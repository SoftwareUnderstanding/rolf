Label;Repo;Text
Semantic web;https://github.com/jindrichmynarz/sparqlab;"""# SPARQLab    SPARQLab is a lab for exercising the SPARQL query language. Your task in SPARQLab is to formulate SPARQL queries that produce results satisfying the given requirements. Each exercise is provided with an exemplary solution. The application is based on data on pension statistics from the Czech social security administration. The exercises focus on data exploration, such as for finding the dimensions used in the data, but also on domain-specific analyses, such as for detecting regions with the largest pay gap between male and female retirees.    ## Deployment    SPARQLab can be deployed via [docker-compose](https://docs.docker.com/compose). It uses [Stardog Community Edition](http://stardog.com) as its RDF store, which can be installed via [this Docker image](https://github.com/jindrichmynarz/sparqlab-stardog).    SPARQLab is a web application based on the [Luminus](https://luminusweb.com) framework. If you need to run it on a non-root path of your web server, such as when in a web application container, you can set the path via the `APP_CONTEXT` enviroment variable in `docker-compose.yml`. Similarly, you can customize the SPARQL endpoint SPARQLab connects to via the `SPARQL_ENDPOINT` environment variable.    ## Acknowledgements    SPARQLab was funded by developement grants of the University of Economics no. 6/2016 and no. 50/2017. The application received additional support from the prize for the best student application based on Czech open data in the Czech Open Data Challenge 2016.    ## License    Copyright © 2016 Jindřich Mynarz, Vojtěch Svátek, and Jan Kučera    Distributed under the Eclipse Public License version 1.0. """
Semantic web;https://github.com/kasei/swift-serd;"""serd-swift  ===    Swift package wrapper for the [Serd RDF library](http://drobilla.net/software/serd). """
Semantic web;https://github.com/gh-rdf3x/gh-rdf3x;"""GH-RDF3X  ========  *GH-RDF3X* is a more complete and modified version of the original *RDF-3X* engine.    Changes over original version ( *rdf3x* ):    - **OPTIONAL** clause was implemented.    - new **GJOIN** clause was added.    - fixed bugs in **lang**, **langMatches**, **bound** and aritmetics relationals operators.    - *translatesparql* now translate more complex queries from SPARQL to SQL for postgresql and monetdb.    RDF-3X was created by: Thomas Neumann. Web site: http://www.mpi-inf.mpg.de/~neumann/rdf3x (c) 2008   RDF-3X was modified by: Hancel Gonzalez and Giuseppe De Simone (Advisor: Maria Esther Vidal). http://github.com/gh-rdf3x/gh-rdf3x (c) 2013 """
Semantic web;https://github.com/jbmusso/awesome-graph;"""# awesome-graph    A curated list of resources for graph databases and graph computing tools    ## Graph databases    * [AgensGraph](https://bitnine.net/agensgraph-2/) - multi-model graph database with SQL and Cypher support  * [AnzoGraph](https://www.cambridgesemantics.com/anzograph/) - Massively parallel graph database with advanced analytics (SPARQL, Cypher, OWL/RDFS+, LPG)   * [Atomic-Server](https://crates.io/crates/atomic-server/) - open-source type-safe graph database server with GUI, written in rust. Supports [Atomic Data](docs.atomicdata.dev/), JSON & RDF.  * [ArangoDB](https://www.arangodb.com/) - highly available Multi-Model NoSQL database  * [Blazegraph](https://github.com/blazegraph/database) - GPU accelerated graph database  * [Cayley](https://github.com/cayleygraph/cayley) - open source database written in Go  * [CosmosDB](https://docs.microsoft.com/en-us/azure/cosmos-db/graph-introduction) - cloud-based multi-model database with support for TinkerPop3  * [Dgraph](https://dgraph.io) - Fast, Transactional, Distributed Graph Database (open source, written in Go)  * [DSE Graph](https://www.datastax.com/products/datastax-enterprise-graph) - Graph layer on top of DataStax Enterprise (Cassandra, SolR, Spark)  * [Grafito](https://github.com/arturo-lang/grafito) - Portable, Serverless & Lightweight SQLite-based Graph Database in Arturo  * [Grakn.AI](https://grakn.ai/) - a distributed hyper-relational database for knowledge-oriented systems, i.e. a distributed knowledge base  * [Graphd](https://github.com/google/graphd) - the Metaweb/Freebase Graph Repository  * [JanusGraph](http://janusgraph.org) - an open-source, distributed graph database with pluggable storage and indexing backends  * [Memgraph](https://memgraph.com/) - High Performance, In-Memory, Transactional Graph Database  * [Neo4j](http://tinkerpop.apache.org/docs/current/#neo4j-gremlin) - OLTP graph database  * [Nebula Graph](http://nebula-graph.io/) - A distributed, fast open-source graph database featuring horizontal scalability and high availability  * [RedisGraph](https://oss.redislabs.com/redisgraph/) - Property graph database, based on linear algebra constructs (GraphBLAS)  * [Sparksee](http://www.sparsity-technologies.com/#sparksee) - makes space and performance compatible with a small footprint and a fast analysis of large networks  * [Stardog](http://stardog.com/) - RDF graph database with OLTP and OLAP support  * [OrientDB](http://orientdb.com/orientdb/) - Distributed Multi-Model NoSQL Database with a Graph Database Engine  * [TerminusDB](https://github.com/terminusdb/terminusdb) is an open source graph database and document store. It is designed for collaboratively building data-intensive applications and knowledge graphs.  * [TigerGraph](https://www.tigergraph.com/) - The First Native Parallel Graph capable of real-time analytics on web-scale data  * [Weaviate](https://github.com/semi-technologies/weaviate) - Weaviate is a cloud-native, modular, real-time vector search engine with a graph data model (GraphQL interface) built to scale your machine learning models.    ### Triple stores  * [Akutan](https://github.com/eBay/akutan) - Akutan is a distributed knowledge graph store, sometimes called an RDF store or a triple store  * [AllegroGraph](https://franz.com/agraph/allegrograph/) - high-performance, persistent graph database that scales to billions of quads  * [Apache Jena](https://jena.apache.org/) - open source Java framework for building Semantic Web and Linked Data applications  * [Dydra]( http://docs.dydra.com/dydra) - Dydra is a cloud-based graph database. Dydra stores data is natively stored as a property graph, directly representing the relationships in the underlying data.  * [Eclipse RDF4J](http://rdf4j.org/) - (formerly known as Sesame) is an open source Java framework for processing RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.  * [GraphDB](http://graphdb.ontotext.com/graphdb/) - enterprise ready Semantic Graph Database, compliant with W3C Standards  * [Virtuoso](https://virtuoso.openlinksw.com/) - a ""Data Junction Box"" that drives enterprise and individual agility by deriving a Semantic Web of Linked Data from existing data silos  * [Hoply](https://github.com/amirouche/hoply/) - explore bigger than RAM relational data in the comfort of Python.    ## Graph computing frameworks    * [Apache Giraph](https://giraph.apache.org/) - an iterative graph processing system built for high scalability  * [Apache TinkerPop](https://tinkerpop.apache.org/) - a graph computing framework for both graph databases (OLTP) and graph analytic systems (OLAP)  * [Apache Spark - GraphX](https://spark.apache.org/graphx/) - Apache Spark's API for graphs and graph-parallel computation  * [GraphScope](https://github.com/alibaba/GraphScope) - A one-stop large-scale graph computing system from Alibaba    ## Languages    * [Cypher](http://www.opencypher.org/)  * [Datalog](https://en.wikipedia.org/wiki/Datalog)  * [Gremlin](https://tinkerpop.apache.org/gremlin.html)  * [SPARQL](https://en.wikipedia.org/wiki/SPARQL)  * [GSQL](https://docs.tigergraph.com/)    ## Managed hosting services    * [CosmosDB @ Microsoft](https://docs.microsoft.com/en-us/azure/cosmos-db/graph-introduction)  * [JanusGraph @ IBM Compose](https://www.compose.com/databases/janusgraph)  * [JanusGraph @ Google Cloud Platform](https://cloud.google.com/solutions/running-janusgraph-with-bigtable) - JanusGraph on Google Kubernetes Engine backed by Google Cloud Bigtable  * [JanusGraph @ Amazon Web Services Labs](https://github.com/awslabs/dynamodb-janusgraph-storage-backend)  * [Neo4j @ Graphene](https://www.graphenedb.com/)  * [Neptune @ Amazon Web Services](https://aws.amazon.com/neptune/) - a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets    ## Learning materials    ### Official documentations  * [Cypher](https://neo4j.com/developer/cypher-query-language/) - reference documentation  * [Gremlin](http://tinkerpop.apache.org/docs/current/reference/#traversal) - reference documentation    ### Community effort  * [Graph Book](https://github.com/krlawrence/graph) - TinkerPop3 centric book written by [Kelvin R. Lawrence](https://twitter.com/gfxman)  * [SQL2Gremlin](http://sql2gremlin.com/) - transition from SQL to Gremlin by [Daniel Kuppitz](https://twitter.com/dkuppitz)  * [The Gremlin Compendium](http://www.doanduyhai.com/blog/?p=13460) - minimum survival kit for any Gremlin user, 10 blog post series by [Doan DuyHai](https://twitter.com/doanduyhai)    ### Blogs  * [TigerGraph Blog](https://www.tigergraph.com/blog/)    ## Conferences    * [Graph Connect](http://graphconnect.com/) - powered by Neo4j  * [Graph Day](http://graphday.com/) - an Independent Graph Conference from the Data Day folks    ## License    [![CC0](http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](https://creativecommons.org/publicdomain/zero/1.0/)    To the extent possible under law, [Jean-Baptiste Musso](https://github.com/jbmusso) has waived all copyright and related or neighboring rights to this work. """
Semantic web;https://github.com/vandenoever/rome;"""[![Build Status](https://travis-ci.org/vandenoever/rome.svg?branch=master)](https://travis-ci.org/vandenoever/rome)  [![Current Version](http://meritbadge.herokuapp.com/rome)](https://crates.io/crates/rome)    **Rome** is an **RDF library** written in safe Rust.    [Documentation](https://www.vandenoever.info/rust/rome/)    # Features    - Access any data in a uniform way as RDF by implementing a Graph.  - Read/write Turtle and N-Triple files.  - Iterate over triples in graphs.  - Wrap a graph in code generated from an ontology.  - Use the type system to distinguish between blank nodes, IRIs and literals at    compile time.    # Generated code    The ontology code is generated by these commands:    ```bash  cargo run --example generate_code src/ontology ontologies/*  cargo fmt  ```    # Testing    The Turtle parser passes the [W3 test suite](https://www.w3.org/2013/TurtleTests/).    Run the tests like this:    ```bash  wget https://www.w3.org/2013/TurtleTests/TESTS.tar.gz  tar xf TESTS.tar.gz  cargo run --example w3tests TurtleTests/manifest.ttl  ```    # License    Rome is licensed under AGPLv3.0 or any later version.    ## Contribution    Unless you explicitly state otherwise, any contribution intentionally submitted  for inclusion in the work by you, as defined in the AGPLv3.0 license, shall be licensed as above, without any additional terms or conditions. """
Semantic web;https://github.com/EBIBioSamples/java2rdf;"""# java2rdf    A simple library to map Java objects and Java beans onto RDF/OWL. Contrary to other similar tools, java2rdf is based on declaring mappings between JavaBeans and RDF/OWL entities in dedicated mapping Java classes, so not in configuration files (you don't have to learn yet another XML schema), not via Java annotations (you don't always have access to, or want to spoil the source model).      We have a [5-min presentation about java2rdf](http://www.slideshare.net/mbrandizi/java2rdf). That shows code exerpts from [this example](https://github.com/EBIBioSamples/java2rdf/tree/master/src/test/java/uk/ac/ebi/fg/java2rdf/mapping/foaf_example), we also have  [another example](https://github.com/EBIBioSamples/java2rdf/blob/master/src/test/java/uk/ac/ebi/fg/java2rdf/mapping/MappersTest.java), showing a slightly different, 'quick-n-dirty' way to define object mappings (we recommend the former approach in real applications).      [Here](http://www.marcobrandizi.info/mysite/node/153) you can read something more about java2rdf and the BioSD Linked Data Project, for which it was built.      [Ondex](https://github.com/Rothamsted/ondex-knet-builder) is another example where java2rdf is used to [export RDF from Ondex knowledge network format](https://github.com/Rothamsted/ondex-knet-builder/tree/master/modules/rdf-export-2).     Note that version 2 has been re-implemented based on Commons-RDF, so that now you can choose to configure java2rdf   to use Jena or RDF4j as underlining RDF framework (we don't support OWLAPI anymore).   """
Semantic web;https://github.com/trellis-ldp/trellis;"""# Trellis Linked Data Server    A scalable platform for building [linked data](https://www.w3.org/TR/ldp/) applications.    ![Build Status](https://github.com/trellis-ldp/trellis/workflows/GitHub%20CI/badge.svg)  [![Coverage](https://sonarcloud.io/api/project_badges/measure?project=org.trellisldp%3Atrellis&metric=coverage)](https://sonarcloud.io/dashboard?id=org.trellisldp%3Atrellis)  ![Maven Central](https://img.shields.io/maven-central/v/org.trellisldp/trellis-api.svg)    Trellis is a rock-solid, enterprise-ready linked data server.  The quickest way to get started with Trellis is to use  a pre-built [docker container](https://hub.docker.com/r/trellisldp/trellis).    Trellis is built on existing [Web standards](https://github.com/trellis-ldp/trellis/wiki/Web-Standards).  It is modular, extensible and fast.    * [Wiki](https://github.com/trellis-ldp/trellis/wiki)  * [Mailing List](https://groups.google.com/group/trellis-ldp)  * [API Documentation](https://www.trellisldp.org/docs/trellis/current/apidocs/) (JavaDocs)  * [Website](https://www.trellisldp.org)    All source code is open source and licensed as Apache 2. Contributions are always welcome.    ## Docker Containers    Docker containers for Trellis are published on [Docker Hub](https://hub.docker.com/u/trellisldp).  Container environments are published with every commit to the `main` branch and are available for all stable  releases. More details are available on the  [Trellis Wiki](https://github.com/trellis-ldp/trellis/wiki/Dockerized-Trellis).    Docker pull command    ```bash  docker pull trellisldp/trellis-triplestore  ```    Or, for the PostgreSQL-based persistence layer    ```bash  docker pull trellisldp/trellis-postgresql  ```    ## Building Trellis    In most cases, you won't need to compile Trellis. Released components are available on Maven Central,  and the deployable application can be [downloaded](https://www.trellisldp.org/download.html) directly  from the Trellis website. However, if you want to build the latest snapshot, you will need, at the very least,  to have Java 11+ available. The software can be built with [Gradle](https://gradle.org) using this command:    ```bash  ./gradlew install  ```    ## Related projects    * [py-ldnlib](https://github.com/trellis-ldp/py-ldnlib) A Python3 library for linked data notifications  * [static-ldp](https://github.com/trellis-ldp/static-ldp) A PHP application that serves static files as LDP resources  * [camel-ldp-recipes](https://github.com/trellis-ldp/camel-ldp-recipes) Integration workflows built with [Apache Camel](https://camel.apache.org)   """
Semantic web;https://github.com/larsga/Duke;"""# Duke    Duke is a fast and flexible deduplication (or entity resolution, or  record linkage) engine written in Java on top of Lucene.  The latest  version is 1.2 (see [ReleaseNotes](https://github.com/larsga/Duke/wiki/ReleaseNotes)).    Duke can find duplicate customer records, or other kinds of records in  your database. Or you can use it to connect records in one data set  with other records representing the same thing in another data set.  Duke has sophisticated comparators that can handle spelling  differences, numbers, geopositions, and more. Using a probabilistic  model Duke can handle noisy data with good accuracy.    Features      * High performance.    * Highly configurable.    * Support for [CSV, JDBC, SPARQL, NTriples, and JSON](https://github.com/larsga/Duke/wiki/DataSources).    * Many built-in [comparators](https://github.com/larsga/Duke/wiki/Comparator).    * Plug in your own data sources, comparators, and [cleaners](https://github.com/larsga/Duke/wiki/Cleaner).    * [Genetic algorithm](https://github.com/larsga/Duke/wiki/GeneticAlgorithm) for automatically tuning configurations.    * Command-line client for getting started.    * [API](https://github.com/larsga/Duke/wiki/UsingTheAPI) for embedding into any kind of application.    * Support for batch processing and continuous processing.    * Can maintain database of links found via JNDI/JDBC.    * Can run in multiple threads.    The [GettingStarted page](https://github.com/larsga/Duke/wiki/GettingStarted) explains how to get started and has links to  further documentation. The [examples of use](https://github.com/larsga/Duke/wiki/ExamplesOfUse) page  lists real examples of using Duke, complete with data and  configurations. [This  presentation](http://www.slideshare.net/larsga/linking-data-without-common-identifiers)  has more of the big picture and background.    Contributions, whether issue reports or patches, are very much  welcome.  Please fork the repository and make pull requests.    Supports Java 1.7 and 1.8.    [![Build status](https://travis-ci.org/larsga/Duke.png?branch=master)](https://travis-ci.org/larsga/Duke)    If you have questions or problems, please register an issue in the  issue tracker, or post to the [the mailing  list](http://groups.google.com/group/duke-dedup). If you don't want to  join the list you can always write to me at `larsga [a]  garshol.priv.no`, too.    ## Using Duke with Maven    Duke is hosted in Maven Central, so if you want to use Duke it's as  easy as including the following in your pom file:    ```  <dependency>    <groupId>no.priv.garshol.duke</groupId>    <artifactId>duke</artifactId>    <version>1.2</version>  </dependency>  ```    ## Building the source    If you have [Maven](https://maven.apache.org/) installed, this is as  easy as giving the command `mvn package` in the root directory. This  will produce a `.jar` file in the `target/` subdirectory of each  module.    ## Older documentation    [This blog post](http://www.garshol.priv.no/blog/217.html) describes  the basic approach taken to match records. It does not deal with the  Lucene-based lookup, but describes an early, slow O(n^2)  prototype. [This early  presentation](http://www.slideshare.net/larsga/deduplication)  describes the ideas behind the engine and the intended architecture"""
Semantic web;https://github.com/yyz1989/NoSPA-RDF-Data-Cube-Validator;"""NoSPA RDF Data Cube Validator  =============================    ### Introduction    This is an RDF Data Cube Validator. Its significant difference from other existing validators is that it is not based on SPARQL queries, as its name ""NoSPA"". Jena library is used to manipulate RDF models. The official SPARQL queries for constraint checks are interpreted and parsed by this validator to search functions with nested statement listing functions provided by Jena and filters for different conditions. It has an outstanding performance because the entire process is executed in memory. I believe that it is valuable to sacrifice some memory for saving time.    Here are some references and knowledge background for this tool:    * The official RDF data cube spec: [The RDF Data Cube Vocabulary](http://www.w3.org/TR/vocab-data-cube/)    * Jena API: [Apache Jena](http://jena.apache.org/index.html)    * The official SPARQL spec: [SPARQL 1.1 Query Language](http://www.w3.org/TR/sparql11-query/)    ### Updates in the latest release 0.9.9    1.  Rewrote some functions to boost the performance on validating constraints 11 and 12, which occupies more than 99% computation time among all constraints. Now NoSPA is capable of handling data cube with million level observations.    2.  Added a progress monitor for the validation of 11 and 12.    ### Requirements    JDK (>=5) and Maven if you want to compile by yourself    or     JVM (>=5) if you want to execute a jar directly    ### Installation    This tool is written in Java and managed by Maven so you can compile it easily by yourself. The first thing you need to do is ``git clone`` this repository.    *Updates: now the packaged jar files are already uploaded and can be found at the release page so you don't need to do it by yourself any more*    Then you need to do a ``mvn package`` at the root directory of this repository and find the jar file at ``NoSPA-RDF-Data-Cube-Validator/target/nospa-rdf-data-cube-validator-0.9.9.jar``. Note that in this case the library for Jena and Log4j is not included in this package.    In the case that you need to run this package independently, you will need to do a ``mvn package assembly:single`` at the root directory of this repository and find the jar file at ``NoSPA-RDF-Data-Cube-Validator/target/nospa-rdf-data-cube-validator-0.9.9-jar-with-dependencies.jar``, which includes all the required libraries to run it.    ### Validation    Basically, there are 3 ways to use it:    1.  Use an IDE to hack into the code by yourself and run the Main class, without making any packages.    2.  In the case that you need to integrate it into your own project, you have to import the package ``rdf-data-cube-validator-0.9.9.jar``, create a new validator instance and call the functions to normalize and validate the cube.        ``Validator validator = ValidatorFactory.createValidator(""NOSPA"", inputPath, inputFormat);``            ``validator.normalize();``            ``validator.validateAll();``        The first argument for the createValidaotr method is the type of validator. Options are ""NOSPA"" and ""SPARQL"" since they are implemented in this software. The ``inputPath`` is the path of the cube file and ``inputFormat`` indicates the RDF format of the cube file such as RDF/XML, N3, TURTLE, N-TRIPLES, etc.        You may also want to check constraints selectively, in that case you cannot use the ValidatorFactory because the two types of validator have different implementions to validate constraints individually and it is a bit difficulty to unify them with an interface. For example, validate with NoSPA validator:            ``NospaValidator nospaValidator = new NospaValidator(inputPath, inputFormat);``            ``nospaValidator.normalize();``            ``nospaValidator.validateIC1();``            ``nospaValidator.validateIC20_21();``            Validate with SPARQL validator:            ``SparqlValidator sparqlValidator = new SparqlValidator(inputPath, inputFormat);``            ``sparqlValidator.normalize();``            ``sparqlValidator.validate(""IC1"");``            ``sparqlValidator.validateIC20_21(""IC20"");``            You will know why there is such difference if you can take a look at the code. Maybe I will get better ideas to unify them in the future. Besides, please make sure that you have normalized the cube before checking constraints if it is in the abbreviated form. You don't need to normalize it if you are sure that it is in the normalized form.        Note that the validation result of this tool will be recorded as logs so you need to turn on the logs for this package in the log configuration of your own project. Additionally you have to set a system property ``current.timestamp`` with the value of current time as part of the name of the validation result. Finally, the validation result can be found at ``${user.dir}/validation_result_${current.timestamp}.md``.    3.  In the case that you need to validate the cube file manually and independently, you need to run ``java -jar nospa-rdf-data-cube-validator-0.9.9-jar-with-dependencies.jar <cube-file.(xml|rdf|nt|n3|ttl)> <(nospa|sparql)>``, where the first argument is the file path of the cube to be validated and the second argument is the name of validator respectively. Currently only 5 RDF format are supported, as can be seen from the file extension name. The validator can be ""nospa"" power by this tool, or ""sparql"" which runs the official validation SPARQL queries against the cube with Jena ARQ.    ### Performance    The constraint check IC-12, ""No duplicate observations"", is the most time-consuming procedure for the entire validation. The motivation of developing this tool is mainly to tackle this issue.     Test file: a data cube containing 13970 observations    Test environment: Ubuntu 14.04 with VMWare, 2 CPU cores of I5-2450M @ 2GHz, 2 GB memory, ordinary HHD    Time consumption for validating IC-12:      * Validation by SPARQL queries with Virtuoso: 1 hour 22 min      * Validation by SPARQL queries with Jena Parser: 58 min      * Validation by NoSPA: 10 sec    *Updates for the performance of the latest release:*    Test file: a 230MB cube file including 540K observations    Test environment: A Web server with 4 Intel(R) Xeon(R) CPUs E5-2630L 0 @ 2.00GHz and 8 GB memory    Time consumption: 52 sec    ### Prospects    Due to lack of faluty datasets, my tests may not cover all cases. Please give me any feedback and suggestion if you are using this software so I can keep improving its quality.    I am still working on some minor changes related to the functionalities. I am planning to make a fair front end when it gets stabilized. """
Semantic web;https://github.com/joshsh/ripple;"""<!-- This README can be viewed at https://github.com/joshsh/ripple/wiki -->    ![Ripple logo|width=420px|height=100px](https://github.com/joshsh/ripple/wiki/graphics/ripple-logo-text-medium.png)    Welcome to the Ripple wiki!  Ripple is a functional, stack-based query language for Linked Data and other RDF data sources.  Ripple programs resemble path expressions as in [XPath](http://www.w3.org/TR/xpath/)  and postfix-style procedures as in  [Forth](http://en.wikipedia.org/wiki/Forth_&#40;programming_language\&#41;).  Every program has an [RDF](http://www.w3.org/RDF/) representation,  so you can embed programs in the Web of Data as well as querying against it.  The implementation is written in Java and includes an interactive command-line interpreter as well as a query API which interoperates with [Sesame 4.1](http://rdf4j.org/).    ## Contents    * [Running Ripple](https://github.com/joshsh/ripple/wiki/Running-Ripple): getting the software, using the command-line interpreter, and embedding Ripple in Java programs  * Examples      * Ripple on [Linked Data](https://github.com/joshsh/ripple/wiki/ripple-on-linked-data)      * Ripple on [JSON](https://github.com/joshsh/ripple/wiki/ripple-on-json)      * [The Web of Programs](https://github.com/joshsh/ripple/wiki/The-Web-of-Programs): publishing Ripple programs as Linked Data  * Language reference      * [Syntax](https://github.com/joshsh/ripple/wiki/Syntax): Ripple's RDF-oriented syntax for commands and queries      * [Commands](https://github.com/joshsh/ripple/wiki/Commands): how to define programs and inspect the scripting environment  * Libraries and primitives      * Core libraries          * [control library](https://github.com/joshsh/ripple/wiki/control-library): mappings and program flow, regular expressions, looping and branching          * [data library](https://github.com/joshsh/ripple/wiki/data-library): atomic values and datatypes, comparison, type conversion          * [graph library](https://github.com/joshsh/ripple/wiki/graph-library): reading and writing RDF statements, SPARQL support, key/value objects and JSON          * [logic library](https://github.com/joshsh/ripple/wiki/logic-library): boolean algebra          * [math library](https://github.com/joshsh/ripple/wiki/math-library): arithmetic, roots and exponentials, trigonometry           * [stack library](https://github.com/joshsh/ripple/wiki/stack-library): list- and stack-oriented primitives inherited from [Joy](http://en.wikipedia.org/wiki/Joy_(programming_language))          * [stream library](https://github.com/joshsh/ripple/wiki/stream-library): stream splitting and intersection, filters for deduplication and pruning, closed-world operations          * [string library](https://github.com/joshsh/ripple/wiki/string-library): string manipulation          * [system library](https://github.com/joshsh/ripple/wiki/system-library): system calls and network operations, other scripting languages      * Extensions          * [media library](https://github.com/joshsh/ripple/wiki/media-library): primitives for playing audio, showing images, and speaking text          * [blueprints library](https://github.com/joshsh/ripple/wiki/blueprints-library): graph traversal on the [Blueprints](https://github.com/tinkerpop/blueprints/wiki/) API  * Miscellaneous      * [Ripple configuration properties](https://github.com/joshsh/ripple/wiki/Ripple-configuration-properties)      * [LinkedDataSail](https://github.com/joshsh/ripple/wiki/LinkedDataSail): Ripple's dynamic view of the Web of Data      * [Naming conventions](https://github.com/joshsh/ripple/wiki/Naming-conventions) for Ripple programs      * [creating a Ripple library](https://github.com/joshsh/ripple/wiki/creating-a-Ripple-library)  * External links      * Ripple [JavaDocs](http://fortytwo.net/projects/ripple/api/latest)      * [Functional programs as Linked Data](http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-248/paper10.pdf) (the original paper on Ripple)      * The [demo screencast](http://ripple.googlecode.com/svn/trunk/docs/screencast/index.html) from the [SFSW 2007 Scripting Challenge](http://web.archive.org/web/20120326083323/http://www.semanticscripting.org/SFSW2007/)   """
Semantic web;https://github.com/lambdamusic/Sparql-cli;"""  # Sparql-cli    A command line SPARQL console. Works with endpoints as a well as local/remote graphs.       ### Status    Prototype. Runs but largely untested.       ### How-to    Suggested:    - Install via  `python setup.py install` then call `sparql-cli`      Otherwise, you can run in dev mode:     - get to the source top folder and run `python -m sparql-cli.main -h`      ### Summary    This is an attempt to build a sparql console using python-prompt toolkit    Idea:  either pass a sparql endpoint / or a graph URI which is loaded in memory using RDFLib  = useful to test things out quickly!    REQUIREMENTS  - click  - colorama  - rdflib  - pygments      @todo  - export results as html / web  - allow passing an endpoint @done  - add more saved queries   - store endpoints? eg via an extra command line  - add meta level cli eg .show or .info etc..  - namespaces and shortened URIs      ### Note     This tool relies on pygments for the syntax highlighting, however the current Sparql Lexer included in Pygments stable releases is broken (https://bitbucket.org/birkenfeld/pygments-main/issues/1236/sparql-lexer-error) hence you either have to update it from the dev branch or wait for the 2.2 release of  Sparql Lexer.        ### Changelog      **October 22, 2016**  - various improvements to print out  - added Endpoint connection and tested with DBpedia      **October 22, 2016**  - improved rendering of results   - added options via click       **October 21, 2016**  NOTE: > problem with Sparql Lexer  https://bitbucket.org/birkenfeld/pygments-main/issues/1236/sparql-lexer-error  ""fixed in 2.2 release soon""    Improved by installing this commit  https://bitbucket.org/birkenfeld/pygments-main/commits/60afc531aa2b  >> installed manually 'hg clone https://bitbucket.org/birkenfeld/pygments-main'  and it works     """
Semantic web;https://github.com/magnetik/node-webid;"""#node-webid    Node.js module with tools to help using WebID (http://www.webid.info).    ##Installation    ### [npm](https://npmjs.org/package/webid)    Just require the module `webid`:    ```  var webid = require('webid');  ```    ### Manual    Start `cake build` and get the webid.js in the bin folder.    ## Usage    Check the project [webid-demo](https://github.com/magnetik/node-webid-demo) to see a working example.     Basic usage:    ```  var webid = require('webid');  var verifAgent = new webid.VerificationAgent(certificate);  	verifAgent.verify(function (result) {  		//Success! User is identified  		var foaf = new webid.Foaf(result);  		req.session.profile = foaf.parse();  	}, function(result) {  		//An error occured  	});  ```    ##Licence    The lib is available under MIT Licence: http://www.opensource.org/licenses/MIT   """
Semantic web;https://github.com/AKSW/cubeviz.ontowiki;"""# CubeViz - THE RDF DATACUBE BROWSER     CubeViz is an [OntoWiki](http://aksw.org/Projects/OntoWiki.html) component providing faceted browsing in statistical   data in the Linked Data Web. It was set up and adopted to be part of the   [Data Portal](https://ec.europa.eu/digital-agenda/en/scoreboard) of the European Union.   CubeViz utilizing the  [RDF DataCube vocabulary](http://www.w3.org/TR/vocab-data-cube/)   which is the state-of-the-art in representing statistical data in [Resource Description Framework (RDF)](http://www.w3.org/RDF/).   This vocabulary is compatible with [SDMX](http://en.wikipedia.org/wiki/SDMX)   ([User Guide](http://sdmx.org/wp-content/uploads/2012/11/SDMX_2-1_User_Guide_draft_0-1.pdf)) and increasingly being adopted.   Based on the vocabulary and the encoded Data Cube, CubeViz is generating a facetted browsing widget that   can be used to filter interactively observations to be visualized in charts.   Based on the selected structure, CubeViz offer beneficiary chart types and options which can be selected by users.    Do you want to read further information about the project and its background, please have a look into   [**about**](https://github.com/AKSW/cubeviz.ontowiki/wiki/About-the-project) page.    ![](https://raw.github.com/wiki/AKSW/cubeviz.ontowiki/images/v0.5_IndexActionScreenshot.png)    ![](https://raw.github.com/wiki/AKSW/cubeviz.ontowiki/images/v0.6_visualization.png)    ## Getting started    You will find various information in our [Wiki](https://github.com/AKSW/cubeviz.ontowiki/wiki/Home).  For new users, please visit Page [installation](https://github.com/AKSW/cubeviz.ontowiki/wiki/Installation-and-setup-main)   to get an introduction about installation and setup of CubeViz.    Further information about the repository structure or other internals can be also found in the Wiki.    ### Docker container available    We providing a Docker container for everybody, who don't want to bother about getting OntoWiki running or struggles with Virtuoso. That container only needs a Docker and can be used with `docker pull` + `docker run`. How easy is that?!     Basically it ships with a fully fledged OntoWiki, pre-filled Virtuoso store and up and running CubeViz. After you started the container, you can use your browser and directly use CubeViz.    To pull the container just run:     `docker pull aksw/dld-present-cubeviz`    To run it, please use:    `docker run -d -p 8080:80 -p 8890:8890 aksw/dld-present-cubeviz`    For further information, please look in following the project page.    **Project page:** [Dockerizing/CubeViz](https://github.com/Dockerizing/CubeViz)    ## CubeViz compared to other tools    *(Last updated 2015-11-30)*    We created a comparison of CubeViz and other tools, which are supporting DataCube vocabulary. It is based on the support of elements which are mentioned in the vocabulary. The purpose of that overview is to provide users, who are interested in using CubeViz, further information of its DataCube vocabulary support in comparison to other tools.    *If you encounter erros or wanna mention something, please make a pull request, create an issue or send us an email.*    `(✓)` - Similar like CubeViz but not equivalent    | Supported Features                | CubeViz | OpenCube | LDCX | LSD Analysis    |  |-----------------------------------|:-------:|:--------:|:----:|:---------------:|  | **Explore RDF Data**              |         |          |      |                 |  | Select parts of the Dataset       |    ✓    |    ✓     |  ✓   |       (✓)       |  | Select Units and Measurements     |    ✓    |    ✓     |  ✓   |       (✓)       |  | Multiple Chart Visualization      |    ✓    |   (✓)    |  \-  |       \-        |  | Configure Dimensions              |    ✓    |    \-    |  ✓   |       \-        |  |                                   |         |          |      |                 |  | Collaborative Exploration/Edit    |    ✓    |    \-    |  \-  |   ✓ (Explore)   |  |                                   |         |          |      |                 |  | **Compare Datasets**              |         |          |      |       (✓)       |  | Compare Meta Information          |    ✓    |    \-    |  \-  |       \-        |  | Observation Values Normalization  |    ✓    |    \-    |  \-  |       \-        |  | Set Dimension Elements            |    ✓    |    \-    |  \-  |       \-        |  | Show Cluster                      |    ✓    |    \-    |  \-  |       \-        |  |                                   |         |          |      |                 |  | **Data Download**                 |         |          |      |                 |  | Download as CSV                   |    ✓    |    ✓     |  \-  |        ✓        |  | Download as Turtle                |    ✓    |    \-    |  \-  |       \-        |  |                                   |         |          |      |                 |  | Hierarchy Slices                  |   \-    |    \-    |  \-  |       \-        |  | OLAP Operations (Sum, Avg, Pivot) |   \-    |    ✓     |  \-  |       \-        |  | Mobile UI                         |   \-    |    \-    |  \-  |       \-        |  | Analysis Task (R Script)          |   \-    |    ✓     |  \-  | ✓ (Server-Side) |  | Geospatial Data Visualization     |   \-    |    ✓     |  \-  |       \-        |  | Visualizations with dimensions >2 |   \-    |    \-    |  \-  |       \-        |    #### Links to the mentioned tools    -	[OpenCube](http://opencube-toolkit.eu)    -	[LDCX](http://km.aifb.kit.edu/projects/ldcx/)    -	[LSD Analysis](http://stats.270a.info/analysis/worldbank:SP.DYN.IMRT.IN/transparency:CPI2011/year:2011.html)    ## Missing support of the Data Cube vocabulary    The following list contains all elements of the DataCube vocabulary which are not supported by CubeViz (yet):        *	**qb:ObservationGroup**: Indicates a group of observations. The domain of this property is left open so that a group may be attached to different resources and need not be restricted to a single DataSet.      *	**qb:CodedProperty**: Superclass of all coded component properties.      *	**qb:HierarchicalCodeList**: Represents a generalized hierarchy of concepts which can be used for coding. The hierarchy is defined by one or more roots together with a property which relates concepts in the hierarchy to their child concept . The same concepts may be members of multiple hierarchies provided that different qb:parentChildProperty values are used for each hierarchy.    *	**qb:hierarchyRoot**: Specifies a root of the hierarchy. A hierarchy may have multiple roots but must have at least one.    *	**qb:parentChildProperty**: Specifies a property which relates a parent concept in the hierarchy to a child concept. Note that a child may have more than one parent.    *	**qb:componentAttachment**: Indicates the level at which the component property should be attached, this might be an qb:DataSet, qb:Slice or qb:Observation, or a qb:MeasureProperty.    *	**qb:componentRequired:** Indicates whether a component property is required (true) or optional (false) in the context of a DSD. Only applicable to components corresponding to an attribute. Defaults to false (optional).      *	**qb:order**: Indicates a priority order for the components of sets with this structure, used to guide presentations - lower order numbers come before higher numbers, un-numbered components come last.    *	**qb:concept**: Gives the concept which is being measured or indicated by a ComponentProperty.    *	SKOS or SDMX related entities.    ## License    CubeViz is licensed under the terms of GNU General Public License 2 and it uses foreign libraries.   For further details have a look in [here](https://github.com/AKSW/cubeviz.ontowiki/blob/master/LICENSE.md). """
Semantic web;https://github.com/egonw/jqudt;"""[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3883588.svg)](https://doi.org/10.5281/zenodo.3883588)  [![Build Status](https://travis-ci.org/egonw/jqudt.svg?branch=master)](https://travis-ci.org/egonw/jqudt)  [![Maven Central](https://img.shields.io/maven-central/v/com.github.egonw/jqudt.svg?label=Maven%20Central)](https://search.maven.org/search?q=g:%22com.github.egonw%22%20AND%20a:%22jqudt%22)    Introduction  ============    Java Library to deal with QUDT units and conversions between them.    QUDT is ""Quantities, Units, Dimensions and Data Types in OWL and XML""        http://www.qudt.org/    QUDT is a CC-SA-BY project by NASA Ames Research Center and TopQuadrant, Inc.    License of this Java library: new BSD    Installation  ============    Maven:    ```xml  <dependency>    <groupId>com.github.egonw</groupId>    <artifactId>jqudt</artifactId>    <version>1.4.0</version>  </dependency>  ```    Groovy:    ```groovy  @Grab(group='com.github.egonw', module='jqudt', version='1.4.0')  ```    Quick demo  ==========    Keep in mind, that the below conversions are purely derived from the information  defined in the QUDT ontology, taking advantage from the fact that the have the  same unit type, qudt:MolarConcentrationUnit and qudt:TemperatureUnit respectively.    Source:    ```java  Quantity obs = new Quantity(0.1, ConcentrationUnit.MICROMOLAR);  System.out.println(obs + "" = "" +  obs.convertTo(ConcentrationUnit.NANOMOLAR));    Quantity temp = new Quantity(20, TemperatureUnit.CELSIUS);  System.out.println(temp + "" = "" +  temp.convertTo(TemperatureUnit.KELVIN));  ```    Output    ```  0.1 μM = 100.00000000000001 nM  20.0 C = 293.0 K  ``` """
Semantic web;https://github.com/wbsg/sieve;"""sieve  =====    Sieve - Linked Data Quality Assessment and Fusion"""
Semantic web;https://github.com/ropensci/rdflib;"""  # rdflib <img src=""man/figures/logo.svg"" align=""right"" alt="""" width=""120"" />    [![Project Status: Active – The project has reached a stable, usable  state and is being actively  developed.](http://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/)  [![Travis-CI Build  Status](https://travis-ci.org/ropensci/rdflib.svg?branch=master)](https://travis-ci.org/ropensci/rdflib)  [![Build  status](https://ci.appveyor.com/api/projects/status/n81e9wsh5bh0xrm6?svg=true)](https://ci.appveyor.com/project/cboettig/rdflib)  [![Coverage  Status](https://img.shields.io/codecov/c/github/ropensci/rdflib/master.svg)](https://codecov.io/github/ropensci/rdflib?branch=master)  [![CircleCI](https://app.circleci.com/pipelines/github/ropensci/rdflib.svg?style=svg)](https://app.circleci.com/pipelines/github/ropensci/rdflib ""Docker tests"")  [![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/rdflib)](https://cran.r-project.org/package=rdflib)  [![](http://badges.ropensci.org/169_status.svg)](https://github.com/ropensci/software-review/issues/169)  [![CRAN RStudio mirror  downloads](http://cranlogs.r-pkg.org/badges/rdflib)](https://CRAN.R-project.org/package=rdflib)  [![DOI](https://zenodo.org/badge/100521776.svg)](https://zenodo.org/badge/latestdoi/100521776)    <!-- README.md is generated from README.Rmd. Please edit that file -->    A friendly and consise user interface for performing common tasks on rdf  data, such as parsing and converting between formats including rdfxml,  turtle, nquads, ntriples, and trig, creating rdf graphs, and performing  SPARQL queries. This package wraps the redland R package which provides  direct bindings to the redland C library. Additionally, the package  supports parsing and serialization of rdf into json-ld through the  json-ld package, which binds the official json-ld javascript API. The  package interface takes inspiration from the Python rdflib library.    ## Installation    You can install rdflib from GitHub with:    ``` r  # install.packages(""devtools"")  devtools::install_github(""ropensci/rdflib"")  ```    ## Basic use    While not required, `rdflib` is designed to play nicely with `%>%`  pipes, so we will load the `magrittr` package as well:    ``` r  library(magrittr)  library(rdflib)  ```    Parse a file and serialize into a different format:    ``` r  system.file(""extdata/dc.rdf"", package=""redland"") %>%    rdf_parse() %>%    rdf_serialize(""test.nquads"", ""nquads"")  ```    Perform SPARQL queries:    ``` r  sparql <-   'PREFIX dc: <http://purl.org/dc/elements/1.1/>    SELECT ?a ?c    WHERE { ?a dc:creator ?c . }'    system.file(""extdata/dc.rdf"", package=""redland"") %>%  rdf_parse() %>%  rdf_query(sparql)  #> Rows: 1 Columns: 2  #> ── Column specification ────────────────────────────────────────────────────────  #> Delimiter: "",""  #> chr (2): a, c  #>   #> ℹ Use `spec()` to retrieve the full column specification for this data.  #> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.  #> # A tibble: 1 × 2  #>   a                      c             #>   <chr>                  <chr>         #> 1 http://www.dajobe.org/ Dave Beckett  ```    Initialize graph a new object or add triples statements to an existing  graph:    ``` r  x <- rdf()  x <- rdf_add(x,       subject=""http://www.dajobe.org/"",      predicate=""http://purl.org/dc/elements/1.1/language"",      object=""en"")  x  #> Total of 1 triples, stored in hashes  #> -------------------------------  #> <http://www.dajobe.org/> <http://purl.org/dc/elements/1.1/language> ""en"" .  ```    Change the default display format (`nquads`) for graph objects:    ``` r  options(rdf_print_format = ""jsonld"")  x  #> Total of 1 triples, stored in hashes  #> -------------------------------  #> {  #>   ""@id"": ""http://www.dajobe.org/"",  #>   ""http://purl.org/dc/elements/1.1/language"": ""en""  #> }  ```    ## JSON-LD    We can also work with the JSON-LD format through additional functions  provided in the R package, `jsonld`.    ``` r  out <- tempfile()  rdf_serialize(x, out, ""jsonld"")  rdf_parse(out, format = ""jsonld"")  #> Total of 1 triples, stored in hashes  #> -------------------------------  #> {  #>   ""@id"": ""http://www.dajobe.org/"",  #>   ""http://purl.org/dc/elements/1.1/language"": ""en""  #> }  ```    For more information on the JSON-LD RDF API, see  <https://json-ld.org/spec/latest/json-ld-rdf/>.    ## Advanced Use    See [articles](https://docs.ropensci.org/rdflib/articles/) from the  documentation for advanced use including applications to large  triplestores, example SPARQL queries, and information about additional  database backends.    ------------------------------------------------------------------------    ## Citing rdflib    Please also cite the underlying `redland` library when citing `rdflib`    Carl Boettiger. (2018). rdflib: A high level wrapper around the redland  package for common rdf applications (Version 0.1.0). Zenodo.  <https://doi.org/10.5281/zenodo.1098478>    Jones M, Slaughter P, Ooms J, Boettiger C, Chamberlain S (2021).  *redland: RDF Library Bindings in R*. doi: 10.5063/F1VM496B (URL:  <https://doi.org/10.5063/F1VM496B>), R package version 1.0.17-15, \<URL:  <https://github.com/ropensci/redland-bindings/tree/master/R/redland>\>.    [![rofooter](https://ropensci.org//public_images/github_footer.png)](https://ropensci.org/) """
Semantic web;https://github.com/lukostaz/prissma-studio;"""PRISSMA Studio  ===========  ### A Prism Designer for PRISSMA    PRISSMA Studio is an in-browser web application that creates [PRISSMA](http://wimmics.inria.fr/projects/prissma) Prisms, using the [PRISSMA vocabulary](ns.inria.fr/prissma/v2/prissma_v2.html).    PRISSMA Studio supports designers in the creation of [Fresnel](http://www.w3.org/2005/04/fresnel-info/manual/) Lenses and Formats, and eases the definition of the associated [PRISSMA context](http://ns.inria.fr/prissma/v2/prissma_v2.html#Context).    Once created, Prisms can be stored on the file system of PRISSMA-equipped mobile devices, where they will be processed by the [PRISSMA selection algorithm](http://2014.eswc-conferences.org/sites/default/files/papers/paper_81.pdf) .        ## Installation    Clone the repository:    	$ git clone https://github.com/lukostaz/prissma-studio.git    Install dependencies with [Bower](http://bower.io/):    	$ bower install      ## Demo    [Check out PRISSMA Studio demo here](http://luca.costabello.info/prissma-studio/).      ## Licence  	      Copyright (C) 2014-2015 Luca Costabello, v1.0.0        This program is free software; you can redistribute it and/or modify it      under the terms of the GNU General Public License as published by the      Free Software Foundation; either version 2 of the License, or (at your      option) any later version.        This program is distributed in the hope that it will be useful, but      WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY      or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License      for more details.        You should have received a copy of the GNU General Public License along      with this program; if not, see <http://www.gnu.org/licenses/>.    ## Contacts  Further details on the [PRISSMA Project Page](http://wimmics.inria.fr/projects/prissma/), or contact [Luca Costabello](http://luca.costabello.info).     """
Semantic web;https://github.com/opencube-toolkit/tarql-component;"""OpenCube Toolkit - Tarql Extension  ===============      [Tarql](https://github.com/cygri/tarql) is a command-line tool for converting CSV files to RDF using SPARQL 1.1 syntax. It's written in Java and based on Apache ARQ.    ### How it works    TARQL Extension for data cubes is a tool for converting CSV files to RDF accordingly to SPARQL 1.1 syntax. The data cubes are generated based on the provided, easy to modify, query templates. The extension is integrated with the Information Workbench as a standard data provider. The interface delivered enables user to specify the basic information about the provider along with the location of the CSV file, polling intervals and to modify the cube mapping query via provided SPARQL editor. The interface includes information on status and the time that was needed to transform the data and enables to browse the output triples generated.    The TARQL extension can be stacked with other data components for instance the output RDF can be visualised with i.e. the OpenCube Map View.      ###Functionality    The component is built on top of Apache ARQ2. The OpenCube TARQL component includes the new release of TARQL. It brings several improvements, such as: streaming capabilities, multiple query patterns in one mapping file, convenient functions for typical mapping activities, validation rules included in mapping file and increased flexibility (dealing with CSV variants like TSV).    The OpenCube Toolkit user is able to create Cube directly from the input files and store the output in the SPARQL endpoint. The OpenCube TARQL extension offers the following options:    + Stream CSV processing  + Use of column headers as variable names  + Translate the CSV imported table into RDF by using a prepared mapping file (SPARQL construct schema)  + Test the mapping file (shows only the CONSTRUCT template, variable names, and a few input rows) """
Semantic web;https://github.com/jpcik/srbench;"""srbench  =======    SRBench SPARQL RDF Bench"""
Semantic web;https://github.com/webr3/js3;"""# JS3 - An insane integration of RDF in ECMAScript-262 V5 (Javascript) #    In short, with this library, all your javascript is also RDF, there are no special types or classes,  each variable and value is also an RDF Node, List or Graph.    All values are both the standard javascript values you expect (no extension or suchlike), and are the RDF types you expect    ## Example ##    This library doesn't inspect objects and then generate RDF, rather each value *is* RDF, and javascript:        true.toNT();         // ""true""^^<http://www.w3.org/2001/XMLSchema#boolean>      (12 * 1.4).toNT();   // ""12.3""^^<http://www.w3.org/2001/XMLSchema#decimal>        Here's a complicated yet simple example to illustrate, this is just a standard Object in js:        var me = {        a: 'foaf:Person',                                         // a String, a CURIE and a full IRI        name: 'Nathan',                                           // a String, and an RDF Plain Literal        age: new Date().getFullYear() - 1981,                     // a Number, and a Typed Literal with the type xsd:integer        homepage: 'http://webr3.org',                             // a String, and an IRI,         holdsAccount: {                                           // an Object, with a BlankNode reference for the .id          label: ""Nathan's twitter account"".l('en'),              // a String, and a Literal with a .language          accountName: 'webr3',                                   // noticed that you don't need the prefixes yet?          homepage: 'http://twitter.com/webr3'                  },        knows: bob,                                               // works with variables too of course        nick: ['webr3','nath']                                    // an Array, also a list of values, like in turtle and n3      }.ref("":me"");                                               // still an Object, but also has a .id now, it's subject is set.    If we now call *me.n3()* we'll get the following output:        <http://webr3.org/nathan#me> rdf:type foaf:Person;        foaf:name ""Nathan"";        foaf:age 29;        foaf:homepage <http://webr3.org>;        foaf:holdsAccount [          rdfs:label ""Nathan's twitter account""@en;          foaf:accountName ""webr3"";          foaf:homepage <http://twitter.com/webr3> ];        foaf:knows <http://example.com/bob#me>;        foaf:nick ""webr3"", ""nath"" .    It's just that simple, your javascript is your RDF, it's just plain old javascript:        me.gender = ""male"";                   // .gender will resolve to foaf:gender to http://xmlns.com/foaf/0.1/gender       if(me.age > 18) return true;          // it's all native values, just use like normal!    ### Implementation Notice ###    This library requires ECMAScript-262 V5, specifically it makes heavy usage of Object.defineProperties.    You can check the [compatibility chart](http://kangax.github.com/es5-compat-table/) to see if your platform / browser supports it.  The short version is that chrome 5+, ff4, webkit (safari) and ie9 all support this script, and on the server side node.js, rhino and besen are all fine.    Objects and values are not modified in the usual manner and they are not converted in to different types, rather this library automagically redefines  the property descriptors on objects to allow each value to be both a native javascript value, and an RDF compatible value.    ## Nodes & Values ##    All values of type string, number, boolean and date are also RDF Nodes, and are fully aligned (and compatible) with the Interfaces  from the RDFa API.    ### Standard Methods ####  All of the basic js types (string, number, boolean and date) are augmented with the following methods:    *   **.nodeType()** - returns one of PlainLiteral, TypedLiteral, BlankNode or IRI                true.nodeType();                  // TypedLiteral          (12 * 1.4).nodeType();            // TypedLiteral          new Date().nodeType();            // TypedLiteral          ""hello world"".nodeType();         // PlainLiteral          ""_:b12"".nodeType();               // BlankNode          ""foaf:name"".nodeType();           // IRI          ""http://webr3.org/"".nodeType();   // IRI        *   **.equals(other)** - returns boolean        RDF type safe equality test.                ""hello"" == ""hello"".l('en')        // true          ""hello"".equals( ""hello"".l('en') ) // false    *   **.toNT()** - returns string        Does what it says on the tin, returns the N-Triples formatted value.                true.toNT();                      // ""true""^^<http://www.w3.org/2001/XMLSchema#boolean>          (12 * 1.4).toNT();                // ""12.3""^^<http://www.w3.org/2001/XMLSchema#decimal>          new Date().toNT();                // ""2010-11-20T21:06:42Z""^^<http://www.w3.org/2001/XMLSchema#dateTime>          ""hello world"".toNT();             // ""hello world""          ""hello world"".l('en').toNT();     // ""hello world""@en          ""_:b12"".toNT();                   // _:b12          ""foaf:name"".toNT();               // <http://xmlns.com/foaf/0.1/name>          ""http://webr3.org/"".toNT();       // <http://webr3.org/>      *   **.toCanonical()**        Alias of .toNT(), RDFa API compatibility method.    *   **.n3()** returns string        Returns the value formatted for N3/Turtle.            true.n3();                        // true          (12 * 1.4).n3();                  // 12.3          new Date().n3();                  // ""2010-11-20T21:06:42Z""^^<http://www.w3.org/2001/XMLSchema#dateTime>          ""hello world"".n3();               // ""hello world""          ""hello world"".l('en').n3();       // ""hello world""@en          ""_:b12"".n3();                     // _:b12          ""foaf:name"".n3();                 // foaf:name          ""http://webr3.org/"".n3();         // <http://webr3.org/>       ### String Methods ####  A string can represent any of the RDF Node types, PlainLiteral (+language), TypedLiteral, BlankNode or IRI.  In js3 string exposes the following methods (in addition to the standard methods outlined above):    *   **.l()** - returns this        Set the language of a PlainLiteral - exposes the **.language** attribute after calling. (.language is non-enumerable, read-only)            var s = ""Hello World"".l('en');                                  s.language                        // 'en'          s.nodeType();                     // PlainLiteral          s.toNT();                         // ""Hello World""@en     *   **.tl()** - returns this        Set the type of a TypedLiteral - exposes the **.type** attribute after calling. (.type is non-enumerable, read-only)            var s = ""0FB7"".tl('xsd:hexBinary');          s.type                            // http://www.w3.org/2001/XMLSchema#hexBinary          s.nodeType();                     // TypedLiteral          s.toNT();                         // ""0FB7""^^<http://www.w3.org/2001/XMLSchema#hexBinary>                Note: this method also caters for the situations when you want a PlainLiteral to be an xsd:string, or an IRI to be a PlainLiteral                var u = ""http://webr3.org/"";      // <http://webr3.org/>          u.tl(""rdf:PlainLiteral);          // ""http://webr3.org/""                    var h = ""hello"";                  // ""hello""          ""hello"".tl('xsd:string');         // ""hello""^^<http://www.w3.org/2001/XMLSchema#string>     *   **.resolve()** - returns string IRI        Resolve a CURIE to a full IRI - note this is done automatically by .n3 and .toNT methods.            ""foaf:name"".resolve()             // returns string ""http://xmlns.com/foaf/0.1/name"" with nodeType IRI    Remember, all javascript values and types remain unchanged, so it's entirely backwards compatible with all existing data, and will not modify any js values,  ""Hello World"".l('en') is still a String, properties like .language and .type are non-enumerable, so they won't show up in for loops or when you JSON.stringify  the values. You cannot implement this library in non v5 ecmascript by simply adding a .language property to the String object, that simply won't work.    ### Numbers ####    js3 is fully aware of number types, it knows when a number is an integer, a double or a decimal.        12 .toNT()                            // ""12""^^<http://www.w3.org/2001/XMLSchema#integer>      12.1 .toNT()                          // ""12.1""^^<http://www.w3.org/2001/XMLSchema#decimal>      1267.43233E32 .toNT()                 // ""1.26743233e+35""^^<http://www.w3.org/2001/XMLSchema#double>    **Gotcha:** do note that you need to add a space between the number and the .method, or wrap the number in braces *(12.1).toNT()*,  since js expects any integer followed immediately by a period to be a decimal, like 12.145 - this only applies to hard coded in the source-code numbers, and not  those in variables or returned from functions, so generally isn't noticable, in the same way that you don't normally write 12.toString()!      ## Arrays and Lists ##    JS3 uses arrays for both lists of objects in property-object chains { name: ['nathan','nath'] } and as rdf Lists.    You can determine whether an array is a list or not by inspecting the boolean property **.list** on any array. To specify that an array is a list you simply call .toList() on it.    ### Array Methods and Properties ###    *   **.list** - boolean        Boolean flag indicating whether an array is an RDF list.         *   **.toList()** - returns this        Specifies that an array is to be used as an RDF list, sets the .list property to true.    *   **.n3()** returns string        Returns the value formatted for N3/Turtle.            [1,2,3,4].n3()                        // 1, 2, 3, 4          [1,2,3,4].toList().n3()               // ( 1 2 3 4 )            Note that there are no .toNT or .nodeType methods, or related, arrays and lists are not RDF Nodes.    ## Objects and Descriptions ##    In js3 each Object is by default just an Object with a single additional method exposed **.ref()**. When you call this method the object is RDF enabled,  whereby it is set to denote the description of something - identified by a blanknode or an IRI - the keys (properties) are mapped to RDF Properties,  a **.id** attribute is exposed on the object, and four methods are also exposed: **.n3()**, **.toNT()**, **.using()** and **.graphify()**.    ### The Basics ###  It's all really simple tbh, the properties on each object can either be:    - obj['http://xmlns.com/foaf/0.1/name'] - a full IRI  - obj['foaf:name'] - a normal CURIE  - obj.foaf$name - a more javascript friendly CURIE where the : is swapped for a $  - obj.name - a single property which maps up to a CURIE, which maps to an IRI    Each value can be a single value (of any type covered), or an array of values (which might be a list), or an object (which can be named with an IRI or a blanknode identifier).    And thus, just like normal javascript or JSON you can make an object structure as simple or as complicated as you like.    Objects can also have methods on them, and these are stripped from any output, so any existing object whether dumb or a full class with properties can be used.    They're just javascript objects with a .id set on them (non-enumerable and read-only), and where the properties are mapped to RDF properties. So, each object can be seen to describe one thing, one subject, the .id is the subject.  To set the .id all you do is call **.ref()** on the object, if you pass in a CURIE or an IRI as a param then that is set as the subject/.id, if you call .ref() with no argument then it is given a blanknode identifier as the .id.    The methods exposed after .ref'ing are also simple, .n3 dumps an n3 string of the object, .toNT dumps it out as ntriples, and .graphify gives you back an RDFGraph from the RDFa API, making it completely compatible and exposing all the functionality of my [rdfa-api](http://github.org/webr3/rdfa-api) library (and other compatible implementations of the RDFa API).    **.using()** is a bit more subtle, you can throw in the names of ontologies which properties your using come from, in order to provide an unambiguous mapping, for instance:        var article = {        description: ""A dc11:, not dc:, description"",        label: ""An rdfs:label""      }.ref(':me').using('dc11','rdfs');    If you don't pass in any names, then they are mapped up on a first-hit-first-used basis. This is covered more in the section about *propertymap* and *curiemap*.    ### Syntax, Variables and References ###    There is no special syntax, and variables + references are part of javascript, so they ""just work"". which means you can do things like this:        article.maker = me;      me.made = article;      article.maker.knows = bob;     // the same as me.knows      article.created = new Date();      { a: 'foaf:Document', primaryTopicOf: article }.ref(':this').graphify().turtle();        Because we referenced article by value then it'll be in the output graph too, we could use article.id instead then it won't be included.    You can also have X many Objects with the same .id, then when you .graphify them they all get smashed together as one - which is nice.    As for migrating IRIs or renaming subjects, that's as simple as calling .ref(':newid') on any object, no complex rdf replace routines needed.    ### Data Structures ###     When Objects are nested, they are by default considered to be blanknodes, *however!*, you can of course call .ref() on them in place, and thus  describe things in context, and name them there too.    So in this case the object in holdsAccount will be a blanknode:        var me = {        name: 'Nathan',        holdsAccount: {          label: ""Nathan's twitter account"".l('en'),          accountName: 'webr3',          homepage: 'http://twitter.com/webr3'                  },      }.ref("":me"");    But in this case it'll have it's own IRI:            var me = {        name: 'Nathan',        holdsAccount: {          label: ""Nathan's twitter account"".l('en'),          accountName: 'webr3',          homepage: 'http://twitter.com/webr3'                  }.ref(':twitter'),                                        // here's where we named it      }.ref("":me"");    ... of course we can code this however we want to get the same results, for example:        var me = { name: 'Nathan' }.ref("":me"");      var account = { accountName: 'webr3' };      account.label = ""Nathan's twitter account"";      account.label.l('en');      me.holdsAccount = account;      me.holdsAccount.foaf$homepage = ""twitter:webr3"".resolve();      me.holdsAccount.ref(':twitter');    ... or create structures just as complex as we like:        { deep: [        ""item1"",        [1, 2.745, [me,bob,""x:mary""].toList(), new Date(), bob].toList(),        ""something"".substr(3,4).l('en'),        [bob.id, me.id].toList(), { foo: ""bar"" }      ]};    ... and interact with our data however we want:        var somedata = {        values: [1,10,25,50].toList(),        created: new Date()      }.ref(':results');       with(Math) {        somedata.result = somedata.values.map(sqrt).reduce(function(p,c) { return max(p,c) });      }    ... and then call .n3():        :results        dc:created ""2010-11-20T21:06:42Z""^^<http://www.w3.org/2001/XMLSchema#dateTime>;        seq:values ( 1 10 25 50 );        seq:result 7.0710678118654755 .          It's all very flexible - as you can see as we just map reduced an RDF List and updated a graph in one line :)    ### Object Methods and Properties - after .ref()'ing ###    *   **.id** - string (read-only, non-enumerable)        BlankNode or IRI in a string, the subject / .id of this object.         *   **.n3()** returns string        Returns the object as N3/Turtle.    *   **.toNT()** returns string        Returns the object as NTriples.    *   **.graphify()** - returns RDFGraph        Returns the structure as an RDFGraph of RDFTriples as per the RDFa API core interfaces - compat++.    *   **.using(arg1, arg2 ... argN)** - returns this        Pass in string prefixes for ontologies to consider when mapping simple properties.            Do see the [wiki page on .using() js3.propertymap](https://github.com/webr3/js3/wiki/using-and-propertymap) for more details.      ## js3.curiemap and js3.propertymap ##    **js3.curiemap** is a simple object which maps prefixes to IRIs.    *   to add a CURIE mapping:                js3.curiemap.foaf = ""http://xmlns.com/foaf/0.1/"";            *   to get an IRI for a prefix:                var iri = js3.curiemap.foaf;            *   **.setDefault(iri)** - to set the default prefix ""**:**"" :                js3.curiemap.setDefault('http://webr3.org/nathan#');    *   **.getPrefix(iri)** - get the registered prefix for an IRI, returns null if no prefix is found:                js3.curiemap.getPrefix('http://xmlns.com/foaf/0.1/'); // 'foaf'            *   **.shrink(iri)** - turn an IRI in to a CURIE :        This method returns either a CURIE or the original IRI if no prefix is found.                js3.curiemap.shrink('http://xmlns.com/foaf/0.1/name'); // 'foaf:name'              **js3.propertymap** is a simple object which makes the lib aware of properties in ontologies.    *   to add the properties for an ontology:                js3.propertymap.foaf = ['name','mbox','page', ...];    note: the value must always be an array.    *   to get the properties for an ontology:                var properties = js3.propertymap.foaf;            *   **.ambiguities()** - returns an array of ambiguous properties:                var gotchas = js3.propertymap.ambiguities();    Do see the [wiki page on .using() js3.propertymap](https://github.com/webr3/js3/wiki/using-and-propertymap) for more details.    ## js3.graphify() ##    A simple method which can accept any number of objects, or an array of objects, and will return back an RDFGraph.      # Coming Soon #    *   **IRI.deref( callback )**        A web aware function that will get the description of a subject from the web (negotiating formats) and return back a js3 object to work with:                ""http://webr3.org/nathan#me"".deref( function(me) {            print( me.name ); // etc          });            *   **Obj.save()**        A web aware function that will PUT an updated description:                ""http://webr3.org/nathan#me"".deref( function(me) {            me.friends.push( ""somebody:new );            me.save();          });            *   **Full rdfa-api integration**        From the other side, so you can do graph.describe(subject) and get back an object, and suchlike.        Probably much more..    # Feedback #    All feedback, bugs etc via issues here, or, well you can get all my details from my FOAF profile using this lib if you like ;) """
Semantic web;https://github.com/sparksrdf/sparks;"""Sparks JavaScript Library  ==================    Sparks is a set of JavaScript libraries designed for simplifying the access to RDF data.  Sparks is licensed under the MIT license.    Disclaimer  ---------------  This is the first release of the library and it only contains an initial version of the Prism filtering framework.  There is some known bugs and the library doesn't support the upcoming Sparks Plug library.      Install  --------  Sparks depends on [JQuery](http://jquery.com/) and [JavascriptMVC](http://javascriptmvc.com/) in order to work properly.  Download [JavascriptMVC](http://javascriptmvc.com/)  and put the Sparks code in sparks directory inside [JavascriptMVC](http://javascriptmvc.com/).    Alternatively, you might use the file ''*sparks.prism.js*'' which includes everything except [JQuery](http://jquery.com/).    Usage  ---------  ```javascript  //You need to include the sparks library before doing the following:  //- Create a new prism endpoint:  var prism = new Sparks.Prism(""http://nebula.dcs.shef.ac.uk/sparks/sparql"",""?root a <http://ext.dcs.shef.ac.uk/~u0080/linkedPOI/core#POI>"");    //- Instantiate some lenses:  $('#tags-list').sparks_prism_lens_tags({prism: prism});  $('#search').sparks_prism_lens_search({prism: prism});  $('#list').sparks_prism_lens_list({prism: prism});  ``` """
Semantic web;https://github.com/aschaetzle/Sempala;"""# Sempala      Sempala is a SPARQL-over-SQL approach to provide interactive-time SPARQL query processing on Hadoop. It stores RDF data in a columnar layout (Parquet) on HDFS and uses either Impala or Spark as the execution layer on top of it. SPARQL queries are translated into Impala/Spark SQL for execution.    http://dbis.informatik.uni-freiburg.de/forschung/projekte/DiPoS/Sempala.html      ### LICENSE  Unless explicitly stated otherwise all files in this repository are licensed under the Apache Software License 2.0    >   Copyright 2017 University of Freiburg  >  >   Licensed under the Apache License, Version 2.0 (the ""License"");  >   you may not use this file except in compliance with the License.  >   You may obtain a copy of the License at  >  >       http://www.apache.org/licenses/LICENSE-2.0  >  >   Unless required by applicable law or agreed to in writing, software  >   distributed under the License is distributed on an ""AS IS"" BASIS,  >   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  >   See the License for the specific language governing permissions and  >   limitations under the License.      ## BUILD project  You need to have Maven installed on your system.  Then simply run ""mvn package"" from the root directory.  It will build 'sempala-loader', 'sempala-translator' and finally 'sempala'.  You can also build 'sepala-loader' or 'sempala-translator' only by  running ""mvn package"" from the corresponding subdirectory.  NOTE: Two jars are generated for sempala translator - one for Impala (sempala-translator)   and one for Spark (spark-sempala-translator)      ## PURPOSE OF project_repo DIRECTORY    Cloudera Impala JDBC connector ships with several libraries. All but the   connector itself are available in the maven or cloudera central repositories  and are pulled at build time by maven. To fit in the maven architecture the  connector is installed in a in-project repository, which behaves like a remote   central repository.    To update the version of the Impala JDBC connector in the in-project repository,  you can install a newer version of it and update the POM of sempala-parent  (main POM in root directory of this project) to use that version.  To do this, get the JDBC driver by downloading it from cloudera.com [1]  and install it with the maven install plugin. This will take care of checksums:    ```    mvn install:install-file      -DlocalRepositoryPath=project_repo      -DcreateChecksum=true      -Dpackaging=jar      -Dfile=<path_to:jdbc_driver.jar>      -DgroupId=com.cloudera.impala      -DartifactId=impala-jdbc-4.1-connector      -Dversion=<version>  ```      ## Official guide to installing 3rd party JARs    Although rarely, but sometimes you will have 3rd party JARs that you need to put  in your local repository for use in your builds, since they don't exist in any  public repository like Maven Central. The JARs must be placed in the local  repository in the correct place in order for it to be correctly picked up by  Apache Maven. To make this easier, and less error prone, we have provide a goal  in the maven-install-plugin which should make this relatively painless. To  install a JAR in the local repository use the following command:    ```    mvn install:install-file -Dfile=<path-to-file> -DgroupId=<group-id> \      -DartifactId=<artifact-id> -Dversion=<version> -Dpackaging=<packaging>  ```    If there's a pom-file as well, you can install it with the following command:    ```    mvn install:install-file -Dfile=<path-to-file> -DpomFile=<path-to-pomfile>  ```    With version 2.5 of the maven-install-plugin it gets even better. If the JAR was  built by Apache Maven, it'll contain a pom.xml in a subfolder of the META-INF  directory, which will be read by default. In that case, all you need to do is:    ```    mvn install:install-file -Dfile=<path-to-file>  ```    (Source: https://maven.apache.org/guides/mini/guide-3rd-party-jars-local.html)        [1] http://www.cloudera.com/downloads/connectors/impala/jdbc.html """
Semantic web;https://github.com/amgadmadkour/knowledgecubes;"""![KNOWLEDGECUBES_LOGO](src/main/resources/logo-svg.png)    ## About    A Knowledge Cube, or KC for short, is a semantically-guided data management architecture, where data semantics influences the data management architecture rather than a predefined scheme. KC relies on semantics to define how the data is fetched, organized, stored, optimized, and queried. Knowledge cubes use RDF to store data. This allows knowledge cubes to store Linked Data from the Web of Data. Knowledge cubes envisions breaking down the centralized architecture into multiple specialized cubes, each having its own index and data store.    ## Quick Start Guide    #### Create Encode Data    ```bash  java -cp uber-knowledgecubes-0.1.0.jar:scala-library-2.11.0.jar edu.purdue.knowledgecubes.DictionaryEncoderCLI -i src/main/resources/datasets/original/sample.nt -o /home/amadkour/kclocal/encoded.nt -l /home/amadkour/kclocal -s space  ```  The ```kclocal``` will contain the created dictionaries, the initial data structure used by the store    #### Create Store    ```bash  spark-submit --master local[*] --class edu.purdue.knowledgecubes.StoreCLI target/uber-knowledgecubes-0.1.0.jar -i /home/amadkour/kclocal/encoded.nt -l /home/amadkour/kclocal -f 0.01 -t roaring -d /home/amadkour/kcdb  ```  The database ```kcdb``` directory contains the actual data and reductions for the input NT file. The following is the directory structure of the local store:    #### Run Query Workload    ```bash  spark-submit --master local[*] --class edu.purdue.knowledgecubes.BenchmarkReductionsCLI target/uber-knowledgecubes-0.1.0.jar -l /home/amadkour/kclocal -f 0.01 -t roaring -d /home/amadkour/kcdb -q src/main/resources/queries/original  ```  The command generates the workload reductions under the ```kcdb/reductions/join``` directory. The partitions are saved using parquet format.     #### Local Store Overview    ```  $ ls  amadkour@amadkour:~/kclocal$ ls  GEFI  dbinfo.yaml  dictionary  encoded.nt  join-reductions.yaml    results-20200625115017.txt  tables.yaml  ```    * ```GEFI```: directory represents the generalized filters created for the input datasets.   * ```dbinfo.yaml```: file lists meta-data about the store datasets.   * ```dictionary```: directory containts the string to id mappings created by the dictionary module.   * ```join-reductions.yaml```: directory contains metadata about the generated reductions.  * ```results-20200625115017.txt```: is the output file containing the query performance output when running the benchmarking modules.   * ```tables.yaml```: file lists the meta-data about the tables.    #### Database Directory Overview    ```  amadkour@amadkour:~/kcdb$ ls  data  reductions  ```  The database contains parquet formatted files that represents the original data and reductions:  * ```data```: contains the original data created based on the input NT files.  * ```reductions```: contains the workload-driven reductions created after running a query workload (e.g. after running the Benchmark CLI tool mentioned below).     Program such as spark-shell can be used to view the parquet file content:    ```bash  scala> var data = spark.read.parquet(""/home/amadkour/kcdb/reductions/join/13_TRPO_JOIN_13_TRPS"")  data: org.apache.spark.sql.DataFrame = [s: int, p: int ... 1 more field]    scala> data.show()  +---+---+---+  |  s|  p|  o|  +---+---+---+  | 11| 13|  3|  | 11| 13|  4|  | 12| 13|  5|  |  8| 13|  1|  +---+---+---+  ```    ## WORQ: Workload-Driven RDF Query Processing    KC uses a workload-driven RDF query processing technique, or WORQ for short, for filtering non-matching entries during join evaluation as early as possible to reduce the communication and computation overhead. WORQ generates a reduced sets of triples (or reductions, for short) to represent join pattern(s) of query workloads. WORQ can materialize the reductions on disk or in memory and reuses the reductions that share the same join pattern(s) to answer queries. Furthermore, these reductions are not computed beforehand, but are rather computed in an online fashion. KC also answer complex analytical queries that involve unbound properties. Based on a realization of KC on top of Spark, extensive experimentation demonstrates an order of magnitude enhancement in terms of preprocessing, storage, and query performance compared to the state-of-the-art cloud-based solutions.    ## Features    * A spark-based API for SPARQL querying  * Efficient execution of frequent workload join patterns  * Materialze workload join patterns in memory or on disk  * Efficiently answer unbound property queries    ## Usage    KC provide spark-based API for issuing RDF related operations. There are three steps necessary for running the system:     * Dictionary Encoding  * Store Creation  * Querying    #### Dictionary Encoding     KC requires that the dataset be dictionary encoded. The dictionary encoding allows adding resources (subjects or objects) as integers to the filters.     ```bash  java -cp target/uber-knowledgecubes-0.1.0.jar edu.purdue.knowledgecubes.DictionaryEncoderCLI -i [NT File] -o [Output File] -l [Local Path for the new store] -s space  ```    The command generates a dictionary encoded version of the dataset. This encoded NT file is used for creating the store. KC automatically encodes and decodes SPARQL queries and the corresponding results.     #### Store Creation    KC provide the Store class for creation of an RDF store. The input to the store is a spark session, database path where the RDF dataset will be stored, and a local configuration path.    ```scala  import org.apache.spark.sql.SparkSession    import edu.purdue.knowledgecubes.GEFI.GEFIType  import edu.purdue.knowledgecubes.GEFI.join.GEFIJoinCreator  import edu.purdue.knowledgecubes.storage.persistent.Store    val localPath = ""/path/to/local/path""  val dbPath = ""/path/to/db/path""  val ntPath = ""/path/to/rdf/file""    val spark = SparkSession.builder              .appName(s""KnowledgeCubes Store Creator"")              .getOrCreate()    val store = Store(spark, dbPath, localPath)  store.create(ntPath)  ```    #### SPARQL Querying    KC provides a SPARQL query processor that takes as input the spark session, database path of where the RDF dataset was created, local configuration file path, a filter type, and a false postivie rate (if any).    ```scala  import org.apache.spark.sql.SparkSession    import edu.purdue.knowledgecubes.queryprocessor.QueryProcessor  import edu.purdue.knowledgecubes.GEFI.GEFIType    val spark = SparkSession.builder              .appName(s""Knowledge Cubes Query"")              .getOrCreate()    val localPath = ""/path/to/local/path""  val dbPath = ""/path/to/db/path""  val filterType = GEFIType.ROARING // Roaring bitmap  val falsePositiveRate = 0    val queryProcessor = QueryProcessor(spark, dbPath, localPath, filterType, falsePositiveRate)    val query =    """"""      SELECT ?GivenName ?FamilyName WHERE{          ?p <http://yago-knowledge.org/resource/hasGivenName> ?GivenName .           ?p <http://yago-knowledge.org/resource/hasFamilyName> ?FamilyName .           ?p <http://yago-knowledge.org/resource/wasBornIn> ?city .           ?p <http://yago-knowledge.org/resource/hasAcademicAdvisor> ?a .          ?a <http://yago-knowledge.org/resource/wasBornIn> ?city .      }    """""".stripMargin    // Returns a Spark DataFrame containing the results  val r = queryProcessor.sparql(query)    ```    #### Constructing Filters    Additionaly, KC provides an API for creating additional filters. KC provides exact and approximate structures for filtering data. Currently KC supports ```GEFIType.BLOOM```, ```GEFIType.ROARING```, and ```GEFIType.BITSET```.    ```scala  import org.apache.spark.sql.SparkSession    import edu.purdue.knowledgecubes.GEFI.GEFIType  import edu.purdue.knowledgecubes.GEFI.join.GEFIJoinCreator  import edu.purdue.knowledgecubes.utils.Timer    val spark = SparkSession.builder              .appName(s""KnowledgeCubes Filter Creator"")              .getOrCreate()                var localPath = ""/path/to/db/path""  var dbPath = ""/path/to/local/path""  var filterType = GEFIType.ROARING  var fp = 0    val filter = new GEFIJoinCreator(spark, dbPath, localPath)  filter.create(filterType, fp)  ```    #### Query Execution Benchmarking    KC provides a set of benchmarking classes    * **BenchmarkFilteringCLI:** For benchmarking the query execution when using filters  * **BenchamrkReductionsCLI:** For benchmarking the query execution when using reductions only          ## Publications    * Amgad Madkour, Ahmed M. Ali, Walid G. Aref, ""WORQ: Workload-driven RDF Query Processing"", ISWC 2018 [[Paper](https://amgadmadkour.github.io/files/papers/worq.pdf)][[Slides](https://amgadmadkour.github.io/files/presentations/WORQ-ISWC2018.pdf)]    * Amgad Madkour, Walid G. Aref, Ahmed M. Aly, ""SPARTI: Scalable RDF Data Management Using Query-Centric Semantic Partitioning"", Semantic Big Data (SBD18) [[Paper](https://amgadmadkour.github.io/files/papers/sparti.pdf)][[Slides](https://amgadmadkour.github.io/files/presentations/SPARTI-SBD2018.pdf)]    * Amgad Madkour, Walid G. Aref, Sunil Prabhakar, Mohamed Ali, Siarhei Bykau, ""TrueWeb: A Proposal for Scalable Semantically-Guided Data Management and Truth Finding in Heterogeneous Web Sources"", Semantic Big Data (SBD18) [[Paper](https://amgadmadkour.github.io/files/papers/trueweb.pdf)][[Slides](https://amgadmadkour.github.io/files/presentations/TrueWeb-SBD2018.pdf)]    * Amgad Madkour, Walid G. Aref, Saleh Basalamah, “Knowledge Cubes - A Proposal for Scalable and Semantically-Guided Management of Big Data”, IEEE BigData 2013 [[Paper](https://amgadmadkour.github.io/files/papers/bigdata2013.pdf)][[Slides](https://amgadmadkour.github.io/files/presentations/KnowledgeCubes.pdf)]    ## Contact    If you have any problems running KC please feel free to send an email.     * Amgad Madkour <amgad@alumni.purdue.edu> """
Semantic web;https://github.com/gushakov/sparql-template;"""## SPARQL Template    [![Build status](https://travis-ci.org/gushakov/sparql-template.svg?branch=master)](https://travis-ci.org/gushakov/sparql-template)    Small library for traversing an RDF store using automatic mapping of triples to annotated POJOs.    ## Highlights     * Support of any store exposing HTTP SPARQL endpoint   * Uses [Jena API](https://jena.apache.org/) to load and process RDF triples   * Uses [MappingContext](https://github.com/spring-projects/spring-data-commons/blob/master/src/main/java/org/springframework/data/mapping/context/MappingContext.java) from Spring Data Commons to process class annotations   * On-demand (lazy) loading of relations using automatic proxying with [ByteBuddy](http://bytebuddy.net/)   * Easily extended for conversion from any `org.apache.jena.graph.Node` to a custom Java type   * Some useful converters are registered by default, see `ch.unil.sparql.template.convert.ExtendedRdfJavaConverter`    + `java.util.Date`    + `java.time.ZonedDateTime`    + `java.time.Duration`    + `java.net.URL`     ## Examples    Assume we want to retrieve some information about a person from the [DBPedia](http://dbpedia.org) using the [SPARQL endpoint](http://dbpedia.org/sparql).  We annotate our domain POJO as following.    ```java  // marks this as an RDF entity  @Rdf  public class Person {        // will be mapped from the value of http://dbpedia.org/ontology/birthName      @Predicate(DBP_NS)      private String birthName;        // will be mapped from the value of http://www.w3.org/2000/01/rdf-schema#label for the Russian language      @Predicate(value = RDFS_NS, language = ""ru"")      private String label;        // will be mapped from the value of http://dbpedia.org/property/birthDate, automatic conversion to java.time.ZonedDateTime      @Predicate(DBP_NS)      private ZonedDateTime birthDate;        // will be mapped from the values of http://dbpedia.org/property/spouse, lazy load of relationships      @Predicate(DBP_NS)      @Relation      private Collection<Person> spouse;  }  ```    Then we can just use `ch.unil.sparql.template.SparqlTemplate` to load the triples from the DBPedia converting  them automatically to the required Java instance.    ```java      // get the default SPARQL template      final SparqlTemplate sparqlTemplate = new SparqlTemplate(""https://dbpedia.org/sparql"");        // load information about Angelina Jolie      final Person person = sparqlTemplate.load(DBR_NS + ""Angelina_Jolie"", Person.class);        System.out.println(person.getBirthName());      // Angelina Jolie Voight        System.out.println(person.getLabel());      // Джоли, Анджелина        System.out.println(person.getBirthDate().format(DateTimeFormatter.ofPattern(""dd/MM/yyyy (EEE)"", Locale.ENGLISH)));      // 04/06/1975 (Wed)        System.out.println(person.getSpouse().stream()              .filter(p -> p.getBirthName() != null && p.getBirthName().contains(""Pitt""))              .findAny().get().getBirthName());      // William Bradley Pitt    ``` """
Semantic web;https://github.com/garlik/4store;"""# 4store    4store is an efficient, scalable and stable RDF database.    4store was designed by Steve Harris and developed at Garlik to underpin their  Semantic Web applications. It has been providing the base platform for around 3  years. At times holding and running queries over databases of 15GT, supporting a  Web application used by thousands of people.    ## Getting started    In this section:    1. Installing prerequisites.  2. Installing 4store.  3. Running 4store.  4. Installing frontend tools only.  5. Other installation hints.    ### Installing prerequisites    To install [Raptor](https://github.com/dajobe/raptor) (RDF parser) and  [Rasqal](https://github.com/dajobe/rasqal) (SPARQL parser):        # install a 64-bit raptor from freshly extracted source      ./configure --libdir=/usr/local/lib64 && make      sudo make install        # similarly for 64-bit rasqal      ./configure ""--enable-query-languages=laqrs sparql rdql"" \       --libdir=/usr/local/lib64 && make      sudo make install        # ensure PKG_CONFIG_PATH is set correctly      # ensure /etc/ld.so.conf.d/ includes /usr/local/lib64      sudo ldconfig    ### Installing 4store        ./autogen.sh      ./configure      make      make install    ### Running 4store        /usr/local/bin/4s-boss -D    ### Installing frontend tools only    To install just the frontend tools on non-cluster frontends:        # pre-requisites for installing the frontend tools      yum install pcre-devel avahi avahi-tools avahi-devel        # src/common      (cd src/common && make)        # src/frontend      (cd src/frontend && make && make install)      ### Other installation hints    Make sure `/var/lib/4store/` exists (in a cluster, it only needs to exist on  backend nodes) and that the user or users who will create new KBs have  permission to write to this directory.    For clusters (or to test cluster tools on a single machine) the frontend must  have a file `/etc/4s-cluster`, which lists all machines in the cluster.      To avoid problems running out of Avahi DBUS connections, modify  `/etc/dbus-1/system.d/avahi-dbus.conf` to:    * Increase `max_connections_per_user` to 200 or so  * Increase `max_match_rules_per_connection` to 512 or so (optional) """
Semantic web;https://github.com/UCLALibrary/fester;"""# Fester  [![Maven Build](https://github.com/uclalibrary/fester/workflows/Maven%20PR%20Build/badge.svg)](https://github.com/UCLALibrary/fester/actions) [![Codacy Badge](https://app.codacy.com/project/badge/Grade/990b5c316e0a45d092c83d58f148e0e8)](https://www.codacy.com/gh/UCLALibrary/fester?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=UCLALibrary/fester&amp;utm_campaign=Badge_Grade) [![Codacy Badge](https://app.codacy.com/project/badge/Coverage/990b5c316e0a45d092c83d58f148e0e8)](https://www.codacy.com/gh/UCLALibrary/fester?utm_source=github.com&utm_medium=referral&utm_content=UCLALibrary/fester&utm_campaign=Badge_Coverage) [![Known Vulnerabilities](https://snyk.io/test/github/uclalibrary/fester/badge.svg)](https://snyk.io/test/github/uclalibrary/fester)    A microservice for facilitating the creation, storage, and retrieval of IIIF manifests and collections.    ## Prerequisites    There are just a few prerequisites that must be installed, and configured correctly, in order to build Fester:    * [Java Development Kit (JDK)](https://openjdk.java.net/install/): version 11 or greater  * [Docker](https://docs.docker.com/get-docker/): version 19.03 or greater  * [Maven](https://maven.apache.org/download.cgi): version 3.6 or greater  * [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html) [Not required, but useful]    These packages may also be available through your system's package repository. If they are, it's better to install from that source so that they will be kept up to date for you.    You will also need an account on AWS and have the ability to create [IAM](https://aws.amazon.com/iam/) accounts and [S3](https://aws.amazon.com/s3/) buckets.    ## Configuring the Build    Fester uses an S3 bucket for back-end storage. To be able to run the project's tests, several configuration values must be supplied:    * fester.s3.bucket  * fester.s3.access_key  * fester.s3.secret_key  * fester.s3.region    These values can be set as properties in your system's Maven settings.xml file (or be supplied on the command line at build time).    ## Building the Project    The project builds an executable Jar that can be run to start the microservice. To build the project, run:        mvn package    This will put the executable Jar in the `target/build-artifact` directory.    To generate the site's documentation, run:        mvn site    This will generate the documentation in the `target/site` directory.    ## Configuring the Tests    The project contains unit, functional, and integration tests, with controls on how to control which tests are run. In order to run the functional and integration tests, the build machine must have a working Docker environment. Setting up Docker on your machine will depend on the type of machine you have (e.g., Linux, Mac, or Windows). Docker's [documentation](https://docs.docker.com/get-docker/) should be consulted on how to do this.    When running the build using the 'package' phase (as described above), only the unit tests are run. If you want to run all the possible tests, the project can be built with:        mvn integration-test    or        mvn verify    This will run the functional, feature flag, and integration tests, in addition to the unit tests. If you want to skip a particular type of test but still run the 'verify' phase, you can use one of the following arguments to your Maven command:        -DskipUTs      -DskipITs      -DskipFTs      -DskipFfTs    The first will skip the unit tests; the second will skip the integration tests; the third will skip the functional tests; and, the fourth will skip the feature flag tests. They can also be combined so that two types of tests are skipped. For instance, only the functional tests will be run if the following is typed:        mvn verify -DskipUTs -DskipITs    When running the integration and functional tests, it may be desirable to turn on logging for the containers that run the tests. This can be useful in debugging test failures that happen within the container. To do this, supply one (or any) of the following arguments to your build:        -DseeLogsFT      -DseeLogsIT      -DseeLogsFfT    This will tunnel the container's logs (including the application within the container's logs) to Maven's logging mechanism so that you will be able to see what's happening in the container as the tests are being run against it.    You might also want to adjust the logging level on the tests themselves. By default, the test loggers are configured to write DEBUG logs to a log file in the `target` directory and ERROR logs to standard out. To change the log level of the standard out logging, run Maven with the `logLevel` argument; for instance:        mvn -DlogLevel=DEBUG test    If you want more fine-grained control over the logging, you can copy the `src/test/resources/logback-test.xml` file to the project's root directory and modify it. A `logback-test.xml` file in the project's home directory will be used instead of the standard one in `src/rest/resources` if it's available. That hypothetical file has also been added to the project's `.gitignore` so you don't need to worry about checking it into Git.    ## Running a Single Test    It is sometimes useful to run a single test (instead of the whole test suite). The Surefire Maven plugin allows for this, but it's worth noting that when a single test is run in this way the test suite's pre-configured system properties are not picked up from the plugin's configuration. To work around this, a dev who wants to run a single test must supply the necessary properties theirself. For example, if one wanted to run the functional test that checks that missing images get a placeholder image in the manifest, the command to do that would be:        mvn integration-test -Dtest=MissingImageFT -Dfester.s3.bucket=iiif-fester -Dfester.placeholder.url=""https://iiif.library.ucla.edu/iiif/2/blank"" -Dfester.logs.output=true    You would want to supply your own values for `fester.s3.bucket` and `fester.placeholder` of course. This command will spin up the Docker container that the functional test is run against, but it will only run the `MissingImageFT` test, skipping all the integration and other functional tests in the suite.    ## Running the Application for Development    You can run a development instance of Fester by typing the following within the project root:        mvn -Plive test    Once run, the service can be verified/accessed at [http://localhost:8888/fester/status](http://localhost:8888/fester/status). The API documentation can be accessed at [http://localhost:8888/fester/docs](http://localhost:8888/fester/docs)    ## Debugging with Eclipse IDE    There are two ways to debug Fester:    - **Debugging the tests.** This enables the developer to step through both the test and application code as the test suite runs.  - **Debugging a running instance.** This enables the developer to step through the application code as they interact with the HTTP API.    The following setup instructions were tested with [Eclipse IDE](https://www.eclipse.org/eclipseide/) 4.14.0 (2019-12).    ### Debugging the tests    From within Eclipse:    1. Create a new debug configuration      - In the top-level menu, select *Run* > *Debug Configurations...*      - In the pop-up window:          - Create a new configuration of type *Remote Java Application*          - Set *Name* to something like `Fester (JDWP server for containerized instances created by test suite)`          - In the *Connect* tab:              - Set *Project* to the Fester project directory              - Set *Connection Type* to `Standard (Socket Listen)`              - Set *Port* to `5556`              - Set *Connection limit* to `16`              - Check *Allow termination of remote VM* (optional)  2. Create another debug configuration *      - In the top-level menu, select *Run* > *Debug Configurations...*      - In the pop-up window:          - Create a new configuration of type *Maven Build*          - Set *Name* to something like `Fester (debug test suite)`          - In the *Main* tab:              - Set *Base directory* to the Fester project directory              - Set *Goals* to `integration-test`              - Set *Profiles* to `debug`              - Set *User settings* to the path to a `settings.xml` that contains your AWS S3 credentials  3. Run the debug configuration created in Step 1 **  4. Run the debug configuration created in Step 2 **    _* As an alternative to step 2 (and 4), run the following from the command line (after completing steps 1 and 3):_        mvn -Pdebug integration-test    _** If you're doing this for the first time, you may need to bring back the pop-up window where you created the configuration in order to invoke it. Otherwise, you can use toolbar buttons, or hotkeys <kbd>Ctrl</kbd> <kbd>F11</kbd> (Run) or <kbd>F11</kbd> (Debug)._    ### Debugging a running instance    This procedure will start an instance of Fester with port `5555` open for incoming JDWP connections.    From within Eclipse:    1. Create a new run configuration ***      - In the top-level menu, select *Run* > *Run Configurations...*      - In the pop-up window:          - Create a new configuration of type *Maven Build*          - Set *Name* to something like `Fester (debugging mode)`          - In the *Main* tab:              - Set *Base directory* to the Fester project directory              - Set *Goals* to `test`              - Set *Profiles* to `runDebug`              - Set *User settings* to the path to a `settings.xml` that contains your AWS S3 credentials  2. Create a new debug configuration      - In the top-level menu, select *Run* > *Debug Configurations...*      - In the pop-up window:          - Create a new configuration of type *Remote Java Application*          - Set *Name* to something like `Fester (JDWP client)`          - In the *Connect* tab:              - Set *Project* to the Fester project directory              - Set *Connection Type* to `Standard (Socket Attach)`              - Set *Host* to `localhost`              - Set *Port* to `5555`              - Check *Allow termination of remote VM* (optional)  3. Run the new run configuration created in Step 1  4. Run the new debug configuration created in Step 2    _*** As an alternative to step 1 (and 3), run the following from the command line:_        mvn -PrunDebug test    _and then proceed with steps 2 and 4._    ## Load Testing    A [Locust](https://docs.locust.io/en/stable/index.html) test file is included, it only tests PUTs of manifests. If you wish to run the test, you need to have Locust installed, and then run the following command from the src/test/scripts/locust folder:        locust --host=url-of-the-server-you-are-testing    For example, if you wish to run a Locust test against a dev instance on your own machine, you would enter:        locust --host=http://localhost:8888    ## Git Hooks    To prevent accidentally pushing commits that would cause the CI build to fail, you can configure your Git client to use a pre-push hook:        ln -s ../../src/test/scripts/git-hooks/pre-push .git/hooks    ## Working with Pinned OS Packages    We pin the versions of packages that we install into our base image. What this means is that periodically a pinned version will become obsolete and the build will break. We have a nightly build that should catch this issues for us, but in the case that you find the breakage before us, there is a handy way to tell which pinned version has broken the build. To see the current versions inside the base image, run:        mvn validate -Dversions    This will output a list of current versions, which can be compared to the pinned versions defined in the project's POM file (i.e., pom.xml).    ## Festerize    [Festerize](https://github.com/UCLALibrary/festerize) may be used to interact with Fester, as an alternative to the built-in CSV upload form.    ## Contact    We use an internal ticketing system, but we've left the GitHub [issues](https://github.com/UCLALibrary/fester/issues) open in case you'd like to file a ticket or make a suggestion. You can also contact Kevin S. Clarke at <a href=""mailto:ksclarke@ksclarke.io"">ksclarke@ksclarke.io</a> if you have a question about the project. """
Semantic web;https://github.com/ssrangan/gm-sparql;"""# gm-sparql  Graph Mining Using SPARQL    The Resource Description Framework (RDF) and SPARQL Protocol and RDF Query Language (SPARQL) were introduced about a decade ago to enable flexible schema-free data interchange on the Semantic Web. Today, data scientists use the framework as a scalable graph representation for integrating, querying, exploring and analyzing data sets hosted at different sources. With increasing adoption, the need for graph mining capabilities for the Semantic Web has emerged. We address that need through implementation of popular iterative Graph Mining algorithms (degree distribution, diameter, radius, node eccentricity, triangle count, connected component analysis, and PageRank). We implement these algorithms as SPARQL queries, wrapped within Python scripts. These graph mining algorithms (that have a linear-algebra formulation) can indeed be unleashed on data represented as RDF graphs using the SPARQL query interface.    Although primarily developed for Cray's Urika hosted at the Department of Energy's Oak Ridge National Laboratory, this open source version works on Apache Jena triplestore. We have tested EAGLE to work on desktops, laptops and cloud services such as Amazon EC2. EAGLE can play a critical role in the exploratory analysis of massive heterogeneous graph data (e.g. semantic knowledge graphs). We believe with more support and user feedback we can create a ""MATLAB"" for LinkedData.    We really appreciate citing the following paper if our code was useful in anyway to your work.    S.Lee (lees4@ornl.gov) , S.R. Sukumar (sukumarsr@ornl.gov) and S- H. Lim (lims1@ornl.gov), ”Graph mining meets the Semantic Web”, in the Proc. of the Workshop on Data Engineering meets the Semantic Web in conjunction with International Conference on Data Engineering, Korea, 2015.    List of ORNL team members (in alphabetical order)  * Sangkeun Lee (lees4@ornl.gov, leesangkeun@gmail.com) - Post-doc  * Seung-Hwan. Lim (lims1@ornl.gov)  * Seokyong Hong (shong3@ncsu.edu) - Intern  * Sreenivas R. Sukumar (sukumarsr@ornl.gov)  * Tyler C. Brown (browntc@ornl.gov) - Intern   """
Semantic web;https://github.com/ontola/ontologies;"""# Ontologies  Never manage a namespace object map again, scrap typo's for well-known ontologies. Like DefinitelyTyped, but for ontologies.    ## Usage    ### @ontologies/core  When working with RDF (linked data) ontologies are very important, but being able to quickly create  and work with the fundamental building blocks of RDF is equally important. `@ontologies/core` exports  an object (_data factory_) which aims to to just that.    Assuming the following import in the examples:    ```javascript   import rdf from ""@ontologies/core""  ```    #### Create literals  ```javascript  // Strings  rdf.literal(""Hello world!"")   // { termType: ""Literal"", value: ""Hello World!"", datatype: { termType: ""NamedNode"", value: ""http://www.w3.org/2001/XMLSchema#string"" } }  ```    ```javascript  // Numbers  rdf.literal(9001)   // { termType: ""Literal"", value: ""9001"", datatype: { termType: ""NamedNode"", value: ""http://www.w3.org/2001/XMLSchema#integer"" } }  ```    Most JS literals will be mapped to their RDF (xsd) counterparts, passing the datatype explicitly is  also possible. Please note that data types in RDF must be IRIs.    ```javascript  rdf.literal(""(5,2)"", rdf.namedNode(""http://example.com/types/myCoordinate""))   // { termType: ""Literal"", value: ""(5,2)"", datatype: { termType: ""NamedNode"", value: ""http://example.com/types/myCoordinate"" } }  ```    #### Create links  Use `namedNode` to create links to other resources which have been _named_, meaning an authority has  given the resource a fixed identifier on their domain. This can be a resource on the web (e.g.   `http:`, `https:`) but also in the internet (e.g. `ftp:` or `magnet:`) or elsewhere (e.g. `urn:isbn:`  or `doi:`). Note that choosing schemes which are widely deployed (e.g. `https:`) will allow others   easier access to find, access, and share your data.     ```javascript  rdf.namedNode(""https://schema.org/Thing"")   // { termType: ""NamedNode"", value: ""https://schema.org/Thing"" }    rdf.namedNode(""https://example.com/myDocument#paragraph"")   // { termType: ""NamedNode"", value: ""https://example.com/myDocument#paragraph"" }    rdf.namedNode(""urn:isbn:978-0-201-61622-4"")   // { termType: ""NamedNode"", value: ""urn:isbn:978-0-201-61622-4"" }  ```    [Cool iris don't change](https://www.w3.org/Provider/Style/URI), but designing systems is difficult,  so there will be resources which don't have their own name or are too expensive to assign a name.  This is where blank nodes can be used, these are resources as well, but with _""an identifier yet to  be assigned""_. This is a bit of a tricky way of saying ""it thing has a name, but I don't know it yet"".    ```javascript  // Create blank nodes (links which have no place in the web yet)  rdf.blankNode()   // { termType: ""BlankNode"", value: ""b0"" }    rdf.blankNode(""fixed"")   // { termType: ""BlankNode"", value: ""fixed"" }  ```    Note that most of the time blank nodes can be replaced with fragment (`#`) IRIs without much trouble.    ```json5  {    ""@id"": ""http://example.com/myCollection"",    ""members"": {      ""@id"": ""http://example.com/myCollection#members"", // Append `#members` to the base rather than use a blank node      ""@type"": ""http://www.w3.org/1999/02/22-rdf-syntax-ns#Seq"",      ""http://www.w3.org/1999/02/22-rdf-syntax-ns#_0"": ""First item"",      ""http://www.w3.org/1999/02/22-rdf-syntax-ns#_1"": ""Second item"",      ""http://www.w3.org/1999/02/22-rdf-syntax-ns#_2"": ""Third item"",    }  }  ```     #### Create statements  ```javascript  // Create quads (Statements about some thing in the world)  rdf.quad(s, p, o)   // { subject: <s>, predicate: <p>, object: <o>, graph: <defaultGraph> }  ```    ```javascript  // Compare rdf objects  import { Thing } from ""@ontologies/schema"";  console.log(rdf.equals(rdf.namedNode(""https://schema.org/Thing""), Thing))   // true      // Serialize to n-triples/quads  console.log(rdf.toNQ())  ```    #### Other exports  Overview of the exports of @ontologies/core;    * A default export which is a proxy to the currently assigned global [Data Factory](http://rdf.js.org/data-model-spec/#datafactory-interface).  * A named export `globalSymbol` which is a symbol to identify the Data Factory  used by the other @ontologies/ packages to create rdf objects with.  * A named export `setup` which binds the PlainFactory to the global scope under the globalSymbol      identifier if it was previously undefined.  * A named export `globalFactory` which should be a reference to the last .  * A named export `PlainFactory` which implements the Data Factory interface (with slight adjustments)     in a functional way (e.g. no instance methods, but provides an `equals` function on the factory itself).   * A named export `createNS` which you can use to create namespaces with which ease NamedNode      creation using the global factory.  * A small set of types useful for working with RDF.    ### @ontologies/*  The other packages are generated from their respective ontologies, providing client applications with  importable symbols and a named export `ns` with which custom additional terms can be created within  the given namespace.    ```javascript  import { name } from '@ontologies/schema'    console.log(name) // http://schema.org/name  ```    All terms    ```javascript  import * as schema from '@ontologies/schema'    console.log(schema.name) // http://schema.org/name  ```    Custom terms    ```javascript  import { ns } from '@ontologies/schema'    console.log(ns('extension')) // http://schema.org/extension  ```    Use `.value` for the string representation    ```javascript  import { name } from '@ontologies/schema'    console.log(name.value) // ""http://schema.org/name""  ```    ### Overriding the default factory  The default factory used is the `PlainFactory` from this package. This is a factory which should  suffice most needs, but certain JS RDF libraries expect more methods to be available on the factory.  It is possible to override the factory with a custom one.    Initialize a custom factory by calling `setup()` from `@ontologies/core`    ```javascript  import { setup } from ""@ontologies/core"";  import myFactory from ""./myFactory"";    setup(new myFactory);  ```    Library authors who want to provide an alternate default than the PlainFactory but who don't want to  override the end-users setting can soft-override the factory;    ```javascript  import { setup } from ""@ontologies/core"";  import LibFactory from ""./LibFactory"";    setup(new LibFactory(), false); // Passing false will override the default but not a user-set factory.  ```     ### Help, my factory isn't loaded!  Chances are you have called `setup` too late in the module initialization cycle. Be sure to;  1. Move the setup call in a _separate_ file.  2. Don't import _any_ package using the default export from `@ontologies/core` in that file.  3. **Import that file before any other import** which uses default export from `@ontologies/core` in that file.    ### Non-js symbols  Dashes in term names are replaced with underscores. The default export contains both the verbatim  and the underscored values.    ```javascript  import dcterms, { ISO639_2 } from '@ontologies'    console.log(ISO639_2) // NamedNode(http://purl.org/dc/terms/ISO639-2)  console.log(dcterms.ISO639_2) // NamedNode(http://purl.org/dc/terms/ISO639-2)  console.log(dcterms['ISO639-2']) // NamedNode(http://purl.org/dc/terms/ISO639-2)  ```    ### Collisions with ES reserved keywords  If a term collides with an [ES 5/6 reserved keyword](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Lexical_grammar#Keywords)  or certain built-in classes, the term is prepended with the symbol of the ontology:    ```javascript  import { name, schemayield } from '@ontologies/schema'    // 'name' is not a JS reserved keyword  console.log(name.value) // ""http://schema.org/name""  // 'yield' is a reserved keyword, so the package name is prepended to the js identifier.  console.log(schemayield.value) // ""http://schema.org/yield""  ``` """
Semantic web;https://github.com/lanthaler/HydraClient;"""HydraClient  ===========    This is work in progress. Documentation will follow. In the meantime, check  out [Hydra](http://www.markus-lanthaler.com/hydra/). """
Semantic web;https://github.com/nelson-ai/semantic-graphql;"""# Semantic GraphQL    *UNMAINTAINED:* [Go to the new repo location](https://github.com/dherault/semantic-graphql) for issues and pull requests. """
Semantic web;https://github.com/streamreasoning/CSPARQL-engine;"""CSPARQL-engine  ==============    Reasoning over RDF stream made easy.  The project contains parent pom in the root and a number of module that inherit from parent pom.  To install the csparql-core jar, run mvn install on parent pom. """
Semantic web;https://github.com/anno4j/anno4j;"""# Anno4j    > This library mainly provides programmatic access to the [W3C Web Annotation Data Model](http://www.w3.org/TR/annotation-model/) (formerly known as the [W3C Open Annotation Data Model](http://www.openannotation.org/spec/core/)) to allow Annotations to be written from and to local or remote SPARQL endpoints. An easy-to-use and extensible Java API allows creation and querying of Annotations even for non-experts. This API is augmented with various supporting functionalities to increase the usability of using the W3C Web Annotations.  >   > With the last iteration, Anno4j has also been developed to be able to work with generic metadata models. It is now possible to parse a RDFS or OWL Lite schema and generate the respective Anno4j classes on the fly via code generation.    ## Build Status  master branch: [![Build Status](https://travis-ci.org/anno4j/anno4j.svg?branch=master)](https://travis-ci.org/anno4j/anno4j) develop branch: [![Build Status](https://travis-ci.org/anno4j/anno4j.svg?branch=develop)](https://travis-ci.org/anno4j/anno4j)    ## Table of Content    The use of the Anno4j library and its features is documented in the respective [GitHub Anno4j Wiki](https://github.com/anno4j/anno4j/wiki). Its features are the following:    - Extensible creation of Web/Open Annotations based on Java Annotations syntax (see [Getting Started](https://github.com/anno4j/anno4j/wiki/Getting-started))  - Built-in and predefined implementations for nearly all RDF classes conform to the W3C Web Annotation Data Model  - Created (and annotated) Java POJOs are transformed to RDF and automatically transmitted to local/remote SPARQL 1.1 endpoints using the SPARQL Update functionality  - Querying of annotations with path-based criteria (see [Querying](https://github.com/anno4j/anno4j/wiki/Querying))      - [x] Basic comparisons like ""equal"", ""greater"", and ""lower""      - [x] String comparisons: ""equal"", ""contains"", ""starts with"", and ""ends with""      - [x] Union of different paths      - [x] Type condition      - [x] Custom filters  - Addition of custom behaviours of otherwise simple Anno4j classes through partial/support classes (see [Support Classes](https://github.com/anno4j/anno4j/wiki/Support-Classes))  - Input and Output to and from different standardised RDF serialisation standards (see [RDF Input and Output](https://github.com/anno4j/anno4j/wiki/RDF-Input-and-Output))  - Parsing of RDFS or OWL Lite schemata to automatically generate respective Anno4j classes (see [Java File Generation](https://github.com/anno4j/anno4j/wiki/Java-File-Generation))  - Schema/Validation annotations that can be added to Anno4j classes to induce schema-correctness which is indicated at the point of creation (see [Schema Validation](https://github.com/anno4j/anno4j/wiki/Schema-Validation) and [Schema Annotations](https://github.com/anno4j/anno4j/wiki/Schema-Annotations))  - A tool to support the generation of so-called proxy classes, that speed up the creation of instances of large and deep schemata    ## Status of Anno4j and the implemented WADM specification    The current version 2.4 of Anno4j supports the [most current W3C recommendation of the Web Annotation Data Model](https://www.w3.org/TR/annotation-model/).    ## Development Guidelines    ### Snapshot  Each push on the development branch triggers the build of a snapshot version. Snapshots are publicly available:  ```xml  	<dependency>   	<groupId>com.github.anno4j</groupId>     	<artifactId>anno4j-core</artifactId>     	<version>X.X.X-SNAPSHOT</version>  	</dependency>  ```         ### Compile, Package and Install    Package with:  ```        mvn package  ```         Install to your local repository  ```        mvn install  ```         ### Participate  1. Create an issue  2. Fork Anno4j  3. Add features  4. Add JUnit Tests  5. Create pull request to anno4j/develop      ### 3rd party integration of custom LDPath expressions    To contribute custom LDPath (test) functions and thereby custom LDPath syntax, the following two classes have to be provided:    1. Step:     Create a Java class that extends either the *SelectorFunction* class or the *TestFunction* class. This class defines the actual syntax  that has to be injected into the Anno4j evaluation process.    ```java      public class GetSelector extends SelectorFunction<Node> {                @Override          protected String getLocalName() {              return ""getSelector"";          }                @Override          public Collection<Node> apply(RDFBackend<Node> backend, Node context, Collection<Node>... args) throws IllegalArgumentException {              return null;          }                @Override          public String getSignature() {              return ""fn:getSelector(Annotation) : Selector"";          }                @Override          public String getDescription() {              return ""Selects the Selector of a given annotation object."";          }      }    ```     2. Step:    Create a Java class that actually evaluates the newly provided LDPath expression. This class needs  to be flagged with the *@Evaluator* Java annotation. The *@Evaluator* annotation requires the class   of the description mentioned in the first step. Besides that, the evaluator has to implement either  the *QueryEvaluator* or the *TestEvaluator* interface. Inside the prepared evaluate method, the actual  SPARQL query has to be generated using the Apache Jena framework.    ```java      @Evaluator(GetSelector.class)      public class GetSelectorFunctionEvaluator implements QueryEvaluator {          @Override          public Var evaluate(NodeSelector nodeSelector, ElementGroup elementGroup, Var var, LDPathEvaluatorConfiguration evaluatorConfiguration) {              Var evaluate = new SelfSelectionEvaluator().evaluate(nodeSelector, elementGroup, var, evaluatorConfiguration);              Var target = Var.alloc(""target"");              Var selector = Var.alloc(""selector"");                    elementGroup.addTriplePattern(new Triple(evaluate.asNode(), new ResourceImpl(OADM.HAS_TARGET).asNode(), target));              elementGroup.addTriplePattern(new Triple(target.asNode(), new ResourceImpl(OADM.HAS_SELECTOR).asNode(), selector));              return selector;          }      }  ```     ## Contributors    - Kai Schlegel (University of Passau)  - Andreas Eisenkolb (University of Passau)  - Emanuel Berndl (University of Passau)  - Thomas Weißgerber (University of Passau)  - Matthias Fisch (University of Passau)    > This software was partially developed within the [MICO project](http://www.mico-project.eu/) (Media in Context - European Commission 7th Framework Programme grant agreement no: 610480) and the [ViSIT project](http://www.phil.uni-passau.de/dh/projekte/visit/) (Virtuelle Verbund-Systeme und Informations-Technologien für die touristische Erschließung von kulturellem Erbe - Interreg Österreich-Bayern 2014-2020, project code: AB78).    ## License   Apache License Version 2.0 - http://www.apache.org/licenses/LICENSE-2.0    """
Semantic web;https://github.com/mff-uk/ODCS;"""ODCleanStore  ============    The tool uses data processing pipelines for obtaining, processing, and storing  RDF data. It makes data processing highly customizable by employing custom data  processing units, also provides data processing monitoring, debugging, and  scheduling capabilities.      Documentation  -------------    Please see http://www.ksi.mff.cuni.cz/~knap/odcs      Installation steps  ------------------    Please see http://www.ksi.mff.cuni.cz/~knap/odcs/doc.html#install      Licenses  -------    The following modules of the software are licensed under GNU Lesser General Public License, Version 3, https://www.gnu.org/licenses/lgpl-3.0.txt:       * commons    * commons-module    * commons-web    * dataunit-rdf    * dataunit-file    * ontology    * module-test    * module-base      The following modules of the software are licensed under GNU General Public License, Version 3, https://www.gnu.org/licenses/gpl-3.0.txt:      * commons-app    * frontend    * backend    * dataunit    * dataunit-file-impl    * dataunit-rdf-impl    * RDF_File_Extractor    * RDF_File_Loader     * SPARQL_Extractor     * SPARQL_Loader     * SPARQL_Transformer    * RDF_Data_Validator     * Silk_Linker_Extractor    For details, please see the particular module.       """
Semantic web;https://github.com/leipert/vsb;"""# Visual SPARQL Builder    Have a look at the [project page](http://leipert.github.io/vsb)    The Visual SPARQL Builder (VSB) is a tool which allows users to create and run SPARQL queries with a graphical interface within the browser.  For the creation of a query basic understanding of linked data is needed.    ## Deployment    1. Clone this repository and checkout the `dist` branch (or download a release).  1. Serve the VSB with an webserver like apache or nginx.  1. You probably want to configure the VSB for your own endpoint.     Therefore you need to create a file named `overwrite.js` in the root folder of your VSB copy.     For documentation of the structure of the file, please have a look [here](docs/overwrite.js.md)    ## Development    1. Clone this repository (or download a release).  1. Install [node.js](http://nodejs.org/) and npm  1. Install grunt `npm install -g bower gulp`  1. Run `npm install`  1. Run `bower install`  1. Run `gulp develop` to see the app running on `http://localhost:8123/`  1. Happy Development!"""
Semantic web;https://github.com/aldonline/fbrs;"""# FBRS = Facebook RDF Sync    Facebook's Graph API can return (almost) proper Linked Data when asked to in a polite manner. ( Accept: text/turtle ).  However, we can still not SPARQL the complete dataset.    Our solution is to load a subset of Facebook's graph at any given time.  Which subset to load is a tricky question and will depend on the use cases.  This framework supports some common ways of loading data from Facebook, including batch, incremental,  as well as keeping data in sync using Facebook's Real-Time updates.    ## Setup    * You will need the following dependencies:   * Node v0.6.x   * OpenLink Virtuoso >= 6.1.3   * Virtuoso's iSQL client library on the path    FBRS will access Virtuoso via both the iSQL client library and the SPARQL HTTP endpoint.  You can configure the ports later on.    Clone this repo:        git@github.com:aldonline/fbrs.git    And install dependencies:        cd fbrs      npm install -d    For example:    * Load all data   ...    ## Usage    You can run FBRS in batch mode or via an HTTP Server. Bath is simpler, but does not   keep the data in sync.    ### Batch ( load everything once or incremental )        ### Web Server ( With Real-Time Updates )        FBRS_SPARQL_ENDPOINT=""http://localhost:8890/sparql""      FBRS_VIRTUOSO_PORT=1111      FBRS_VIRTUOSO_USERNAME=""dba""      FBRS_VIRTUOSO_PASSWORD=""dba""      FBRS_PORT=3008      FBRS_CALLBACK_PORT=3009      FBRS_ACCESS_TOKEN=...       """
Semantic web;https://github.com/tenforce/SPARQL-parser;"""# SPARQL parser  The SPARQL parser is a library that helps to parse and investigate a SPARQL query and to build up and generate SPARQL queries.    ## Usage  To parse a query you can run:  ```  SPARQLQuery parsedQuery = new SPARQLQuery(""SELECT * FROM <http://graph1> WHERE { ?s ?p ?o . }"");  ```  The parsedQuery object will then contain a java object representation of that query. It will have a hashMap with prefix objects, a type , a list of IStatements, a set of unknowns, possibly a graph and the original query.    ### Prefix objects  Those are quiet simple, they map a name on a URL.    ### Type  The following types are supported  * SPARQLQuery.Type.ASK  * SPARQLQuery.Type.DESCRIBE  * SPARQLQuery.Type.SELECT  * SPARQLQuery.Type.CONSTRUCT  * SPARQLQuery.Type.UPDATE     ### IStatement  The IStatement interface is an abstraction of SPARQL statements. It includes 'select blocks', 'where blocks', 'update blocks', 'parentheses blocks' and 'simple statements'. These blocks support methods to extract the unknowns, inner blocks (for instance a select block contains a parentheses block and that again contains multiple simple statements), the graph on which it operates as well as some functional methods (to change the graph for instance).    ### Unknown  This is just a java String that holds the name of the variable in the query without the '?' (ie. ?mu becomes ""mu"")    ### Graph  This is also just a java String.    ## Installation  Adding to the pom:  ```  <dependency>    <groupId>com.tenforce.semtech</groupId>    <artifactId>SPARQL-parser</artifactId>    <version>0.0.3</version>  </dependency>  ``` """
Semantic web;https://github.com/tomayac/ldf-client;"""&lt;ldf-client&gt;  ==========================    A declarative [Linked Data Fragments](http://linkeddatafragments.org) client in the form of a [Web Component](http://webcomponents.org/).  Simply insert the ```<ldf-client>``` in your page, fill in the required attribute values,  and you are ready to go.    Live Demo  =========    You can see a [live demo](http://tomayac.github.io/ldf-client/)  of the Web Component in action hosted on GitHub pages.    Installation  ============    This assumes that you have [bower](http://bower.io/) and  [polyserve](https://github.com/PolymerLabs/polyserve) installed.    ```sh  $ mkdir web-components  $ cd web-components  $ git clone git@github.com:tomayac/ldf-client.git  $ cd ldf-client  $ bower install  $ polyserve  ```    Finally visit [http://localhost:8080/components/ldf-client/](http://localhost:8080/components/ldf-client/)  and click the [demo](http://localhost:8080/components/ldf-client/demo/) link in the upper right corner.    Basic Usage  ===========    The example below shows basic usage instructions for the element.    ```html  <!doctype html>  <html>    <head>      <script src=""../webcomponentsjs/webcomponents-lite.min.js""></script>      <link rel=""import"" href=""ldf-client.html"">    </head>    <body>      <!-- Streaming example -->      <ldf-client          id=""ldf-client-streaming""          response-format=""streaming""          query=""SELECT DISTINCT ?frag WHERE {                   ?a a &lt;http://advene.org/ns/cinelab/ld#Annotation&gt; ;                     &lt;http://advene.org/ns/cinelab/ld#hasFragment&gt; ?frag ;                     &lt;http://advene.org/ns/cinelab/ld#taggedWith&gt;                       [ &lt;http://purl.org/dc/elements/1.1/title&gt; 'personnages: Margaret'],                       [ &lt;http://purl.org/dc/elements/1.1/title&gt; 'personnages: Grand Papa Pollitt'];                     .                 }""          start-fragment=""http://spectacleenlignes.fr/ldf/spectacle_en_lignes"">      </ldf-client>        <!-- Polling example -->      <ldf-client          id=""ldf-client-polling""          response-format=""polling""          query=""SELECT DISTINCT ?tag WHERE {                   [ a &lt;http://advene.org/ns/cinelab/ld#Annotation&gt; ;                     &lt;http://advene.org/ns/cinelab/ld#taggedWith&gt;                       [ &lt;http://purl.org/dc/elements/1.1/title&gt;  ?tag ]                    ]                 }""          start-fragment=""http://spectacleenlignes.fr/ldf/spectacle_en_lignes"">      </ldf-client>        <button value=""Poll"" id=""button"">Poll</button>        <script>        document.addEventListener('polymer-ready', function() {          /* Streaming example */          var ldfClientStreaming = document.querySelector('#ldf-client-streaming');          // Process data as it appears          ldfClientStreaming.addEventListener('ldf-query-streaming-response-partial',              function(e) {            var pre = document.createElement('pre');            pre.textContent = JSON.stringify(e.detail.response);            document.body.appendChild(pre);          });          // Get notified once all data is received          ldfClientStreaming.addEventListener('ldf-query-streaming-response-end', function() {            alert('Received all data');          });            /* Polling example */          var ldfClientPolling = document.querySelector('#ldf-client-polling');          // Poll for data          ldfClientPolling.addEventListener('ldf-query-polling-response', function(e) {            var pre = document.createElement('pre');            pre.textContent = JSON.stringify(e.detail.response);            document.body.appendChild(pre);          });          // Manually trigger polling          var button = document.querySelector('#button');          button.addEventListener('click', function() {            ldfClientPolling.showNext();          });        });      </script>    </body>  </html>  ```    About Linked Data Fragments  ===========================    The Web is full of high-quality Linked Data,  but we can't reliably query it.  Public SPARQL endpoints are often unavailable,  because they need to answer many unique queries.  One could set up a local endpoint using data dumps,  but that's not Web querying and  the data is never up-to-date.    [Linked Data Fragments](http://linkeddatafragments.org/) offer interfaces to  solve queries at the client side with server data.  Servers can offer data at low processing cost  in a way that enables client-side querying.  You can read more about [the concept of Linked Data Fragments](http://linkeddatafragments.org/concept/)  and learn about available [Linked Data Fragments software](http://linkeddatafragments.org/software/).    Acknowledgements  ================    Building this Web Component was pretty straight-forward thanks to  [Ruben Verborgh](http://ruben.verborgh.org/)'s  Linked Data Fragments browser client  [Browser.js](https://github.com/LinkedDataFragments/Browser.js).    License  =======  Copyright 2015 Thomas Steiner (tomac@google.com)    Licensed under the Apache License, Version 2.0 (the ""License"");  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)    Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. """
Semantic web;https://github.com/Accenture/AmpliGraph;"""# ![AmpliGraph](docs/img/ampligraph_logo_transparent_300.png)    [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2595043.svg)](https://doi.org/10.5281/zenodo.2595043)    [![Documentation Status](https://readthedocs.org/projects/ampligraph/badge/?version=latest)](http://ampligraph.readthedocs.io/?badge=latest)    [Join the conversation on Slack](https://join.slack.com/t/ampligraph/shared_invite/enQtNTc2NTI0MzUxMTM5LTRkODk0MjI2OWRlZjdjYmExY2Q3M2M3NGY0MGYyMmI4NWYyMWVhYTRjZDhkZjA1YTEyMzBkMGE4N2RmNTRiZDg)  ![](docs/img/slack_logo.png)    **Open source library based on TensorFlow that predicts links between concepts in a knowledge graph.**    **AmpliGraph** is a suite of neural machine learning models for relational Learning, a branch of machine learning  that deals with supervised learning on knowledge graphs.      **Use AmpliGraph if you need to**:    * Discover new knowledge from an existing knowledge graph.  * Complete large knowledge graphs with missing statements.  * Generate stand-alone knowledge graph embeddings.  * Develop and evaluate a new relational model.      AmpliGraph's machine learning models generate **knowledge graph embeddings**, vector representations of concepts in a metric space:    ![](docs/img/kg_lp_step1.png)    It then combines embeddings with model-specific scoring functions to predict unseen and novel links:    ![](docs/img/kg_lp_step2.png)      ## Key Features      * **Intuitive APIs**: AmpliGraph APIs are designed to reduce the code amount required to learn models that predict links in knowledge graphs.  * **GPU-Ready**: AmpliGraph is based on TensorFlow, and it is designed to run seamlessly on CPU and GPU devices - to speed-up training.  * **Extensible**: Roll your own knowledge graph embeddings model by extending AmpliGraph base estimators.      ## Modules    AmpliGraph includes the following submodules:    * **Datasets**: helper functions to load datasets (knowledge graphs).  * **Models**: knowledge graph embedding models. AmpliGraph contains **TransE**, **DistMult**, **ComplEx**, **HolE**, **ConvE**, **ConvKB**. (More to come!)  * **Evaluation**: metrics and evaluation protocols to assess the predictive power of the models.  * **Discovery**: High-level convenience APIs for knowledge discovery (discover new facts, cluster entities, predict near duplicates).      ## Installation    ### Prerequisites    * Linux, macOS, Windows  * Python 3.7    #### Provision a Virtual Environment    Create and activate a virtual environment (conda)    ```  conda create --name ampligraph python=3.7  source activate ampligraph  ```    #### Install TensorFlow    AmpliGraph is built on TensorFlow 1.x.  Install from pip or conda:    **CPU-only**    ```  pip install ""tensorflow>=1.15.2,<2.0""    or    conda install tensorflow'>=1.15.2,<2.0.0'  ```    **GPU support**    ```  pip install ""tensorflow-gpu>=1.15.2,<2.0""    or    conda install tensorflow-gpu'>=1.15.2,<2.0.0'  ```        ### Install AmpliGraph      Install the latest stable release from pip:    ```  pip install ampligraph  ```    If instead you want the most recent development version, you can clone the repository  and install from source (your local working copy will be on the latest commit on the `develop` branch).  The code snippet below will install the library in editable mode (`-e`):    ```  git clone https://github.com/Accenture/AmpliGraph.git  cd AmpliGraph  pip install -e .  ```      ### Sanity Check    ```python  >> import ampligraph  >> ampligraph.__version__  '1.4.0'  ```      ## Predictive Power Evaluation (MRR Filtered)    AmpliGraph includes implementations of TransE, DistMult, ComplEx, HolE, ConvE, and ConvKB.  Their predictive power is reported below and compared against the state-of-the-art results in literature.  [More details available here](https://docs.ampligraph.org/en/latest/experiments.html).    |                              |FB15K-237 |WN18RR   |YAGO3-10   | FB15k      |WN18           |  |------------------------------|----------|---------|-----------|------------|---------------|  | Literature Best              | **0.35***| 0.48*   | 0.49*     | **0.84**** | **0.95***     |  | TransE (AmpliGraph)          |  0.31    | 0.22    | **0.51**  | 0.63       | 0.66          |  | DistMult (AmpliGraph)        |  0.31    | 0.47    | 0.50      | 0.78       | 0.82          |  | ComplEx  (AmpliGraph)        |  0.32    | **0.51**| 0.49      | 0.80       | 0.94          |  | HolE (AmpliGraph)            |  0.31    | 0.47    | 0.50      | 0.80       | 0.94          |  | ConvE (AmpliGraph)           |  0.26    | 0.45    | 0.30      | 0.50       | 0.93          |  | ConvE (1-N, AmpliGraph)      |  0.32    | 0.48    | 0.40      | 0.80       | **0.95**      |  | ConvKB (AmpliGraph)          |  0.23    | 0.39    | 0.30      | 0.65       | 0.80          |    <sub>  * Timothee Lacroix, Nicolas Usunier, and Guillaume Obozinski. Canonical tensor decomposition for knowledge base   completion. In International Conference on Machine Learning, 2869–2878. 2018. <br/>  **  Kadlec, Rudolf, Ondrej Bajgar, and Jan Kleindienst. ""Knowledge base completion: Baselines strike back.   "" arXiv preprint arXiv:1705.10744 (2017).  </sub>    <sub>  Results above are computed assigning the worst rank to a positive in case of ties.   Although this is the most conservative approach, some published literature may adopt an evaluation protocol that assigns   the best rank instead.   </sub>      ## Documentation    **[Documentation available here](http://docs.ampligraph.org)**    The project documentation can be built from your local working copy with:    ```  cd docs  make clean autogen html  ```    ## How to contribute    See [guidelines](http://docs.ampligraph.org) from AmpliGraph documentation.      ## How to Cite    If you like AmpliGraph and you use it in your project, why not starring the project on GitHub!    [![GitHub stars](https://img.shields.io/github/stars/Accenture/AmpliGraph.svg?style=social&label=Star&maxAge=3600)](https://GitHub.com/Accenture/AmpliGraph/stargazers/)      If you instead use AmpliGraph in an academic publication, cite as:    ```  @misc{ampligraph,   author= {Luca Costabello and            Sumit Pai and            Chan Le Van and            Rory McGrath and            Nicholas McCarthy and            Pedro Tabacof},   title = {{AmpliGraph: a Library for Representation Learning on Knowledge Graphs}},   month = mar,   year  = 2019,   doi   = {10.5281/zenodo.2595043},   url   = {https://doi.org/10.5281/zenodo.2595043}  }  ```    ## License    AmpliGraph is licensed under the Apache 2.0 License."""
Semantic web;https://github.com/jimregan/wordnet-lemon-to-w3c;"""wordnet-lemon-to-w3c  ====================    sameAs links for the Lemon conversion of WordNet 2.0 to the W3C conversion"""
Semantic web;https://github.com/jpcik/morph;"""morph  =====    **If you don't care about compiling you can use morph**   as in this sample Java project: https://github.com/jpcik/morph-starter  using the library through Maven or Sbt.      To build morph you need:    * jvm7  * sbt 0.13 (www.scala-sbt.org)    The scala version is 2.10.3, but sbt will take care of that ;)  To compile it, run sbt after downloading the code:    ```  >sbt  >compile  ```    To run the R2RML test cases:    ```  >sbt  >project morph-r2rml-tc  >test  ```   """
Semantic web;https://github.com/Claudenw/PA4RDF;"""# PA4RDF  Persistence Annotation for RDF (PA4RDF) is a set of annotations and an entity manager that provides JPA like functionality on top of an RDF store while accounting for and exploiting the fundamental differences between graph storage and relational storage.  	PA4RDF introduces three (3) annotations that map a RDF triple (subject, predicate, object) to a Plain Old Java Object (POJO) using Java's dynamic proxy capabilities.    Documentation is at http://claudenw.github.io/PA4RDF/ """
Semantic web;https://github.com/levelgraph/levelgraph-n3;"""LevelGraph-N3  ===========    ![Logo](https://github.com/levelgraph/node-levelgraph/raw/master/logo.png)    [![Build Status](https://travis-ci.org/levelgraph/levelgraph-n3.png)](https://travis-ci.org/levelgraph/levelgraph-n3)  [![Coverage Status](https://coveralls.io/repos/levelgraph/levelgraph-n3/badge.png)](https://coveralls.io/r/levelgraph/levelgraph-n3)  [![Dependency Status](https://david-dm.org/levelgraph/levelgraph-n3.png?theme=shields.io)](https://david-dm.org/levelgraph/levelgraph-n3)  [![Sauce Labs  Tests](https://saucelabs.com/browser-matrix/levelgraph-n3.svg)](https://saucelabs.com/u/levelgraph-n3)    __LevelGraph-N3__ is a plugin for  [LevelGraph](http://github.com/levelgraph/levelgraph) that adds the  ability to store, fetch and process N3 and turtle files.    ## Install    ### Node.js    Adding support for N3 to LevelGraph is easy:  ```shell  $ npm install level levelgraph levelgraph-n3 --save  ```  Then in your code:  ```js  var level = require('level'),      levelgraph = require('levelgraph'),      levelgraphN3 = require('levelgraph-n3'),      db = levelgraphN3(levelgraph(level('yourdb')));  ```        ### Browser    If you use [browserify](http://browserify.org/) you can use this package  in a browser just as in node.js. Please also take a look at [Browserify  section in LevelGraph package](https://github.com/levelgraph/levelgraph#browserify)      ## Usage    We assume in following examples that you created database as explained  above!  ```js  var db = levelgraphN3(levelgraph(level(""yourdb"")));  ```    ### Importing n3 files    In code:    ```js  var fs = require(""fs"");    var stream = fs.createReadStream(""./triples.n3"")                 .pipe(db.n3.putStream());    stream.on(""finish"", function() {    console.log(""Import completed"");  });  ```    Alternatively, you can run the import CLI tool by running `npm install`, then:    ```  ./import.js path/to/n3/file(s)  ```    with the following optional flags:    `-o` or `--output` followed by the desired DB path. If not specified, path will be at `./db`.    `-q` or `--quiet` will silence status updates during the import process. Otherwise, progress information is displayed.    File extensions must be `.n3` or `.nt`. Additionally, there is glob support, so for example `*.nt` will import all the matching n-triple files.      ### Get and Put    Storing an N3 file in the database is extremey easy:  ```js  var turtle = ""@prefix c: <http://example.org/cartoons#>.\n"" +               ""c:Tom a c:Cat.\n"" +               ""c:Jerry a c:Mouse;\n"" +               ""        c:smarterThan c:Tom;\n"" +               ""        c:place \""fantasy\""."";    db.n3.put(turtle, function(err) {    // do something after the triple is inserted  });  ```    Retrieving it through pattern-matching is extremely easy:  ```js  db.n3.get({ subject: ""http://example.org/cartoons#Tom"" }, function(err, turtle) {    // turtle is ""<http://example.org/cartoons#Tom> a <http://example.org/cartoons#Cat> .\n"";  });  ```    It even support a Stream interface:  ```js  var stream = db.n3.getStream({ subject: ""http://example.org/cartoons#Tom"" });  stream.on(""data"", function(data) {    // data is ""<http://example.org/cartoons#Tom> a <http://example.org/cartoons#Cat> .\n"";  });  stream.on(""end"", done);  ```    ### Exporting NTriples from LevelGraph    __LevelGraph-N3__ allows to export ntriples from a __LevelGraph__ database.  __LevelGraph-N3__ augments the a standard `search` method with a `{ n3: ... }` option  that specifies the subject, predicate and object of the created triples.  It follows the same structure of the `{ materialized: ... }` option (see https://github.com/levelgraph/levelgraph#searches).    Here is an example:  ```js  db.search([{    subject: db.v(""s""),    predicate: ""http://example.org/cartoons#smarterThan"",    object: db.v(""o"")  }], {    n3: {      subject: db.v(""o""),      predicate: ""http://example.org/cartoons#dumberThan"",      object: db.v(""s"")    }  }, function(err, turtle) {    // turtle is ""<http://example.org/cartoons#Tom> <http://example.org/cartoons#dumberThan> <http://example.org/cartoons#Jerry> .\n""  });  ```    It also supported by the `searchStream` method.    ## Changes    [CHANGELOG.md](https://github.com/levelgraph/levelgraph-n3/blob/master/CHANGELOG.md)  **including migration info for breaking changes**      ## Contributing to LevelGraph-N3    * Check out the latest master to make sure the feature hasn't been    implemented or the bug hasn't been fixed yet  * Check out the issue tracker to make sure someone already hasn't    requested it and/or contributed it  * Fork the project  * Start a feature/bugfix branch  * Commit and push until you are happy with your contribution  * Make sure to add tests for it. This is important so I don't break it    in a future version unintentionally.  * Please try not to mess with the Makefile and package.json. If you    want to have your own version, or is otherwise necessary, that is    fine, but please isolate to its own commit so I can cherry-pick around    it.    ## LICENSE - ""MIT License""    Copyright (c) 2013-2015 Matteo Collina (http://matteocollina.com)    Permission is hereby granted, free of charge, to any person  obtaining a copy of this software and associated documentation  files (the ""Software""), to deal in the Software without  restriction, including without limitation the rights to use,  copy, modify, merge, publish, distribute, sublicense, and/or sell  copies of the Software, and to permit persons to whom the  Software is furnished to do so, subject to the following  conditions:    The above copyright notice and this permission notice shall be  included in all copies or substantial portions of the Software.    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,  EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES  OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND  NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT  HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,  WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR  OTHER DEALINGS IN THE SOFTWARE. """
Semantic web;https://github.com/Wimmics/gephi-semantic-web-import;"""# Gephi Plugins    This repository is an out-of-the-box development environment for Gephi plugins. Gephi plugins are implemented in Java and can extend [Gephi](https://gephi.org) in many different ways, adding or improving features. Getting started is easy with this repository but also checkout the [Bootcamp](https://github.com/gephi/gephi-plugins-bootcamp) for examples of plugins you can create.     ## Migrate Gephi 0.8 plugins    The process in which plugins are developed and submitted had an overhaul when Gephi 0.9 was released. Details can be read on this article: [Plugin development gets new tools and opens-up to the community](https://gephi.wordpress.com/2015/12/16/plugin-development-gets-new-tools-and-opens-up-to-the-community/).    This section is a step-by-step guide to migrate 0.8 plugins. Before going through the code and configuration, let's summerize the key differences between the two environements.    - The 0.8 base is built using Ant, whereas the 0.9 uses Maven. These two are significantly different. If you aren't familiar with Maven, you can start with [Maven in 5 Minutes]( https://maven.apache.org/guides/getting-started/maven-in-five-minutes.html). Maven configurations are defined in the `pom.xml` files.  - The 0.8 base finds the Gephi modules into the `platform` folder checked in the repository, whereas the 0.9 base downloads everything from the central Maven repository, where all Gephi modules are available.  - Maven requires to separate source files (e.g. .java) and resources files (e.g. .properties) into distinct folders. Sources are located in `src/main/java` and resources in `src/main/resources`.    A custom `migrate` goal is available in the [Gephi Maven Plugin](https://github.com/gephi/gephi-maven-plugin) to facilitate the migration from 0.8 to 0.9. This automated process migrates ant-based plugins to maven and takes care of copying the configuration and code. Follow these steps to migrate your plugin:    - Fork and checkout this repository:            git clone git@github.com:username/gephi-plugins.git    If you've already had a forked repository based on 0.8 we suggest to save your code somewhere, delete it and fork again as the history was cleared.    - Copy your plugin folder at the root of this directory.    - Run this command:            mvn org.gephi:gephi-maven-plugin:migrate    This command will detect the ant-based plugin and migrate it. The resulting folder is then located into the `modules` folder.    The plugin code can then be inspected in Netbeans or built via command line with `mvn clean package`.    ## Get started    ### Requirements    Developing Gephi plugins requires [JDK 7](http://www.oracle.com/technetwork/java/javase/downloads/index.html) or later and [Maven](http://maven.apache.org/). Although any IDE/Editor can be used, [Netbeans IDE](https://netbeans.org/) is recommend as Gephi itself is based on [Netbeans Platform](https://netbeans.org/features/platform/index.html).    ### Create a plugin    The creation of a new plugin is simple thanks to our custom [Gephi Maven Plugin](https://github.com/gephi/gephi-maven-plugin). The `generate` goal asks a few questions and then configures everything for you.    - Fork and checkout the latest version of this repository:            git clone git@github.com:username/gephi-plugins.git  - Run the following command and answer the questions:            mvn org.gephi:gephi-maven-plugin:generate    This is an example of what this process will ask:            Name of organization (e.g. my.company): org.foo          Name of artifact (e.g my-plugin): my-plugin          Version (e.g. 1.0.0): 1.0.0          Directory name (e.g MyPlugin): MyPlugin          Branding name (e.g My Plugin): My Plugin          Category (e.g Layout, Filter, etc.): Layout          Author: My Name          Author email (optional):          Author URL (optional):          License (e.g Apache 2.0): Apache 2.0          Short description (i.e. one sentence): Plugin catch-phrase          Long description (i.e multiple sentences): Plugin features are great          Would you like to add a README.md file (yes|no): yes    The plugin configuration is created. Now you can (in any order):    - Add some Java code in the `src/main/java` folder of your plugin  - Add some resources (e.g. Bundle.properties, images) into the `src/main/resources/` folder of your plugin  - Change the version, author or license information into the `pom.xml` file, which is in your plugin folder  - Edit the description or category details into the `src/main/nbm/manifest.mf` file in your plugin folder     ### Build a plugin    Run the following command to compile and build your plugin:           mvn clean package    In addition of compiling and building the JAR and NBM, this command uses the `Gephi Maven Plugin` to verify the plugin's configuration. In care something is wrong it will fail and indicte the reason.    ### Run Gephi with plugin    Run the following command to run Gephi with your plugin pre-installed. Make sure to run `mvn package` beforehand to rebuild.           mvn org.gephi:gephi-maven-plugin:run    In Gephi, when you navigate to `Tools` > `Plugins` you should see your plugin listed in `Installed`.    ## Submit a plugin    Submitting a Gephi plugin for approval is a simple process based on GitHub's [pull request](https://help.github.com/articles/using-pull-requests/) mechanism.    - First, make sure you're working on a fork of [gephi-plugins](https://github.com/gephi/gephi-plugins). You can check that by running `git remote -v` and look at the url, it should contain your GitHub username, for example `git@github.com:username/gephi-plugins.git`.    - Add and commit your work. It's recommended to keep your fork synced with the upstream repository, as explained [here](https://help.github.com/articles/syncing-a-fork/), so you can run `git merge upstream/master` beforehand.    - Push your commits to your fork with `git push origin master`.    - Navigate to your fork's URL and create a pull request. Select `master-forge` instead of `master` as base branch.    - Submit your pull request.    ## Update a plugin    Updating a Gephi plugin has the same process as submiting it for the first time. Don't forget to merge from upstream's master branch.    ## IDE Support    ### Netbeans IDE    - Start Netbeans and go to `File` and then `Open Project`. Navigate to your fork repository, Netbeans automatically recognizes it as Maven project.   - Each plugin module can be found in the `Modules` folder.    To run Gephi with your plugin pre-installed, right click on the `gephi-plugins` project and select `Run`.    To debug Gephi with your plugin, right click on the `gephi-plugins` project and select `Debug`.    ### IntelliJ IDEA    - Start IntelliJ and `Open` the project by navigating to your fork repository. IntelliJ may prompt you to import the Maven project, select yes.    To run Gephi with your plugin pre-installed when you click `Run`, create a `Maven` run configuration and enter `org.gephi:gephi-maven-plugin:run` in the command field. The working directory is simply the current project directory.    To debug Gephi with your plugin, create a `Remote` configuration and switch the `Debugger mode` option to `Listen`. Then create a `Maven` run configuration like abobe but add `-Drun.params.debug=""-J-Xdebug -J-Xnoagent -J-Xrunjdwp:transport=dt_socket,suspend=n,server=n,address=5005""` into the `Runner` > `VM Options` field. Then, go to the `Run` menu and first run debug with the remote configuration and then only run debug with the Maven configuration.    ## FAQ    #### What kind of plugins can I create?    Gephi can be extended in many ways but the major categories are `Layout`, `Export`, `Import`, `Data Laboratory`, `Filter`, `Generator`, `Metric`, `Preview`, `Tool`, `Appearance` and `Clustering`. A good way to start is to look at examples with the [bootcamp](https://github.com/gephi/gephi-plugins-bootcamp).    #### In which language can plugins be created?    Plugins can use any JVM languages (e.g. Scala, Python, Groovy) but the default option is Java.     #### Can native librairies be used?    Yes, native librairies can be used in modules.    #### How is this repository structured?    The `modules` folder is where plugin modules go. Each plugin is defined in a in single folder in this directory. A plugin can be composed of multiple modules (it's called a suite then) but usually one is enough to do what you want.    The `pom.xml` file in `modules` is the parent pom for plugins. A Maven pom can inherit configurations from a parent and that is something we use to keep each plugin's pom very simple. Notice that each plugin's pom (i.e. the `pom.xml` file in the plugin folder) has a `<parent>` defined.    The `pom.xml` file at the root folder makes eveything fit together and notably lists the modules.    #### How are the manifest settings defined?    There are two options. The first option is what the `generate` task does: it puts entries `OpenIDE-Module-Short-Description`, `OpenIDE-Module-Long-Description`, `OpenIDE-Module-Display-Category` and `OpenIDE-Module-Name` into the `src/main/nbm/manifest.mf` file. The second option sets a `  OpenIDE-Module-Localizing-Bundle` entry into the `manifest.mf` so values are defined elsewhere in `Bundle.properties` file. The value is then simply the path to the file (e.g. `OpenIDE-Module-Localizing-Bundle: org/project/Bundle.properties`).    The second option is preferable when the short or long description have too many characters as the manifest format is pretty restrictive.      #### How to add a new module?    This applies for suite plugins with multiple modules. Besides creating the module folder, edit the `pom.xml` file and add the folder path to `<modules>`, like in this example:    ```      <!-- List of modules -->      <modules>          <!-- Add here the paths of all modules (e.g. <module>modules/MyModule</module>) -->          <module>modules/ExampleModule</module>       </modules>  ```    #### Where are dependencies configured?    Dependencies are configured in the `<dependencies>` section in the plugin folder's `pom.xml`. Each dependency has a `groupId`, an `artifactId` and a `version`. There are three types of dependencies a plugin can have: an external library, a Gephi module or a Netbeans module.    The list of Gephi and Netbeans dependencies one can use can be found in the `modules/pom.xml` file. All possible dependencies are listed in the `<dependencyManagement>` section. Because each plugin module inherits from this parent pom the version can be omitted when the dependency is set. For instance, this is how a plugin depends on `GraphAPI` and Netbeans's `Lookup`.    ```  <dependencies>       <dependency>           <groupId>org.netbeans.api</groupId>           <artifactId>org-openide-util-lookup</artifactId>       </dependency>       <dependency>           <groupId>org.gephi</groupId>           <artifactId>graph-api</artifactId>      </dependency>  </dependencies>  ```    #### What are public packages for?    This applies for suite plugins with multiple modules. A module should declare the packages it wants to nake accessible to other modules. For instance, if a module `B` depends on the class `my.org.project.ExampleController` defined in a module `A`, the `A` module should declare `my.org.project` as public package.    Public packages are configured in the module's `pom.xml` file. Edit the `<publicPackages>` entry. Example:    ```  <publicPackages>      <publicPackage>my.org.project</publicPackage>  </publicPackages>  ```    #### What is the difference between plugin and module?    It's the same thing. We say module because Gephi is a modular application and is composed of many independent modules. Plugins also are modules but we call them plugin because they aren't in the _core_ Gephi.    #### When running the plugin in Netbeans I get an error ""Running standalone modules or suites requires...""    This error appears when you try to run a module. To run Gephi with your plugin you need to run the `gephi-plugins` project, not your module. """
Semantic web;https://github.com/sebneu/csvw-parser;"""# pycsvw    Python implementation of the W3C CSV on the Web specification, cf. http://w3c.github.io/csvw/      ## Authors    - Sebastian Neumaier  - Jürgen Umbrich  - Mao Li """
Semantic web;https://github.com/arces-wot/SEPA;"""<div align=""center"">    <a href=""https://github.com/arces-wot/SEPA"">      <img width=""300px"" src=""./doc/logo.png"">    </a>    <br>    <br>    <a href=""https://travis-ci.org/arces-wot/SEPA"">      <img  src=""https://travis-ci.org/arces-wot/SEPA.svg?branch=master"">    </a>    <a href=""https://bintray.com/arces-wot/sepa-java-libs/client-api"">      <img  src=""https://img.shields.io/badge/client%20api-latest-cyan.svg"">    </a>    <a href=""https://github.com/arces-wot/SEPA/releases"">      <img  src=""https://img.shields.io/github/downloads/arces-wot/SEPA/total.svg?colorB=blue"">    </a>    <a href=""https://github.com/arces-wot/SEPA/tree/dev"">      <img  src=""https://img.shields.io/badge/unstable-dev-violet.svg"">    </a>    <a href=""https://gitter.im/sepa_dev/Lobby#"">      <img  src=""https://img.shields.io/badge/chat-on%20gitter-red.svg"">    </a>    <br>     <a href=""https://www.gnu.org/licenses/gpl-3.0"">      <img  src=""https://img.shields.io/badge/License-GPLv3-blue.svg"">    </a>    <a href=""hhttps://www.gnu.org/licenses/lgpl-3.0"">      <img  src=""https://img.shields.io/badge/License-LGPL%20v3-blue.svg"">    </a>      </div>    ## Table of Contents  - [Introduction](#introduction)  - [Demo](#demo)  - [Quick start](#quick-start)  - [Configuration](#configuration)  - [Usage](#usage)  - [Contributing](#contributing)  - [History](#history)  - [Credits](#credits)    ## Introduction  SEPA (**S**PARQL **E**vent **P**rocessing **A**rchitecture) is a publish-subscribe architecture designed to support information level interoperability. The architecture is built on top of generic SPARQL endpoints (conformant with [SPARQL 1.1 protocol](https://www.w3.org/TR/sparql11-protocol/)) where publishers and subscribers use standard **SPARQL 1.1** [Updates](https://www.w3.org/TR/sparql11-update/) and [Queries](https://www.w3.org/TR/sparql11-query/). Notifications about events (i.e., changes in the **RDF** knowledge base) are expressed in terms of added and removed SPARQL binding results since the previous notification. To know more about SEPA architecture and vision please refer to this [paper](https://www.mdpi.com/1999-5903/10/4/36/htm). SEPA proposal has been formalized in the following *unofficial dratfs*:  - [SPARQL Event Processing Architecture (SEPA)](http://mml.arces.unibo.it/TR/sepa.html) contribute [here](https://github.com/arces-wot/SEPADocs/blob/master/sepa.html)  - [SPARQL 1.1 Secure Event Protocol](http://mml.arces.unibo.it/TR/sparql11-se-protocol.html) contribute [here](https://github.com/arces-wot/SEPADocs/blob/master/sparql11-se-protocol.html)  - [SPARQL 1.1 Subscribe Language](http://mml.arces.unibo.it/TR/sparql11-subscribe.html) contribute [here](https://github.com/arces-wot/SEPADocs/blob/master/sparql11-subscribe.html)  - [JSON SPARQL Application Profile (JSAP)](http://mml.arces.unibo.it/TR/jsap.html) contribute [here](https://github.com/arces-wot/SEPADocs/blob/master/jsap.html)    ## Demo    ![Demo showing subscription and notifications](./doc/SEPADemo.gif)    ## Quick start    - Download the [SEPA Engine](https://github.com/arces-wot/SEPA/releases/latest) and run it: `java -jar engine-x.y.z.jar`    - Download [Blazegraph](https://sourceforge.net/projects/bigdata/files/latest/download) (or use any other SPARQL 1.1 Protocol compliant service) and run it as shown [here](https://wiki.blazegraph.com/wiki/index.php/Quick_Start)     - Use the [SEPA Playground](http://mml.arces.unibo.it/apps/dashboard?mode=local) to check basic functionalities of the engine.    ### For Hackers 💻👩‍💻👨‍💻  <a href=""https://asciinema.org/a/251211"">    <img width=""300px"" src=""https://asciinema.org/a/251211.svg"">  </a>    ## Configuration  The SEPA engine can be used with different SPARQL endpoints which must support SPARQL 1.1 protocol. The endpoint can be configured using  a JSON file `endpoint.jpar`. Furthermore, the engine has various parameters that can be used to configure the standard behavior; they  can be set using another JSON file called `engine.jpar`.    In the repository, you will find some versions of `endpoint-{something}.jpar` file. According to your underlying SPARQL endpoint, you have to rename the correct file to `endpoint.jpar`.  The default version of `endpoint.jpar` configures the engine to use use a local running instance of Blazegraph as [SPARQL 1.1 Protocol Service](https://www.w3.org/TR/sparql11-protocol/).    ```json  {  ""host"":""localhost"",  ""sparql11protocol"":{    ""protocol"":""http"",    ""port"":9999,    ""query"":{      ""path"":""/blazegraph/namespace/kb/sparql"",      ""method"":""POST"",      ""format"":""JSON""},    ""update"":{      ""path"":""/blazegraph/namespace/kb/sparql"",      ""method"":""POST"",      ""format"":""JSON""}}}  ```  The default version of  `engine.jpar` configures the engine to listen for incoming [SPARQL 1.1 SE Protocol](http://mml.arces.unibo.it/TR/sparql11-se-protocol/) requests at the following URLs:    1. Query: http://localhost:8000/query  2. Update: http://localhost:8000/update  3. Subscribe/Unsubscribe: ws://localhost:9000/subscribe  4. SECURE Query: https://localhost:8443/secure/query  5. SECURE Update: https://localhost:8443/secure/update  6. SECURE Subscribe/Unsubscribe: wss://localhost:9443/secure/subscribe   7. Regitration: https://localhost:8443/oauth/register  8. Token request: https://localhost:8443/oauth/token  ```json  {""parameters"":{    ""scheduler"":{     ""queueSize"":100,     ""timeout"":5000},    ""processor"":{     ""updateTimeout"":5000,     ""queryTimeout"":5000,     ""maxConcurrentRequests"":5,     ""reliableUpdate"":true},    ""spu"":{""timeout"":5000},    ""gates"":{     ""security"":{      ""tls"":false,      ""enabled"":false,      ""type"":""local""},     ""paths"":{      ""secure"":""/secure"",      ""update"":""/update"",      ""query"":""/query"",      ""subscribe"":""/subscribe"",      ""unsubscribe"":""/unsubscribe"",      ""register"":""/oauth/register"",      ""tokenRequest"":""/oauth/token""},     ""ports"":{      ""http"":8000,      ""https"":8443,      ""ws"":9000,      ""wss"":9443}}}}  ```  ### Logging  SEPA uses [log4j2](http://logging.apache.org/log4j/2.x/) by Apache. A default configuration is stored in the file log4j2.xml provided with the distribution. If the file resides in the engine folder, but it is not used, add the following JVM directive to force using it:    java `-Dlog4j.configurationFile=./log4j2.xml` -jar engine-x.y.z.jar    ### Security  By default, the engine implements a simple in-memory [OAuth 2.0 client-credential flow](https://auth0.com/docs/flows/client-credentials-flow). It uses a JKS for storing the keys and certificates for [SSL](http://docs.oracle.com/cd/E19509-01/820-3503/6nf1il6ek/index.html) and [JWT](https://tools.ietf.org/html/rfc7519) signing/verification. A default `sepa.jks` is provided including a single X.509 certificate (the password for both the store and the key is: `sepa2017`). If you face problems using the provided JKS, please delete the `sepa.jks` file and create a new one as follows: `keytool -genkey -keyalg RSA -alias sepakey -keystore sepa.jks -storepass sepa2017 -validity 360 -keysize 2048`  Run `java -jar engine-x.y.z.jar -help` for a list of options. The Java [Keytool](https://docs.oracle.com/javase/6/docs/technotes/tools/solaris/keytool.html) can be used to create, access and modify a JKS.   SEPA also implements other two security mechanisms:  - LDAP: it extends the default one by storing clients's information into an LDAP server (tested with [Apache Directory](https://directory.apache.org/))  - KEYCLOAK: authentication based on OpenID Connect in managed by [Keycloak](https://www.keycloak.org/)    Security is configured within the `engine.jpar` as follows:  ```json  {""gates"":{    ""security"":{      ""tls"": false,      ""enabled"": true,      ""type"": ""local""  }}}  ```  where   - `type` can assume one of the following values: `local`,`ldap`,`keycloak`  - `tls` is used when `type`=`ldap` to enable or not LDAP StartTLS    ### JMX monitoring  The SEPA engine is also distributed with a default [JMX](http://www.oracle.com/technetwork/articles/java/javamanagement-140525.html) configuration `jmx.properties` (including the `jmxremote.password` and `jmxremote.access` files for password and user grants). Remember to change password file permissions using: `chmod 600 jmxremote.password`. To enable remote JMX, the engine must be run as follows: `java -Dcom.sun.management.config.file=jmx.properties -jar engine-x.y.z.jar`. Using [`jconsole`](http://docs.oracle.com/javase/7/docs/technotes/guides/management/jconsole.html) is possible to monitor and control the most important engine parameters. By default, the port is `5555` and the `root:root` credentials grant full control (read/write).    ### Usage  The SEPA engine can be configured from the command line. Run `java -jar engine-x.y.z.jar -help` for the list of available settings.    `java [JMX] [JVM] [LOG4J] -jar SEPAEngine_X.Y.Z.jar [-help] [-secure=true] [-engine=engine.jpar] [-endpoint=endpoint.jpar] [JKS OPTIONS] [LDAP OPTIONS] [ISQL OPTIONS]`    - `secure` : overwrite the current secure option of engine.jpar  - `engine` : can be used to specify the JSON configuration parameters for the engine (default: engine.jpar)  - `endpoint` : can be used to specify the JSON configuration parameters for the endpoint (default: endpoint.jpar)  - `help` : to print this help    [JMX]  - `Dcom.sun.management.config.file=jmx.properties` : to enable JMX remote managment    [JVM]  - `XX:+UseG1GC`    [LOG4J]  - `Dlog4j.configurationFile=path/to/log4j2.xml`    [JKS OPTIONS]  - `sslstore` <jks> : JKS for SSL CA      			(default: ssl.jks)  - `sslpass` <pwd> : password of the JKS        	(default: sepastore)  - `jwtstore` <jks> : JKS for the JWT key       	(default: jwt.jks)  - `jwtalias` <alias> : alias for the JWT key   	(default: jwt)  - `jwtstorepass` <pwd> : password for the JKS  	(default: sepakey)  - `jwtaliaspass` <pwd> : password for the JWT key  (default: sepakey)  		  [LDAP OPTIONS]  - `ldaphost` <name> : host     		         (default: localhost)  - `ldapport` <port> : port                      (default: 10389)  - `ldapdn` <dn> : domain                        (default: dc=sepatest,dc=com)  - `ldapusersdn` <dn> : domain                   (default: null)  - `ldapuser` <usr> : username                   (default: null)  - `ldappwd` <pwd> : password                    (default: null)  		  [ISQL OPTIONS]  - `isqlpath` <path> : location of isql     		 (default: /usr/local/virtuoso-opensource/bin/)  - `isqlhost` <host> : host of Virtuoso     		 (default: localhost)  - `isqluser` <user> : user of Virtuoso     		 (default: dba)  - `isqlpass` <pass> : password of Virtuoso     	 (default: dba)    ## Contributing  You are very welcome to be part of SEPA community. If you find any bug feel free to open an issue here on GitHub, but also feel free to  ask any question. For more details check [Contributing guidelines](CONTRIBUTING.md). Besides, if you want to help the SEPA development follow this simple steps:    1. Fork it!  2. Create your feature branch: `git checkout -b my-new-feature`  3. Check some IDE specific instruction below  4. Do your stuff  5. Provide tests for your features if applicable  5. Commit your changes: `git commit -am 'Add some feature'`  6. Push to the branch: `git push origin my-new-feature`  7. Submit a pull request :D    Pull request with unit tests have an higher likelihood to be accepted, but we are not to restrictive. So do not be afraid to send your contribution!    ### Clone in Eclipse  There is no particular restriction in your IDE choice. Here we provide a short guide to import the GitHub cloned project inside Eclipse. Any   other IDEs work fine.     1. Open Eclipse  2. File > Import > Maven  3. Choose ""Check out Maven Projects from SCM""  4. In the field SCM URL choose 'git' and add the clone address from Github. If 'git' is not found, tap into ""Find more SCM connectors in the m2e Marketplace""  5. go on...  The project is cloned. Enjoy!    ### Build with Maven  SEPA engine is a Maven project composed by two sub-projects:  - Client-api  - Engine    As first, you need to build client-api skipping JUnit tests:  ```bash  mvn install -DskipTests  ```  In fact, clien-api JUnit tests include integration tests that require a SEPA engine running    Then you can build the engine with this command:  ```bash  mvn install  ```  That create an executable inside the target directory. To know more about Maven please refer to the [official documentation](https://maven.apache.org/).    ## History    SEPA has been inspired and influenced by [Smart-M3](https://sourceforge.net/projects/smart-m3/). SEPA authors have been involved in the development of Smart-M3 since its [origin](https://artemis-ia.eu/project/4-sofia.html).     The main differences beetween SEPA and Smart-M3 are the protocol (now compliant with the [SPARQL 1.1 Protocol](https://www.w3.org/TR/sparql11-protocol/)) and the introduction of a security layer (based on TLS and JSON Web Token for client authentication).     All the SEPA software components have been implemented from scratch.    ## Credits    SEPA stands for *SPARQL Event Processing Architecture*. SEPA is promoted and maintained by the [**Dynamic linked data and Web of Things Research Group**](https://site.unibo.it/wot/en) @ [**ARCES**](http://www.arces.unibo.it), the *Advanced Research Center on Electronic Systems ""Ercole De Castro""* of the [**University of Bologna**](http://www.unibo.it).    ## License    SEPA Engine is released under the [GNU GPL](https://github.com/arces-wot/SEPA/blob/master/engine/LICENSE), SEPA APIs are released under the  [GNU LGPL](https://github.com/arces-wot/SEPA/blob/master/client-api/LICENSE)"""
Semantic web;https://github.com/AKSW/SemanticPingback;"""# Semantic Pingback Vocabulary    This small vocabulary defines resources which are used in the context  of Semantic Pingback. The Semantic Pingback mechanism is an extension  of the well-known Pingback method, a technological cornerstone of the  blogosphere, thus supporting the interlinking within the Data Web.    More information about Semantic Pingback are available at:  * http://aksw.org/Projects/SemanticPingback  * https://aksw.github.io/SemanticPingback/    Semantic Pingback also used in the architecture of the  [Distributed Semantic Social Network (DSSN)](http://aksw.org/Projects/DSSN)  and in the [Structured Feedback](http://feedback.aksw.org/) protocol. """
Semantic web;https://github.com/AKSW/ShacShifter;"""# The ShacShifter    [![Travis CI Build Status](https://travis-ci.org/AKSW/ShacShifter.svg)](https://travis-ci.org/AKSW/ShacShifter/)  [![Coverage Status](https://coveralls.io/repos/github/AKSW/ShacShifter/badge.svg?branch=master)](https://coveralls.io/github/AKSW/ShacShifter?branch=master)    The *ShacShifter* is a shape shifter for the [*Shapes Constraint Language (SHACL)*](https://www.w3.org/TR/shacl/) to various other format.  Currently our focus is on convertig a SHACL NodeShape to an [RDForms template](http://rdforms.org/#!templateReference.md).    ## Installation and Usage    You have to install the python dependencies with `pip install -r requirements.txt`.    To run start with:        $ bin/ShacShifter --help      usage: ShacShifter [-h] [-s SHACL] [-o OUTPUT] [-f {rdforms,wisski,html}]        optional arguments:        -h, --help            show this help message and exit        -s SHACL, --shacl SHACL                              The input SHACL file        -o OUTPUT, --output OUTPUT                              The output file        -f {rdforms,wisski,html}, --format {rdforms,wisski,html}                              The output format """
Semantic web;https://github.com/peta/turtle.tmbundle;"""# turtle.tmbundle  ---------------------------------------------------------------------    Totally awesome bundle for Turtle – the terse RDF triple language.    It consists of:    + Language grammar for Turtle and SPARQL 1.1  + Powerful auto-completion (live-aggregated)  + Documentation for classes and roles/properties at your fingertips (live-aggregated)  + Interactive SPARQL query scratchpad  + Some snippets (prefixes and document skeleton)  + Solid syntax validation  + Commands for instant graph visualization of a knowledge base (requires Graphviz and Raptor)  + Automatic removal of unused prefixes  + Conversion between all common RDF formats    See [Screenshots](#screenshots)    __NOTE: If the HTML output of this GitHub-flavored Markdown document looks broken, please visit [https://github.com/peta/turtle.tmbundle/blob/master/README.md] to read the original version__    ## Language grammar     The language grammar now covers the official W3C parser spec (as proposed in the latest CR released on Feb 19th 2013). However, there are still one/two particularities that differ, but you shouldn't notice them during your daily work. In the case you notice some weird behaviour (most obvious sign: broken syntax highlighting), please file a bug in the [project's issue tracker](https://github.com/peta/turtle.tmbundle/issues ""Here at GitHub"").    The language grammar also recognizes keywords and builtin functions from the latest SPAR[QU]L 1.1 language specification. Further there is basic autocompletion (`⌥ + ⎋`) for the aforementioned.    ## Powerful auto-completion    The Turtle bundle offers auto-completion at two levels:    __NOTE: *When determining IRIs associated with a given prefix name, local prefix declarations always have precedence over those given by prefix.cc. So when you mess up IRIs in your @prefix directives, auto-completion might not work as expected.*__    ### Auto-completion for prefix directives    When you invoke the `Autocomplete` command (`⌥ + ⎋`) within the scope of a prefix directive (right after `@prefix` or `PREFIX`), the Turtle bundle fetches a list of all prefixes registered at [prefix.cc](http://prefix.cc) and displays them nicely in a auto-complete dropdown box. Once you have chosen an and confirmed your selection, the prefix directive is automagically updated with the prefix and its according URI. (Note: the fetched data is locally cached for 24h)    __NOTE: *Auto-completion for prefix declarations is case-insensitive*__    ### Auto-completion for the local part of prefixed name IRIs    When you invoke the `Autocomplete` command (`⌥ + ⎋`) within the scope of a prefixed name (e.g. right after `my:` or at `my:a...`), the Turtle bundle determines the actual URI that is abbreviated by the prefixed namespace and checks if there is a machine readable Vocabulary/Ontology document available (currently only RDF/S and OWL documents in the XML serialization format are supported -- but with known issues). When one is found, it is live-aggregated and all of its Classes and Roles/Properties are extracted (along with their documentation) and nicely presented in a auto-complete dropdown box. (Note: the fetched data is locally cached for 24h)    __NOTE: *Auto-completion for prefixed names is case-sensitive*__    ### Known issues    For now, the Turtle bundle relies on [prefix.cc](http://prefix.cc) for mapping prefixes to URIs (required for all live-aggregations). The problem however is, that the available listings contain only one IRI per prefix (the one with the highest ranking) and not every IRI offers a machine readable vocabulary/ontology representation, what in turn means that for certain prefixes no auto-completion data might be available. You can help to fix this, by visiting the according page at prefix.cc (URL scheme looks like `http://prefix.cc/<THE_PREFIX>`; without angle brackets ofc) and up/downvoting the according URIs.    The automatic aggregation of machine-readable vocabulary/ontology descriptions is working in principle but still has some shortcomings. (See the Github issue tracker) I will overwork that part when I have some more spare time and/or the need just arises. When you're told that data for a given prefix was fetched (green tooltip after a few seconds of freeze) but you will see no autocompletion dropdown later on, it probably means that the aggregator script failed and/or the IRI could not redirect to resource in a compatible format.    ## Documentation for classes, roles/properties and individuals    When you invoke the `Documentation for Resource` command (`⌃ + H`) within the scope of a prefixed name IRI (e.g. `my:Dog`), the Turtle bundle looks up if there are any informal descriptions available (like description texts, HTTP URLs to human-readable docs, asf.) and if so, displays them to the user. (Note: the fetched data is locally cached for 24h)    __NOTE: *That function also suffers from the issues outlined in the previous section.*__    ## Interactive SPARQL query scratchpad ##    All (web-based) query forms that crossed by cursor had one thing in common – they suck(ed). Syntax highlighting? Efficient workflows (like trial-error-roundtripping)? Of course NOT. So I decided to add something similar to TextMate. The *Execute SPARQL Query* command may seem self-explaining, but it hides some important features:    + first of all, it supports multiple sections in a single document  + it is aware of custom magic comments types (called ""markers"") for   	+ specifying different SPARQL Query/Update services (aka *endpoints*)  	+ including/calling other section from within the same document  + query results are nicely displayed together with the query log in a preview window    Just press `⌘ + R` and your query will be executed.    ### Multiple sections/snippets syntax    You can have one document contain multiple (independent) sections. This is e.g. useful when developing complex queries where we usually follow an iterative approach. When running the `Execute SPARQL Query` command, it will automatically figure out what your intention is and act accordingly. It assumes that you will always do one of the following tasks:    1. Execute the whole document  2. Execute your current selection  3. Execute the section where your cursor is currently positioned in     A simple document with multiple sections could look like this:    ```  #QUERY  <http://example1.com/ds/query>  #UPDATE <http://example1.com/ds/update>    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>  PREFIX : <urn:example#>    INSERT DATA { :instanceA rdfs:label 'Human-readable label of instanceA'. }    #---  PREFIX : <urn:example#>  SELECT (COUNT(?subj) AS ?n_subjs)  WHERE { ?subj a :ClassA }    #---  PREFIX : <urn:example#>  SELECT ?g  WHERE { GRAPH ?g {} }  ```    As you probably notice, multiple sections are separated with the marker string `#---` written on a separate line.    ###  Magic comment syntax for endpoints    These magic comments have the following syntax:    + `#QUERY <http://example.org/sparql` to describe a SPARQL endpoint (read-only)  + `#UPDATE <http://example.org/update` to describe a SPARUL endpoint (write-only)    You can specify multiple magic comments throughout your document. When executing the query command, it will automatically determine the required endpoint type depending on the SPARQL commands that occur in your query. After that, it scans your document for magic comments where it uses the following strategies:    + _Top-down:_ When the whole document is used as query, it scans every line beginning at the top until it finds a suitable magic comment   + _Bottom-up:_ When only the current/selected section is queried, it first checks if that section contains the required endpoints, and, when not the case, it then scans every line from bottom to top beginning at the line where the section/selection starts until it finds a suitable magic comment    When no suitable magic comment was found, the command consults TextMate's environment variables (can be set in the application preferences). More precisely it looks for the following two variables:     + `TM_SPARQL_QUERY`  + `TM_SPARQL_UPDATE`    that should contain the HTTP(S) URL to SPAR[QU]L endpoints. When even this fails, YOU (the user) are prompted to enter an endpoint URL. If you cancel, the execution is aborted and a puppy dies.    Here is an example document with multiple sections and multiple magic comments for endpoints:    ```  #QUERY  <http://example.com/ds/query>                   #UPDATE <http://example.com/tbox/update>  PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>  \  PREFIX : <urn:example#>                               |  INSERT DATA {                                         | http://example.com/tbox/update      :ClassB a rdfs:Class;                             |          rdfs:subClassOf :ClassA.                      |  }                                                     /    #---  #UPDATE <http://example.com/abox/update>  PREFIX : <urn:example#>                               \  INSERT DATA {                                         |      :instA a :ClassB;                                 | http://example.com/abox/update          rdfs:label 'Instance of ClassB'.              |  }                                                     /    #---  PREFIX : <urn:example#>                               \  SELECT (COUNT(?subj) AS ?n_subjs)                     | http://example.com/ds/query  WHERE { ?subj a :ClassA }                             /    #---  #QUERY <http://dbpedia.org/sparql>                      SELECT DISTINCT ?s ?label                             \  WHERE {                                               | http://dbpedia.org/sparql      ?s <http://dbpedia.org/property/season> ?o .      |      ?s rdfs:label ?label                              |  }                                                     /    #---  BASE <http://dbpedia.org/>                            \  SELECT DISTINCT ?s ?label                             |  WHERE {                                               | http://dbpedia.org/sparql      ?s rdfs:label ?label                              |      FILTER( CONTAINS( STR(?label), 'The Wire') )      |  }                                                     /  ```    ### Additional magic comment types    #### Name your sections    You can create a named section by adding a `#name= my-section` marker. Named sections can be included from other sections within your document. For more details see the next section:       #### #INCLUDE <section-name>    By using Includes, you can reuse named sections all over the place. This way, it is possible to write typically used snippets once, and then just ""call"" them from anywhere else when needed. This is especially helpful for SPARQL beginners or when iteratively developing complex queries where one wants to see the effect of a previous query immediately.    When executed, every included snippet is listed separately in the web preview window of TextMate. That way it is easy to track down issues with intermediate states (and not just only after all queries have finished).    The order in which sections appear and are included in your document doesn't matter. So an included section doesn't have to be defined before the section it is included from. The following example shows a simple use-case:    ```  #INCLUDE <drop-all>  #INCLUDE <insert-some-bnode>  #INCLUDE <select-all-bnodes>  DELETE { ?s ?p ?o }  WHERE {   	[] ?p ?o .   	?s <urn:example#label> ?o .   	?s ?p 'bnode 2' .   	FILTER( isBLANK( ?s ) )  }  #INCLUDE <select-all-bnodes>    #---  #name= insert-some-bnode  PREFIX : <urn:example#>  INSERT DATA {  	[ a :MyClass ] :label 'bnode 1' .  	[ a :MyClass ] :label 'bnode 2' .  	[ a :MyClass ] :label 'bnode 3' .  }    #---  #name= drop-all  #descr= Drop all graphs – both, the default graph and all named ones  DROP ALL    #---  #name= select-all-bnodes  #descr= Selects all bnodes in the default graph  SELECT DISTINCT *  WHERE {  	?s ?p ?o   	FILTER isBLANK(?s)  }  ```    ![Screenshot of SPARQL result preview window](./Support/img/screenshot-sparql.png ""Screenshot of SPARQL result preview window"")    #### Notes    There is also basic support for simple metadata properties in the following form: `#propname= some prop value`. It's the SPARQL pendant to jsdoc or phpdoc comments. However, beside section names an description texts I have no idea how this could be used. Do you?    I have the vague idea of a pastebin for SPARQL in mind, where one can host reusable SPARQL snippets with support for parametrized SPARQL calls. What do you think about it?        ## Snippets    Right now the following snippets are included:    + Basic document skeleton  + ""Smart"" prefix/base directives (hit tab to see it work)  + A set of basic prefix directives (Boring! The cool kids instead use the fancy auto-completion)    ## Syntax validation    You can trigger a syntax validation of your Turtle by pressing `CTRL + SHIFT + V`. In order to make use of syntax validation you must a have a working installation of the [Raptor RDF syntax library](http://librdf.org/raptor/). For detailed instructions about wiring up Raptor with Textmate, see the [#graph-visualization](section below).    ![Screenshot of syntax validation error message](./Support/img/screenshot-syntaxval-error.png ""Screenshot of syntax validation error message"")    ![Screenshot of syntax validation success message](./Support/img/screenshot-syntaxval-success.png ""Screenshot of syntax validation success message"")    ## Graph visualization    In order to use this functionality you need a working installation of [Graphviz](http://graphviz.org) (especially the dot command) and the [Raptor RDF syntax library](http://librdf.org/raptor/). When properly installed (locatable through PATHs) everything should work fine ootb. However, in some cases you must explicitly tell Textmate where to find them. You can do this by introducing two configuration variables (Textmate -> Preferences -> Variables):    + `TM_DOT` absolute path to the dot binary (part of Graphviz)    + `TM_RAPPER` absoluter path to the rapper binary (part of Raptor)    By hitting `CMD + R` the active TTL document will be visualized on-the-fly in a Textmate HTML preview window. Because these preview windows are driven by Safari's WebKit rendering engine, PDF documents will be rendered right in-line. That way your ""edit knowledge base --> visualize"" workflow will be super intuitive and wont get interrupted by switching to separate PDF viewer app for viewing the visualization.    By hitting `SHIFT + ALT + CMD + S` the active TTL document will be visualized and saved to a PDF document.    ## Conversion between all common RDF formats    In order to make use of the converter functionality, you must need a working installation of the [Raptor RDF syntax library](http://librdf.org/raptor/). For detailed instructions about wiring up Raptor with Textmate, see the [#graph-visualization](section above).    ![Screenshot showing list of available target formats the user may choose from](./Support/img/screenshot-converter.png ""Screenshot showing list of available target formats the user may choose from"")    ## Installation    The Turtle bundle is now officially available through the Textate bundle installer (_Textmate -> Preferences -> Bundles_). However, it usually takes a few days until new releases are available through the bundle installer (make sure that you enabled 'Keep bundles updated' in the application preferences). If you know what you do, you can also install bundles (like Turtle) by hand. Just download/clone this repository, and place its root directory at `~/Library/Application Support/Avian/Bundles/Turtle.tmbundle`. That way it's kept distinct from bundles installed through the bundle installer. Textmate should notice the new bundle automatically; but when in doubt, just restart Textmate (`⌃ + ⌘ + Q`).     ## Screenshots    ![Screenshot of expanded bundle menu](./Support/img/screenshot-menu.png ""Screenshot of expanded bundle menu"")    ![Screenshot showing SPARQL execution](./Support/img/screenshot-sparql-exec.png ""Screenshot showing SPARQL execution"")    ![Screenshot editor showing auto-completion for resource identifier and documentation](./Support/img/screenshot-editor.png ""Screenshot editor showing auto-completion for resource identifier and documentation"")    ![Screenshot of knowledge base visualization](./Support/img/screenshot-visu.png ""Screenshot of knowledge base visualization"")    ## Meta    Turtle.tmbundle was created by [Peter Geil](http://github.com/peta). Feedback is highly welcome – if you find a bug, have a feature request or simply want to contribute something, please let me know. Just visit the official GitHub repository at [https://github.com/peta/turtle.tmbundle](https://github.com/peta/turtle.tmbundle) and open an [issue](https://github.com/peta/turtle.tmbundle/issues).    ### Please help making TextMate2 even more awesome!    One of the features EVERY user could greatly benefit from, is a more powerful auto-completion feature. However, the implementation of such a feature takes a considerable amount of time. Unfortunately time is one of those goods, Allan (the creator of TextMate) and the other guy(s) from Macromates don't have enough from. So I had to idea to start a crowdfunding campaign to raising enough funds for working two months as full-time contributor to the TextMate 2 project. [Visit my campaign page and contribute!](http://www.indiegogo.com/projects/textmate-dialog2-sprint).    ## Roadmap    + Extract individuals (for both, autocompletion and documentation)  + Work out documentation component  	+ Display resource documentation as HTML text (with clickable links to official sources) in a notification window  + Polish language grammar  + Add additional caching layer for speeding up things (vanilla MacOS Ruby 1.8.7 has only sloooow REXML module)  + Convert RDF/S and OWL documents from XML into Turtle and ""link"" resource identifiers to them, so that users can jump/navigate across all involved documents  + To be fixed  	+ Fix PN_LOCAL pattern so that semicolons inside POLs are marked up as terminators  + Rewrite Turtle.tmBundle as nodejs module and publish as plugin for web-based code editors like Cloud9 and Atom """
Semantic web;https://github.com/usc-isi-i2/Web-Karma;"""Karma: A Data Integration Tool  ================================    ![travis ci](https://travis-ci.org/usc-isi-i2/Web-Karma.svg?branch=master)    The Karma tutorial at https://github.com/szeke/karma-tcdl-tutorial, also check out our [DIG web site](http://usc-isi-i2.github.io/dig/), where we use Karma extensively to process > 90M web pages.    See our [release stats](http://www.somsubhra.com/github-release-stats/?username=usc-isi-i2&repository=Web-Karma)      ## What is Karma?    Karma is an information integration tool that enables users to quickly and easily integrate data from a variety of data sources including databases, spreadsheets, delimited text files, XML, JSON, KML and Web APIs. Users integrate information by modeling it according to an ontology of their choice using a graphical user interface that automates much of the process. Karma learns to recognize the mapping of data to ontology classes and then uses the ontology to propose a model that ties together these classes. Users then interact with the system to adjust the automatically generated model. During this process, users can transform the data as needed to normalize data expressed in different formats and to restructure it. Once the model is complete, users can published the integrated data as RDF or store it in a database.    You can find useful tutorials on the project Website: [http://www.isi.edu/integration/karma/](http://www.isi.edu/integration/karma/)    ## Installation and Setup ##    Look in the Wiki [Installation](https://github.com/InformationIntegrationGroup/Web-Karma/wiki/Installation)    ## Frequently Asked Questions ##  ### How to perform offline RDF generation for a data source using a published model? ###  1. Model your source and publish it's model (the published models are located at `src/main/webapp/publish/R2RML/` inside the Karma directory).  2. To generate RDF of a CSV/JSON/XML file, go to the top level Karma directory and run the following command from terminal:  ```  cd karma-offline  mvn exec:java -Dexec.mainClass=""edu.isi.karma.rdf.OfflineRdfGenerator"" -Dexec.args=""--sourcetype   <sourcetype> --filepath <filepath> --modelfilepath <modelfilepath> --sourcename <sourcename> --outputfile <outputfile> --JSONOutputFile<outputJSON-LD>"" -Dexec.classpathScope=compile  ```    	Valid argument values for sourcetype are: CSV, JSON, XML. Also, you need to escape the double quotes that go inside argument values. Example invocation for a JSON file:  ```	  mvn exec:java -Dexec.mainClass=""edu.isi.karma.rdf.OfflineRdfGenerator"" -Dexec.args=""  --sourcetype JSON   --filepath \""/Users/shubhamgupta/Documents/wikipedia.json\""   --modelfilepath \""/Users/shubhamgupta/Documents/model-wikipedia.n3\""  --sourcename wikipedia  --outputfile wikipedia-rdf.n3  --JSONOutputFile wikipedia-rdf.json"" -Dexec.classpathScope=compile  ```  3. To generate RDF of a database table, go to the top level Karma directory and run the following command from terminal:  ```  cd karma-offline  mvn exec:java -Dexec.mainClass=""edu.isi.karma.rdf.OfflineRdfGenerator"" -Dexec.args=""--sourcetype DB  --modelfilepath <modelfilepath> --outputfile <outputfile> --dbtype <dbtype> --hostname <hostname>   --username <username> --password <password> --portnumber <portnumber> --dbname <dbname> --tablename <tablename> --JSONOutputFile<outputJSON-LD>"" -Dexec.classpathScope=compile  ```  	Valid argument values for `dbtype` are Oracle, MySQL, SQLServer, PostGIS, Sybase. Example invocation:  ```  mvn exec:java -Dexec.mainClass=""edu.isi.karma.rdf.OfflineRdfGenerator"" -Dexec.args=""  --sourcetype DB --dbtype SQLServer   --hostname example.com --username root --password secret   --portnumber 1433 --dbname Employees --tablename Person   --modelfilepath \""/Users/shubhamgupta/Documents/db-r2rml-model.ttl\""  --outputfile db-rdf.n3  --JSONOutputFile db-rdf.json"" -Dexec.classpathScope=compile  ```    You can do `mvn exec:java -Dexec.mainClass=""edu.isi.karma.rdf.OfflineRdfGenerator"" -Dexec.args=""--help""` to get information about required arguments.    ### How to set up password protection for accessing Karma? ###  - in /src/main/config/jettyrealm.properties change user/password (if you wish)  - in /src/main/webapp/WEB-INF/web.xml uncomment security section at the end of the file  - in pom.xml uncomment security section (search for loginServices)    ### Are there additional steps required to import data from Oracle database? ###  Yes. Due to Oracles binary license issues, we can't distribute the JAR file that is required for importing data from an Oracle database. Following are the steps to resolve the runtime error that you will get if you try to do it with the current source code:    1. Download the appropriate JDBC drive JAR file (for JDK 1.5 and above) that matches your Oracle DB version. Link: http://www.oracle.com/technetwork/database/features/jdbc/index-091264.html  2. Put the downloaded JAR file inside `lib` folder of the Karma source code.   3. Add the following snippet in the `pom.xml` file (present inside the top level folder of Karma source code) inside the dependencies XML element:     ```  <dependency>       <groupId>com.oracle</groupId>       <artifactId>ojdbc</artifactId>       <version>14</version>       <scope>system</scope>       <systemPath>/Users/karma/Web-Karma/lib/ojdbc14.jar</systemPath>   </dependency>   ```  Make sure that the filename mentioned in the `systemPath` element matches with your downloaded JAR file; it is likely that your installation folder is different from `/Users/karma` so make sure you use the correct one.    ### Are there additional steps required to import data from MySQL database? ###  Yes. Due to MySQL binary license issues, we can't distribute the JAR file that is required for importing data from an MySQL database. Following are the steps to resolve the runtime error that you will get if you try to do it with the current source code:    1. Download the appropriate MySQL driver JAR file (for JDK 1.5 and above) that matches your MySQL version. Link: http://dev.mysql.com/downloads/connector/j/  2. Put the downloaded JAR file inside `lib` folder of the Karma source code.   3. Add the following snippet in the `pom.xml` file of the `karma-jdbc` project inside the dependencies XML element:     ```  <dependency>       <groupId>mysql</groupId>       <artifactId>mysql-connector-java</artifactId>       <version>5.1.32</version>       <scope>system</scope>       <systemPath>/Users/karma/Web-Karma/lib/mysql-connector-java-5.1.32-bin.jar</systemPath>   </dependency>   ```  Make sure that the filename mentioned in the `systemPath` element matches with your downloaded JAR file; it is likely that your installation folder is different from `/Users/karma` so make sure you use the correct one. The `version` will be the version of the JAR that you downloaded.     """
Semantic web;https://github.com/sparql-generate/sparql-generate;"""# SPARQL-Generate    This project contains the sources of the implementations of SPARQL-Generate and STTL (a.k.a. SPARQL-Template) over Apache Jena.  """
Semantic web;https://github.com/rollxx/antlr-sparql-grammar;"""SPARQL grammars  ================    This project includes parser and lexer grammars in ANTLR-specific syntax for the following SPARQL specifications:    - SPARQL 1.1 update and query   - SPARQL 1.0   - OpenLink's Virtuoso-specific SPARQL Parser grammar """
Semantic web;https://github.com/kasei/URITemplate;"""URITemplate  ===========    Swift implementation of URI Template ([RFC6570](https://tools.ietf.org/html/rfc6570)).    ## Installation    [CocoaPods](http://cocoapods.org/) is the recommended installation method.    ```ruby  pod 'URITemplate'  ```    ## Example    ### Expanding a URI Template    ```swift  let template = URITemplate(template: ""https://api.github.com/repos/{owner}/{repo}/"")  let url = template.expand([""owner"": ""kylef"", ""repo"": ""URITemplate.swift""])  => ""https://api.github.com/repos/kylef/URITemplate.swift/""  ```    ### Determine which variables are in a template    ```swift  let variables = template.variables  => [""owner"", ""repo""]  ```    ### Extract the variables used in a given URL    ```swift  let variables = template.extract(""https://api.github.com/repos/kylef/PathKit/"")  => [""owner"":""kylef"", ""repo"":""PathKit""]  ```    ## [RFC6570](https://tools.ietf.org/html/rfc6570)    The URITemplate library follows the [test suite](https://github.com/uri-templates/uritemplate-test).    We have full support for level 4 of RFC6570 when expanding a template and retrieving the variables in a template.    For extraction of variables from an already expanded template, level 3 is supported.    ## License    URITemplate is licensed under the MIT license. See [LICENSE](LICENSE) for more  info. """
Semantic web;https://github.com/D2KLab/sparql-transformer;"""SPARQL Transformer  ===================    Write your SPARQL query directly in the JSON-LD you would like to have in output.    JavaScript package. Try it with the [Playground](https://d2klab.github.io/sparql-transformer/).    > Looking for the [Python one](https://github.com/D2KLab/py-sparql-transformer)?    ## News    - The parameter `$libraryMode` allows to perform the pagination on the merged objects, obtaining exactly `n=$limit` objects  - It is now possible to set a different **merging anchor** instead of `id`/`@id` using the `$anchor` modifier.    **Table of Contents**    - [Motivation](./motivation.md)  - [Query in JSON](#query-in-json)  - [How to use](#how-to-use)  - [Credits](#credits)    You want to learn more? Watch this [Tutorial](https://d2klab.github.io/swapi2020/slides.html)    ## Query in JSON    The core idea of this module is writing in a single file the query and the expected output in JSON.    Two syntaxes are supported: plain JSON and JSON-LD.  Here the examples in the 2 formats for the query of cities.    - plain JSON    ```json  {    ""proto"": [{      ""id"" : ""?id"",      ""name"": ""$rdfs:label$required"",      ""image"": ""$foaf:depiction$required""    }],    ""$where"": [      ""?id a dbo:City"",      ""?id dbo:country dbr:Italy""    ],    ""$limit"": 100  }  ```    - JSON-LD    ```json  {    ""@context"": ""http://schema.org/"",    ""@graph"": [{      ""@type"": ""City"",      ""@id"" : ""?id"",      ""name"": ""$rdfs:label$required"",      ""image"": ""$foaf:depiction$required""    }],    ""$where"": [      ""?id a dbo:City"",      ""?id dbo:country dbr:Italy""    ],    ""$limit"": 100  }  ```    The syntax is composed by two main parts.    ### The prototype    The `@graph`/`proto` property contains the prototype of the result as I expect it. When the value should be taken from the query result, I declare it using the following syntax:        $<SPARQL PREDICATE>[$modifier[:option...]...]    The subject of the predicate is the variable (declared of automatically assigned) of the closer **mergin anchor** in the structure, which is the `@id`/`id` property (if it exists, otherwise is the default `?id`).  The SPARQL variable name is manually (with the `$var` modifier) or automatically assigned.    Some modifiers can be present after, separated by the `$` sign. The `:` prepend the options for a given modifier.    |MODIFIER|OPTIONS|NOTE|  |---|---|---|  |`$required`|n/a| When omitted, the clause is wrapped by `OPTIONAL { ... }`.|  |`$sample`|n/a|Extract a single value for that property by adding a `SAMPLE(?v)` in the SELECT|  |`$lang`|`:lang`[string, optional]| FILTER by language. In absence of a language, pick the first value of `$lang` in the root.<br>Ex. `$lang:it`, `$lang:en`, `$lang`. |  |`$bestlang`|`:acceptedLangs`[string, optional]| Choose the best match (using `BEST_LANGMATCH`) over the languages according to the list expressed through the [Accept-Language standard](https://tools.ietf.org/html/rfc7231#section-5.3.5). This list can be appended after the `:` or expressed as `$lang` in the root.<br>Ex. `$bestlang`, `$bestlang:en;q=1, it;q=0.7 *;q=0.1`|  |`$var`|`:var`[string]| Specify the variable that will be assigned in the query, so that it can be referred in the root properties (like `$filter`). If missing, a `?` is prepended. <br> Ex. `$var:myVariable`, `$var:?name`|  |`$anchor`|n/a|Set this property as merging anchor. The set is valid for the current level in the JSON tree, ignoring eventual `id`/`@id` sibling properties. Ex. `""a"":""?example$anchor""` sets`?example` as subject of SPARQL statements and merges the final results on the `a` property.|  |`$reverse`|n/a|Set this property for use the current variable as subject of the SPARQL predicate, rather than object.|  |`$count` `$sum` `$min` `$max` `$avg`| n/a | Return the respective aggregate function (COUNT, SUM, MIN, MAX, AVG) on the variable. |  |`$langTag`|`""hide""`, `""show""` (default)| When `hide`, language tags are not included in the output.<br> Ex. `hide` => `""label"":""Bologna""` ;<br>  `show` => `""label"":{""value"": ""Bologna"", ""language"": ""it""}` |  |`$accept`|`""string""`, `""number""`, `""boolean""`| If set, values of type different from the specified one are discarded. |  |`$asList`|n/a| When set, the interested property value would always be a list, even if with a single element. |      In this way, I specify a mapping between the JSON-LD output properties and the ones in the endpoint. The values non prepended by a `$` are transferred as is to the output.    ### The root `$` properties    The `$`-something root properties allow to make the query more specific. They will be not present in the output, being used only at query level.  The supported properties are:    |PROPERTY|INPUT|NOTE|  |--------|-----|----|  |`$where`|string, array| Add where clause in the triple format.<br>Ex. `""$where"": ""?id a dbo:City""`|  |`$values`|object| Set `VALUES` for specified variables as a map. The presence of a lang tag or of the '$lang' attribute attached to the related property is taken in account. <br>Ex. `""$values"": {""?id"": [""dbr:Bari"", ""http://dbpedia.org/resource/Bologna""]}`|  |`$limit` |number| `LIMIT` the SPARQL results |  |`$limitMode` |`query` (default) or `library`| Perform the LIMIT operation in the query or on the obtained results (`library`) |  |`$from` |string(uri)| Define the graph `FROM` which selecting the results |  |`$offset` |number| `OFFSET` applied to the SPARQL results |  |`$distinct`|boolean (default `true`)| Set the `DISTINCT` in the select|  |`$orderby`|string, array| Build an `ORDER BY` on the variables in the input.<br> Ex. `""$orderby"":[""DESC(?name)"",""?age""]`|  |`$groupby`| string, array | Build an `GROUP BY` on the variables in the input. <br> Ex. `""$groupby"":""?id""`|  |`$having`| string, array | Allows to declare the content of `HAVING`. If it is an array, the items are concatenated by `&&`. |  |`$filter`| string, array |Add the content as a `FILTER`.<br>`""$filter"": ""?myNum > 3""`|  |`$prefixes`| object | set the prefixes in the format `""foaf"": ""http://xmlns.com/foaf/0.1/""`.|  |`$lang`|`:acceptedLangs`[string]| The default language to use as `$bestlang` (see above), expressed through the [Accept-Language standard](https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.4). <br>Ex. `$lang:en;q=1, it;q=0.7 *;q=0.1`|  |`$langTag`|`""hide""`, `""show""` (default)| When `hide`, language tags are not included in the output. Similar to the inline `$langTag`, but acting at a global level.<br> Ex. `hide` => `""label"":""Bologna""` ;<br>  `show` => `""label"":{""value"": ""Bologna"", ""language"": ""it""}` |    The `@context` property (for the JSON-LD version) will be transferred to the output.    The output of this query is intended to be:  - for the plain JSON, an array of object with the shape of the prototype;  - for the JSON-LD, an array of object with the shape of the prototype in the `@graph` property and with a sibling `@context`.    ## How to use    #### Install in nodeJS  Install by npm.    ```bash  npm install sparql-transformer  ```      Add to the application.    ```js  import sparqlTransformer from 'sparql-transformer';  ```    #### Install in the browser    SPARQL Transformer is exposed as [ES Module](https://jakearchibald.com/2017/es-modules-in-browsers/). We rely on [getlibs](https://www.npmjs.com/package/getlibs) until the technology will allow to use [""bare"" import specifier](https://github.com/WICG/import-maps#bare-specifiers).    ```html  <script src=""https://unpkg.com/getlibs""></script>  <script>sparqlTransformer = System.import('https://unpkg.com/sparql-transformer')</script>  ```    #### Use  ```js  sparqlTransformer(query, options)    .then(res => console.log(res))    .catch(err => console.error(err););    ```    The first parameter (`query`) is the query in the JSON-LD format. The JSON-LD can be:  - an already parsed JS object (or defined real time),  - **ONLY if running in NodeJS**, the local path of a JSON file (that will then be read and parsed).    The `options` parameter is optional, and can define the following:    | OPTION | DEFAULT | NOTE |  | --- | --- | --- |  |context | http://schema.org/ | The value in `@context`. It overwrites the one in the query.|  | sparqlFunction | `null` | A function receiving in input the transformed query in SPARQL, returning a Promise. If not specified, the module performs the query on its own<sup id=""a1"">[1](#f1)</sup> against the specified endpoint.  |  | endpoint | http://dbpedia.org/sparql | Used only if `sparqlFunction` is not specified. |  | debug | `false` | Enter in debug mode. This allow to print in console the generated SPARQL query. |      See [`test.js`](./test.js) for further examples.      ## Credits    If you use this module for your research work, please cite:    > Pasquale Lisena, Albert Meroño-Peñuela, Tobias Kuhn and Raphaël Troncy. Easy Web API Development with SPARQL Transformer. In 18th International Semantic Web Conference (ISWC), Auckland, New Zealand, October 26-30, 2019.    [BIB file](./bib/lisena2019easyweb.bib)      > Pasquale Lisena and Raphaël Troncy. Transforming the JSON Output of SPARQL Queries for Linked Data Clients. In WWW'18 Companion: The 2018 Web Conference Companion, April 23–27, 2018, Lyon, France.  https://doi.org/10.1145/3184558.3188739    [BIB file](./bib/lisena2018sparqltransformer.bib)    ---    <b id=""f1"">1</b>: Using a [lightweight SPARQL client](./src/sparql-client.mjs). """
Semantic web;https://github.com/JeniT/linked-csv;"""# Linked CSV    Many open data sets are essentially tables, or sets of tables, which follow the same regular structure. Linked CSV is a set of conventions for CSV files that enable them to be linked together and to be interpreted as JSON, XML or RDF.    This repository contains the (work in progress) spec for linked CSV.   """
Semantic web;https://github.com/shaoxiongji/awesome-knowledge-graph;"""# Knowledge Graphs  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)  [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)  [![GitHub issues](https://img.shields.io/github/issues/shaoxiongji/knowledge-graphs)](https://github.com/shaoxiongji/knowledge-graphs/issues)  [![GitHub forks](https://img.shields.io/github/forks/shaoxiongji/knowledge-graphs)](https://github.com/shaoxiongji/knowledge-graphs/network)  [![GitHub stars](https://img.shields.io/github/stars/shaoxiongji/knowledge-graphs)](https://github.com/shaoxiongji/knowledge-graphs/stargazers)  [![Twitter](https://img.shields.io/twitter/url?style=social)](https://twitter.com/intent/tweet?text=Wow:&url=https%3A%2F%2Fgithub.com%2Fshaoxiongji%2Fknowledge-graphs)    A collection of knowledge graph papers, codes, and reading notes.    - [Knowledge Graphs](#knowledge-graphs)    - [Survey](#survey)    - [Papers by venues](#papers-by-venues)    - [Papers by categories](#papers-by-categories)    - [Data](#data)      - [General Knowledge Graphs](#general-knowledge-graphs)      - [Domain-specific Data](#domain-specific-data)      - [Entity Recognition](#entity-recognition)      - [Other Collections](#other-collections)    - [Libraries, Softwares and Tools](#libraries-softwares-and-tools)      - [KRL Libraries](#krl-libraries)      - [Knowledge Graph Database](#knowledge-graph-database)      - [Others](#others)      - [Interactive APP](#interactive-app)    - [Courses, Tutorials and Seminars](#courses-tutorials-and-seminars)      - [Courses](#courses)    - [Related Repos](#related-repos)    - [Acknowledgements](#acknowledgements)      ## Survey  __A Survey on Knowledge Graphs: Representation, Acquisition and Applications__. IEEE TNNLS 2021. _Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, Philip S. Yu_. [[Paper](https://arxiv.org/pdf/2002.00388)]     __Knowledge Graphs__. Preprint 2020. _Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia d'Amato, Gerard de Melo, Claudio Gutierrez, José Emilio Labra Gayo, Sabrina Kirrane, Sebastian Neumaier, Axel Polleres, Roberto Navigli, Axel-Cyrille Ngonga Ngomo, Sabbir M. Rashid, Anisa Rula, Lukas Schmelzeisen, Juan Sequeda, Steffen Staab, Antoine Zimmermann_. [[Paper](https://arxiv.org/abs/2003.02320)]     __Knowledge Representation Learning: A Quantitative Review__. Preprint 2018. _Lin, Yankai and Han, Xu and Xie, Ruobing and Liu, Zhiyuan and Sun, Maosong_. [[Paper](https://arxiv.org/pdf/1812.10901)]    __Knowledge graph embedding: A survey of approaches and applications__. TKDE 2017. _Wang, Quan and Mao, Zhendong and Wang, Bin and Guo, Li_. [[Paper](https://persagen.com/files/misc/Wang2017Knowledge.pdf)]    __Knowledge graph refinement: A survey of approaches and evaluation methods__. Semantic Web 2017. _Paulheim, Heiko_. [[Paper](http://www.semantic-web-journal.net/system/files/swj1167.pdf)]    __A review of relational machine learning for knowledge graphs__. Proceedings of the IEEE 2015. _Nickel, Maximilian and Murphy, Kevin and Tresp, Volker and Gabrilovich, Evgeniy_. [[Paper](https://arxiv.org/pdf/1503.00759)]    ## Papers by venues  | Year      | WWW                           | AAAI                            |  ACL  |   | --------  | --------                      | --------                        |  --------                        |                | 2020      | [20](./conferences/www20.md)  | [28](./conferences/aaai20.md)   |  [53](./conferences/acl20.md)    |    ## Papers by categories  - [Knowledge Graph Embedding](./papers/KG-embedding.md)  - [Cross-Modal KG Embedding](./papers/KG-cross-modal.md)  - Knowledge Acquisition    - [Knowledge Graph Completion](./papers/KG-KGC.md)    - [Relation Extraction](./papers/KG-RE.md)    - [Entity Discovery](./papers/KG-entity.md)  - Knowledge-aware Applications    - [Natural Language Understanding](./papers/KG-applications.md#natural-langauge-understanding)    - [Commonsense Knowledge](./papers/KG-applications.md#commonsense-knowledge)    - [Question Answering](./papers/KG-applications.md#question-answering)    - [Dialogue Systems](./papers/KG-applications.md#dialogue-systems)    - [Recommendation Systems](./papers/KG-applications.md#recommendation-systems)    - [Information Retrieval](./papers/KG-applications.md#information-retrieval)  - [Temporal Knowledge Graph](./papers/KG-temporal.md)  - [Knowledge Graph Reasoning](./papers/KG-reasoning.md)  - [One/few-Shot and Zero-Shot Learning](./papers/KG-few-shot.md)  - [Domain-specific Knowledge Graphs](./papers/KG-domain.md)  - [KG Database Systems](./papers/KG-database.md)    ## Data  ### General Knowledge Graphs  - WordNet, https://wordnet.princeton.edu  - OpenCyc, https://www.cyc.com/opencyc/  - Cyc, https://www.cyc.com  - YAGO, http://www.mpii.mpg.de/∼suchanek/yago  - DBpedia, https://wiki.dbpedia.org/develop/datasets  - Freebase, https://developers.google.com/freebase/  - NELL, http://rtw.ml.cmu.edu/rtw/  - Wikidata, https://www.wikidata.org/wiki  - Probase IsA, https://concept.research.microsoft.com/Home/Download  - Google KG, https://developers.google.com/knowledge-graph  - A large-scale Chinese knowledge graph from [OwnThink](https://github.com/ownthink/KnowledgeGraph)  - GDELT（Global Database of Events, Language, and Tone）[Web](https://www.gdeltproject.org)    ### Domain-specific Data  __OpenKG knowledge graphs about the novel coronavirus COVID-19__  - 新冠百科图谱 [[链接](http://www.openkg.cn/dataset/2019-ncov-baike)] Knowledge graph from encyclopedia[[Link](http://www.openkg.cn/dataset/2019-ncov-baike)]    - 新冠科研图谱 [[链接](http://www.openkg.cn/dataset/2019-ncov-research)] Knowledge graph of COVID-19 research [[Link](http://www.openkg.cn/dataset/2019-ncov-research)]    - 新冠临床图谱 [[链接](http://www.openkg.cn/dataset/2019-ncov-clinic)] Clinical knowledge graph [[Link](http://www.openkg.cn/dataset/2019-ncov-clinic)]    - 新冠英雄图谱 [[链接](http://www.openkg.cn/dataset/2019-ncov-hero)] Knowledge graph of people, experts, and heroes [[Link](http://www.openkg.cn/dataset/2019-ncov-hero)]    - 新冠热点事件图谱 [[链接](http://www.openkg.cn/dataset/2019-ncov-event)] Knowledge graph of public events [[Link](http://www.openkg.cn/dataset/2019-ncov-event)]    __COVID❋GRAPH  COVID-19 virus__  [[Web](http://www.odbms.org/2020/03/we-build-a-knowledge-graph-on-covid-19/)]    __KgBase COVID-19 knowledge graph__ [[Web](https://covid19.kgbase.com)]  __Academic graphs__  - OAG, Open Academic Graph, https://www.aminer.cn/open-academic-graph    ### Entity Recognition  CORD-19, a comprehensieve named entity annotation dataset, CORD-NER, on the COVID-19 Open Research Dataset Challenge (CORD-19) corpus [[Data](https://xuanwang91.github.io/2020-03-20-cord19-ner/)]    ### Other Collections  Baidu BROAD datasets [[Web](https://ai.baidu.com/broad/introduction)]    __ASER: A Large-scale Eventuality Knowledge Graph__  WWW 2020. _Zhang et al._ [[Paper](https://dl.acm.org/doi/abs/10.1145/3366423.3380107)]      ## Libraries, Softwares and Tools  ### KRL Libraries  [Grakn](https://github.com/graknlabs/kglib), Grakn Knowledge Graph Library (ML R&D) https://grakn.ai    [AmpliGraph](https://github.com/Accenture/AmpliGraph), Python library for Representation Learning on Knowledge Graphs https://docs.ampligraph.org    [OpenKE](https://github.com/thunlp/OpenKE), An Open-Source Package for Knowledge Embedding (KE)    [Fast-TransX](https://github.com/thunlp/Fast-TransX), An Efficient implementation of TransE and its extended models for Knowledge Representation Learning    [scikit-kge](https://github.com/mnick/scikit-kge), Python library to compute knowledge graph embeddings    [OpenNRE](https://github.com/thunlp/OpenNRE), An Open-Source Package for Neural Relation Extraction (NRE)    ### Knowledge Graph Database  [akutan](https://github.com/eBay/akutan), A distributed knowledge graph store    ### Others  - [OpenNRE](https://github.com/thunlp/OpenNRE)    ### Interactive APP  Knowledge graph APP, Simple knowledge graph applications can be easily built using JSON data managed entirely via a GraphQL layer. [[Github](https://github.com/epistemik-co/staple-api-kg-demo)] [[Website](http://demo.staple-api.org)]    ## Courses, Tutorials and Seminars  ### Courses  - Stanford CS 520 Knowledge Graphs: How should AI explicitly represent knowledge? _Vinay K. Chaudhri, Naren Chittar, Michael Genesereth_. [[Web](https://web.stanford.edu/class/cs520/)]  - Stanford CS 224W: Machine Learning with Graphs. _Jure Leskovec_. [[Web](http://web.stanford.edu/class/cs224w/index.html)]  - University of Bonn: Analysis of Knowledge Graphs. _Jens Lehmmann_. [[Web](https://sewiki.iai.uni-bonn.de/teaching/lectures/kga/2017/start)] [[GitHub](https://github.com/SmartDataAnalytics/Knowledge-Graph-Analysis-Programming-Exercises)]  - Knowledge Graphs. _Harald Sack, Mehwish Alam_. [[Web](https://open.hpi.de/courses/knowledgegraphs2020)]      ## Related Repos  A repo about knowledge graph in Chinese - [husthuke/awesome-knowledge-graph](https://github.com/husthuke/awesome-knowledge-graph)    A repo about NLP, KG, Dialogue Systems in Chinese - [lihanghang/NLP-Knowledge-Graph](https://github.com/lihanghang/NLP-Knowledge-Graph)    Top-level Conference Publications on Knowledge Graph - [wds-seu/Knowledge-Graph-Publications](https://github.com/wds-seu/Knowledge-Graph-Publications)    Geospatial Knowledge Graphs - [semantic-geospatial](https://github.com/laurentlefort/semantic-geospatial/wiki/Geospatial-Knowledge-Graphs)    ## Acknowledgements    Acknowledgments give to the following people who comment or contribute to this repository (listed chronologically).    - [DonaldTsang](https://github.com/DonaldTsang)  - [NYXFLOWER](https://github.com/NYXFLOWER)  - [Arun-George-Zachariah](https://github.com/Arun-George-Zachariah)      __[⬆](#awesome-knowledge-graph)__ """
Semantic web;https://github.com/stoewer/fluent-sparql;"""[![Build Status](https://travis-ci.org/stoewer/fluent-sparql.png?branch=master)](https://travis-ci.org/stoewer/fluent-sparql)  [![Coverage Status](https://coveralls.io/repos/stoewer/fluent-sparql/badge.png?branch=master)](https://coveralls.io/r/stoewer/fluent-sparql?branch=master)    ## About Fluent SPARQL    This project aims to provide an interface for creating and executing SPARQL queries using a DSL-like fluent API that  allows to write code that resembles the SPARQL syntax, such as:    ```java  Query q = sparql.select(""name"")                      .add(""?a"", RDF.type, FOAF.Person)                      .add(""?a"", FOAF.givenname, ""Frodo"")                      .add(""?a"", FOAF.family_name, ""?name"")                      .filter(""?name"").regexp(""^Baggins$"")                  .orderByAsc(""name"")                  .limit(1)                  .query();  ```    ## Status    This project is still work in progress and at this time there is no stable release! """
Semantic web;https://github.com/cosminbasca/jvmrdftools;"""jvmrdftools  ===========    Simple collection of RDF tools in scala    Important Notes  ---------------  This software is the product of research carried out at the [University of Zurich](http://www.ifi.uzh.ch/ddis.html) and comes with no warranty whatsoever. Have fun!    TODO's  ------  * unit tests  * documentation  * examples    Gotcha's  --------  Every time the project version information is changed, BuildInfo needs to be regenerated. To do that simply run:    ```sh  $ sbt compile  ```    to generate the assembly (used by the [rdftools](https://github.com/cosminbasca/rdftools) python module) simply run    ```sh  $ sbt assembly  ```    Thanks a lot to  ---------------  * [University of Zurich](http://www.ifi.uzh.ch/ddis.html) and the [Swiss National Science Foundation](http://www.snf.ch/en/Pages/default.aspx) for generously funding the research that led to this software. """
Semantic web;https://github.com/mommi84/Mandolin;"""![logo](https://github.com/mommi84/Mandolin/raw/master/mandolin-400px.png ""Mandolin logo"")    MANDOLIN  ========    *Markov Logic Networks for the Discovery of Links.*    ## Requirements    * Java 1.8+  * PostgreSQL 9.4.x  * Gurobi solver  * Maven  * Wget, Unzip    ## Quick start    * Download and decompress [Mandolin v0.4.0-alpha](https://github.com/mommi84/Mandolin/releases/download/v0.4.0-alpha/mandolin-binaries-v0.4.0-alpha.zip)  * Run `bash install.sh`    ## Experiments    The following command will discover new links of any predicate (`--aim`) on the WordNet dataset (`--input`) with mining threshold 0.8 (`--mining`) and 1 million Gibbs sampling iterations (`--sampling`).    ```bash  java -Xmx1g -jar target/Mandolin-0.4.0-jar-with-dependencies.jar plain --input data/benchmark/wn18/wordnet-mlj12-train.nt,data/benchmark/wn18/wordnet-mlj12-valid.nt --output eval/wn18 --mining 0.8 --sampling 1000000 --aim ""*""  ```    Discovered links can be found in the `--output` folder at `./eval/wn18/discovered_X.nt`, where `X` is the output threshold, meaning that a file contains all links whose confidence is greater or equal than `X`.    An excerpt of the discovered **rules and weights**:    ```text  0.990517419  wn18:_part_of(b, a) => wn18:_has_part(a, b)  0.862068966  wn18:_instance_hypernym(a, c) AND wn18:_synset_domain_topic_of(f, b) => wn18:_synset_domain_topic_of(a, b)  ```    An excerpt of the discovered **links** with confidence > 0.9:    ```text  wn18:08131530 wn18:_has_part wn18:08132046 .  wn18:09189411 wn18:_has_part wn18:08707917 .  wn18:10484858 wn18:_synset_domain_topic_of wn18:08441203 .  wn18:01941987 wn18:_synset_domain_topic_of wn18:00300441 .  ```    ### Basic documentation    Mandolin can be launched as follows.    ```bash  java -Xmx1g -jar target/Mandolin-0.4.0-jar-with-dependencies.jar <GOAL> <PARAMETERS>  ```    #### Goals    **Goal**|**Description**  :-----|:-----  `plain`|Launch a plain Mandolin execution.  `eval`|Evaluate MRR and hits@k.    #### Plain execution    Parameters for `plain` goal:    **Parameter**|**Description**|**Example value**  :-----|:-----|:-----  `--input`|Comma-separated N-Triple files.|`data1.nt,data2.nt`  `--output`|Workspace and output folder.|`eval/experiment1`  `--aim`|Aim predicate. For all predicates use wildcard `*`.|`http://www.w3.org/2002/07/owl#sameAs`  `--mining`|Rule mining threshold.|`0.9` (default: `0.0` support)  `--sampling`|Gibbs sampling iterations.|`1000000` (default: 100 x evidence size)  `--rules`|Maximum number of rules.|`1500` (default: none)  `--sim`|Enable similarity among literals as `min,step,max`.|`0.8,0.1,0.9` (default: none)  `--onto`|Enable ontology import.|`true` (default: `false`)  `--fwc`|Enable forward-chain.|`true` (default: `false`)    #### Evaluation    The `eval` goal takes two parameters: the N-Triples file of the test set and Mandolin's output directory.    Example run:    ```bash  java -Xmx1g -jar target/Mandolin-0.4.0-jar-with-dependencies.jar eval data/benchmark/wn18/wordnet-mlj12-test.nt eval/wn18  ```    ## Manual install    * Clone project:    ```bash  git clone https://github.com/mommi84/Mandolin.git  cd Mandolin  ```    * Get PostgreSQL 9.4.x - [Ubuntu/Debian binaries](http://oscg-downloads.s3.amazonaws.com/packages/postgresql-9.4.8-1-x64-bigsql.deb)    ### Alternative 1    * Launch `bash install.sh -c`    ### Alternative 2    * Insert PostgreSQL setting parameters into a file `./mandolin.properties`. Example:    ```properties  # GENERAL CONFIGURATION FOR MANDOLIN  pgsql_home=/usr/local/Cellar/postgresql/9.4.1  pgsql_username=tom  pgsql_password=  pgsql_url=localhost  ```    * Download [data](https://s3-eu-west-1.amazonaws.com/anonymous-folder/data.zip)    * Compile project:    ```bash  export MAVEN_OPTS=-Xss4m  mvn clean compile assembly:single  ```    ## Database handler    After using Mandolin, stop the DB instance with:    ```bash  sh pgsql-stop.sh  ```    The instance can be restarted with:    ```bash  sh pgsql-start.sh  ```    ## License(s)    **Mandolin** is licensed under GNU General Public License v2.0.  **AMIE** is licensed under Creative Commons Attribution-NonComercial license v3.0.  **ProbKB** is licensed under the BSD license.  **RockIt** is licensed under the MIT License.  **Gurobi** can be activated using a [free academic license](http://www.gurobi.com/academia/academia-center). """
Semantic web;https://github.com/RubenVerborgh/N3.js;"""# Lightning fast, asynchronous, streaming RDF for JavaScript  [![Build Status](https://github.com/rdfjs/N3.js/workflows/CI/badge.svg)](https://github.com/rdfjs/N3.js/actions)  [![Coverage Status](https://coveralls.io/repos/github/rdfjs/N3.js/badge.svg)](https://coveralls.io/github/rdfjs/N3.js)  [![npm version](https://badge.fury.io/js/n3.svg)](https://www.npmjs.com/package/n3)  [![DOI](https://zenodo.org/badge/3058202.svg)](https://zenodo.org/badge/latestdoi/3058202)    The N3.js library is an implementation of the [RDF.js low-level specification](http://rdf.js.org/) that lets you handle [RDF](https://www.w3.org/TR/rdf-primer/) in JavaScript easily.  It offers:    - [**Parsing**](#parsing) triples/quads from    [Turtle](https://www.w3.org/TR/turtle/),    [TriG](https://www.w3.org/TR/trig/),    [N-Triples](https://www.w3.org/TR/n-triples/),    [N-Quads](https://www.w3.org/TR/n-quads/),    [RDF*](https://blog.liu.se/olafhartig/2019/01/10/position-statement-rdf-star-and-sparql-star/)    and [Notation3 (N3)](https://www.w3.org/TeamSubmission/n3/)  - [**Writing**](#writing) triples/quads to    [Turtle](https://www.w3.org/TR/turtle/),    [TriG](https://www.w3.org/TR/trig/),    [N-Triples](https://www.w3.org/TR/n-triples/),    [N-Quads](https://www.w3.org/TR/n-quads/)    and [RDF*](https://blog.liu.se/olafhartig/2019/01/10/position-statement-rdf-star-and-sparql-star/)  - [**Storage**](#storing) of triples/quads in memory    Parsing and writing is:  - **asynchronous** – triples arrive as soon as possible  - **streaming** – streams are parsed as data comes in, so you can parse files larger than memory  - **fast** – once the [fastest spec-compatible parser in JavaScript](https://github.com/rdfjs/N3.js/tree/master/perf)    (but then [graphy.js](https://github.com/blake-regalia/graphy.js) came along)    ## Installation  For Node.js, N3.js comes as an [npm package](https://npmjs.org/package/n3).    ```Bash  $ npm install n3  ```    ```JavaScript  const N3 = require('n3');  ```    N3.js seamlessly works in browsers via [webpack](https://webpack.js.org/)  or [browserify](http://browserify.org/).  If you're unfamiliar with these tools,  you can read  [_webpack: Creating a Bundle – getting started_](https://webpack.js.org/guides/getting-started/#creating-a-bundle)  or  [_Introduction to browserify_](https://writingjavascript.org/posts/introduction-to-browserify).  You will need to create a ""UMD bundle"" and supply a name (e.g. with the `-s N3` option in browserify).    You can also load it via CDN:  ```html  <script src=""https://unpkg.com/n3/browser/n3.min.js""></script>  ```    ## Creating triples/quads  N3.js follows the [RDF.js low-level specification](http://rdf.js.org/).    `N3.DataFactory` will give you the [factory](http://rdf.js.org/#datafactory-interface) functions to create triples and quads:    ```JavaScript  const { DataFactory } = N3;  const { namedNode, literal, defaultGraph, quad } = DataFactory;  const myQuad = quad(    namedNode('https://ruben.verborgh.org/profile/#me'),    namedNode('http://xmlns.com/foaf/0.1/givenName'),    literal('Ruben', 'en'),    defaultGraph(),  );  console.log(myQuad.termType);              // Quad  console.log(myQuad.value);                 // ''  console.log(myQuad.subject.value);         // https://ruben.verborgh.org/profile/#me  console.log(myQuad.object.value);          // Ruben  console.log(myQuad.object.datatype.value); // http://www.w3.org/1999/02/22-rdf-syntax-ns#langString  console.log(myQuad.object.language);       // en  ```    In the rest of this document, we will treat “triples” and “quads” equally:  we assume that a quad is simply a triple in a named or default graph.    ## Parsing    ### From an RDF document to quads    `N3.Parser` transforms Turtle, TriG, N-Triples, or N-Quads document into quads through a callback:  ```JavaScript  const parser = new N3.Parser();  parser.parse(    `PREFIX c: <http://example.org/cartoons#>     c:Tom a c:Cat.     c:Jerry a c:Mouse;             c:smarterThan c:Tom.`,    (error, quad, prefixes) => {      if (quad)        console.log(quad);      else        console.log(""# That's all, folks!"", prefixes);    });  ```  The callback's first argument is an optional error value, the second is a quad.  If there are no more quads,  the callback is invoked one last time with `null` for `quad`  and a hash of prefixes as third argument.  <br>  Pass a second callback to `parse` to retrieve prefixes as they are read.  <br>  If no callbacks are provided, parsing happens synchronously.    By default, `N3.Parser` parses a permissive superset of Turtle, TriG, N-Triples, and N-Quads.  <br>  For strict compatibility with any of those languages, pass a `format` argument upon creation:    ```JavaScript  const parser1 = new N3.Parser({ format: 'N-Triples' });  const parser2 = new N3.Parser({ format: 'application/trig' });  ```    Notation3 (N3) is supported _only_ through the `format` argument:    ```JavaScript  const parser3 = new N3.Parser({ format: 'N3' });  const parser4 = new N3.Parser({ format: 'Notation3' });  const parser5 = new N3.Parser({ format: 'text/n3' });  ```    It is possible to provide the base IRI of the document that you want to parse.  This is done by passing a `baseIRI` argument upon creation:  ```JavaScript  const parser = new N3.Parser({ baseIRI: 'http://example.org/' });  ```    By default, `N3.Parser` will prefix blank node labels with a `b{digit}_` prefix.  This is done to prevent collisions of unrelated blank nodes having identical  labels. The `blankNodePrefix` constructor argument can be used to modify the  prefix or, if set to an empty string, completely disable prefixing:  ```JavaScript  const parser = new N3.Parser({ blankNodePrefix: '' });  ```    ### From an RDF stream to quads    `N3.Parser` can parse [Node.js streams](http://nodejs.org/api/stream.html) as they grow,  returning quads as soon as they're ready.    ```JavaScript  const parser = new N3.Parser(),        rdfStream = fs.createReadStream('cartoons.ttl');  parser.parse(rdfStream, console.log);  ```    `N3.StreamParser` is a [Node.js stream](http://nodejs.org/api/stream.html) and [RDF.js Sink](http://rdf.js.org/#sink-interface) implementation.  This solution is ideal if your consumer is slower,  since source data is only read when the consumer is ready.    ```JavaScript  const streamParser = new N3.StreamParser(),        rdfStream = fs.createReadStream('cartoons.ttl');  rdfStream.pipe(streamParser);  streamParser.pipe(new SlowConsumer());    function SlowConsumer() {    const writer = new require('stream').Writable({ objectMode: true });    writer._write = (quad, encoding, done) => {      console.log(quad);      setTimeout(done, 1000);    };    return writer;  }  ```    A dedicated `prefix` event signals every prefix with `prefix` and `term` arguments.    ## Writing    ### From quads to a string    `N3.Writer` serializes quads as an RDF document.  Write quads through `addQuad`.    ```JavaScript  const writer = new N3.Writer({ prefixes: { c: 'http://example.org/cartoons#' } });  writer.addQuad(    namedNode('http://example.org/cartoons#Tom'),    namedNode('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),    namedNode('http://example.org/cartoons#Cat')  );  writer.addQuad(quad(    namedNode('http://example.org/cartoons#Tom'),    namedNode('http://example.org/cartoons#name'),    literal('Tom')  ));  writer.end((error, result) => console.log(result));  ```    By default, `N3.Writer` writes Turtle (or TriG if some quads are in a named graph).  <br>  To write N-Triples (or N-Quads) instead, pass a `format` argument upon creation:    ```JavaScript  const writer1 = new N3.Writer({ format: 'N-Triples' });  const writer2 = new N3.Writer({ format: 'application/trig' });  ```    ### From quads to an RDF stream    `N3.Writer` can also write quads to a Node.js stream.    ```JavaScript  const writer = new N3.Writer(process.stdout, { end: false, prefixes: { c: 'http://example.org/cartoons#' } });  writer.addQuad(    namedNode('http://example.org/cartoons#Tom'),    namedNode('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),    namedNode('http://example.org/cartoons#Cat')  );  writer.addQuad(quad(    namedNode('http://example.org/cartoons#Tom'),    namedNode('http://example.org/cartoons#name'),    literal('Tom')  ));  writer.end();  ```    ### From a quad stream to an RDF stream    `N3.StreamWriter` is a [Node.js stream](http://nodejs.org/api/stream.html) and [RDF.js Sink](http://rdf.js.org/#sink-interface) implementation.    ```JavaScript  const streamParser = new N3.StreamParser(),        inputStream = fs.createReadStream('cartoons.ttl'),        streamWriter = new N3.StreamWriter({ prefixes: { c: 'http://example.org/cartoons#' } });  inputStream.pipe(streamParser);  streamParser.pipe(streamWriter);  streamWriter.pipe(process.stdout);  ```    ### Blank nodes and lists  You might want to use the `[…]` and list `(…)` notations of Turtle and TriG.  However, a streaming writer cannot create these automatically:  the shorthand notations are only possible if blank nodes or list heads are not used later on,  which can only be determined conclusively at the end of the stream.    The `blank` and `list` functions allow you to create them manually instead:  ```JavaScript  const writer = new N3.Writer({ prefixes: { c: 'http://example.org/cartoons#',                                         foaf: 'http://xmlns.com/foaf/0.1/' } });  writer.addQuad(    writer.blank(      namedNode('http://xmlns.com/foaf/0.1/givenName'),      literal('Tom', 'en')),    namedNode('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),    namedNode('http://example.org/cartoons#Cat')  );  writer.addQuad(quad(    namedNode('http://example.org/cartoons#Jerry'),    namedNode('http://xmlns.com/foaf/0.1/knows'),    writer.blank([{      predicate: namedNode('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),      object:    namedNode('http://example.org/cartoons#Cat'),    },{      predicate: namedNode('http://xmlns.com/foaf/0.1/givenName'),      object:    literal('Tom', 'en'),    }])  ));  writer.addQuad(    namedNode('http://example.org/cartoons#Mammy'),    namedNode('http://example.org/cartoons#hasPets'),    writer.list([      namedNode('http://example.org/cartoons#Tom'),      namedNode('http://example.org/cartoons#Jerry'),    ])  );  writer.end((error, result) => console.log(result));  ```    ## Storing    `N3.Store` allows you to store triples in memory and find them fast.    In this example, we create a new store and add the triples `:Pluto a :Dog.` and `:Mickey a :Mouse`.  <br>  Then, we find triples with `:Mickey` as subject.    ```JavaScript  const store = new N3.Store();  store.addQuad(    namedNode('http://ex.org/Pluto'),    namedNode('http://ex.org/type'),    namedNode('http://ex.org/Dog')  );  store.addQuad(    namedNode('http://ex.org/Mickey'),    namedNode('http://ex.org/type'),    namedNode('http://ex.org/Mouse')  );    const mickey = store.getQuads(namedNode('http://ex.org/Mickey'), null, null)[0];  console.log(mickey);  ```    ### Addition and deletion of quads  The store provides the following manipulation methods  ([documentation](http://rdfjs.github.io/N3.js/docs/N3Store.html)):  - `addQuad` to insert one quad  - `addQuads` to insert an array of quads  - `removeQuad` to remove one quad  - `removeQuads` to remove an array of quads  - `remove` to remove a stream of quads  - `removeMatches` to remove all quads matching the given pattern  - `deleteGraph` to remove all quads with the given graph  - `createBlankNode` returns an unused blank node identifier    ### Searching quads or entities  The store provides the following search methods  ([documentation](http://rdfjs.github.io/N3.js/docs/N3Store.html)):  - `getQuads` returns an array of quads matching the given pattern  - `match` returns a stream of quads matching the given pattern  - `countQuads` counts the number of quads matching the given pattern  - `forEach` executes a callback on all matching quads  - `every` returns whether a callback on matching quads always returns true  - `some`  returns whether a callback on matching quads returns true at least once  - `getSubjects` returns an array of unique subjects occurring in matching quads  - `forSubjects` executes a callback on unique subjects occurring in matching quads  - `getPredicates` returns an array of unique predicates occurring in matching quad  - `forPredicates` executes a callback on unique predicates occurring in matching quads  - `getObjects` returns an array of unique objects occurring in matching quad  - `forObjects` executes a callback on unique objects occurring in matching quads  - `getGraphs` returns an array of unique graphs occurring in matching quad  - `forGraphs` executes a callback on unique graphs occurring in matching quads    ## Compatibility  ### Format specifications  The N3.js parser and writer is fully compatible with the following W3C specifications:  - [RDF 1.1 Turtle](https://www.w3.org/TR/turtle/)    – [EARL report](https://raw.githubusercontent.com/rdfjs/N3.js/earl/n3js-earl-report-turtle.ttl)  - [RDF 1.1 TriG](https://www.w3.org/TR/trig/)    – [EARL report](https://raw.githubusercontent.com/rdfjs/N3.js/earl/n3js-earl-report-trig.ttl)  - [RDF 1.1 N-Triples](https://www.w3.org/TR/n-triples/)    – [EARL report](https://raw.githubusercontent.com/rdfjs/N3.js/earl/n3js-earl-report-ntriples.ttl)  - [RDF 1.1 N-Quads](https://www.w3.org/TR/n-quads/)    – [EARL report](https://raw.githubusercontent.com/rdfjs/N3.js/earl/n3js-earl-report-nquads.ttl)    In addition, the N3.js parser also supports [Notation3 (N3)](https://www.w3.org/TeamSubmission/n3/) (no official specification yet).    The N3.js parser and writer are also fully compatible with the RDF* variants  of the W3C specifications.    The default mode is permissive  and allows a mixture of different syntaxes, including RDF*.  Pass a `format` option to the constructor with the name or MIME type of a format  for strict, fault-intolerant behavior.  If a format string contains `star` or `*`  (e.g., `turtlestar` or `TriG*`),  RDF* support for that format will be enabled.    ### Interface specifications  The N3.js submodules are compatible with the following [RDF.js](http://rdf.js.org) interfaces:    - `N3.DataFactory` implements    [`DataFactory`](http://rdf.js.org/data-model-spec/#datafactory-interface)    - the terms it creates implement [`Term`](http://rdf.js.org/data-model-spec/#term-interface)      and one of      [`NamedNode`](http://rdf.js.org/data-model-spec/#namednode-interface),      [`BlankNode`](http://rdf.js.org/data-model-spec/#blanknode-interface),      [`Literal`](http://rdf.js.org/data-model-spec/#literal-interface),      [`Variable`](http://rdf.js.org/data-model-spec/#variable-interface),      [`DefaultGraph`](http://rdf.js.org/data-model-spec/#defaultgraph-interface)    - the triples/quads it creates implement      [`Term`](http://rdf.js.org/data-model-spec/#term-interface),      [`Triple`](http://rdf.js.org/data-model-spec/#triple-interface)      and      [`Quad`](http://rdf.js.org/data-model-spec/#quad-interface)  - `N3.StreamParser` implements    [`Stream`](http://rdf.js.org/stream-spec/#stream-interface)    and    [`Sink`](http://rdf.js.org/stream-spec/#sink-interface)  - `N3.StreamWriter` implements    [`Stream`](http://rdf.js.org/stream-spec/#stream-interface)    and    [`Sink`](http://rdf.js.org/stream-spec/#sink-interface)  - `N3.Store` implements    [`Store`](http://rdf.js.org/stream-spec/#store-interface)    [`Source`](http://rdf.js.org/stream-spec/#source-interface)    [`Sink`](http://rdf.js.org/stream-spec/#sink-interface)    ## License and contributions  The N3.js library is copyrighted by [Ruben Verborgh](https://ruben.verborgh.org/)  and released under the [MIT License](https://github.com/rdfjs/N3.js/blob/master/LICENSE.md).    Contributions are welcome, and bug reports or pull requests are always helpful.  If you plan to implement a larger feature, it's best to contact me first. """
Semantic web;https://github.com/architolk/Linked-Data-Studio;"""# Linked Data Studio  The Linked Data Studio (LDS) is a platform for the creation of Linked Data.    The LDS is an extension to the [Linked Data Theatre](https://github.com/architolk/Linked-Data-Theatre), and you should have a working version of the Linked Data Theatre if you want to use the Linked Data Theatre!    See [BUILD.md](BUILD.md) for instructions to build the Linked Data Studio. You can also try one of the releases:    - [lds-1.7.0.war](https://github.com/architolk/Linked-Data-Studio/releases/download/v1.7.0/lds-1.7.0.war ""ldt-1.7.0.war"")    If you want to create a new release of the LDS, please look into the instructions for creating a new release of the [Linked Data Theatre](https://github.com/architolk/Linked-Data-Theatre), the instructions are the same.    To deploy the Linked Data Studio in a tomcat container, follow the instructions in [DEPLOY.md](DEPLOY.md). You can also opt for a docker installation, see [DOCKER.md](DOCKER.md).    The Linked Data Studio uses a configuration graph containing all the triples that make up the LDS configuration. Instructions and examples how to create such a configuration can be found at the [wiki](https://github.com/architolk/Linked-Data-Theatre/wiki) of the Linked Data Theatre.    For security, firewall, proxy and certificates: use the documentation of the [Linked Data Theatre](https://github.com/architolk/Linked-Data-Theatre). """
Semantic web;https://github.com/notruthless/csv2rdf;"""csv2rdf  =======    Ruth Helfinstein  6/24/2012    CSV to RDF translator  Does a simple CSV to RDF translation.    - Assumes the first line contains the attribute names  - Assumes all instances are separated by newlines and contain the right number of elements  - Items in the csv file may have quotes or not, it works either way.      Outputs the rdf file as <filename>.rdf  NOTE: It does not currently check if the output file already exists and will overwrite it if it does.    Looks for a configuration file called <filename>-config.csv, where <filename>.csv is the input file.  Each line in the configuration file describes one item in the header line of the input csv file  <csv_name>, <type>(<optional rdf_name>), <class>    where <csv_name> is the name seen in the header of the column in the csv file.   <type> is either ""class"" or ""property"" or ""ignore"" (without quotes)  <rdf_name> in parenthesis is optional, indicates name to use for this property or class in the rdf file  <class> is the class (for a property) or superclass (for a class)     the lines in the config file do not have to be in the same order as the columns in the csv file  with the exception of any columns with a blank name.  These will be renamed ""unlabeled1"" ""unlabeled2""   and will only match if they are in the same order.    NOTE: any columns NOT listed in the configuration file will be ignored in the rdf file.    If no configuration file is found, the program will create a basic one    Files:  csv2rdf.jar  example.csv  - sample input file  example-config.csv - sample config file  src - source files    To run CSV to RDF from the console:  java -jar csv2rdf.jar <input csv file name>  *** NOTE You can specify the name of the .csv file to use on the command line, otherwise it will default to input.csv        for example   	java -jar csv2rdf.jar input.csv  creates  	input.rdf    Changes 2012.08.02  • Add configuration file    Changes 2012.07.04  • Ignore leading or trailing spaces in the csv  • Add a unlabeled1,unlabled2, unlabeld3... to any attribute in the csv that is not named.  • If there is a blank in the data, do not  add the property name to instance    • Add additional input question: Do you want an 'all' class?   If the user answers yes an ""all"" class is created as the  only subclass to Thing and everything  is subclassed to ""all"".         """
Semantic web;https://github.com/benjimor/odmtp-tpf;"""# ODMTP TPF Server  odmtp-tpf is a Triple Pattern Fragment server using Python, Django and ODMTP.    ODMTP (On Demand Mapper with Triple pattern matching) enables triple pattern matching over non-RDF datasources.    ODMTP support inference (see Semantic Reasoner (Inference) section)    # Online demo    ODMTP's implemented for Twitter API, Github API and Linkedin API are available online. You can run SPARQL queries using the online TPF client demo: [Here](http://query.linkeddatafragments.org/#datasources=http%3A%2F%2Fodmtp.priloo.univ-nantes.fr%2Ftwitter%2Fextended%2F&query=PREFIX%20it%3A%20%3Chttp%3A%2F%2Fwww.influencetracker.com%2Fontology%23%3E%0A%0ASELECT%20%3Fs%20WHERE%20%7B%0A%20%20%3Fs%20it%3AincludedHashtag%20%22SemanticWeb%22.%0A%7D) for Twitter,  [Here](http://query.linkeddatafragments.org/#datasources=http%3A%2F%2Fodmtp.priloo.univ-nantes.fr%2Fgithub%2Fextended%2F&query=PREFIX%20schema%3A%20%3Chttp%3A%2F%2Fschema.org%2F%3E%0A%0ASELECT%20%3Frepo%0AWHERE%20%7B%0A%09%3Frepo%20schema%3Alanguage%20%22Java%22%0A%7D) for Github (limited to 60 request per hour) and [Here](http://odmtp.priloo.univ-nantes.fr/linkedin/authentification/) for your Linkedin profile (you will need to login to access your personal LI profile).    ODMTP approach is already used on [OpenDataSoft Plateform](https://data.opendatasoft.com) and can be tested [Here](http://query.linkeddatafragments.org/#datasources=https%3A%2F%2Fpublic.opendatasoft.com%2Fapi%2Ftpf%2Froman-emperors%2F&query=PREFIX%20roman%3A%20%3Chttps%3A%2F%2Fpublic.opendatasoft.com%2Fld%2Fontologies%2Froman-emperors%2F%3E%0A%0ASELECT%20%3Fname%20WHERE%20%7B%0A%20%20%3Fs%20roman%3Abirth_cty%20%22Rome%22%5E%5Exsd%3Astring%20.%0A%20%20%3Fs%20roman%3Areign_start%20%3Fdate%20.%0A%20%20%20%20FILTER%20(%3Fdate%20%3E%20%220014-12-31T00%3A00%3A00%2B00%3A00%22%5E%5Exsd%3AdateTime)%0A%20%20%3Fs%20%20roman%3Aname%20%3Fname%20.%0A%7D).    # Instructions  ## Installation    #### macOS  You need to install [Homebrew](http://brew.sh/).    and then install Python 2.7:  ```bash  brew install python  ```  #### Ubuntu  ```  sudo apt-get install python-dev python-setuptools  ```    ## Dependencies  You need to install dependencies with pip:  ```bash  pip install -r requirements.txt  ```    ## Running The Platform  #### macOS & Ubuntu  From `~/odmtp-tpf` run the comand:  ```bash  python manage.py runserver  ```    The TPF server should run at: http://127.0.0.1:8000/    # Semantic Reasoner (Inference)  RDF data contains explicit and implicit triples that can be inferred using rules described in ontologies.  To support inference, ODMTP use ontologies to materialize implicit triples of mappings (extended mappings).  Each API can be queried using extended mappings at: `http://127.0.0.1:8000/{api}/extended`  Example: http://127.0.0.1:8000/twitter/extended    However, mappings only contains schema of the corresponding RDF dataset.  Thus, rules that apply on RDF instances cannot be applied on mappings.    ## Supported rules    All rules that apply to schema (class and properties) are supported by ODMTP Semantic Reasoner.    ### Implemented rules    | Rule Name                        |                    if dataset contains ...                   | ... then add                 |  |----------------------------------|:---------------------------------------------------------:|------------------------------|  | rdfs2 (domain)                   | aaa rdfs:domain xxx .<br>uuu aaa yyy .                       | uuu rdf:type xxx .           |  | rdfs3 (range)                    | aaa rdfs:range xxx  .<br>uuu aaa vvv .                        | vvv rdf:type xxx .           |  | rdfs5 (subProperty transitivity) | uuu rdfs:subPropertyOf vvv .<br>vvv rdfs:subPropertyOf xxx . | uuu rdfs:subPropertyOf xxx . |  | rdfs7 (subProperty)              | aaa rdfs:subPropertyOf bbb .<br>uuu aaa yyy .                | uuu bbb yyy .                |  | rdfs9 (subClassOf)               | uuu rdfs:subClassOf xxx .<br>vvv rdf:type uuu .              | vvv rdf:type xxx .           |  | rdfs11 (subClassOf transitivity) | uuu rdfs:subClassOf vvv .<br>vvv rdfs:subClassOf xxx .       | uuu rdfs:subClassOf xxx .    |  | owl sameAs Class                 | uuu owl:sameAs xxx .<br>vvv rdf:type uuu .                   | vvv rdf:type xxx .           |  | owl sameAs Property              | aaa owl:sameAs bbb .<br>uuu aaa yyy .                        | uuu bbb yyy .                |  | owl equivalentClass              | uuu owl:equivalentClass xxx .<br>vvv rdf:type uuu .          | vvv rdf:type xxx .           |  | owl equivalentProperty           | aaa owl:equivalentProperty bbb .<br>uuu aaa yyy .            | uuu bbb yyy .                |    ### Not implemented rules    Not all supported rules are yet implemented.  Complete list of [RDFS](https://www.w3.org/TR/rdf11-mt/#rdfs-entailment) and [OWL-LD](http://semanticweb.org/OWLLD/#Rules) rules.    ## Not supported rules    Rules that applies on instances are not supported by ODMTP Semantic Reasoner.  examples:    | Rule Name             |            if data contains ...           | ... then add     | Comment                                                                        |  |-----------------------|:-----------------------------------------:|------------------|--------------------------------------------------------------------------------|  | owl-ld eq-rep-subject | subj1 owl:sameAs subj2 .<br> subj1 aaa obj1 . | subj2 aaa obj1 . | Not supported if one of the two subject<br> is not defined in ontology or mapping  |  | owl-ld eq-rep-object  | obj1 owl:sameAs obj2 .<br> subj1 aaa obj1 .   | subj1 aaa obj2 . | Not supported if subject or objects are<br> not defined in ontology or mapping     |    # Mappings  Mappings are accessible at: `http://127.0.0.1:8000/{api}/mapping`  Example: http://127.0.0.1:8000/twitter/mapping    Extended mapping are accessible at `http://127.0.0.1:8000/{api}/mapping/extended`  Example: http://127.0.0.1:8000/twitter/mapping/extended    # Examples of Simple Queries  You can use any Triple Pattern Fragment client: http://linkeddatafragments.org/software/  to run SPARQL queries over twitter API and github Repo API V3  ## SPARQL queries over Twitter API  You can run this SPARQL query over http://127.0.0.1:8000/twitter/ to retrieve tweets using #iswc2017 hashtag.  ```sparql  PREFIX it: <http://www.influencetracker.com/ontology#>    SELECT ?s WHERE {   ?s it:includedHashtag ""SemanticWeb"".  }  ```    Retrieve tweets from a specific user.  ```sparql  PREFIX schema: <http://schema.org/>    SELECT ?s WHERE {   ?s schema:author ""NantesTech"".  }  ```    SPO query to browse tweets  ```sparql  SELECT ?s ?p ?o WHERE {   ?s ?p ?o  }  ```    To retrieve a specific tweet by ID  ```sparql  SELECT ?p ?o WHERE {   <https://twitter.com/statuses/889775221452005377> ?p ?o  }  ```    ## SPARQL queries over Github API  You can run this SPARQL query over http://127.0.0.1:8000/github/ to retrieve repositories using Java programming language.  ```sparql  PREFIX schema: <http://schema.org/>    SELECT ?repo  WHERE {  ?repo schema:language ""Java""  }  ```    SPO query to browse repositories  ```sparql  SELECT ?s ?p ?o WHERE {   ?s ?p ?o  }  ```    # Extras  To understand how ODTMP approach is working, the ISWC 2017 poster is available [here](https://docs.google.com/presentation/d/e/2PACX-1vT7fstdxp9LrqPdYpVpbDopBjBLJB5oUysFDp8iS3Z33MCqk-6Yq-2OrWZuWT1tqyFWLeAYcv2kshXe/embed?).    This is just a prototype, feel free to optimize it, improve mapping files, work on new api's etc. """
Semantic web;https://github.com/LinkedBrainz/MusicBrainz-R2RML;"""MusicBrainz-R2RML  =================    R2RML mappings for the MusicBrainz schema on an entity-by-entity basis.     These can be run on the MusicBrainz server using ultrawrap, for which a script is provided (`dump.sh`).  You must set an environment variable `ULTRAWRAP_HOME`.    Running `musicbrainz-r2rml/dump.sh entity` (where entity is artist, track, etc.) runs the appropriate set of mappings (e.g. `mappings/artist.ttl`) to produce output in the form of NTriples (e.g. `output/artist.nt`).    A virtual machine is available (for use with VirtualBox, VMware, etc.) with a replicated MusicBrainz database.    Note that the file `musicbrainz_compile_config.properties` must reflect your DB name:  * `musicbrainz_db` is the default for a snapshot  * `musicbrainz_db_slave` is the default for a replicated database    Please report any issues on [our Jira tracker](https://tickets.metabrainz.org/), under the `LINKB` project. """
Semantic web;https://github.com/RENCI-NRIG/gleen;"""GLEEN - A regular path library for ARQ SparQL    The GLEEN library is a property function library for the Jena ARQ SparQL query engine.    GLEEN was developed by:    > Todd Detwiler    > Structural Informatics Group    > University of Washington    GLEEN is currently licensed under the Apache License, version 2.0    ------    This version of GLEEN is forked from the last released version: 0.6.1    The fork was created by Victor J. Orlikowski (Duke University) in support of  the ORCA project (https://github.com/RENCI-NRIG/orca5), which makes use of GLEEN.    This new revision ports GLEEN forward to currently supported versions of JENA  (http://jena.apache.org/). There are two branches in the code - master branch tied to Jena 2.11.0 and producing Gleen artifact with version 0.6.3-jena-2.11.0-SNAPSHOT. The other branch is called JENA_3_3_0 and produces an artifact with version 0.6.3-jena-3.3.0-SNAPSHOT.     ------    ### Building    For this revision of GLEEN, simply check out the source, make sure you have a  recent version of maven, and type:    mvn clean compile """
Semantic web;https://github.com/oxigraph/oxigraph;"""Oxigraph  ========    [![Latest Version](https://img.shields.io/crates/v/oxigraph.svg)](https://crates.io/crates/oxigraph)   [![Released API docs](https://docs.rs/oxigraph/badge.svg)](https://docs.rs/oxigraph)  [![PyPI](https://img.shields.io/pypi/v/pyoxigraph)](https://pypi.org/project/pyoxigraph/)  [![npm](https://img.shields.io/npm/v/oxigraph)](https://www.npmjs.com/package/oxigraph)  [![actions status](https://github.com/oxigraph/oxigraph/workflows/build/badge.svg)](https://github.com/oxigraph/oxigraph/actions)  [![dependency status](https://deps.rs/repo/github/oxigraph/oxigraph/status.svg)](https://deps.rs/repo/github/oxigraph/oxigraph)  [![Gitter](https://badges.gitter.im/oxigraph/community.svg)](https://gitter.im/oxigraph/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)  [![Twitter URL](https://img.shields.io/twitter/url?style=social&url=https%3A%2F%2Ftwitter.com%2Foxigraph)](https://twitter.com/oxigraph)    Oxigraph is a graph database implementing the [SPARQL](https://www.w3.org/TR/sparql11-overview/) standard.    Its goal is to provide a compliant, safe, and fast graph database based on the [RocksDB](https://rocksdb.org/) key-value store.  It is written in Rust.  It also provides a set of utility functions for reading, writing, and processing RDF files.    Oxigraph is in heavy development and SPARQL query evaluation has not been optimized yet.  The development roadmap is using [GitHub milestones](https://github.com/oxigraph/oxigraph/milestones?direction=desc&sort=completeness&state=open).  Oxigraph internal design [is described on the wiki](https://github.com/oxigraph/oxigraph/wiki/Architecture).    It is split into multiple parts:  * [The database written as a Rust library](https://crates.io/crates/oxigraph). Its source code is in the `lib` directory.  [![Latest Version](https://img.shields.io/crates/v/oxigraph.svg)](https://crates.io/crates/oxigraph)   [![Released API docs](https://docs.rs/oxigraph/badge.svg)](https://docs.rs/oxigraph)  * [`pyoxigraph` that exposes Oxigraph to the Python world](https://oxigraph.org/pyoxigraph/). Its source code is in the `python` directory. [![PyPI](https://img.shields.io/pypi/v/pyoxigraph)](https://pypi.org/project/pyoxigraph/)  * [JavaScript bindings for Oxigraph](https://www.npmjs.com/package/oxigraph). WebAssembly is used to package Oxigraph into a NodeJS compatible NPM package. Its source code is in the `js` directory.  [![npm](https://img.shields.io/npm/v/oxigraph)](https://www.npmjs.com/package/oxigraph)  * [Oxigraph server](https://crates.io/crates/oxigraph_server) that provides a standalone binary of a web server implementing the [SPARQL 1.1 Protocol](https://www.w3.org/TR/sparql11-protocol/) and the [SPARQL 1.1 Graph Store Protocol](https://www.w3.org/TR/sparql11-http-rdf-update/). Its source code is in the `server` directory.  [![Latest Version](https://img.shields.io/crates/v/oxigraph_server.svg)](https://crates.io/crates/oxigraph_server)   [![Docker Image Version (latest semver)](https://img.shields.io/docker/v/oxigraph/oxigraph?sort=semver)](https://hub.docker.com/r/oxigraph/oxigraph)    Oxigraph implements the following specifications:  * [SPARQL 1.1 Query](https://www.w3.org/TR/sparql11-query/), [SPARQL 1.1 Update](https://www.w3.org/TR/sparql11-update/), and [SPARQL 1.1 Federated Query](https://www.w3.org/TR/sparql11-federated-query/).  * [Turtle](https://www.w3.org/TR/turtle/), [TriG](https://www.w3.org/TR/trig/), [N-Triples](https://www.w3.org/TR/n-triples/), [N-Quads](https://www.w3.org/TR/n-quads/), and [RDF XML](https://www.w3.org/TR/rdf-syntax-grammar/) RDF serialization formats for both data ingestion and retrieval using the [Rio library](https://github.com/oxigraph/rio).  * [SPARQL Query Results XML Format](http://www.w3.org/TR/rdf-sparql-XMLres/), [SPARQL 1.1 Query Results JSON Format](https://www.w3.org/TR/sparql11-results-json/) and [SPARQL 1.1 Query Results CSV and TSV Formats](https://www.w3.org/TR/sparql11-results-csv-tsv/).    A preliminary benchmark [is provided](bench/README.md). There is also [a document describing Oxigraph technical architecture](https://github.com/oxigraph/oxigraph/wiki/Architecture).      ## Help    Feel free to use [GitHub discussions](https://github.com/oxigraph/oxigraph/discussions) or [the Gitter chat](https://gitter.im/oxigraph/community) to ask questions or talk about Oxigraph.  [Bug reports](https://github.com/oxigraph/oxigraph/issues) are also very welcome.    If you need advanced support or are willing to pay to get some extra features, feel free to reach out to [Tpt](https://github.com/Tpt/).      ## License    This project is licensed under either of     * Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or     http://www.apache.org/licenses/LICENSE-2.0)   * MIT license ([LICENSE-MIT](LICENSE-MIT) or     http://opensource.org/licenses/MIT)       at your option.      ### Contribution    Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in Oxigraph by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions. """
Semantic web;https://github.com/Remixman/Vedas;"""# VEDAS    **VEDAS** is a RDF store engine that be able to query with SPARQL and run on single GPU.     ## Dependencies  - ModernGPU  - Thrust  - [Raptor RDF Syntax Library](http://librdf.org/raptor/INSTALL.html)  - [Rasqal RDF Query Library](http://librdf.org/rasqal/INSTALL.html)    ## Build  ```bash  make  ```    ## Build the VEDAS database  First, you should prepare the RDF data in N-triple format or .nt extension. **vdBuild** is used for load the triple data into VEDAS internal format  ```bash  ./vdBuild <database_name> <path_to_nt_file>  ```  For example  ```bash  ./vdBuild watdiv500M /home/username/data/watdiv/watdiv.500M.nt  ```  The internal database file <database_name>.vdd and <database_name>.vds will be generated.      ## Query RDF data  VEDAS support query only from file. The **vdQuery** is the query engine that load the RDF data and wait for the input file.  ```bash  ./vdQuery <database_name>  ```  The prompt will shown after finish loaded data. To submit the query, use command *sparql <path_to_sparql_query_file>* and *exit* to terminate the program.    You can use *-sparql-path* option to speccify the sparql file path.  ```bash  ./vdQuery <database_name> -sparql-path=<path_to_sparql_query_file>  ```    ## Visualize the RDF Graph  After load the database with **vdBuild**, it will construct the graph vertex and edge files, named *tools/nodes.txt* and *edges/nodes.txt*. You can generate the GraphML file with the following command  ```bash  cd tools  pip install -r requirements.txt  python graphml.py  ```  The output file *triple-data.graphml* can opened with any supported software e.g. Graphia, Gephi etc. """
Semantic web;https://github.com/ldp4j/ldp4j;"""LDP4j  =====    A Java-based framework for the development of read-write Linked Data applications based on the W3C Linked Data Platform 1.0 (LDP) specification.    LDP4j is distributed under the Apache License, version 2.0.    For more information, please visit [LDP4j website](http://www.ldp4j.org/).    [![Build Status](https://travis-ci.org/ldp4j/ldp4j.svg?branch=master)](https://travis-ci.org/ldp4j/ldp4j)  [![Coverage](https://img.shields.io/sonar/http/analysis.ldp4j.org/sonar/org.ldp4j:ldp4j-parent:master/coverage.svg)](http://analysis.ldp4j.org/sonar/)  [![Technical Debt](https://img.shields.io/sonar/http/analysis.ldp4j.org/sonar/org.ldp4j:ldp4j-parent:master/tech_debt.svg)](http://analysis.ldp4j.org/sonar/)  [![Version](https://img.shields.io/maven-central/v/org.ldp4j/ldp4j-parent.svg?style=flat)](https://github.com/ldp4j/ldp4j/releases)  [![License](https://img.shields.io/github/license/ldp4j/ldp4j.svg)](http://www.apache.org/licenses/LICENSE-2.0)"""
Semantic web;https://github.com/gbv/ssso;"""  This repository contains the specification of **Simple Service Status Ontology  (SSSO)**.    The URI of this ontology is <http://purl.org/ontology/ssso> and it's URI  namespace is <http://purl.org/ontology/ssso#>.    See <http://gbv.github.io/ssso> for a full documentation. RDF serializations  are available at <http://gbv.github.io/ssso/ssso.ttl> and  <http://gbv.github.io/ssso/ssso.owl>.    [Feedback](https://github.com/gbv/ssso/issues) is welcome!    # Overview    The following diagram illustrates the classes and properties definied in this ontology:    ``` {.ditaa}      nextService / previousService                 ------                |      |                v      v         +--------------------+         |    ServiceEvent    |         |                    |         |   ReservedService  |         |   PreparedService  |         |   ProvidedService  |         |   ExecutedService  |         |   RejectedService  |         |                    |         | ServiceFulfillment |         +-----^--------------+               |      ^               |      |                ------  dcterms:hasPart / dcterms:partOf  ```   """
Semantic web;https://github.com/djogopatrao/SPARQLFederator;"""SPARQLFederator  ===============    Expand SPARQL queries to perform inference on multiple endpoints.      Main documentation  ==================    Find more information on our wiki page:    https://github.com/djogopatrao/SPARQLFederator/wiki    Example  =======    java -jar target/SPARQLFederator-1.1-SNAPSHOT-jar-with-dependencies.jar -domain_ontology examples/domain.owl -federation_ontology examples/federation.owl  -exec print -query 'http://www.cipe.accamargo.org.br/ontologias/domain.owl#A'    ""-exec print"" will print the expanded query; ""-exec run"" would execute it and yield results (that is, if there are working endpoints as defined on example/federation.owl)    ""http://www.cipe.accamargo.org.br/ontologias/domain.owl#A"" is the full IRI of the class you're querying for (see -domain_ns for saving space); try it with other classes (like B, or C). Add axioms and your own classes to domain.owl, but keep in mind SPARQLFederator implemented semantics (see the wiki).      Arguments  =========    It is mandatory to specify at least the domain ontology file, the federation ontology file, and one or more classes for querying.    usage: SPARQLFederator [options] <DOMAIN_CLASS> [...]     -query <query>               The query to be expanded (or run) in the syntax specified by -query_type     -domain_ns <arg>             The domain namespace (if specified, will be appended before each of the queryied DOMAIN_CLASSes)      -domain_ontology <arg>       The domain ontology file      -federation_ontology <arg>   The federation ontology file      -help                        Shows the help message      -ontocloud_ns <arg>          The federation namespace (default value: http://www.cipe.accamargo.org.br/ontologias/ontocloud2.owl)      -optimizer <arg>             Execute a query optimizer: 'simple' (default) or 'none'      -planner <arg>               Execute a query planner: 'simple' (default) or 'none'      -query_type <arg>            The accepted query type: 'simple' (default) or 'sparql' (not implemented)      -stats                       Display statistics for queries      DEBUG compiling and running:  ===========================    mvn clean package     mvn exec:java -Dexec.mainClass=""br.org.accamargo.cipe.gqe.SPARQLFederatorRun"" -Dexec.args=""-federation_ontology federation_ontology.owl -domain_ontology domain_ontology.owl  -domain_ns domainNamespace# class1 [,classn]""      Production Compiling  ====================    mvn clean compile assembly:single     """
Semantic web;https://github.com/jeeger/ttl-mode;"""This is an Emacs mode for editing Turtle (RDF) files.    I've changed the indenting for ttl-mode somewhat to support graphs  (`{}`) better.    Original readme:    It is based on an excellent start made by Hugo Haas.  I've extended it to support indentation, some electric punctuation,  and hungry delete.    To use, download the file `ttl-mode.el` from [Bitbucket](https://bitbucket.org/nxg/ttl-mode)  (or clone the project), put the file in the emacs load path (look at the variable  load-path to find where emacs currently searches), and add something  like the following to your `.emacs` file.        (autoload 'ttl-mode ""ttl-mode"" ""Major mode for OWL or Turtle files"" t)      (add-hook 'ttl-mode-hook    ; Turn on font lock when in ttl mode                'turn-on-font-lock)      (setq auto-mode-alist            (append             (list              '(""\\.n3"" . ttl-mode)              '(""\\.ttl"" . ttl-mode))             auto-mode-alist))    Comments and contributions most welcome.      * Copyright 2003-2007, [Hugo Haas](http://www.hugoh.net)    * Copyright 2011-2012, [Norman Gray](https://nxg.me.uk)    * Copyright 2013, [Daniel Gerber](https://danielgerber.net)    * Copyright 2016, [Peter Vasil](http://petervasil.net)    `ttl-mode.el` is released under the terms of the  [two-clause BSD licence](https://opensource.org/licenses/bsd-license.php)  (see the `ttl-model.el` header for the licence text).    Norman Gray    https://nxg.me.uk """
Semantic web;https://github.com/SmartDataAnalytics/Beast;"""# BEAST - Benchmarking, Evaluation, and Analysis Stack    Beast is a lightweight framework that makes it easy to build RDF-in/RDF-out workflows using Java8 streams and Jena.  For instance, if you want to execute a set of tasks described in RDF, Beast easily lets you create workflows that execute them as often as desired and record   any measurements directly in  RDF using the vocabulary of your choice (such as DataCube).    ## Charts in RDF - the Chart Vocabulary    The chart vocabulary enables embedding information about which charts to render from a dataset directly in RDF.  The full dataset example is [here](beast-core/src/test/resources/statistical-data.ttl).    ```turtle  eg:exp1    a cv:StatisticalBarChart ;    rdfs:label ""Performance Histogram"" ;    cv:xAxisTitle ""Workload"" ;    cv:yAxisTitle ""Time (s)"" ;    cv:width 1650 ;    cv:height 1050 ;    cv:style eg:exp1-style ;    cv:series eg:exp1-series ;    .    eg:exp1-style    a cv:ChartStyle ;    cv:legendPosition ""InsideNW"" ;    cv:yAxisLogarithmic true ;    cv:yAxisTicksVisible true ;    cv:xAxisLabelRotation 45 ;    cv:yAxisDecimalPattern ""###,###,###,###,###.#####"" ;    .            eg:exp1-series     a cv:ConceptBasedSeries ;    cv:sliceProperty bsbm:experimentId ;    cv:series ""some-triple-store"" ;    cv:valueProperty <http://bsbm.org/avgQet> ;    bsbm:experimentId eg:bsbm-exp1 ;    .  ```    Charts can be rendered using the class [`org.aksw.beast.cli.MainBeastChart`](beast-cli/src/main/java/org/aksw/beast/cli/MainBeastChart.java).  Installing the beast debian package gives you the convenient `ldcharts` command, which invokes the main class for rendering charts.    ```bash  cd beast-core/src/test/resources  ldcharts statistical-data.ttl  ```      ```bash  Usage [Options] file(s)    Option                 Description  ------                 -----------  --png                  Output charts in png format (Default if no other format is given)  --svg                  Output charts in svg format  --jgp                  Output charts in jpg format  --gui                  Display charts in a window  -o, --output <String>  Output folder  ```    ![LDChartScreenshot](docs/images/2018-02-10-ldchart-screenshot.png)      ## Features    * Construction of Resource-centric Java streams. Hence, plain RDF properties can be attached to resources as part of the stream execution.  * Extension to Jena which enhances Resources with support for attaching and retrieving Java objects by class. This means you can e.g. attach a parsed Jena Query object to a resource that represents a SPARQL query string.  * Looping with the loops state getting attached to the resource.  * No need to know the URI for resources in advance. You can painlessly give them a proper name *at the end* of the workflow *based on its properties*.    While technically Beast essentially provides utilities for chaining functions and streams, a great share of Beast's contribution lies in its the conceptual considerations.    ## Components    * Jena Extension: Attach Java objects to Jena resources by casting them to the enhanced resource `ResourceEnh`. Requires the `Model` to be created with `ModelFactoryEnh`:  ```java  Model m = ModelFactoryEnh.createModel();  m.createResource().as(ResourceEnh.class)      .addTrait(myObj.getClass(), myObj);      // Short-hand of above version      .addTrait(myObj);  ```  * RdfStream API: Enables construction of RDF Resource based workflows using the usual streaming methods, such as *map*, *flatMap*, *peek*, and additional ones such as *repeat*.  * Analysis: Compute new resources representing observations of aggregated values such as averages and standard deviations.  * Visualization: Plot series as charts.    ```java  RdfStream      .startWithCopy()      .peek(workloadRes -> workloadRes.as(ResourceEnh.class)          .addTrait(QueryFactory.create(workloadRes.getProperty(LSQ.text).getString())))      .map(workloadRes ->          // Create the blank observation resource          workloadRes.getModel().createResource().as(ResourceEnh.class)          // Copy the query object attached to the workload resource over to this observation resource          .copyTraitsFrom(workloadRes)          // Add some properties to the observation          .addProperty(RDF.type, QB.Observation)          .addProperty(IguanaVocab.workload, workloadRes)          .as(ResourceEnh.class))      .seq(          // Warm up run - the resources are processed, but filtered out          RdfStream.<ResourceEnh>start().repeat(1, IV.run, 1)              .peek(r -> r.addLiteral(IV.warmup, true))              .filter(r -> false),          // Actual evaluation          RdfStream.<ResourceEnh>start().repeat(3, IV.run, 1).peek(r -> r.addLiteral(IV.warmup, false))      )      // Give the observation resource a proper name      .map(r -> r.rename(""http://example.org/observation/{0}-{1}"", r.getProperty(IguanaVocab.workload).getResource().getLocalName(), IV.run))      .apply(() -> workloads.stream()).get()      // write out every observation resource      .forEach(observationRes -> RDFDataMgr.write(System.out, observationRes.getModel(), RDFFormat.TURTLE_BLOCKS));      ;  ```    Which generates output such as:  ```java  <http://example.org/observation/q1a-5>          <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>  <http://purl.org/linked-data/cube#Observation> ;          <http://iguana.aksw.org/ontology#workload>  <http://example.org/query/q1a> ;          <http://www.w3.org/ns/prov#startedAtTime>  ""2016-12-20T02:57:07.608Z""^^<http://www.w3.org/2001/XMLSchema#dateTime> ;          <http://www.w3.org/ns/prov#endAtTime>  ""2016-12-20T02:57:07.672Z""^^<http://www.w3.org/2001/XMLSchema#dateTime> ;          <http://www.w3.org/2006/time#numericDuration>  ""0.063747601""^^<http://www.w3.org/2001/XMLSchema#double> ;          <http://iv.aksw.org/vocab#run>  ""5""^^<http://www.w3.org/2001/XMLSchema#long> ;          <http://iv.aksw.org/vocab#warmup>  false .  ```    ## Examples    * [Performance Measurement](beast-examples/src/main/java/org/aksw/beast/examples/MainQueryPerformance.java) - [Test Data (queries.ttl)](beast-examples/src/main/resources/queries.ttl)  * [KFoldCrossValidation](beast-examples/src/main/java/org/aksw/beast/examples/MainKFoldCrossValidation.java) - [Test Data (folds.ttl)](beast-examples/src/main/resources/folds.ttl)      ## Dependencies    Beast only aggregates features from other (lower-level) projects, among them:    Core:  * jena  * guava    Visualization (Optional):  * JFreeChart  * XChart (probably in the future)           """
Semantic web;https://github.com/TopQuadrant/shacl;"""# TopBraid SHACL API    **An open source implementation of the W3C Shapes Constraint Language (SHACL) based on Apache Jena.**    Contact: Holger Knublauch (holger@topquadrant.com)    Can be used to perform SHACL constraint checking and rule inferencing in any Jena-based Java application.  This API also serves as a reference implementation of the SHACL spec.    Coverage:  * [SHACL Core and SHACL-SPARQL validation](https://www.w3.org/TR/shacl/)  * [SHACL Advanced Features (Rules etc)](https://www.w3.org/TR/shacl-af/)    Former Coverage until version 1.4.0  * [SHACL Compact Syntax](https://w3c.github.io/shacl/shacl-compact-syntax/)    Former Coverage until version 1.3.2  * [SHACL JavaScript Extensions](https://www.w3.org/TR/shacl-js/)    The TopBraid SHACL API is internally used by the European Commission's generic [SHACL-based RDF validator](https://www.itb.ec.europa.eu/shacl/any/upload) (used to validate RDF content against SHACL shapes)  and [SHACL shape validator](https://www.itb.ec.europa.eu/shacl/shacl/upload) (used to validate SHACL shapes themselves).    The same code is used in the TopBraid products (currently aligned with the TopBraid 7.1 release).    Feedback and questions should become GitHub issues or sent to TopBraid Users mailing list:  https://groups.google.com/forum/#!forum/topbraid-users  Please prefix your messages with [SHACL API]    To get started, look at the class ValidationUtil in  the package org.topbraid.shacl.validation.  There is also an [Example Test Case](../master/src/test/java/org/topbraid/shacl/ValidationExample.java)    ## Application dependency    Releases are available in the central maven repository:    ```  <dependency>    <groupId>org.topbraid</groupId>    <artifactId>shacl</artifactId>    <version>*VER*</version>  </dependency>  ```  ## Command Line Usage    Download the latest release from:    `https://repo1.maven.org/maven2/org/topbraid/shacl/`    The binary distribution is:    `https://repo1.maven.org/maven2/org/topbraid/shacl/*VER*/shacl-*VER*-bin.zip`.    Two command line utilities are included: shaclvalidate (performs constraint validation) and shaclinfer (performs SHACL rule inferencing).    To use them, set up your environment similar to https://jena.apache.org/documentation/tools/ (note that the SHACL download includes Jena).    For example, on Windows:    ```  SET SHACLROOT=C:\Users\Holger\Desktop\shacl-1.4.1-bin  SET PATH=%PATH%;%SHACLROOT%\bin  ```    As another example, for Linux, add to .bashrc these lines:    ```  # for shacl  export SHACLROOT=/home/holger/shacl/shacl-1.4.1-bin/shacl-1.4.1/bin  export PATH=$SHACLROOT:$PATH   ```    Both tools take the following parameters, for example:    `shaclvalidate.bat -datafile myfile.ttl -shapesfile myshapes.ttl`    where `-shapesfile` is optional and falls back to using the data graph as shapes graph.  Add -validateShapes in case you want to include the metashapes (from the tosh namespace in particular).    Currently only Turtle (.ttl) files are supported.    The tools print the validation report or the inferences graph to the output screen. """
Semantic web;https://github.com/AKSW/GeoLift;"""GeoLift  =======    Framework to enrich geographic content in the Semantic Web"""
Semantic web;https://github.com/kasei/swift-hdt;"""# swift-hdt    ## An HDT RDF Parser    ### Build    On MacOS 10.14:    ```  % swift build -Xswiftc ""-target"" -Xswiftc ""x86_64-apple-macosx10.14""  ```    On Linux:    ```  % swift build  ```    ### Parse an HDT file    ```  % ./.build/release/hdt-parse swdf-2012-11-28.hdt  _:b1 <http://www.w3.org/1999/02/22-rdf-syntax-ns#_1> <http://data.semanticweb.org/person/barry-norton> .  _:b1 <http://www.w3.org/1999/02/22-rdf-syntax-ns#_2> <http://data.semanticweb.org/person/reto-krummenacher> .  _:b1 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/1999/02/22-rdf-syntax-ns#Seq> .  _:b10 <http://www.w3.org/1999/02/22-rdf-syntax-ns#_1> <http://data.semanticweb.org/person/robert-isele> .  _:b10 <http://www.w3.org/1999/02/22-rdf-syntax-ns#_2> <http://data.semanticweb.org/person/anja-jentzsch> .  _:b10 <http://www.w3.org/1999/02/22-rdf-syntax-ns#_3> <http://data.semanticweb.org/person/christian-bizer> .  _:b10 <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/1999/02/22-rdf-syntax-ns#Seq> .  ...  ```    ### Limitations    This project is early in development, and has many limitations:    * Only serializing the entire HDT file is possible (triple pattern matching is planned for the future)  * Only ""Four Part"" dictionary encoding is currently supported  * Only ""Log64"" encoding of bitmap triples values is currently supported    ### References    * [HDT](http://www.rdfhdt.org)  * [HDT Binary Format](http://www.rdfhdt.org/hdt-binary-format/) """
Semantic web;https://github.com/ktym/d3sparql;"""d3sparql.js  ===========    JavaScript library for executing SPARQL query and transforming resulted JSON for visualization in D3.js.    ### Description    Semantic Web technologies are getting widely used in information sciences along with the Linked Open Data (LOD) initiative and RDF data are exposed at SPARQL endpoints around the world. SPARQL query is used to search those endpoints and the results are obtained as a SPARQL Query Results XML Format or a SPARQL Query Results JSON Format, both are essentially tabular structured data. To effectively represent the SPARQL results, appropriate visualization methods are highly demanded. To create and control dynamic graphical representation of data on the Web, the D3.js JavaScript library is getting popularity as a generic framework based on the widely accepted Web standards such as SVG, JavaScript, HTML5 and CSS. A variety of visualization examples implemented with the D3.js library are already freely available, however, each of them depends on a predefined JSON data structure that differs from the JSON structure returned from SPARQL endpoints. Therefore, it is expected to largely reduce development costs of Semantic Web visualization if a JavaScript library is available which can transform SPARQL Query Results JSON Format into JSON data structures consumed by the D3.js. The d3sparql.js is developed as a generic JavaScript library to fill this gap which can be used to query SPARQL endpoints as an AJAX call and provides various callback functions to visualize the obtained results.    ### Currently supports    * Charts    * barchart, piechart, scatterplot  * Graphs    * force graph, sankey graph  * Trees    * roundtree, dendrogram, treemap, sunburst, circlepack  * Maps    * coordmap, namedmap  * Tables    * htmltable, htmlhash    ### Usage    ```html  <!DOCTYPE html>  <meta charset=""utf-8"">  <html>   <head>    <script src=""http://d3js.org/d3.v3.min.js""></script>    <script src=""d3sparql.js""></script>    <script>    function exec() {      /* Uncomment to see debug information in console */      // d3sparql.debug = true      var endpoint = d3.select(""#endpoint"").property(""value"")      var sparql = d3.select(""#sparql"").property(""value"")      d3sparql.query(endpoint, sparql, render)    }    function render(json) {      /* set options and call the d3spraql.xxxxx function in this library ... */      var config = {  	  ""selector"": ""#result""  	}      d3sparql.xxxxx(json, config)    }    </script>    <style>    <!-- customize CSS -->    </style>   </head>   <body onload=""exec()"">    <form style=""display:none"">     <input id=""endpoint"" value=""http://dbpedia.org/sparql"" type=""text"">     <textarea id=""sparql"">      PREFIX ...      SELECT ...      WHERE { ... }     </textarea>    </form>    <div id=""result""></div>   </body>  </html>  ```    ### Live demo    * http://biohackathon.org/d3sparql    ### Codebase    * https://github.com/ktym/d3sparql    ### Publication    * http://ceur-ws.org/Vol-1320/paper_39.pdf    ### Presentation    * http://www.slideshare.net/ToshiakiKatayama/d3sparqljs-demo-at-swat4ls-2014-in-berlin    ### License    * The d3sparql.js library is distributed under the same license as D3.js's ([BSD license](http://opensource.org/licenses/BSD-3-Clause)).    ### ChangeLog    See details at https://github.com/ktym/d3sparql/commits/master/d3sparql.js    * 2013-01-28 Project started  * 2014-07-03 Made publicly available at GitHub  * 2014-07-14 Added bar/line charts ```barchart()``` with scales  * 2014-07-17 Added default SVG attributes equivalent to CSS styles    * Visualizations look good without CSS by default (user can customize style by CSS)    * Added descriptions to each visualization function  * 2014-07-19 Introduced ```d3sparql``` name space  * 2014-07-20 Added Pie, Doughnut ```piechart()```, Sankey diagram ```sankey()``` and a name based map ```namedmap()```  * 2014-11-13 Merged a pull request to visualize a coordination based map ```coordmap()```  * 2014-12-11 Updated to set default values in options  * 2015-02-03 Added README file    * updated namedmap to use an option for scale    * merged a pull request to insert visualization at the specified DOM ID instead of appending to the body  * 2015-02-04 Improved to customize the target DOM ID  * 2015-02-06 Changed to clear the DOM contents before appending elements to update the visualization  * 2015-05-21 Updated ```tree()``` and ```graph()``` to keep values associated to nodes    * Values are reflected in the ```treemap()```, ```sunburst()``` and ```forcegraph()``` visualizations  * 2015-05-21 Debug mode is introduced    * Assign ```d3sparql.debug = true``` at anytime to enable verbose console log  * 2015-05-25 Incorporated ```treemapzoom()``` useful to dig into a nested tree (in which each leaf may have a value)     """
Semantic web;https://github.com/ontodia-org/ontodia;"""# Ontodia [![npm](https://img.shields.io/npm/v/ontodia.svg)](https://www.npmjs.com/package/ontodia) [![CircleCI](https://circleci.com/gh/sputniq-space/ontodia.svg?style=svg)](https://circleci.com/gh/sputniq-space/ontodia) #    Ontodia is a JavaScript library that allows to visualize, navigate and explore data in the form of an interactive graph based on underlying data sources.    ## What is Ontodia for?    Ontodia allows you to create and persist diagrams made from existing data - relational, object, semantic.    It was designed to visualize RDF data sets in particular, but could be tailored to almost any data source by implementing a data provider interface.      ## Core features    - Visual navigation and diagramming over large graph data sets  - Rich graph visualization and context-aware navigation features   - Ability to store and retrieve diagrams  - User friendly - no graph query language or prior knowledge of the schema required  - Customizable user interface (by modifying templates for nodes and links) and data storage back-end    ## How to try it?    You can follow developer tutorials at the [developer documentation page](https://github.com/metaphacts/ontodia/wiki)    ## License    The Ontodia library is distributed under LGPL-2.1. A commercial license with additional features, support and custom development is available, please contact us at [info@metaphacts.com](info@metaphacts.com).         ## Developer documentation and contributing    Developer documentation is available at [wiki page](https://github.com/metaphacts/ontodia/wiki).    ## Giving Ontodia people credit    If you use the Ontodia library in your projects, please provide a link to this repository in your publication and a citation reference to the following paper:     Mouromtsev, D., Pavlov, D., Emelyanov, Y., Morozov, A., Razdyakonov, D. and Galkin, M., 2015. The Simple Web-based Tool for Visualization and Sharing of Semantic Data and Ontologies. In International Semantic Web Conference (Posters & Demos).    ```  @inproceedings{Mouromtsev2015,      author = {Mouromtsev, Dmitry and Pavlov, Dmitry and Emelyanov, Yury and          Morozov, Alexey and Razdyakonov, Daniil and Galkin, Mikhail},      year = {2015},      month = {10},      title = {The Simple Web-based Tool for Visualization and Sharing of Semantic Data and Ontologies},      booktitle = {International Semantic Web Conference (Posters & Demos)}  }  ```    It really helps our team to gain publicity and acknowledgment for our efforts.  Thank you for being considerate! """
Semantic web;https://github.com/ldodds/geonames;"""geonames  ========    Simple utility scripts for downloading, unpacking and reformatting the Geonames   RDF data dumps.    Author  ------    Leigh Dodds (leigh.dodds@talis.com)    Download  --------    [http://github.com/ldodds/geonames]    Overview  --------    Geonames is a great resource for geographical information. Helpfully they   publish data exports in a variety of formats, allowing others to process and   manipulate the data locally.    Unfortunately the RDF data dump that is available from:    [http://download.geonames.org/export/dump/all-geonames-rdf.txt.zip]    is a little idiosyncratic. Rather than provide a single ntriples or even RDF/XML file   the dump consists of a text file that consists of alternating lines like this:      ...feature URI....    <rdf:RDF>...RDF/XML description of feature....</rdf:RDF>    This means you need to script up unpacking the file in order to load it into a triple   store.    The Rakefile and script provided here make it easy to download, unpack and convert   the data dump into ntriples.    The conversion is written in Ruby and uses the RDF.rb and rdf/raptor libraries.  So before running the scripts you'll need to execute:      sudo gem install rdf rdf-raptor    To download and convert the files into ntriples simply run:      rake download convert    The converted data is stored in geonames.nt.    License  -------    These scripts are free and unencumbered public domain software. For more information, see http://unlicense.org/ or the accompanying UNLICENSE file. """
Semantic web;https://github.com/carml/carml;"""  <p align=""center"">  <img src=""https://raw.githubusercontent.com/carml/carml.github.io/master/carml-logo.png"" height=""100"" alt=""carml"">  </p>    CARML  =====================  **A pretty sweet RML engine**    CARML was first developed by [Taxonic](http://www.taxonic.com) in cooperation with [Kadaster](https://www.kadaster.com/). And is now being maintained and developed further by [Skemu](https://skemu.com).      [![Build Status](https://api.travis-ci.org/carml/carml.svg?branch=master)](https://travis-ci.org/carml/carml)  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.taxonic.carml/carml/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.taxonic.carml/carml)    Table of Contents  -----------------  - [CARML](#carml)    - [Table of Contents](#table-of-contents)    - [Releases](#releases)    - [Introduction](#introduction)    - [Getting started](#getting-started)    - [Validating your RML mapping](#validating-your-rml-mapping)    - [Input stream extension](#input-stream-extension)    - [Function extension](#function-extension)    - [XML namespace extension](#xml-namespace-extension)    - [Supported Data Source Types](#supported-data-source-types)    - [CARML in RML Test Cases](#carml-in-rml-test-cases)    - [Roadmap](#roadmap)    Releases  ----  20 Sep 2017 - CARML 0.0.1    21 Oct 2017 - CARML 0.1.0    05 Dec 2017 - CARML 0.1.1    12 Feb 2018 - CARML 0.1.2    20 May 2018 - CARML 0.2.0    03 Aug 2018 - CARML 0.2.1    13 Nov 2018 - CARML 0.2.2    17 Nov 2018 - CARML 0.2.3    11 Sep 2020 - CARML 0.3.0    31 Oct 2020 - CARML 0.3.1    22 Mar 2021 - CARML 0.3.2    Introduction  ------------  CARML is a java library that transforms structured sources to RDF based as declared in and [RML](http://rml.io) mapping, in accordance with the [RML spec](http://rml.io/spec.html).    The best place to start learning about RML is at the [source](http://rml.io), but basically  RML is defined as a superset of [R2RML](https://www.w3.org/TR/r2rml/) which is a W3C recommendation that describes a language for expressing mappings from relational databases to RDF datasets. RML allows not only the expression of mappings for relational databases, but generalizes this to any structured source. All you need is a way to iterate over and query the source.    Getting started  ---------------    CARML is available from the Central Maven Repository.    ```xml  <dependency>      <groupId>com.taxonic.carml</groupId>      <artifactId>carml-engine</artifactId>      <version>0.3.2</version>  </dependency>    <!-- Choose the resolvers to suit your need -->  <dependency>    <groupId>com.taxonic.carml</groupId>    <artifactId>carml-logical-source-resolver-jsonpath</artifactId>    <version>0.3.2</version>  </dependency>  <dependency>    <groupId>com.taxonic.carml</groupId>    <artifactId>carml-logical-source-resolver-xpath</artifactId>    <version>0.3.2</version>  </dependency>  <dependency>    <groupId>com.taxonic.carml</groupId>    <artifactId>carml-logical-source-resolver-csv</artifactId>    <version>0.3.2</version>  </dependency>    ```    CARML is built on [RDF4J](http://rdf4j.org/), and currently the Mapper directly outputs an [RDF4J Model](http://docs.rdf4j.org/javadoc/2.0/org/eclipse/rdf4j/model/package-summary.html).    ```java  Set<TriplesMap> mapping =    RmlMappingLoader      .build()      .load(RDFFormat.TURTLE, Paths.get(""path-to-mapping-file""));    RmlMapper mapper =    RmlMapper      .newBuilder()      // Add the resolvers to suit your need      .setLogicalSourceResolver(Rdf.Ql.JsonPath, new JsonPathResolver())      .setLogicalSourceResolver(Rdf.Ql.XPath, new XPathResolver())      .setLogicalSourceResolver(Rdf.Ql.Csv, new CsvResolver())        //-- optional: --        // specify IRI unicode normalization form (default = NFC)        // see http://www.unicode.org/unicode/reports/tr15/tr15-23.html      .iriUnicodeNormalization(Form.NFKC)        // set file directory for sources in mapping      .fileResolver(""/some/dir/"")        // set classpath basepath for sources in mapping      .classPathResolver(""some/path"")        // specify casing of hex numbers in IRI percent encoding (default = true)        // added for backwards compatibility with IRI encoding up until v0.2.3      .iriUpperCasePercentEncoding(false)      //---------------        .build();    Model result = mapper.map(mapping);  ```    Validating your RML mapping  ---------------------------  We're not set up for full mapping validation yet. But, to help you get those first nasty mapping errors out, we've created a [SHACL](https://www.w3.org/TR/shacl/) shapes graph ([here](https://github.com/carml/carml/tree/master/rml.sh.ttl)) that validates RML mappings. You can use the [SHACL playground](http://shacl.org/playground/) to easily test your mapping.    Input stream extension  ---------------------  When it comes to non-database sources, the current RML spec only supports the specification of file based sources in an `rml:LogicalSource`. However, it is often very useful to be able to transform stream sources.    To this end CARML introduces the notion of 'Named Streams'.  Which follows the ontology defined [here](https://github.com/carml/carml/tree/master/carml.ttl).    So now, you can define streams in your mapping like so:  ```  :SomeLogicalSource    rml:source [      a carml:Stream ;      # NOTE: name is not mandatory and can be left unspecified, when working with a single stream      carml:streamName ""stream-A"" ;    ];    rml:referenceFormulation ql:JSONPath;    rml:iterator ""$""  .  ```  In order to provide access to the input stream, it needs to be registered on the mapper.  ```java  RmlMapper mapper =    RmlMapper    .newBuilder()    .setLogicalSourceResolver(Rdf.Ql.JsonPath, new JsonPathResolver())    .build();  mapper.bindInputStream(""stream-A"", inputStream);  ```  Note that it is possible to register several streams, allowing you to combine several streams to create your desired RDF output.    Function extension  ------------------  A recent development related to RML is the possibility of adding functions to the mix. This is a powerful extension that increases the expressivity of mappings immensely. Do note that the function ontology is still under development at UGhent.    Because we believe that this extension can already be of great value, we've already adopted it in CARML.    <!--- TODO: explain that the function execution is a finisher, that is it runs the normal mapping, which creates the function execution triples, and the described execution is in turn evaluated and results in the term map value. --->  The way it works is, you describe the execution of a function in terms of the [Function Ontology (FnO)](https://fno.io/).    Take for example the SumFunction example of the [FnO spec](http://users.ugent.be/~bjdmeest/function/#complete-example). This defines an instance `ex:sumFunction` of class `fno:Function` that is able to compute the sum of two values provided as parameters of the function at execution time.    It also describes an instance `ex:sumExecution` of `fno:execution`, which `fno:executes` `ex:sumFunction` which descibes an instance of an execution of the defined sum function. In this case with parameters 2 and 4.    To be able to use this in RML mappings we use executions of instances of `fno:Function` to determine the value of a term map. The execution of a function can be seen as a post-processing step in the evaluation of a term map.    ```  @prefix rr: <http://www.w3.org/ns/r2rml#> .  @prefix rml: <http://semweb.mmlab.be/ns/rml#> .  @prefix fnml: <http://semweb.mmlab.be/ns/fnml#> .  @prefix xsd: <http://www.w3.org/2001/XMLSchema#> .  @prefix fno: <https://w3id.org/function/ontology#> .  @prefix ex: <http://example.org/> .    ex:sumValuePredicateObjectMap    rr:predicate ex:total ;    rr:objectMap [      a fnml:FunctionMap ;      fnml:functionValue [        rml:logicalSource ex:LogicalSource ;        rr:subjectMap [          rr:template ""functionExec"";          rr:termType rr:BlankNode ;          rr:class fno:Execution        ] ;        rr:predicateObjectMap          [            rr:predicate fno:executes ;            rr:objectMap [              rr:constant ex:sumFunction ;            ]          ] ,          [            rr:predicate ex:intParameterA ;            rr:objectMap [ rml:reference ""foo"" ]          ] ,          [            rr:predicate ex:intParameterB  ;            rr:objectMap [ rml:reference ""bar"" ]          ]      ] ;      rr:datatype xsd:integer ;    ]  .  ```    A function can be created in any `.java` class. The function should be annotated with `@FnoFunction`, providing the string value of the function IRI, and the function parameters with `@FnoParam`, providing the string value of the function parameter IRIs.    ```java  public class RmlFunctions {      @FnoFunction(""http://example.org/sumFunction"")    public int sumFunction(      @FnoParam(""http://example.org/intParameterA"") int intA,      @FnoParam(""http://example.org/intParameterB"") int intB      ) {        return intA + intB;      }    }  ```    The class or classes containing the annotated functions can then be registered on the mapper via the `RmlMapper#addFunctions` method.    ```java  RmlMapper mapper =    RmlMapper      .newBuilder()      .setLogicalSourceResolver(Rdf.Ql.JsonPath, new JsonPathResolver())      .addFunctions(new YourRmlFunctions())      .build();  Model result = mapper.map(mapping);  ```    It is recommended to describe and publish new functions in terms of FnO for interpretability of mappings, and, possibly, reuse of functions, but it's not mandatory for use in CARML.    Note that it is currently possible to specify and use function executions as parameters of other function executions in CARML, although this is not (yet?) expressible in FnO.    XML namespace extension  -----------------------    When working with XML documents, it is often necessary specify namespaces to identify a node's qualified name.  Most XPath implementations allow you to register these namespaces, in order to be able to use them in executing XPath expressions.  In order to convey these expressions to the CARML engine, CARML introduces the class `carml:XmlDocument` that can be used as a value of `rml:source`. An instance of  `carml:XmlDocument` can, if it is a file source, specify a location via the `carml:url` property, and specify namespace declarations via the `carml:declaresNamespace` property.    For example, given the following XML document:  ```xml  <?xml version=""1.0"" encoding=""UTF-8""?>  <ex:bookstore xmlns:ex=""http://www.example.com/books/1.0/"">    <ex:book category=""children"">      <ex:title lang=""en"">Harry Potter</ex:title>      <ex:author>J K. Rowling</ex:author>      <ex:year>2005</ex:year>      <ex:price>29.99</ex:price>    </ex:book>  </ex:bookstore>  ```    one can now use the following mapping, declaring namespaces, to use them in XPath expressions:  ```  @prefix rr: <http://www.w3.org/ns/r2rml#>.  @prefix rml: <http://semweb.mmlab.be/ns/rml#>.  @prefix ql: <http://semweb.mmlab.be/ns/ql#> .  @prefix carml: <http://carml.taxonic.com/carml/> .  @prefix ex: <http://www.example.com/> .    <#SubjectMapping> a rr:TriplesMap ;    rml:logicalSource [      rml:source [        a carml:Stream ;        # or in case of a file source use:        # carml:url ""path-to-source"" ;        carml:declaresNamespace [          carml:namespacePrefix ""ex"" ;          carml:namespaceName ""http://www.example.com/books/1.0/"" ;        ] ;      ] ;      rml:referenceFormulation ql:XPath ;      rml:iterator ""/ex:bookstore/*"" ;    ] ;      rr:subjectMap [      rr:template ""http://www.example.com/{./ex:title}"" ;      rr:class ex:Book ;      rr:termType rr:IRI ;    ] ;  .    ```    which yields:  ```  <http://www.example.com/Harry%20Potter> a <http://www.example.com/Book> .  ```    Supported Data Source Types  ---------------------------    | Data source type | Reference query language                           | Implementation                                                      |  | :--------------- | :------------------------------------------------- | :-------------------------------------------------------------      |  | JSON             | [JsonPath](http://goessner.net/articles/JsonPath/) | [Jayway JsonPath 2.4.0](https://github.com/json-path/JsonPath)      |  | XML              | [XPath](https://www.w3.org/TR/xpath-31/)           | [Saxon-HE 10.2](http://saxon.sourceforge.net/#F10HE)           |  | CSV              | n/a                                                | [Univocity 2.9.0](https://github.com/uniVocity/univocity-parsers)   |      CARML in RML Test Cases  -----------------------  See the [RML implementation Report](https://rml.io/implementation-report/) for how CARML does in the [RML test cases](https://rml.io/test-cases/).    > Note: currently we've raised [issues](https://github.com/RMLio/rml-test-cases/issues?q=is%3Aissue+author%3Apmaria+) for for some of the test cases which we believe are incorrect, or have an adverse effect on mapping data.    Roadmap  -------  * Better support for large sources  * Improved join / parent triples map performance  * Support RDF store connections  * Split off provisional RDF Mapper as separate library """
Semantic web;https://github.com/edumbill/doap;"""# DOAP: Description Of A Project    DOAP is a project to create an XML/RDF vocabulary to describe software  projects, and in particular open source projects.    In addition to developing an RDF schema and examples, the DOAP project aims to  provide tool support in all the popular programming languages.    Homepage: [DOAP wiki](https://github.com/ewilderj/doap/wiki)    Maintainers are:    * Edd Wilder-James [ewilderj](https://github.com/ewilderj)  * Kjetil Kjernsmo [kjetilk](https://github.com/kjetilk)    ## Schema    The live version of the schema is at the namespace URI,  http://usefulinc.com/ns/doap    It will be kept synchronized with the `master` branch in the Github  repository. """
Semantic web;https://github.com/jsonld-java/jsonld-java;"""**JSONLD-Java is looking for a maintainer**    JSONLD-JAVA  ===========    This is a Java implementation of the [JSON-LD 1.0 specification](https://www.w3.org/TR/2014/REC-json-ld-20140116/) and the [JSON-LD-API 1.0 specification](https://www.w3.org/TR/2014/REC-json-ld-api-20140116/).    [![Build Status](https://travis-ci.org/jsonld-java/jsonld-java.svg?branch=master)](https://travis-ci.org/jsonld-java/jsonld-java) [![Coverage Status](https://coveralls.io/repos/jsonld-java/jsonld-java/badge.svg?branch=master)](https://coveralls.io/r/jsonld-java/jsonld-java?branch=master)    USAGE  =====    From Maven  ----------        <dependency>          <groupId>com.github.jsonld-java</groupId>          <artifactId>jsonld-java</artifactId>          <version>0.13.4</version>      </dependency>    Code example  ------------    ```java  // Open a valid json(-ld) input file  InputStream inputStream = new FileInputStream(""input.json"");  // Read the file into an Object (The type of this object will be a List, Map, String, Boolean,  // Number or null depending on the root object in the file).  Object jsonObject = JsonUtils.fromInputStream(inputStream);  // Create a context JSON map containing prefixes and definitions  Map context = new HashMap();  // Customise context...  // Create an instance of JsonLdOptions with the standard JSON-LD options  JsonLdOptions options = new JsonLdOptions();  // Customise options...  // Call whichever JSONLD function you want! (e.g. compact)  Object compact = JsonLdProcessor.compact(jsonObject, context, options);  // Print out the result (or don't, it's your call!)  System.out.println(JsonUtils.toPrettyString(compact));  ```    Processor options  -----------------    The Options specified by the [JSON-LD API Specification](https://json-ld.org/spec/latest/json-ld-api/#the-jsonldoptions-type) are accessible via the `com.github.jsonldjava.core.JsonLdOptions` class, and each `JsonLdProcessor.*` function has an optional input to take an instance of this class.    Controlling network traffic  ---------------------------    Parsing JSON-LD will normally follow any external `@context` declarations.  Loading these contexts from the network may in some cases not be desirable, or  might require additional proxy configuration or authentication.    JSONLD-Java uses the [Apache HTTPComponents Client](https://hc.apache.org/httpcomponents-client-ga/index.html) for these network connections,  based on the [SystemDefaultHttpClient](http://hc.apache.org/httpcomponents-client-ga/httpclient/apidocs/org/apache/http/impl/client/SystemDefaultHttpClient.html) which reads  standard Java properties like `http.proxyHost`.     The default HTTP Client is wrapped with a  [CachingHttpClient](https://hc.apache.org/httpcomponents-client-ga/httpclient-cache/apidocs/org/apache/http/impl/client/cache/CachingHttpClient.html) to provide a   small memory-based cache (1000 objects, max 128 kB each) of regularly accessed contexts.    ### Loading contexts from classpath    Your application might be parsing JSONLD documents which always use the same  external `@context` IRIs. Although the default HTTP cache (see above) will  avoid repeated downloading of the same contexts, your application would still  initially be vulnerable to network connectivity.    To bypass this issue, and even facilitate parsing of such documents in an  offline state, it is possible to provide a 'warmed' cache populated  from the classpath, e.g. loaded from a JAR.    In your application, simply add a resource `jarcache.json` to the root of your  classpath together with the JSON-LD contexts to embed. (Note that you might  have to recursively embed any nested contexts).    The syntax of `jarcache.json` is best explained by example:  ```javascript  [    {      ""Content-Location"": ""http://www.example.com/context"",      ""X-Classpath"": ""contexts/example.jsonld"",      ""Content-Type"": ""application/ld+json""    },    {      ""Content-Location"": ""http://data.example.net/other"",      ""X-Classpath"": ""contexts/other.jsonld"",      ""Content-Type"": ""application/ld+json""    }  ]  ```  (See also [core/src/test/resources/jarcache.json](core/src/test/resources/jarcache.json)).    This will mean that any JSON-LD document trying to import the `@context`   `http://www.example.com/context` will instead be given  `contexts/example.jsonld` loaded as a classpath resource.     The `X-Classpath` location is an IRI reference resolved relative to the  location of the `jarcache.json` - so if you have multiple JARs with a  `jarcache.json` each, then the `X-Classpath` will be resolved within the  corresponding JAR (minimizing any conflicts).    Additional HTTP headers (such as `Content-Type` above) can be included,  although these are generally ignored by JSONLD-Java.     Unless overridden in `jarcache.json`, this `Cache-Control` header is  automatically injected together with the current `Date`, meaning that the  resource loaded from the JAR will effectively never expire (the real HTTP  server will never be consulted by the Apache HTTP client):    ```  Date: Wed, 19 Mar 2014 13:25:08 GMT  Cache-Control: max-age=2147483647  ```    The mechanism for loading `jarcache.json` relies on   [Thread.currentThread().getContextClassLoader()](http://docs.oracle.com/javase/7/docs/api/java/lang/Thread.html#getContextClassLoader%28%29)  to locate resources from the classpath - if you are running on a command line,  within a framework (e.g. OSGi) or Servlet container (e.g. Tomcat) this should  normally be set correctly. If not, try:    ```java  ClassLoader oldContextCL = Thread.currentThread().getContextClassLoader();  try {       Thread.currentThread().setContextClassLoader(getClass().getClassLoader());      JsonLdProcessor.expand(input);   // or any other JsonLd operation  } finally {       // Restore, in case the current thread was doing something else      // with the context classloader before calling our method      Thread.currentThread().setContextClassLoader(oldContextCL);  }  ```    To disable all remote document fetching, when using the default DocumentLoader, set the   following Java System Property to ""true"" using:    ```java  System.setProperty(""com.github.jsonldjava.disallowRemoteContextLoading"", ""true"");  ```    You can also use the constant provided in DocumentLoader for the same purpose:    ```java  System.setProperty(DocumentLoader.DISALLOW_REMOTE_CONTEXT_LOADING, ""true"");  ```    Note that if you override DocumentLoader you should also support this setting for consistency and security.    ### Loading contexts from a string    Your application might be parsing JSONLD documents which reference external `@context` IRIs  that are not available as file URIs on the classpath. In this case, the `jarcache.json`  approach will not work. Instead you can inject the literal context file strings through  the `JsonLdOptions` object, as follows:    ```java  // Inject a context document into the options as a literal string  DocumentLoader dl = new DocumentLoader();  JsonLdOptions options = new JsonLdOptions();  // ... the contents of ""contexts/example.jsonld""  String jsonContext = ""{ \""@context\"": { ... } }"";  dl.addInjectedDoc(""http://www.example.com/context"",  jsonContext);  options.setDocumentLoader(dl);    InputStream inputStream = new FileInputStream(""input.json"");  Object jsonObject = JsonUtils.fromInputStream(inputStream);  Map context = new HashMap();  Object compact = JsonLdProcessor.compact(jsonObject, context, options);  System.out.println(JsonUtils.toPrettyString(compact));  ```    ### Customizing the Apache HttpClient    To customize the HTTP behaviour (e.g. to disable the cache or provide  [authentication  credentials)](https://hc.apache.org/httpcomponents-client-ga/tutorial/html/authentication.html),  you may want to create and configure your own `CloseableHttpClient` instance, which can  be passed to a `DocumentLoader` instance using `setHttpClient()`. This document  loader can then be inserted into `JsonLdOptions` using `setDocumentLoader()`  and passed as an argument to `JsonLdProcessor` arguments.      Example of inserting a credential provider (e.g. to load a `@context` protected  by HTTP Basic Auth):    ```java  Object input = JsonUtils.fromInputStream(..);  DocumentLoader documentLoader = new DocumentLoader();            CredentialsProvider credsProvider = new BasicCredentialsProvider();  credsProvider.setCredentials(          new AuthScope(""localhost"", 443),          new UsernamePasswordCredentials(""username"", ""password""));           CacheConfig cacheConfig = CacheConfig.custom().setMaxCacheEntries(1000)          .setMaxObjectSize(1024 * 128).build();    CloseableHttpClient httpClient = CachingHttpClientBuilder          .create()          // allow caching          .setCacheConfig(cacheConfig)          // Wrap the local JarCacheStorage around a BasicHttpCacheStorage          .setHttpCacheStorage(                  new JarCacheStorage(null, cacheConfig, new BasicHttpCacheStorage(                          cacheConfig)))....  		          // Add in the credentials provider          .setDefaultCredentialsProvider(credsProvider);          // When you are finished setting the properties, call build          .build();    documentLoader.setHttpClient(httpClient);            JsonLdOptions options = new JsonLdOptions();  options.setDocumentLoader(documentLoader);  // .. and any other options          Object rdf = JsonLdProcessor.toRDF(input, options);  ```    PLAYGROUND  ----------    The [jsonld-java-tools](https://github.com/jsonld-java/jsonld-java-tools) repository contains a simple application which provides command line access to JSON-LD functions    ### Initial clone and setup    ```bash  git clone git@github.com:jsonld-java/jsonld-java-tools.git  chmod +x ./jsonldplayground  ```    ### Usage    run the following to get usage details:    ```bash  ./jsonldplayground --help  ```    For Developers  --------------    ### Compiling & Packaging    `jsonld-java` uses maven to compile. From the base `jsonld-java` module run `mvn clean install` to install the jar into your local maven repository.    ### Running tests    ```bash  mvn test  ```    or    ```bash  mvn test -pl core  ```    to run only core package tests    ### Code style    The JSONLD-Java project uses custom Eclipse formatting and cleanup style guides to ensure that Pull Requests are fairly simple to merge.    These guides can be found in the /conf directory and can be installed in Eclipse using ""Properties>Java Code Style>Formatter"", followed by ""Properties>Java Code Style>Clean Up"" for each of the modules making up the JSONLD-Java project.    If you don't use Eclipse, then don't worry, your pull requests can be cleaned up by a repository maintainer prior to merging, but it makes the initial check easier if the modified code uses the conventions.    ### Submitting Pull Requests    Once you have made a change to fix a bug or add a new feature, you should commit and push the change to your fork.    Then, you can open a pull request to merge your change into the master branch of the main repository.    Implementation Reports for JSONLD-Java conformance with JSONLD-1.0  ==================================================================    The Implementation Reports documenting the conformance of JSONLD-Java with JSONLD-1.0 are available at:    https://github.com/jsonld-java/jsonld-java/tree/master/core/reports    ### Regenerating Implementation Report    Implementation Reports conforming to the [JSON-LD Implementation Report](http://json-ld.org/test-suite/reports/#instructions-for-submitting-implementation-reports) document can be regenerated using the following command:    ```bash  mvn test -pl core -Dtest=JsonLdProcessorTest -Dreport.format=<format>  ```    Current possible values for `<format>` include JSON-LD (`application/ld+json` or `jsonld`), NQuads (`text/plain`, `nquads`, `ntriples`, `nq` or `nt`) and Turtle (`text/turtle`, `turtle` or `ttl`). `*` can be used to generate reports in all available formats.    Integration of JSONLD-Java with other Java packages  ===================================================    This is the base package for JSONLD-Java. Integration with other Java packages are done in separate repositories.    Existing integrations  ---------------------    * [Eclipse RDF4J](https://github.com/eclipse/rdf4j)  * [Apache Jena](https://github.com/apache/jena/)  * [RDF2GO](https://github.com/jsonld-java/jsonld-java-rdf2go)  * [Apache Clerezza](https://github.com/jsonld-java/jsonld-java-clerezza)    Creating an integration module  ------------------------------    ### Create a repository for your module    Create a GitHub repository for your module under your user account, or have a JSONLD-Java maintainer create one in the jsonld-java organisation.    Create maven module  -------------------    ### Create pom.xml for your module    Here is the basic outline for what your module's pom.xml should look like    ```xml  <project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""      xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">      <parent>      <groupId>com.github.jsonld-java</groupId>      <artifactId>jsonld-java-parent</artifactId>      <version>0.13.4</version>    </parent>    <modelVersion>4.0.0</modelVersion>    <artifactId>jsonld-java-{your module}</artifactId>    <version>0.13.4-SNAPSHOT</version>    <name>JSONLD Java :: {your module name}</name>    <description>JSON-LD Java integration module for {RDF Library your module integrates}</description>    <packaging>jar</packaging>      <developers>      <developer>        <name>{YOU}</name>        <email>{YOUR EMAIL ADDRESS}</email>      </developer>    </developers>      <dependencies>      <dependency>        <groupId>${project.groupId}</groupId>        <artifactId>jsonld-java</artifactId>        <version>${project.version}</version>        <type>jar</type>         <scope>compile</scope>       </dependency>      <dependency>        <groupId>${project.groupId}</groupId>        <artifactId>jsonld-java</artifactId>        <version>${project.version}</version>        <type>test-jar</type>        <scope>test</scope>      </dependency>      <dependency>        <groupId>junit</groupId>        <artifactId>junit</artifactId>        <scope>test</scope>      </dependency>      <dependency>        <groupId>org.slf4j</groupId>        <artifactId>slf4j-jdk14</artifactId>        <scope>test</scope>      </dependency>    </dependencies>  </project>  ```    Make sure you edit the following:   * `project/artifactId` : set this to `jsonld-java-{module id}`, where `{module id}` usually represents the RDF library you're integrating (e.g. `jsonld-java-jena`)   * `project/name` : set this to `JSONLD Java :: {Module Name}`, wher `{module name}` is usually the name of the RDF library you're integrating.   * `project/description`   * `project/developers/developer/...` : Give youself credit by filling in the developer field. At least put your `<name>` in ([see here for all available options](http://maven.apache.org/pom.html#Developers)).   * `project/dependencies/...` : remember to add any dependencies your project needs    ### Import into your favorite editor    For Example: Follow the first few steps in the section above to import the whole `jsonld-java` project or only your new module into eclipse.    Create RDFParser Implementation  -------------------------------    The interface `com.github.jsonldjava.core.RDFParser` is used to parse RDF from the library into the JSONLD-Java internal RDF format. See the documentation in [`RDFParser.java`](../core/src/main/java/com/github/jsonldjava/core/RDFParser.java) for details on how to implement this interface.    Create TripleCallback Implementation  ------------------------------------    The interface `com.github.jsonldjava.core.JSONLDTripleCallback` is used to generate a representation of the JSON-LD input in the RDF library. See the documentation in [`JSONLDTripleCallback.java`](../core/src/main/java/com/github/jsonldjava/core/JSONLDTripleCallback.java) for details on how to implement this interface.    Using your Implementations  --------------------------    ### RDFParser    A JSONLD RDF parser is a class that can parse your frameworks' RDF model  and generate JSON-LD.    There are two ways to use your `RDFParser` implementation.    Register your parser with the `JSONLD` class and set `options.format` when you call `fromRDF`    ```java  JSONLD.registerRDFParser(""format/identifier"", new YourRDFParser());  Object jsonld = JSONLD.fromRDF(yourInput, new Options("""") {{ format = ""format/identifier"" }});  ```    or pass an instance of your `RDFParser` into the `fromRDF` function    ```java  Object jsonld = JSONLD.fromRDF(yourInput, new YourRDFParser());  ```    ### JSONLDTripleCallback    A JSONLD triple callback is a class that can populate your framework's  RDF model from JSON-LD - being called for each triple (technically quad).    Pass an instance of your `TripleCallback` to `JSONLD.toRDF`    ```java  Object yourOutput = JSONLD.toRDF(jsonld, new YourTripleCallback());  ```    Integrate with your framework  -----------------------------  Your framework might have its own system of readers and writers, where  you should register JSON-LD as a supported format. Remember that here  the ""parse"" direction is opposite of above, a 'reader' may be a class   that can parse JSON-LD and populate an RDF Graph.    Write Tests  -----------    It's helpful to have a test or two for your implementations to make sure they work and continue to work with future versions.    Write README.md  ---------------    Write a `README.md` file with instrutions on how to use your module.    Submit your module  ------------------    Once you've `commit`ted your code, and `push`ed it into your github fork you can issue a [Pull Request](https://help.github.com/articles/using-pull-requests) so that we can add a reference to your module in this README file.    Alternatively, we can also host your repository in the jsonld-java organisation to give it more visibility.    CHANGELOG  =========  ### 2021-12-13  * Release 0.13.4  * Switch test logging from log4j to logback (Patch by @ansell)  * Improve Travis CI build Performance (Patch by @YunLemon)    ### 2021-03-06  * Release 0.13.3  * Fix @type when subject and object are the same (Reported by @barthanssens, Patch by @umbreak)  * Ignore @base if remote context is not relative (Reported by @whikloj, Patch by @dr0i)  * Fix throwing recursive context inclusion (Patch by @umbreak)    ### 2020-09-24  * Release 0.13.2  * Fix Guava dependency shading (Reported by @ggrasso)  * Fix @context issues when using a remote context (Patch by @umbreak)  * Deprecate Context.serialize (Patch by @umbreak)    ### 2020-09-09  * Release 0.13.1  * Fix java.net.URI resolution (Reported by @ebremer and @afs, Patch by @dr0i)  * Shade Guava failureaccess module (Patch by @peacekeeper)  * Don't minimize Guava class shading (Patch by @elahrvivaz)  * Follow link headers to @context files (Patch by @dr0i and @fsteeg)    ### 2019-11-28  * Release 0.13.0  * Bump Jackson versions to latest for security updates (Patch by @afs)  * Do not canonicalise XSD Decimal typed values (Patch by @jhg023)  * Bump dependency and plugin versions    ### 2019-08-03  * Release 0.12.5  * Bump Jackson versions to latest for security updates (Patches by @afs)  * IRI resolution fixes (Patch by @fsteeg)    ### 2019-04-20  * Release 0.12.4  * Bump Jackson version to 2.9.8  * Add a regression test for a past framing bug  * Throw error on empty key  * Add regression tests for workarounds to Text/URL dual definitions  * Persist JsonLdOptions through normalize/toRDF    ### 2018-11-24  * Release 0.12.3  * Fix NaN/Inf/-Inf raw value types on conversion to RDF  * Added fix for wrong rdf:type to @type conversion (Path by @umbreak)  * Open up Context.getTypeMapping and Context.getLanguageMapping for reuse    ### 2018-11-03  * W3c json ld syntax 34 allow container set on aliased type (Patch by @dr0i)  * Release 0.12.2    ### 2018-09-05  * handle omit graph flag (Patch by @eroux)  * Release 0.12.1  * Make pruneBlankNodeIdentifiers false by default in 1.0 mode and always true in 1.1 mode (Patch by @eroux)  * Fix issue with blank node identifier pruning when @id is aliased (Patch by @eroux)  * Allow wildcard {} for @id in framing (Patch by @eroux)    ### 2018-07-07  * Fix tests setup for schema.org with HttpURLConnection that break because of the inability of HttpURLConnection to redirect from HTTP to HTTPS    ### 2018-04-08  * Release 0.12.0  * Encapsulate RemoteDocument and make it immutable    ### 2018-04-03  * Fix performance issue caused by not caching schema.org and others that use ``Cache-Control: private`` (Patch by @HansBrende)  * Cache classpath scans for jarcache.json to fix a similar performance issue  * Add internal shaded dependency on Google Guava to use maintained soft and weak reference maps rather than adhoc versions  * Make JsonLdError a RuntimeException to improve its use in closures  * Bump minor version to 0.12 to reflect the API incompatibility caused by JsonLdError and protected field change and hiding in JarCacheStorage    ### 2018-01-25  * Fix resource leak in JsonUtils.fromURL on unsuccessful requests (Patch by @plaplaige)    ### 2017-11-15  * Ignore UTF BOM (Patch by @christopher-johnson)    ### 2017-08-26  * Release 0.11.1  * Fix @embed:@always support (Patch by @dr0i)    ### 2017-08-24  * Release 0.11.0    ### 2017-08-22  * Add implicit ""flag only"" subframe to fix incomplete list recursion (Patch by @christopher-johnson)  * Support pruneBlankNodeIdentifiers framing option in 1.1 mode (Patch by @fsteeg and @eroux)  * Support new @embed values (Patch by @eroux)    ### 2017-07-11  * Add injection of contexts directly into DocumentLoader (Patch by @ryankenney)  * Fix N-Quads content type (Patch by @NicolasRouquette)  * Add JsonUtils.fromJsonParser (Patch by @dschulten)    ### 2017-02-16  * Make literals compare consistently (Patch by @stain)  * Release 0.10.0    ### 2017-01-09  * Propagate causes for JsonLdError instances where they were caused by other Exceptions  * Remove schema.org hack as it appears to work again now...  * Remove deprecated and unused APIs  * Bump version to 0.10.0-SNAPSHOT per the removed/changed APIs    ### 2016-12-23  * Release 0.9.0  * Fixes schema.org support that is broken with Apache HTTP Client but works with java.net.URL    ### 2016-05-20  * Fix reported NPE in JsonLdApi.removeDependents    ### 2016-05-18  * Release 0.8.3  * Fix @base in remote contexts corrupting the local context    ### 2016-04-23  * Support @default inside of sets for framing    ### 2016-02-29  * Fix ConcurrentModificationException in the implementation of the Framing API    ### 2016-02-17  * Re-release version 0.8.2 with the refactoring work actually in it. 0.8.1 is identical in functionality to 0.8.0  * Release version 0.8.1  * Refactor JSONUtils and DocumentLoader to move most of the static logic into JSONUtils, and deprecate the DocumentLoader versions    ### 2016-02-10  * Release version 0.8.0    ### 2015-11-19  * Replace deprecated HTTPClient code with the new builder pattern  * Chain JarCacheStorage to any other HttpCacheStorage to simplify the way local caching is performed  * Bump version to 0.8.0-SNAPSHOT as some interface method parameters changed, particularly, DocumentLoader.setHttpClient changed to require CloseableHttpClient that was introduced in HttpClient-4.3    ### 2015-11-16  * Bump dependencies to latest versions, particularly HTTPClient that is seeing more use on 4.5/4.4 than the 4.2 series that we have used so far  * Performance improvements for serialisation to N-Quads by replacing string append and replace with StringBuilder  * Support setting a system property, com.github.jsonldjava.disallowRemoteContextLoading, to ""true"" to disable remote context loading.    ### 2015-09-30  * Release 0.7.0    ### 2015-09-27  * Move Tools, Clerezza and RDF2GO modules out to separate repositories. The Tools repository had a circular build dependency with Sesame, while the other modules are best located and managed in separate repositories    ### 2015-08-25  * Remove Sesame-2.7 module in favour of sesame-rio-jsonld for Sesame-2.8 and 4.0  * Fix bug where parsing did not fail if content was present after the end of a full JSON top level element    ### 2015-03-12  * Compact context arrays if they contain a single element during compaction  * Bump to Sesame-2.7.15    ### 2015-03-01  * Use jopt-simple for the playground cli to simplify the coding and improve error messages  * Allow RDF parsing and writing using all of the available Sesame Rio parsers through the playground cli  * Make the httpclient dependency OSGi compliant    ### 2014-12-31  * Fix locale sensitive serialisation of XSD double/decimal typed literals to always be Locale.US  * Bump to Sesame-2.7.14  * Bump to Clerezza-0.14    ### 2014-11-14  * Fix identification of integer, boolean, and decimal in RDF-JSONLD with useNativeTypes  * Release 0.5.1    ### 2014-10-29  * Add OSGi metadata to Jar files  * Bump to Sesame-2.7.13    ### 2014-07-14  * Release version 0.5.0  * Fix Jackson parse exceptions being propagated through Sesame without wrapping as RDFParseExceptions    ### 2014-07-02  * Fix use of Java-7 API so we are still Java-6 compatible  * Ensure that Sesame RDFHandler endRDF and startRDF are called in SesameTripleCallback    ### 2014-06-30  * Release version 0.4.2  * Bump to Sesame-2.7.12  * Remove Jena integration module, as it is now maintained by Jena team in their repository    ### 2014-04-22  * Release version 0.4  * Bump to Sesame-2.7.11  * Bump to Jackson-2.3.3  * Bump to Jena-2.11.1    ### 2014-03-26  * Bump RDF2GO to version 5.0.0    ### 2014-03-24  * Allow loading remote @context from bundled JAR cache  * Support JSON array in @context with toRDF   * Avoid exception on @context with default @language and unmapped key    ### 2014-02-24  * Javadoc some core classes, JsonLdProcessor, JsonLdApi, and JsonUtils  * Rename some core classes for consistency, particularly JSONUtils to JsonUtils and JsonLdTripleCallback  * Fix for a Context constructor that wasn't taking base into account    ### 2014-02-20  * Fix JsonLdApi mapping options in framing algorithm (Thanks Scott Blomquist @sblom)    ### 2014-02-06    * Release version 0.3  * Bump to Sesame-2.7.10  * Fix Jena module to use new API    ### 2014-01-29    * Updated to final Recommendation  * Namespaces supported by Sesame integration module  * Initial implementation of remote document loading  * Bump to Jackson-2.3.1    ### 2013-11-22    * updated jena writer    ### 2013-11-07    * Integration packages renamed com.github.jsonldjava.sesame,     com.github.jsonldjava.jena etc. (Issue #76)      ### 2013-10-07    * Matched class names to Spec   - Renamed `JSONLDException` to `JsonLdError`   - Renamed `JSONLDProcessor` to `JsonLdApi`   - Renamed `JSONLD` to `JsonLdProcessor`   - Renamed `ActiveContext` to `Context`   - Renamed `Options` to `JsonLdOptions`  * All context related utility functions moved to be members of the `Context` class    ### 2013-09-30  * Fixed JSON-LD to Jena to handle of BNodes    ### 2013-09-02    * Add RDF2Go integration  * Bump Sesame and Clerezza dependency versions    ### 2013-06-18    * Bump to version 0.2  * Updated Turtle integration  * Added Caching of contexts loaded from URI  * Added source formatting eclipse config  * Fixed up seasame integration package names  * Replaced depreciated Jackson code    ### 2013-05-19    * Added Turtle RDFParser and TripleCallback  * Changed Maven groupIds to `com.github.jsonld-java` to match github domain.  * Released version 0.1    ### 2013-05-16    * Updated core code to match [JSON-LD 1.0 Processing Algorithms and API / W3C Editor's Draft 14 May 2013](http://json-ld.org/spec/latest/json-ld-api/)  * Deprecated JSONLDSerializer in favor of the RDFParser interface to better represent the purpose of the interface and better fit in with the updated core code.  * Updated the JSONLDTripleCallback to better fit with the updated code.  * Updated the Playground tool to support updated core code.    ### 2013-05-07    * Changed base package names to com.github.jsonldjava  * Reverted version to 0.1-SNAPSHOT to allow version incrementing pre 1.0 while allowing a 1.0 release when the json-ld spec is finalised.  * Turned JSONLDTripleCallback into an interface.    ### 2013-04-18    * Updated to Sesame 2.7.0, Jena 2.10.0, Jackson 2.1.4  * Fixing a character encoding issue in the JSONLDProcessorTests  * Bumping to 1.0.1 to reflect dependency changes    ### 2012-10-30    * Brought the implementation up to date with the reference implementation (minus the normalization stuff)  * Changed entry point for the functions to the static functions in the JSONLD class  * Changed the JSONLDSerializer to an abstract class, requiring the implementation of a ""parse"" function. The JSONLDSerializer is now passed to the JSONLD.fromRDF function.  * Added JSONLDProcessingError class to handle errors more efficiently      Considerations for 1.0 release / optimisations  =========    * The `Context` class is a `Map` and many of the options are stored as values of the map. These could be made into variables, whice should speed things up a bit (the same with the termDefinitions variable inside the Context).  * some sort of document loader interface (with a mockup for testing) is required   """
Semantic web;https://github.com/semarglproject/semargl;"""Welcome to the home of Semargl!  ===============================    Semargl is a modular framework for crawling [linked data](http://en.wikipedia.org/wiki/Linked_data)  from structured documents. The main goal of the project is to provide lightweight  and performant tool without excess dependencies.    At this moment Semargl offers high-performant streaming parsers for RDF/XML,  [RDFa](http://en.wikipedia.org/wiki/Rdfa), N-Triples, JSON-LD,  streaming serializers for Turtle, NTriples, NQuads and integration with Jena, Clerezza and Sesame.    Small memory footprint, and CPU requirements allow framework to be embedded in any system.  It runs seamlessly on Android and GAE.    You can check some framework capabilities via [RDFa parser demo](http://demo.semarglproject.org).    [![Maven Central](https://img.shields.io/maven-central/v/org.semarglproject/semargl-core.svg?style=flat-square)](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.semarglproject%22%20semargl)  [![Build Status](https://img.shields.io/travis/semarglproject/semargl/master.svg?style=flat-square)](https://travis-ci.org/semarglproject/semargl)  [![Coverage Status](https://img.shields.io/coveralls/semarglproject/semargl.svg?style=flat-square)](https://coveralls.io/r/semarglproject/semargl?branch=master)    Why use Semargl?  ================    Lightweight  -----------    Semargl’s code is small and simple to understand. It has no external dependencies and  it will never [read a mail](http://en.wikipedia.org/wiki/Zawinski's_law_of_software_envelopment).  Internally it operates with a raw strings and creates as few objects as possible,  so your Android or GAE applications will be happy.    Standard conformant  -------------------    All parsers and serializers fully support  [corresponding W3C specifications](http://semarglproject.org/conformance.html) and test suites.    Dead Simple  -----------    No jokes!    ```xml  <dependency>      <groupId>org.semarglproject</groupId>      <artifactId>semargl-rdfa</artifactId>      <version>0.7</version>  </dependency>  ```    ```java  // just init triple store you want  MGraph graph = ... // Clerezza calls  // create processing pipe  StreamProcessor sp = new StreamProcessor(NTriplesParser.connect(ClerezzaSink.connect(graph));  // and run it  sp.process(file, docUri);  ```    If you want to use Semargl as a standalone framework, you can find useful internal  serializers and easily extendable API.    Build  =====    To build framework just run `mvn clean install`. RDFa tests require direct Internet connection. """
Semantic web;https://github.com/AKSW/rocker;"""## ROCKER: A Refinement Operator for Key Discovery ##    [![Build Status](http://ci.aksw.org/jenkins/buildStatus/icon?job=Rocker)](http://ci.aksw.org/jenkins/view/All/job/Rocker/)    ### Demo ###    A demo of ROCKER for Data Quality is running at http://rocker.aksw.org. It offers a web interface with accessible APIs. For computations on large datasets, please follow the guide below.    ### Run from terminal ###    First, download the [full jar package](https://github.com/AKSW/rocker/releases/download/v1.2.1/rocker-1.2.1-full.jar), which also contains all required dependencies. Datasets are available here:    OAEI Benchmark 2011 (artificial data)    * [OAEI_2011_Restaurant_1.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/OAEI_2011_Restaurant_1.nt.gz)  * [OAEI_2011_Restaurant_2.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/OAEI_2011_Restaurant_2.nt.gz)    DBpedia 3.9 (real data)    * [album.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/album.nt.gz)  * [animal.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/animal.nt.gz)  * [architecturalStruture.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/architecturalStruture.nt.gz)  * [artist.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/artist.nt.gz)  * [careerstation.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/careerstation.nt.gz)  * [musicalWork.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/musicalWork.nt.gz)  * [organisationMember.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/organisationMember.nt.gz)  * [personFunction.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/personFunction.nt.gz)  * [soccerplayer.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/soccerplayer.nt.gz)  * [village.nt.gz](https://bitbucket.org/mommi84/rocker-servlet/downloads/village.nt.gz)    To run ROCKER:    ```  java -Xmx8g -jar rocker-1.2.1-full.jar <dataset name> <dataset path with protocol> <class name> <find one key> <fast search> <alpha threshold>  ```    Example:    ```  java -Xmx8g -jar rocker-1.2.1-full.jar ""restaurant_1"" ""file:///home/rocker/OAEI_2011_Restaurant_1.nt"" ""http://www.okkam.org/ontology_restaurant1.owl#Restaurant"" false true 1.0  ```    We recommend to run your experiments on a machine with at least 8 GB of RAM.    ### Maven    ```xml  <repository>      <id>maven.aksw.internal</id>      <name>University Leipzig, AKSW Maven2 Repository</name>      <url>http://maven.aksw.org/archiva/repository/internal</url>  </repository>  ...  <dependency>      <groupId>org.aksw.rocker</groupId>      <artifactId>rocker</artifactId>      <version>1.3.1</version>  </dependency>  ```    ### Java library ###    You may also download the [Java library](https://github.com/AKSW/rocker/releases/download/v1.2.1/rocker-1.2.1.jar) without dependencies.    ### Basic usage ###    ```java  Rocker r = null;  r = new Rocker(""restaurant_1"", ""file:///home/rocker/OAEI_2011_Restaurant_1.nt"",          ""http://www.okkam.org/ontology_restaurant1.owl#Restaurant"", false, true, 1.0);  r.run();  Set<CandidateNode> results = r.getKeys();  ```    ### Citing ROCKER ###    Please refer to the paper *T. Soru, E. Marx, A.-C. Ngonga Ngomo, ""ROCKER: A Refinement Operator for Key Discovery""*, in proceedings of the 24th International Conference on World Wide Web, WWW 2015. [[PDF](http://svn.aksw.org/papers/2015/WWW_Rocker/public.pdf)] [[ACM](http://dl.acm.org/citation.cfm?id=2741642)]    ```  @inproceedings{Soru:2015:RRO:2736277.2741642,   author = {Soru, Tommaso and Marx, Edgard and {Ngonga Ngomo}, Axel-Cyrille},   title = {ROCKER: A Refinement Operator for Key Discovery},   booktitle = {Proceedings of the 24th International Conference on World Wide Web},   series = {WWW '15},   year = {2015},   isbn = {978-1-4503-3469-3},   location = {Florence, Italy},   pages = {1025--1033},   numpages = {9},   url = {http://doi.acm.org/10.1145/2736277.2741642},   doi = {10.1145/2736277.2741642},   acmid = {2741642},   publisher = {ACM},   address = {New York, NY, USA},   keywords = {key discovery, link discovery, linked data, refinement operators, semantic web},  }  ``` """
Semantic web;https://github.com/AKSW/xodx;"""xodx  ====    This is an implementation of the basic functionalities of a DSSN Provider:  * [Semantic Pingback](http://aksw.org/Projects/SemanticPingback) for Friending  * [Pubsubhubbub](http://code.google.com/p/pubsubhubbub/) (PuSH) for notification along the edges    It is written in PHP and utilizes the Zend Framework and the [Erfurt Framework](http://erfurt-framework.org/)    Installation  ------------  You need a webserver (tested with Apache, but I hope it also runs with nginx and lighttd) and a database backend which is supported by Erfurt (MySQL and Virtuoso).    ### Erfurt  Run `git submodules init` and `git submodules update` to clone Erfurt.    Take one of the prepared `config.ini-*` files in `xodx/libraries/Erfurt/library/Erfurt`, copy it to `config.ini` and configure it according to your system setup.    ### Zend  You have to place a copy of the Zend framework library into `libraries/Zend/` you can do this by doing the following things (replace `${ZENDVERSION}` e.g. with `1.12.0`):        wget http://packages.zendframework.com/releases/ZendFramework-${ZENDVERSION}/ZendFramework-${ZENDVERSION}-minimal.tar.gz      tar xzf ZendFramework-${ZENDVERSION}-minimal.tar.gz      mv ZendFramework-${ZENDVERSION}-minimal/library/Zend libraries      rm -rf ZendFramework-${ZENDVERSION}-minimal.tar.gz ZendFramework-${ZENDVERSION}-minimal    ### JavaScript  You have to add [twitter bootstrap](http://twitter.github.com/bootstrap/) and [jquery](http://jquery.com/) to the `resources` directory.    Code Conventions  ----------------  Currently, this project is developed using [OntoWiki's coding standard](http://code.google.com/p/ontowiki/wiki/CodingStandard). """
Semantic web;https://github.com/Omer/vim-sparql;"""# VIM-SPARQL    This is an unofficial fork of [http://www.vim.org/scripts/script.php?script_id=1755](http://www.vim.org/scripts/script.php?script_id=1755). This fork simply adds a filetype detection.    This is a first stab at syntax highlighting for SPARQL.  Very useful if you write SPARQL in, for example, `rq` files that you can use with the `roqet` command line query processor.     More information on SPARQL at [http://www.w3.org/TR/rdf-sparql-query/](http://www.w3.org/TR/rdf-sparql-query/).    Original by Jeroen Pulles, 2007-01-07 """
Semantic web;https://github.com/innoq/iqvoc;"""# iQvoc    [![Gem Version](https://badge.fury.io/rb/iqvoc.png)](http://badge.fury.io/rb/iqvoc)  ![CI](https://github.com/innoq/iqvoc/workflows/CI/badge.svg?branch=master)  [![Code Climate](https://codeclimate.com/github/innoq/iqvoc.png)](https://codeclimate.com/github/innoq/iqvoc)    iQvoc is a vocabulary management tool that combines easy-to-use human interfaces  with Semantic Web interoperability.    iQvoc supports vocabularies that are common to many knowledge organization  systems, such as:    * Thesauri  * Taxonomies  * Classification schemes  * Subject heading systems    iQvoc provides comprehensive functionality for all aspects of managing such  vocabularies:    * import of existing vocabularies from a SKOS representation  * multilingual display and navigation in any Web browser  * editorial features for registered users  * publishing the vocabulary in the Semantic Web    iQvoc is built with state-of-the-art technology and can be easily customized according to user's needs.    ## Demo    You can try out iQvoc right now! In our [sandbox](http://try.iqvoc.net/) you can play around with the public views.  If you like to test the collaborative functions simply [request](mailto:iqvoc@innoq.com) your personal demo account.    ## Setup    ### Heroku    You can easily setup your iQvoc instance in under 5 minutes, we wanted to make  this process really easy. In order to deploy to heroku you need to have an  account and [heroku toolbelt](https://toolbelt.heroku.com) installed.    ```  $ bundle install  $ heroku create  $ heroku config:add HEROKU=true RAILS_ENV=heroku RACK_ENV=heroku SECRET_KEY_BASE=$(bundle exec rake secret)  $ git push heroku master  $ heroku run rake db:migrate  $ heroku run rake db:seed  $ heroku restart  ```    `heroku open` opens your app in the browser.    Remember to visit the Users section and change the default passwords!    ### Docker    If you want to try iQvoc using Docker just clone this repository and run:    ```  $ docker-compose up  ```    This Setup uses Postgres as a database. Please make sure that your Docker daemon is running and that you have docker-compose installed. User credentials can be found in https://github.com/innoq/iqvoc/blob/master/db/seeds.rb.    ### Custom    We recommend running [iQvoc as a Rails engine](https://github.com/innoq/iqvoc/wiki/iQvoc-as-a-Rails-Engine).  Running the cloned source code is possible but any modifications would require a  fork.    1. Configure your database via `config/database.template.yml`.     Don't forget to rename it to `database.yml`  2. Run `bundle install`  3. Run `bundle exec rake db:create` to create the database  4. Create the necessary tables by running `rake db:migrate`  5. Load some base data by running `rake db:seed`  6. Make sure you have got `config/secrets.yml` in place  7. Install nodejs dependencies for compiling assets: `npm install` (make sure nodejs is installed)  8. Compile assets using: `npm run compile` (or `npm run watch` to compile and listen for changes in development)  8. Boot up the app using `bundle exec rails s` (or `passenger start` if you use passenger)  9. Log in with ""admin@iqvoc"" / ""admin123"" or ""demo@iqvoc"" / ""cooluri123"" (cf. step #5)  10. Visit the Users section and change the default passwords    ## Background Jobs    Note that some features like ""Import"" and ""Export"" exposed in the Web UI store  their workload as jobs. You can either issue a job worker that runs continuously  and watches for new jobs via    ```  $ rake jobs:work  ```    or process jobs in a one-off way (in development or via cron):    ```  $ rake jobs:workoff  ```    ## Compatibility    iQvoc is fully compatible with Ruby 2.6.    ## Customization    There are many hooks providing support for your own classes and configuration.  The core app also works as a Rails Engine. The config residing in `lib/iqvoc.rb`  provides a basic overview of the possibilities.    ## Documentation    Documentation resources can be found in the [wiki](https://github.com/innoq/iqvoc/wiki).    iQvoc provides an (inline) API documentation which can be found on `APP_URI/apidoc`. Check out our sandbox to see it in action: http://try.iqvoc.net/apidoc/    ## Related projects    We provide several extensions to add additional features to iQvoc:    * [iqvoc_skosxl](https://github.com/innoq/iqvoc_skosxl): SKOS-XL extension for iQvoc  * [iqvoc_compound_forms](https://github.com/innoq/iqvoc_compound_forms): Compound labels for iQvoc  * [iqvoc_inflectionals](https://github.com/innoq/iqvoc_inflectionals): Inflectionals for iQvoc  * [iqvoc_similar_terms](https://github.com/innoq/iqvoc_similar_terms):  iQvoc engine for similar terms    ## Versioning    Releases will follow a semantic versioning format:        <major>.<minor>.<patch>    For more information on SemVer, visit http://semver.org/.    ## Contributing    If you want to help out there are several options:    - Found a bug? Just create an issue on the    [GitHub Issue tracker](https://github.com/innoq/iqvoc/issues) and/or submit a    patch by initiating a pull request  - You're welcome to fix bugs listed under    [Issues](https://github.com/innoq/iqvoc/issues)  - Proposal, discussion and implementation of new features on our mailing list    [iqvoc@lists.innoq.com] or on the issue tracker    If you make changes to existing code please make sure that the test suite stays  green. Please include tests to your additional contributions.    Tests can be run via `bundle exec rake test`. We're using Cuprite for  integration tests with JavaScript support.    ## Maintainer & Contributors    iQvoc was originally created and is being maintained by [innoQ Deutschland GmbH](http://innoq.com).    * Robert Glaser ([mrreynolds](http://github.com/mrreynolds))  * Till Schulte-Coerne ([tillsc](http://github.com/tillsc))  * Frederik Dohr ([FND](http://github.com/FND))  * Marc Jansing ([mjansing](http://github.com/mjansing))    ## License    Copyright 2022 [innoQ Deutschland GmbH](https://www.innoq.com).    Licensed under the Apache License, Version 2.0. """
Semantic web;https://github.com/lorenae/qb4olap-tools;"""QB4OLAP tools    [QB4OLAP] (http://lorenae.github.io/qb4olap/) is a RDFS Vocabulary for Business Intelligence over Linked Data.    In this project we include a set of prototypes that conform a showcase of what can be done using this vocabulary, focusing on exploring and querying.    For querying we propose a high level OLAP language, called QL, which consists on a set of well-known operators: `rollup, drilldown, slice,` and `dice`. Using the cube metadata, also written using QB4OLAP, we automatically generate SPARQL queries to implement sequences of QL operations.      INSTALLATION    1) download [zip] (https://github.com/lorenae/qb4olap-tools/archive/master.zip) file and extract it     2) install [nodejs] (https://nodejs.org/en/)    3) open console, go to qb4olap-tools directory (obtained in step 1)    4) install npm packages needed by the application  (`npm install` command, which install the packages liste in packages.json)    5) run the application  (`node qb4olap.js`)!!   """
Semantic web;https://github.com/dice-group/DEER;"""# DEER  [![Build Status](https://github.com/dice-group/deer/actions/workflows/run-tests.yml/badge.svg?branch=master&event=push)](https://github.com/dice-group/deer/actions/workflows/run-tests.yml)  [![DockerHub](https://badgen.net/badge/dockerhub/dicegroup%2Fdeer/blue?icon=docker)](https://hub.docker.com/r/dicegroup/deer)  [![GNU Affero General Public License v3.0](https://badgen.net/badge/license/GNU_Affero_General_Public_License_v3.0/orange)](./LICENSE)  ![Java 11+](https://badgen.net/badge/java/11+/gray?icon=maven)    <div style=""text-align: center;"">    ![LOGO](https://raw.githubusercontent.com/dice-group/deer/master/docs/_media/deer_logo.svg)  </div>    The RDF Dataset Enrichment Framework (DEER), is a modular, extensible software system for efficient  computation of arbitrary operations on RDF datasets.    The atomic operations involved in this process, dubbed *enrichment operators*,   are configured using RDF, making DEER a native semantic web citizen.    Enrichment operators are mapped to nodes of a directed acyclic graphs to build complex enrichment  models, in which the connections between two nodes represent intermediary datasets.    ## Running DEER    To bundle DEER as a single jar file, do    ```bash  mvn clean package shade:shade -Dmaven.test.skip=true  ```    Then execute it using    ```bash  java -jar deer-cli/target/deer-cli-${current-version}.jar path_to_config.ttl  ```    ## Using Docker    The Docker image declares two volumes:  - /plugins - this is where plugins are dynamically loaded from  - /data - this is where configuration as well as input/output data will reside    For running DEER server in Docker, we expose port 8080.  The image accepts the same arguments as the deer-cli.jar, i.e. to run a configuration at `./my-configuration`:    ```bash  docker run -it --rm \     -v $(pwd)/plugins:/plugins \     -v $(pwd):/data dicegroup/deer:latest \     /data/my-configuration.ttl  ```    To run DEER server:    ```bash  docker run -it --rm \     -v $(pwd)/plugins:/plugins \     -p 8080:8080 \     -s  ```    ## Maven    ```xml  <dependencies>    <dependency>      <groupId>org.aksw.deer</groupId>      <artifactId>deer-core</artifactId>      <version>2.3.1</version>    </dependency>  </dependencies>  ```  ```xml  <repositories>   <repository>        <id>maven.aksw.internal</id>        <name>University Leipzig, AKSW Maven2 Internal Repository</name>        <url>http://maven.aksw.org/repository/internal/</url>      </repository>        <repository>        <id>maven.aksw.snapshots</id>        <name>University Leipzig, AKSW Maven2 Snapshot Repository</name>        <url>http://maven.aksw.org/repository/snapshots/</url>      </repository>  </repositories>  ```      ## Documentation    For more detailed information about how to run or extend DEER, please read the  [manual](https://dice-group.github.io/deer/) and consult the  [Javadoc](https://dice-group.github.io/deer/javadoc/)    ## Developers    ### Release new version    ```bash  ./release ${new-version} ${new-snapshot-version}  ```"""
Semantic web;https://github.com/kbss-cvut/jb4jsonld;"""# Java Binding for JSON-LD    [![Build Status](https://kbss.felk.cvut.cz/jenkins/buildStatus/icon?job=jaxb-jsonld)](https://kbss.felk.cvut.cz/jenkins/job/jaxb-jsonld)    Java Binding for JSON-LD (JB4JSON-LD) is a simple library for serialization of Java objects into JSON-LD and vice versa.    Note that this is the core, abstract implementation. For actual usage, a binding like [https://github.com/kbss-cvut/jb4jsonld-jackson](https://github.com/kbss-cvut/jb4jsonld-jackson)  has to be used.      ## Usage    JB4JSON-LD is based on annotations from [JOPA](https://github.com/kbss-cvut/jopa), which enable POJO attributes  to be mapped to ontological constructs (i.e. to object, data or annotation properties) and Java classes to ontological  classes.    Use `@OWLDataProperty` to annotate data fields and `@OWLObjectProperty` to annotate fields referencing other mapped entities.    See [https://github.com/kbss-cvut/jopa-examples/tree/master/jsonld](https://github.com/kbss-cvut/jopa-examples/tree/master/jsonld) for  an executable example of JB4JSON-LD in action (together with Spring and Jackson).      ## Example    ### Java    ```Java  @OWLClass(iri = ""http://onto.fel.cvut.cz/ontologies/ufo/Person"")  public class User {        @Id      public URI uri;        @OWLDataProperty(iri = ""http://xmlns.com/foaf/0.1/firstName"")      private String firstName;        @OWLDataProperty(iri = ""http://xmlns.com/foaf/0.1/lastName"")      private String lastName;            @OWLDataProperty(iri = ""http://xmlns.com/foaf/0.1/accountName"")      private String username;        @OWLDataProperty(iri = ""http://krizik.felk.cvut.cz/ontologies/jb4jsonld/role"")      private Role role;  // Role is an enum        @Properties      private Map<String, Set<String>> properties;            // Getters and setters follow  }  ```    ### JSON-LD    ```JSON  {    ""@context"": {      ""firstName"": ""http://xmlns.com/foaf/0.1/firstName"",      ""lastName"": ""http://xmlns.com/foaf/0.1/lastName"",      ""accountName"": ""http://xmlns.com/foaf/0.1/accountName"",      ""isAdmin"": ""http://krizik.felk.cvut.cz/ontologies/jb4jsonld/isAdmin"",      ""role"": ""http://krizik.felk.cvut.cz/ontologies/jb4jsonld/role""    },    ""@id"": ""http://krizik.felk.cvut.cz/ontologies/jb4jsonld#Catherine+Halsey"",    ""@type"": [      ""http://onto.fel.cvut.cz/ontologies/ufo/Person"",      ""http://krizik.felk.cvut.cz/ontologies/jb4jsonld/User"",      ""http://onto.fel.cvut.cz/ontologies/ufo/Agent""    ],    ""isAdmin"": true,    ""accountName"": ""halsey@unsc.org"",    ""firstName"": ""Catherine"",    ""lastName"": ""Halsey"",    ""role"": ""USER""  }  ```    ## Configuration    Parameter | Default value | Explanation  ----------|---------------|-----------  `ignoreUnknownProperties` | `false` | Whether to ignore unknown properties when deserializing JSON-LD. Default behavior throws an exception.  `scanPackage` | `""""` | Package in which the library should look for mapped classes. The scan is important for support for polymorphism in object deserialization.  It is highly recommended to specify this value, otherwise the library will attempt to load and scan all classes on the classpath.  `requireId` | `false` | Whether to require an identifier when serializing an object. If set to `true` and no identifier is found (either there is no `@Id` field or its value is `null`), an exception will be thrown. By default a blank node identifier is generated if no id is present.  `assumeTargetType` | `false` | Whether to allow assuming target type in case the JSON-LD object does not contain types (`@type`). If set to `true`, the provided Java type (deserialization invocation argument, field type) will be used as target type.  `enableOptimisticTargetTypeResolution` | `false` | Whether to enable optimistic target type resolution. If enabled, this allows to pick a target type even if there are multiple matching classes (which would normally end with an `AmbiguousTargetTypeException`).  `preferSuperclass` | `false` | Allows to further specify optimistic target type resolution. By default, any of the target classes may be selected. Setting this to `true` will make the resolver attempt to select a superclass of the matching classes (if it is also in the target set).     See `cz.cvut.kbss.jsonld.ConfigParam`.    ## Documentation    Documentation is on the [Wiki](https://github.com/kbss-cvut/jb4jsonld/wiki). API Javadoc is also [available](https://kbss.felk.cvut.cz/jenkins/view/Java%20Tools/job/jaxb-jsonld/javadoc/).    ## Getting JB4JSON-LD    There are two ways to get JB4JSON-LD:    * Clone repository/download zip and build it with Maven,  * Use a [Maven dependency](http://search.maven.org/#search%7Cga%7C1%7Ccz.cvut.kbss.jsonld):    ```XML  <dependency>      <groupId>cz.cvut.kbss.jsonld</groupId>      <artifactId>jb4jsonld</artifactId>  </dependency>  ```    Note that you will most likely need an integration with a JSON-serialization library like [JB4JSON-LD-Jackson](https://github.com/kbss-cvut/jb4jsonld-jackson).      ## License    LGPLv3 """
Semantic web;https://github.com/daedafusion/cyber-ontology;"""#Cyber Intelligence Ontology    <a rel=""license"" href=""http://creativecommons.org/licenses/by-sa/4.0/""><img alt=""Creative Commons License"" style=""border-width:0"" src=""https://i.creativecommons.org/l/by-sa/4.0/88x31.png"" /></a><br /><span xmlns:dct=""http://purl.org/dc/terms/"" href=""http://purl.org/dc/dcmitype/Text"" property=""dct:title"" rel=""dct:type"">Cyber</span> is licensed under a <a rel=""license"" href=""http://creativecommons.org/licenses/by-sa/4.0/"">Creative Commons Attribution-ShareAlike 4.0 International License</a>. """
Semantic web;https://github.com/streamreasoning/TripleWave;"""# TripleWave    All the information regarding how to install, run and customize TripleWave can be found [here](http://streamreasoning.github.io/TripleWave/) """
Semantic web;https://github.com/AKSW/Sparqlify;"""# Sparqlify SPARQL->SQL rewriter  [![Build Status](http://ci.aksw.org/jenkins/job/Sparqlify/badge/icon)](http://ci.aksw.org/jenkins/job/Sparqlify/)      ## Introduction    Sparqlify is a scalable SPARQL-SQL rewriter whose development began in April 2011 in the course of the [LinkedGeoData](http://linkedgeodata.org) project.    This system's features/traits are:  * Support of the ['Sparqlification Mapping Language' (SML)](http://sparqlify.org/wiki/SML), an intuitive language for expressing RDB-RDF mappings with only very little syntactic noise.  * Scalability: Sparqlify does not evaluate expressions in memory. All SPARQL filters end up in the corresponding SQL statement, giving the underlying RDBMS has maximum control over query planning.  * A powerful rewriting engine that analyzes filter expressions in order to eleminate self joins and joins with unsatisfiable conditions.  * Initial support for spatial datatypes and predicates.  * A subset of the SPARQL 1.0 query language plus sub queries are supported.  * Tested with PostgreSQL/Postgis and H2. Support for further databases is planned.  * CSV support  * R2RML will be supported soon    ## Supported SPARQL language features  * Join, LeftJoin (i.e. Optional), Union, Sub queries  * Filter predicates: comparison: (<=, <, =, >, >=) logical: (!, &&; ||) arithmetic: (+, -) spatial: st\_intersects, geomFromText; other: regex, lang, langMatches    * Aggregate functions: Count(\*)  * Order By is pushed into the SQL      ## Debian packages    Sparqlify Debian packages can be obtained by following means:  * Via the [Linked Data Stack](http://stack.linkeddata.org) (recommended)  * Download from the [Sparqlify website's download section](http://sparqlify.org/downloads/releases).  * Directly from source using maven (read down the README)    ### Public repositories    After setting up any of the repositories below, you can install sparqlify with apt using    * apt: `sudo apt-get install sparqlify-cli    #### Linked Data Stack (this is what you want)    Sparqlify is distributed at the [Linked Data Stack](http://stack.linkeddata.org), which offers many great tools done by various contributors of the Semantic Web community.    * The repository is available in the flavors `nightly`, `testing` and `stable` [here](http://stack.linkeddata.org/download/repo.php).    ```bash  # !!! Replace stable with nightly or testing as needed !!!    # Download the repository package  wget http://stack.linkeddata.org/ldstable-repository.deb    # Install the repository package  sudo dpkg -i ldstable-repository.deb    # Update the repository database  sudo apt-get update  ```      #### Bleeding Edge (Not recommended for production)  For the latest development version (built on every commit) perform the following steps    Import the public key with        wget -qO - http://cstadler.aksw.org/repos/apt/conf/packages.precise.gpg.key  | sudo apt-key add -    Add the repository        echo 'deb http://cstadler.aksw.org/repos/apt precise main contrib non-free' | sudo tee -a /etc/apt/sources.list.d/cstadler.aksw.org.list      Note that this also works with distros other than ""precise"" (ubuntu 12.04) such as ubuntu 14.04 or 16.04.        ## Building  Building the repository creates the JAR files providing the `sparqlify-*` tool suite.      ### Debian package  Building debian packages from this repo relies on the [Debian Maven Plugin](http://debian-maven.sourceforge.net]) plugin, which requires a debian-compatible environment.  If such an environment is present, the rest is simple:        # Install all shell scripts necessary for creating deb packages      sudo apt-get install devscripts        # Execute the follwing from the `<repository-root>/sparqlify-core` folder:      mvn clean install deb:package        # Upon sucessful completion, the debian package is located under `<repository-root>/sparqlify-core/target`      # Install using `dpkg`      sudo dpkg -i sparqlify_<version>.deb        # Uninstall using dpkg or apt:      sudo dpkg -r sparqlify      sudo apt-get remove sparqlify      ### Assembly based  Another way to build the project is run the following commands at `<repository-root>`        mvn clean install        cd sparqlify-cli      mvn assembly:assembly      This will generate a single stand-alone jar containing all necessary dependencies.  Afterwards, the shell scripts under `sparqlify-core/bin` should work.    ## Tool suite    If Sparqlify was installed from the debian package, the following commands are available system-wide:    * `sparqlify`: This is the main executable for running individual SPARQL queries, creating dumps and starting a stand-alone server.  * `sparqlify-csv`: This tool can create RDF dumps from CSV file based on SML view definitions.  * `sparqlify-platform`: A stand-alone server component integrating additional projects.    These tools write their output (such as RDF data in the N-TRIPLES format) to STDOUT. Log output goes to STDERR.    ### sparqlify  Usage: `sparqlify [options]`    Options are:    * Setup    * -m   SML view definition file    * Database Connectivity Settings    * -h   Hostname of the database (e.g. localhost or localhost:5432)    * -d   Database name    * -u   User name    * -p   Password    * -j   JDBC URI (mutually exclusive with both -h and -d)    * Quality of Service    * -n   Maximum result set size    * -t   Maximum query execution time in seconds (excluding rewriting time)    * Stand-alone Server Configuration    * -P   Server port [default: 7531]    * Run-Once (these options prevent the server from being started and are mutually exclusive with the server configuration)    * -D   Create an N-TRIPLES RDF dump on STDOUT     * -Q   [SPARQL query] Runs a SPARQL query against the configured database and view definitions    #### Example  The following command will start the Sparqlify HTTP server on the default port.        sparqlify -h localhost -u postgres -p secret -d mydb -m mydb-mappings.sml -n 1000 -t 30    Agents can now access the SPARQL endpoint at `http://localhost:7531/sparql`    ### sparqlify-csv  Usage: `sparqlify-csv [options]`    * Setup    * -m   SML view definition file    * -f   Input data file    * -v   View name (can be omitted if the view definition file only contains a single view)    * CSV Parser Settings    * -d   CSV field delimiter (default is '""')    * -e   CSV field escape delimiter (escapes the field delimiter) (default is '\')    * -s   CSV field separator (default is ',')    * -h   Use first row as headers. This option allows one to reference columns by name additionally to its index.      ### sparqlify-platform (Deprecated; about to be superseded by sparqlify-web-admin)  The Sparqlify Platform (under /sparqlify-platform) bundles Sparqlify with the Linked Data wrapper [Pubby](https://github.com/cygri/pubby) and the SPARQL Web interface [Snorql](https://github.com/kurtjx/SNORQL).    Usage: `sparqlify-platform config-dir [port]`     * `config-dir` Path to the configuration directory, e.g. `<repository-root/sparqlify-platform/config/example>`  * `port` Port on which to run the platform, default 7531.      For building, at the root of the project (outside of the sparqlify-\* directories), run `mvn compile` to build all modules.  Afterwards, lauch the platform using:        cd sparqlify-platform/bin      ./sparqlify-platform <path-to-config> <port>      Assuming the platform runs under `http://localhost:7531`, you can access the following services relative to this base url:  * `/sparql` is Sparqlify's SPARQL endpoint  * `/snorql` shows the SNORQL web frontend  * `/pubby` is the entry point to the Linked Data interface      #### Configuration  The configDirectory argument is mandatory and must contain a *sub-directory* for the context-path (i.e. `sparqlify-platform`) in turn contains the files:  * `platform.properties` This file contains configuration parameters that can be adjusted, such as the database connection.  * `views.sparqlify` The set of Sparqlify view definition to use.    I recommend to first create a copy of the files in `/sparqlify-platform/config/example` under a different location, then adjust the parameters and finally launch the platform with `-DconfigDirectory=...` set appropriately.    The platform *applies autoconfiguration to Pubby and Snorql*:  * Snorql: Namespaces are those of the views.sparqlify file.  * Pubby: The host name of all resources generated in the Sparqlify views is replaced with the URL of the platform (currently still needs to be configured via `platform.properties`)    Additionally you probably want to make the URIs nice by e.g. configuring an apache reverse proxy:    Enable the apache `proxy_http` module:    	sudo a2enmod proxy_http    Then in your `/etc/apache2/sites-available/default` add lines such as    	ProxyRequest Off  	ProxyPass /resource http://localhost:7531/pubby/bizer/bsbm/v01/ retry=1  	ProxyPassReverse /resource http://localhost:7531/pubby/bizer/bsbm/v01/    These entries will enable requests to `http://localhost/resource/...` rather than `http//localhost:7531/pubby/bizer/bsbm/v01/`.    The `retry=1` means, that apache only waits 1 seconds before retrying again when it encounters an error (e.g. HTTP code 500) from the proxied resource.    *IMPORTANT: ProxyRequests are off by default; DO NOT ENABLE THEM UNLESS YOU KNOW WHAT YOU ARE DOING. Simply enabling them potentially allows anyone to use your computer as a proxy.*      ## SML Mapping Syntax:  A Sparqlification Mapping Language (SML) configuration is essentially a set of CREATE VIEW statements, somewhat similar to the CREATE VIEW statement from SQL.  Probably the easiest way to learn to syntax is to look at the following resources:    * The [SML documentation](http://sparqlify.org/wiki/SML)  * The [SML test suite](https://github.com/AKSW/Sparqlify/tree/master/sparqlify-core/src/test/resources/org/aksw/sml/r2rml_tests) which is derived from the [R2RML test suite](https://github.com/AKSW/Sparqlify/tree/master/sparqlify-core/src/test/resources/org/w3c/r2rml_tests).    Two more examples are from    Additionally, for convenience, prefixes can be declared, which are valid throughout the config file.  As comments, you can use //, /\* \*/, and #.     For a first impression, here is a quick example:            /* This is a comment       * /* You can even nest them! */       */      // Prefixes are valid throughout the file      Prefix dbp:<http://dbpedia.org/ontology/>      Prefix ex:<http://ex.org/>        Create View myFirstView As          Construct {              ?s a dbp:Person .              ?s ex:workPage ?w .          }      With          ?s = uri('http://mydomain.org/person', ?id) // Define ?s to be an URI generated from the concatenation of a prefix with mytable's id-column.          ?w = uri(?work_page) // ?w is assigned the URIs in the column 'work_page' of 'mytable'      Constrain          ?w prefix ""http://my-organization.org/user/"" // Constraints can be used for optimization, e.g. to prune unsatisfiable join conditions      From          mytable; // If you want to use an SQL query, the query (without trailing semicolon) must be enclosed in double square brackets: [[SELECT id, work_page FROM mytable]]      ### Notes for sparqlify-csv  For `sparqlify-csv` view definition syntax is almost the same as above; the differences being:    * Instead of `Create View viewname As Construct` start your views with `CREATE VIEW TEMPLATE viewname As Construct`  * There is no FROM and CONSTRAINT clause    Colums can be referenced either by name (see the -h option) or by index (1-based).    #### Example        // Assume a CSV file with the following columns (osm stands for OpenStreetMap)      (city\_name, country\_name, osm\_entity\_type, osm\_id, longitude, latitude)        Prefix fn:<http://aksw.org/sparqlify/> //Needed for urlEncode and urlDecode.      Prefix rdfs:<http://www.w3.org/2000/01/rdf-schema#>      Prefix owl:<http://www.w3.org/2002/07/owl#>      Prefix xsd:<http://www.w3.org/2001/XMLSchema#>      Prefix geo:<http://www.w3.org/2003/01/geo/wgs84_pos#>        Create View Template geocode As        Construct {          ?cityUri            owl:sameAs ?lgdUri .            ?lgdUri            rdfs:label ?cityLabel ;            geo:long ?long ;            geo:lat ?lat .        }        With          ?cityUri = uri(concat(""http://fp7-pp.publicdata.eu/resource/city/"", fn:urlEncode(?2), ""-"", fn:urlEncode(?1)))          ?cityLabel = plainLiteral(?1)          ?lgdUri = uri(concat(""http://linkedgeodata.org/triplify/"", ?4, ?5))          ?long = typedLiteral(?6, xsd:float)          ?lat = typedLiteral(?7, xsd:float)       """
Semantic web;https://github.com/jindrichmynarz/sparql-to-csv;"""# sparql-to-csv    A command-line tool to stream SPARQL results to CSV. The tool is primarily intended to support data preparation for analyses that require tabular input. It helps you avoid writing ad hoc scripts to piece larger tabular datasets out of results of many SPARQL queries. It allows to generate queries from [Mustache](https://mustache.github.io) templates, either to execute paged queries or to execute queries based on results of other queries.     ## Usage    Use a [released executable](https://github.com/jindrichmynarz/sparql-to-csv/releases) or compile using [Leiningen](http://leiningen.org) and [lein-binplus](https://github.com/BrunoBonacci/lein-binplus):    ```sh  git clone https://github.com/jindrichmynarz/sparql-to-csv.git  cd sparql-to-csv  lein bin  ```    Then you can run the created executable file to find out about the configuration options:     ```sh  target/sparql_to_csv --help  ```    Example of use:    ```sh  target/sparql_to_csv --endpoint http://localhost:8890/sparql \                       --page-size 1000 \                       paged_query.mustache > results.csv  ```    There are two main use cases for this tool: paged queries and piped queries.    ### Paged queries    The first one is paged execution of SPARQL `SELECT` queries. RDF stores often limit the number of rows a SPARQL `SELECT` query can retrieve in one go and thus avoid the load such queries impose on the store. For queries that select more results than the limit per one request their execution must be split into several requests if complete results need to be obtained. One way to partition such queries is to split them into pages delimited by `LIMIT` and `OFFSET`, indicating the size and the start index, respectively, of a page. Paging requires the results to have a deterministic order, which can be achieved by using an `ORDER BY` clause. Due to limitations of some RDF stores (see [Virtuoso's documentation on this topic](https://virtuoso.openlinksw.com/dataspace/doc/dav/wiki/Main/VirtTipsAndTricksHowToHandleBandwidthLimitExceed)), the paged queries may need to contain an inner sub-`SELECT` that with an `ORDER BY` clause wrapped by an outer `SELECT` that slices a page from the ordered results using `LIMIT` and `OFFSET`, like this:    ```sparql  PREFIX dbo: <http://dbpedia.org/ontology/>    SELECT ?person   WHERE {    {      SELECT DISTINCT ?person      WHERE {        ?person a dbo:Person .      }      ORDER BY ?person    }  }  LIMIT 10000  OFFSET 40000  ```    In order to run paged queries you need to provide the tool with a Mustache template to generate the queries for the individual pages. These queries must contain a `{{limit}}` and `{{offset}}` parameters, like so:    ```sparql  PREFIX dbo: <http://dbpedia.org/ontology/>    SELECT ?person   WHERE {    {      SELECT DISTINCT ?person      WHERE {        ?person a dbo:Person .      }      ORDER BY ?person    }  }  LIMIT {{limit}}  OFFSET {{offset}}  ```    The `limit` is set by the `--page-size` parameter. The offset is incremented by the page size in each successive request. The execution of paged queries stops when an individual query returns empty results.    ### Piped queries     It may be desirable to decompose complex queries into several simpler queries to avoid limit on demanding queries due to performance. For example, for each person in a dataset we may want to retrieve its complex description. While this may be possible to achieve by using a sub-`SELECT` to page through the individual persons and an outer `SELECT` to compose their descriptions, such query would be more demanding since it both sorts the persons and selects their descriptions. Consequently, it may not be possible to run such query since it would end with a time-out. Instead, this query can be decomposed into two queries. The first one selects persons in the paged manner described above, while the second one receives results of the first query one by one and fetches their descriptions.    This approach is also useful when you need to query one SPARQL endpoint using data from another SPARQL endpoint. While this is feasible using [federated queries](https://www.w3.org/TR/sparql11-federated-query), they too suffer from performance problems.    Piped queries take CSV input and for each line they execute a query generated from a Mustache template that is provided with the line's data as parameters. For example, the CSV generated by the query above contains a column `person`, which can be used in a query template as `{{person}}`:    ```sparql  PREFIX dbo: <http://dbpedia.org/ontology/>  PREFIX dbp: <http://dbpedia.org/property/>    SELECT (<{{person}}> AS ?person) ?name ?birthDate ?deathDate  WHERE {    <{{person}}> dbp:name ?name ;      dbo:birthDate ?birthDate .    OPTIONAL {      <{{person}}> dbo:deathDate ?deathDate .    }  }  ```    The input CSV must have a header with column names. In order to be usable in Mustache template, the column names in the input CSV can contain only ASCII characters, `?`, `!`, `/`, `.`, or `-`. For example, `right!` is allowed, while `mélangé` is not.    Piped queries enable to create data processing pipelines. For instance, if the first query is stored in the `persons.mustache` file and the second query is stored as `describe_person.mustache`, then we can run them in pipeline using the following command using `--piped` to indicate that it is a piped query:     ```sh  sparql_to_csv -e http://dbpedia.org/sparql persons.mustache |    sparql_to_csv -e http://dbpedia.org/sparql --piped describe_person.mustache  ```    By default the piped input is replaced by the output query results. However, using the `--extend` parameter extends the input with the results. Each result row is append to its input row. This allows you to combine data from multiple queries. Piped queries can be arbitrarily chained and allow joining data across many SPARQL endpoints.    ## License    Copyright © 2016 Jindřich Mynarz    Distributed under the Eclipse Public License version 1.0. """
Semantic web;https://github.com/rsgoncalves/module-extractor;"""*owl-me*  ====    #### a Java-based module extractor for OWL ontologies ####    Built using the [OWL API](http://owlapi.sourceforge.net/).       summary  --------------------  *owl-me* is a standalone tool designed to extract different types of [Locality-based modules](http://owl.cs.manchester.ac.uk/research/modularity/) from OWL ontologies.    The tool takes as inputs an ontology and a text file. The latter is the so-called *signature file*, which contains entity (class and object/data property) IRIs. The tool extracts a module for the specified set of IRIs (i.e. signature) onto a chosen location.      usage  --------------------  Build using the Ant script and run the **owl-me.jar** file. For large ontologies you may have to increase the heap space and entity expansion limit (esp. for ontologies in RDF/XML), e.g., for 4GB heap:<br><br>  `java -jar -Xmx4G -DentityExpansionLimit=100000000 owl-me.jar`      signatures for module extraction  --------------------  Signature files should contain entity IRIs as they appear in the original ontology. IRIs can be separated by any of the following delimiters:    * Comma (e.g. CSV files)    * White space    * Vertical bar ""|""    * Tab    * New line    The file may also contain headers or comments, so long as the line or part thereof is preceded with '%'. All text following '%' is ignored. Check the *example signature file contents* below:    % My header<br>  Class_IRI_1, Class_IRI_2 Class_IRI_3<br>  Property_IRI_2 | Property_IRI_3    % Main properties<br>  <br>  % Some comment<br>  Class_IRI_4<br>      SNOMED CT  --------------------  The module extractor accepts signature files for the SNOMED CT ontology in the *UMLS Core Subset format*. Any manually constructed signature files **should have the concept ID's delimited by vertical bars ""|""**, in a similar way as the UMLS Core Subset files.      deployment  --------------------  The module extractor is compatible with **Java 1.6 and above**. It was tested with Java 1.7 and 1.8., and relies mainly on the following project:     * [OWL API](http://owlapi.sourceforge.net/) (v4.0.1)      contact  --------------------  Consider checking the [OWL@Manchester](http://owl.cs.manchester.ac.uk) website (and linked publications) for more information regarding _Locality-based_ modules, before submitting queries.    If you come across any bugs please use the ""Issues"" tab to describe the problem, along with sufficient data to reproduce it (i.e. the ontology and signature used)."""
Semantic web;https://github.com/oeg-upm/gtfs-bench;"""# The GTFS-Madrid-Bench        We present GTFS-Madrid-Bench, **a benchmark to evaluate declarative KG construction engines** that can be used for the provision of access mechanisms to (virtual) knowledge graphs. Our proposal introduces several scenarios that aim at measuring performance and scalability as well as the query capabilities of all this kind of engines, considering their heterogeneity. The data sources used in our benchmark are derived from the [GTFS](https://developers.google.com/transit/gtfs) data files of the subway network of Madrid. They can be transformed into several formats (CSV, JSON, SQL and XML) and scaled up. The query set aims at addressing a representative number of SPARQL 1.1 features while covering usual queries that data consumers may be interested in.    <p align=""center"">    <img src=""misc/logo.png"" />  </p>    ### Main Publication:  David Chaves-Fraga, Freddy Priyatna, Andrea Cimmino, Jhon Toledo, Edna Ruckhaus, & Oscar Corcho (2020). GTFS-Madrid-Bench: A benchmark for virtual knowledge graph access in the transport domain. Journal of Web Semantics, 65. [Online](https://doi.org/10.1016/j.websem.2020.100596)        **Citing GTFS-Madrid-Bench**: If you used GTFS-Madrid-Bench in your work, please cite as:    ```bib  @article{chaves2020gtfs,    title={GTFS-Madrid-Bench: A benchmark for virtual knowledge graph access in the transport domain},    author={Chaves-Fraga, David and Priyatna, Freddy and Cimmino, Andrea and Toledo, Jhon and Ruckhaus, Edna and Corcho, Oscar},    journal={Journal of Web Semantics},    volume={65},    pages={100596},    year={2020},    doi={https://doi.org/10.1016/j.websem.2020.100596},    publisher={Elsevier}    }  ```    ### Results  - Virtual KGC results can be reproduced through the resources provided in [this branch](https://github.com/oeg-upm/gtfs-bench/tree/evaluation-jows2020)  - Materialized KGC results can be reproduced through the resources provided in [this repo](https://github.com/oeg-upm/kgc-eval)    ## Requirements for the use:    To have locally installed [docker](https://docs.docker.com/engine/install/).    Decide the distributions to be used for your testing. They can be:  - Standard distributions: data sources are represented in one format (e.g., GTFS-CSV, GTFS-JSON or GTFS-SQL).  - Custom distributions: each data source is represented in the format selected by the user (e.g., SHAPES in JSON, CALENDAR in CSV, etc.)      ## Using Madrid-GTFS-Bench:    1. Download and run the docker image (run it always to ensure you are using the last version of the docker image).  * Docker v20.10 or later: `docker run --pull always -itv ""$(pwd)"":/output oegdataintegration/gtfs-bench`   * Previous versions: `docker pull oegdataintegration/gtfs-bench` and then `docker run -itv ""$(pwd)"":/output oegdataintegration/gtfs-bench`  2. Choose data scales and formats to obtain the distributions you want to test. You have to provide: first the data scales (in one line, separated by a comma), then, select the standard distributions (from none to all) and if is needed, the configuration for one custom distribution. If you want to generate several custom distributions, you will have to run the generator several times. Example:    ![Demo GIF](misc/gtfs-demo.gif)    3. Result will be available as `result.zip` in the current working directory. The folders structure are: one folder for datasets and other for the queries (for virtual KG). Inside the datasets folder will be one folder for each distribution (e.g., csv, sql, custom), and in each distribution folder we provide the required sizes (each size in one folder), the corresponding mapping associated to the distribution, and the SQL schemes if they are needed. **Consider that for not repeating resources at scale level, the mappings and SQL paths to the data are define at distribution level (e.g, ""data/AGENCY.csv"") and their management for performing a correct evaluation has to be done by the user (with an script, for example)**. You can visit the [utils](https://github.com/oeg-upm/gtfs-bench/tree/master/utils) folder where we provide some ideas on how to manage it. See the following example:    ```  .  ├── datasets  │   ├── csv  │   │   ├── 1  │   │   │   ├── AGENCY.csv  │   │   │   ├── CALENDAR.csv  │   │   │   ├── CALENDAR_DATES.csv  │   │   │   ├── FEED_INFO.csv  │   │   │   ├── FREQUENCIES.csv  │   │   │   ├── ROUTES.csv  │   │   │   ├── SHAPES.csv  │   │   │   ├── STOPS.csv  │   │   │   ├── STOP_TIMES.csv  │   │   │   └── TRIPS.csv  │   │   ├── 2  │   │   │   ├── AGENCY.csv  │   │   │   ├── CALENDAR.csv  │   │   │   ├── CALENDAR_DATES.csv  │   │   │   ├── FEED_INFO.csv  │   │   │   ├── FREQUENCIES.csv  │   │   │   ├── ROUTES.csv  │   │   │   ├── SHAPES.csv  │   │   │   ├── STOPS.csv  │   │   │   ├── STOP_TIMES.csv  │   │   │   └── TRIPS.csv  │   │   ├── 3  │   │   │   ├── AGENCY.csv  │   │   │   ├── CALENDAR.csv  │   │   │   ├── CALENDAR_DATES.csv  │   │   │   ├── FEED_INFO.csv  │   │   │   ├── FREQUENCIES.csv  │   │   │   ├── ROUTES.csv  │   │   │   ├── SHAPES.csv  │   │   │   ├── STOPS.csv  │   │   │   ├── STOP_TIMES.csv  │   │   │   └── TRIPS.csv  │   │   └── mapping.csv.nt  │   ├── json  │   │   ├── 1  │   │   │   ├── AGENCY.json  │   │   │   ├── CALENDAR_DATES.json  │   │   │   ├── CALENDAR.json  │   │   │   ├── FEED_INFO.json  │   │   │   ├── FREQUENCIES.json  │   │   │   ├── ROUTES.json  │   │   │   ├── SHAPES.json  │   │   │   ├── STOPS.json  │   │   │   ├── STOP_TIMES.json  │   │   │   └── TRIPS.json  │   │   ├── 2  │   │   │   ├── AGENCY.json  │   │   │   ├── CALENDAR_DATES.json  │   │   │   ├── CALENDAR.json  │   │   │   ├── FEED_INFO.json  │   │   │   ├── FREQUENCIES.json  │   │   │   ├── ROUTES.json  │   │   │   ├── SHAPES.json  │   │   │   ├── STOPS.json  │   │   │   ├── STOP_TIMES.json  │   │   │   └── TRIPS.json  │   │   ├── 3  │   │   │   ├── AGENCY.json  │   │   │   ├── CALENDAR_DATES.json  │   │   │   ├── CALENDAR.json  │   │   │   ├── FEED_INFO.json  │   │   │   ├── FREQUENCIES.json  │   │   │   ├── ROUTES.json  │   │   │   ├── SHAPES.json  │   │   │   ├── STOPS.json  │   │   │   ├── STOP_TIMES.json  │   │   │   └── TRIPS.json  │   │   └── mapping.json.nt  │   └── sql  │       ├── 1  │       │   ├── AGENCY.csv  │       │   ├── CALENDAR.csv  │       │   ├── CALENDAR_DATES.csv  │       │   ├── FEED_INFO.csv  │       │   ├── FREQUENCIES.csv  │       │   ├── ROUTES.csv  │       │   ├── SHAPES.csv  │       │   ├── STOPS.csv  │       │   ├── STOP_TIMES.csv  │       │   └── TRIPS.csv  │       ├── 2  │       │   ├── AGENCY.csv  │       │   ├── CALENDAR.csv  │       │   ├── CALENDAR_DATES.csv  │       │   ├── FEED_INFO.csv  │       │   ├── FREQUENCIES.csv  │       │   ├── ROUTES.csv  │       │   ├── SHAPES.csv  │       │   ├── STOPS.csv  │       │   ├── STOP_TIMES.csv  │       │   └── TRIPS.csv  │       ├── 3  │       │   ├── AGENCY.csv  │       │   ├── CALENDAR.csv  │       │   ├── CALENDAR_DATES.csv  │       │   ├── FEED_INFO.csv  │       │   ├── FREQUENCIES.csv  │       │   ├── ROUTES.csv  │       │   ├── SHAPES.csv  │       │   ├── STOPS.csv  │       │   ├── STOP_TIMES.csv  │       │   └── TRIPS.csv  │       └── mapping.sql.nt  │       └── schema.sql  └── queries      ├── q10.rq      ├── q11.rq      ├── q12.rq      ├── q13.rq      ├── q14.rq      ├── q15.rq      ├── q16.rq      ├── q17.rq      ├── q18.rq      ├── q1.rq      ├── q2.rq      ├── q3.rq      ├── q4.rq      ├── q5.rq      ├── q6.rq      ├── q7.rq      ├── q8.rq      └── q9.rq  ```      ## Resources    Additionally to the generator engine, that provides the data at desirable scales and distributions, together with corresponding mappings and queries, there are also common resources openly available to be modified or used by any practicioner or developer:    - Folder [mappings](https://github.com/oeg-upm/gtfs-bench/tree/master/mappings) contains RML mappings for CSV, XML, JSON and RDB distributions of the input GTFS dataset, R2RML mapping for RDB and xR2RML mapping for MongoDB. It also includes CSVW annotations for the CSV distributions.  - Folder [queries](https://github.com/oeg-upm/gtfs-bench/tree/master/queries) includes 18 queries with different levels of complexity including a representative set of SPARQL 1.1. operators. Additionally, the folder contains [11 simple queries](https://github.com/oeg-upm/gtfs-bench/tree/master/queries/simple) that will help to test the basic capabilities of virtual KG construction engines (i.e., to understand if the engine is able to translate correctly the SPARQL operators over different GTFS distributions before starting to test performance and scalability).    ## Utils    Our experiences testing (virtual) knowledge graph engines have revealed the difficulties for setting up an infrastructure where many variables and resources are involved: databases, raw data, mappings, queries, data paths, mapping paths, databases connections, etc. For that reason, and in order to facilitate the use of the benchmark to any developer or practitioner, we provide a set of [utils](https://github.com/oeg-upm/gtfs-bench/tree/master/utils) such as docker-compose templates or evaluation bash scripts that, in our opinion, can reduce the time for preparing the testing set up.    ## Desirable Metrics:    We highly recommend that (virutalizers or materializers) KG construction engines tested with this benchmark provide (at least) the following metris:  - Total execution time  - Number of answers	  - Memory consumption  - Initial delay	  - Dief@k (only for continuous/streaming behavior)*	  - Dief@t (only for continuous/streaming behavior)*    For virtual knowledge graphs systems, we also encourage developers and tester to provide:  - Loading time	  - Mapping translation time (if applies)  - Number of requests  - Source selection time	  - Query generation (or disitribution) time  - Query rewritting time  - Query translation time  - Query exececution time  - Results aggregation time    *R Package available at: https://github.com/dachafra/dief (extension from https://github.com/maribelacosta/dief)    ## Data License  All the datasets generated by this benchmark have to follow the license of the Consorcio Regional de Transporte de Madrid: https://www.crtm.es/licencia-de-uso?lang=en    ## Contribute  We know that there are variables and dimensions that we did not take into account in the current version of the benchmark (e.g., transformation function defined in the mapping rules). If you are interested in collaborate with us in a new version of the benchmark, send us an email or open a new [discussion](https://github.com/oeg-upm/gtfs-bench/discussions)!      ## Authors    - David Chaves-Fraga - [dchaves@fi.upm.es](mailto:dchaves@fi.upm.es)  - Freddy Priyatna  - Jhon Toledo  - Daniel Doña  - Edna Ruckhaus  - Andrea Cimmino  - Oscar Corcho    Ontology Engineering Group, October 2019 - Present """
Semantic web;https://github.com/RubenVerborgh/LDflex;"""# LDflex makes Linked Data in JavaScript fun  LDflex is a domain-specific language  for querying Linked Data on the Web  as if you were browsing a local JavaScript graph.    [![npm version](https://img.shields.io/npm/v/ldflex.svg)](https://www.npmjs.com/package/ldflex)  [![Build Status](https://travis-ci.com/LDflex/LDflex.svg?branch=master)](https://travis-ci.com/LDflex/LDflex)  [![Coverage Status](https://coveralls.io/repos/github/LDflex/LDflex/badge.svg?branch=master)](https://coveralls.io/github/LDflex/LDflex?branch=master)  [![Dependency Status](https://david-dm.org/LDflex/LDflex.svg)](https://david-dm.org/LDflex/LDflex)  [![DOI](https://zenodo.org/badge/148931900.svg)](https://zenodo.org/badge/latestdoi/148931900)    You can write things like `person.friends.firstName`  to get a list of your friends.  Thanks to the power of [JSON-LD contexts](https://www.w3.org/TR/json-ld/#the-context)  and [JavaScript's Proxy](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Proxy),  these properties are not hard-coded in LDflex,  but can be chosen at runtime.  They feel as if you're traversing a local object,  while you're actually querying the Web—without  pulling in all data first.    [Tim Berners-Lee](https://www.w3.org/People/Berners-Lee/)  came up with the idea for such a fluid JavaScript interface to Linked Data,  in a discussion on how to make Linked Data easier for developers.    ## Articles and tutorials  - [Tutorial slides](https://comunica.github.io/Tutorial-ISWC2019-Slides-LDflex/)    and [walkthrough](https://github.com/comunica/Tutorial-ISWC2019-LDflex-on-React/wiki/Tutorial-Walkthrough)  - [Cheatsheet](https://vincenttunru.gitlab.io/tripledoc/docs/cheatsheet)  - [Designing a Linked Data developer experience](https://ruben.verborgh.org/blog/2018/12/28/designing-a-linked-data-developer-experience/),    discussing the design of LDflex  - [Solid Chess](https://pieterheyvaert.com/blog/2019/02/10/solid-world-summary),    an app built with LDflex    ## Installation  ```bash  npm install ldflex  ```    In order to execute queries,  you will also need a query engine:  ```bash  npm install @ldflex/comunica  ```    ## Usage  When you have obtained a starting subject,  you can navigate through its properties  using standard JavaScript dot property syntax.    In order to query for the result,  use `await` if you want a single value,  or `for await` to iterate over all values.    ### Initialization  ```javascript  const { PathFactory } = require('ldflex');  const { default: ComunicaEngine } = require('@ldflex/comunica');  const { namedNode } = require('@rdfjs/data-model');    // The JSON-LD context for resolving properties  const context = {    ""@context"": {      ""@vocab"": ""http://xmlns.com/foaf/0.1/"",      ""friends"": ""knows"",      ""label"": ""http://www.w3.org/2000/01/rdf-schema#label"",      ""rbn"": ""https://ruben.verborgh.org/profile/#""    }  };  // The query engine and its source  const queryEngine = new ComunicaEngine('https://ruben.verborgh.org/profile/');  // The object that can create new paths  const path = new PathFactory({ context, queryEngine });  ```    ### Looking up data on the Web  ```javascript  const ruben = path.create({ subject: namedNode('https://ruben.verborgh.org/profile/#me') });  showPerson(ruben);    async function showPerson(person) {    console.log(`This person is ${await person.name}`);      console.log(`${await person.givenName} is interested in:`);    for await (const name of person.interest.label)      console.log(`- ${name}`);      console.log(`${await person.givenName} is friends with:`);    for await (const name of person.friends.givenName)      console.log(`- ${name}`);  }  ```    ### Inspecting the generated path expression  ```javascript  (async person => {    console.log(await person.friends.givenName.pathExpression);  })(ruben);    ```    ### Getting all subjects of a document  ```javascript  (async document => {    for await (const subject of document.subjects)      console.log(`${subject}`);  })(ruben);  ```    ### Getting all properties of a subject  ```javascript  (async subject => {    for await (const property of subject.properties)      console.log(`${property}`);  })(ruben);    ```    ### Converting an LDflex expression into a SPARQL query  ```javascript  (async person => {    console.log(await person.friends.givenName.sparql);  })(ruben);    ```    ### Sorting path results  ```javascript  (async person => {    for await (const uri of person.interest.sort('label'))      console.log(`- ${uri}`);  })(ruben);    ```    The sort function takes multiple arguments,  creating a path that sorts on the last argument.  The path can also continue after the sort:  `person.friends.sort('country', 'label').givenName`  will sort the friends based on the label of their country,  and then return their names.    ### Modifying data    ```javascript  // Add a new value  await person['http://xmlns.com/foaf/0.1/name'].add(literal(name));  await person['http://xmlns.com/foaf/0.1/nick'].add(literal(nickname));    // Set a new value and override existing values  await person['http://xmlns.com/foaf/0.1/name'].set(literal(name));  await person['http://xmlns.com/foaf/0.1/nick'].set(literal(nickname));    // Delete object values  await person['http://xmlns.com/foaf/0.1/name'].delete();  await person['http://xmlns.com/foaf/0.1/nick'].delete();    // Replace object values  await person['http://xmlns.com/foaf/0.1/name'].replace(literal(oldName), literal(name));  ```    ### Accessing collections  Handle `rdf:List`, `rdf:Bag`, `rdf:Alt`, `rdf:Seq` and `rdf:Container`.    For `rdf:List`s  ```javascript  (async publication => {    // Returns an Array of Authors    const authors = await publication['bibo:authorList'].list();  })(ordonez_medellin_2014);  ```    For `rdf:Alt`, `rdf:Seq` and `rdf:Container`s  ```javascript  (async data => {    // Returns an Array of elements    const elements = await data['ex:myContainer'].container();  })(data);  ```    For `rdf:Bag`s  ```javascript  (async data => {    // Returns a Set of elements    const elements = await data['ex:myBag'].containerAsSet();  })(data);  ```    Alternatively, `.collection` can be used for *any* collection (i.e. `rdf:List`, `rdf:Bag`, `rdf:Alt`, `rdf:Seq` and `rdf:Container`) **provided the collection has the correct `rdf:type` annotation in the data source**    ```javascript  (async publication => {    // Returns an Array of Authors    const authors = await publication['bibo:authorList'].collection();  })(ordonez_medellin_2014);  ```    ### NamedNode URI utilities  ```js  ruben.namespace // 'https://ruben.verborgh.org/profile/#'  ruben.fragment // 'me'  await ruben.prefix // 'rbn'  ```    ## Additional Handlers    The following libraries provide handlers that extend the functionality of LDflex:   - [async-iteration-handlers](https://github.com/LDflex/async-iteration-handlers) Provides methods such as `.map`, `.filter` and `.reduce` for the async-iterable results returned by LDflex.    ## License  ©2018–present  [Ruben Verborgh](https://ruben.verborgh.org/),  [Ruben Taelman](https://www.rubensworks.net/).  [MIT License](https://github.com/LDflex/LDflex/blob/master/LICENSE.md). """
Semantic web;https://github.com/sisinflab-swot/ldp-coap-framework;"""LDP-CoAP: Linked Data Platform for the Constrained Application Protocol  ===================    [W3C Linked Data Platform 1.0 specification](http://www.w3.org/TR/ldp/) defines resource management primitives for HTTP only, pushing into the background not-negligible   use cases related to Web of Things (WoT) scenarios where HTTP-based communication and infrastructures are unfeasible.     LDP-CoAP proposes a mapping of the LDP specification for [RFC 7252 Constrained Application Protocol](https://tools.ietf.org/html/rfc7252) (CoAP)   and a complete Java-based framework to publish Linked Data on the WoT.     A general translation of LDP-HTTP requests and responses is provided, as well as a fully comprehensive framework for HTTP-to-CoAP proxying.     LDP-CoAP framework also supports the [W3C Linked Data Notifications](https://www.w3.org/TR/ldn/) (LDN) protocol aiming to generate, share and reuse notifications across different applications.    LDP-CoAP functionalities can be tested using the [W3C Test Suite for LDP](http://w3c.github.io/ldp-testsuite/) and the [LDN Test Suite](http://github.com/csarven/ldn-tests).    Modules  -------------    LDP-CoAP (version 1.2.x) consists of the following sub-projects:    - _ldp-coap-core_: basic framework implementation including the proposed LDP-CoAP mapping;  - _ldp-coap-test_: includes reference client/server implementation used to test the framework according to the test suites cited above;  - _ldp-coap-raspberry_: usage examples exploiting _ldp-coap-core_ on a [Raspberry Pi 1 Model B+](https://www.raspberrypi.com/products/raspberry-pi-1-model-b-plus/) board;  - _ldp-coap-android_: simple project using _ldp-coap-core_ on Android platform;    LDP-CoAP also requires [Californium-LDP](https://github.com/sisinflab-swot/californium-ldp), a fork of the _Eclipse Californium_ framework supporting LDP specification. In particular, the following modules were defined as local Maven dependencies:    - _californium-core-ldp_: a modified version of the [californium-core](https://github.com/eclipse/californium) library extended to support LDP-CoAP features;  - _californium-proxy-ldp_: a modified version of the [californium-proxy](http://github.com/eclipse/californium) used to translate LDP-HTTP request methods and headers   into the corresponding LDP-CoAP ones and then map back LDP-CoAP responses to LDP-HTTP;    Usage with Eclipse and Maven  -------------    Each module can be imported as Maven project in Eclipse. Make sure to have the following plugins before importing LDP-CoAP projects:    - [Eclipse EGit](http://www.eclipse.org/egit/)  - [M2Eclipse - Maven Integration for Eclipse](http://www.eclipse.org/m2e/)    Documentation  -------------    Hands-on introduction to LDP-CoAP using [basic code samples](http://swot.sisinflab.poliba.it/ldp-coap/usage.html) available on the project webpage.    More details about packages and methods can be found on the official [Javadoc](http://swot.sisinflab.poliba.it/ldp-coap/docs/javadoc/v1_1/).    References  -------------    If you want to refer to LDP-CoAP in a publication, please cite one of the following papers:    ```  @InProceedings{ldp-coap-framework,    author       = {Giuseppe Loseto and Saverio Ieva and Filippo Gramegna and Michele Ruta and Floriano Scioscia and Eugenio {Di Sciascio}},    title        = {Linked Data (in low-resource) Platforms: a mapping for Constrained Application Protocol},    booktitle    = {The Semantic Web - ISWC 2016: 15th International Semantic Web Conference, Proceedings, Part II},    series       = {Lecture Notes in Computer Science},    volume       = {9982},    pages        = {131--139},    month        = {oct},    year         = {2016},    editor       = {Paul Groth, Elena Simperl, Alasdair Gray, Marta Sabou, Markus Krotzsch, Freddy Lecue, Fabian Flock, Yolanda Gil},    publisher    = {Springer International Publishing},    address      = {Cham},  }  ```    ```  @InProceedings{ldp-coap-proposal,    author = {Giuseppe Loseto and Saverio Ieva and Filippo Gramegna and Michele Ruta and Floriano Scioscia and Eugenio {Di Sciascio}},    title = {Linking the Web of Things: LDP-CoAP mapping},    booktitle = {The 7th International Conference on Ambient Systems, Networks and Technologies (ANT 2016) / Affiliated Workshops},    series = {Procedia Computer Science},    volume = {83},    pages = {1182--1187},    month = {may},    year = {2016},    editor = {Elhadi Shakshuki},    publisher = {Elsevier}  }  ```    License  -------------    _ldp-coap-core_, _ldp-coap-android_ and _ldp-coap-raspberry_ modules are distributed under the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0).    _californium-core-ldp_ and _ldp-coap-proxy_ are distributed under the [Eclipse Public License, Version 1.0](https://www.eclipse.org/legal/epl-v10.html) as derived projects.      Contact  -------------    For more information, please visit the [LDP-CoAP webpage](http://swot.sisinflab.poliba.it/ldp-coap/).      Contribute  -------------  The main purpose of this repository is to share and continue to improve the LDP-CoAP framework, making it easier to use. If you're interested in helping us any feedback you have about using LDP-CoAP would be greatly appreciated. There are only a few guidelines that we need contributors to follow reported in the CONTRIBUTING.md file.    --------- """
Semantic web;https://github.com/CLARIAH/grlc;"""<p algin=""center""><img src=""https://raw.githubusercontent.com/CLARIAH/grlc/master/src/static/grlc_logo_01.png"" width=""250px""></p>    [![PyPI version](https://badge.fury.io/py/grlc.svg)](https://badge.fury.io/py/grlc)  [![DOI](https://zenodo.org/badge/46131212.svg)](https://zenodo.org/badge/latestdoi/46131212)  [![Build Status](https://travis-ci.org/CLARIAH/grlc.svg?branch=master)](https://travis-ci.org/CLARIAH/grlc)      grlc, the <b>g</b>it <b>r</b>epository <b>l</b>inked data API <b>c</b>onstructor, automatically builds Web APIs using shared SPARQL queries. http://grlc.io/    If you use grlc in your work, please cite it as:    ```  @InProceedings{merono2016grlc,   author = {Mero{\~{n}}o-Pe{\~{n}}uela, Albert and Hoekstra, Rinke},   title = {{grlc Makes GitHub Taste Like Linked Data APIs}},   booktitle = {The Semantic Web: ESWC 2016 Satellite Events, Heraklion, Crete, Greece, May 29 -- June 2,  2016},   year = {2016},   publisher = {Springer},   pages = {342--353},   isbn = {978-3-319-47602-5},   doi = {10.1007/978-3-319-47602-5_48}  }  ```    ## What is grlc?  grlc is a lightweight server that takes SPARQL queries (stored in a GitHub repository, in your local filesystem, or listed in a URL), and translates them to Linked Data Web APIs. This enables universal access to Linked Data. Users are not required to know SPARQL to query their data, but instead can access a web API.    ## Quick tutorial  For a quick usage tutorial check out our wiki [walkthrough](https://github.com/CLARIAH/grlc/wiki/Quick-tutorial) and [list of features](https://github.com/CLARIAH/grlc/wiki/Features).    ## Usage  grlc assumes that you have a collection of SPARQL queries as .rq files (like [this](https://github.com/CLARIAH/grlc-queries)). grlc will create one API operation for each SPARQL query/.rq file in the collection.    Your queries can add API parameters to each operation by using the [parameter mapping](https://github.com/CLARIAH/grlc/wiki/Parameter-Mapping) syntax. This allows your query to define query variables which will be mapped to API parameters for your API operation ([see here](https://github.com/CLARIAH/grlc-queries/blob/master/enumerate.rq) for an example).    Your queries can include special [decorators](#decorator-syntax) to add extra functionality to your API.    ### Query location  grlc can load your query collection from different locations: from a GitHub repository (`api-git`), from local storage (`api-local`), and from a specification file (`api-url`). Each type of location has specific features and is accessible via different paths. However all location types produce the same beautiful APIs.    #### From a GitHub repository  > API path:  `http://grlc-server/api-git/<user>/<repo>`    grlc can build an API from any Github repository, specified by the GitHub user name of the owner (`<user>`) and repository name (`<repo>`).    For example, assuming your queries are stored on a Github repo: `https://github.com/CLARIAH/grlc-queries/`, point your browser to the following location  `http://grlc.io/api-git/CLARIAH/grlc-queries/`    grlc can make use of git's version control mechanism to generate an API based on a specific version of queries in the repository. This can be done by including the commit sha in the URL path (`http://grlc-server/api-git/<user>/<repo>/commit/<sha>`), for example: `http://grlc.io/api-git/CLARIAH/grlc-queries/commit/79ceef2ee814a12e2ec572ffaa2f8212a22bae23`    grlc can also use a subdirectory inside your Github repo. This can be done by including a subdirectory in the URL path (`http://grlc-server/api-git/<user>/<repo>/subdir/<subdir>`).    #### From local storage  > API path:  `http://grlc-server/api-local/`    grlc can generate an API from a local directory in the computer where your grlc server runs. You can configure the location of this directory in your [grlc server configuration file](#grlc-server-configuration). See also [how to install and run your own grlc instance](#install-and-run).    When the API is generated from a local directory, API information can be loaded from a configuration file in that folder. This file must be called `local-api-config.ini` and it has the following format:  ```ini  [repo_info]  repo_title = Some title  api_description = Description of my API  contact_name = My name  contact_url = https://mypage/  licence_url = https://mylicence/  ```    #### From a specification file  > API path:  `http://grlc-server/api-url/?specUrl=<specUrl>`    grlc can generate an API from a yaml specification file accessible on the web.    For example, assuming your queries are listed on spec file: `https://raw.githubusercontent.com/CLARIAH/grlc-queries/master/urls.yml`, point your browser to the following location  `http://grlc.io/api-url?specUrl=https://raw.githubusercontent.com/CLARIAH/grlc-queries/master/urls.yml`    ##### Specification file syntax  A grlc API specification file is a YAML file which includes the necessary information to create a grlc API, most importantly a list of URLs to decorated and HTTP-dereferenceable SPARQL queries. This file should contain the following fields     - `title`: Title of my API   - `description`: API description   - `contact`: Contact details of the API owner. This should include the `name` and `url` properties.   - `licence`: A URL pointing to the licence file for the API.   - `queries`: A list of URLs of SPARQL queries (with header decorators).    For example:  ```YAML  title: Title of my API  description: Description of my API  contact:    name: Contact Name    url: https://www.mywebsite.org  licence: http://example.org/licence.html  queries:    - https://www.mywebsite.org/query1.rq    - https://www.mywebsite.org/query2.rq    - https://www.otherwebsite.org/query3.rq  ```    ### grlc generated API    The API paths of all location types point to the generated [swagger-ui](https://swagger.io/) style API documentation. On the API documentation page, you can explore available API calls and execute individual API calls.    You can also view the swagger spec of your API, by visiting `<API-path>/swagger`, for example: `http://grlc.io/api-git/CLARIAH/grlc-queries/swagger`    ### grlc query execution  When you call an API endpoint, grlc executes the SPARQL query for that endpoint by combining supplied parameters and decorators.    There are 4 options to specify your own endpoint:    * Add a `sparql_endpoint` on your [`config.ini`](#grlc-server-configuration)  * Add a `endpoint` parameter to your request: 'http://grlc.io/user/repo/query?endpoint=http://sparql-endpoint/'. You can add a `#+ endpoint_in_url: False` decorator if you DO NOT want to see the `endpoint` parameter in the swagger-ui of your API.  * Add the `#+ endpoint:` [decorator](#`endpoint`).  * Add the URL of the endpoint on a single line in an `endpoint.txt` file within the GitHub repository that contains the queries.    The endpoint call will return the result of executing the query as a json representation of rdflib.query.QueryResult (for other result formats, you can use content negotiation via HTTP `Accept` headers). For json responses, the schema of the response can be modified by using the `#+ transform:` [decorator](#`transform`).    ## Decorator syntax  Special decorators are available to make your swagger-ui look nicer and to increase functionality. These are provided as comments at the start of your query file, making it still syntactically valid SPARQL. All decorators start with `#+ `, for example:    ```SPARQL  #+ decorator_1: decorator value  #+ decorator_1: decorator value    SELECT * WHERE {    ?s ?p ?o .  }  ```  The following is a list of available decorators and their function:    ### `summary`  Creates a summary of your query/operation. This is shown next to your operation name in the swagger-ui.    Syntax:  ```  #+ summary: This is the summary of my query/operation  ```    Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/summary.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_summary).    ### `description`  Creates a description of your query/operation. This is shown as the description of your operation in the swagger-ui.    Syntax:  ```  #+ description: Extended description of my query/operation.  ```    Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/description.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_description).    ### `endpoint`  Specifies a query-specific endpoint.    Syntax:  ```  #+ endpoint: http://example.com/sparql  ```    Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/endpoint.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_endpoint).    ### `pagination`  Paginates the results in groups of (for example) 100. Links to previous, next, first, and last result pages are provided as HTTP response headers to avoid polluting the payload (see details [here](https://developer.github.com/v3/guides/traversing-with-pagination/))    Syntax:  ```  #+ pagination: 100  ```    Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/pagination.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_pagination).    ### `method`  Indicates the HTTP request method (`GET` and `POST` are supported).    Syntax:  ```  #+ method: GET  ```    Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/method.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/post_method).    ### `tags`  Assign tags to your query/operation. Query/operations with the same tag are grouped together in the swagger-ui.    Syntax:  ```  #+ tags:  #+   - firstTag  #+   - secondTag  ```    Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/tags.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/group1/get_tags).    ### `defaults`  Set the default value in the swagger-ui for a specific parameter in the query.    Syntax:  ```  #+ defaults:  #+   - param_name: default_value  ```    Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/defaults.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_defaults).    ### `enumerate`  Indicates which parameters of your query/operation should get enumerations (and get dropdown menus in the swagger-ui) using the given values from the SPARQL endpoint. The values for each enumeration variable can also be specified into the query decorators to save endpoint requests and speed up the API generation.    Syntax:  ```  #+ enumerate:  #+   - var1:  #+     - value1  #+     - value2  ```    Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/enumerate.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_enumerate).    Notice that these should be plain variable names without SPARQL/BASIL conventions (so `var1` instead of `?_var1_iri`)    ###  `endpoint_in_url`  Allows/disallows the `endpoint` parameter from being provided as a URL parameter (allowed by default).    Syntax:  ```  #+ endpoint_in_url: False  ```    Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/endpoint_url.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_endpoint_url).    ###  `transform`  Allows  query results to be converted to the specified JSON structure, by using [SPARQLTransformer](https://github.com/D2KLab/py-sparql-transformer) syntax. Notice that the response content type must be set to `application/json` for the transformation to take effect.    Syntax:  ```  #+ transform: {  #+     ""key"": ""?p"",  #+     ""value"": ""?o"",  #+     ""$anchor"": ""key""  #+   }  ```    Example [query](https://github.com/CLARIAH/grlc-queries/blob/master/transform.rq) and the equivalent [API operation](http://grlc.io/api-git/CLARIAH/grlc-queries/#/default/get_transform).    ### Example APIs    Check these out:  - http://grlc.io/api-git/CLARIAH/grlc-queries  - http://grlc.io/api-url?specUrl=https://raw.githubusercontent.com/CLARIAH/grlc-queries/master/urls.yml  - http://grlc.io/api-git/CLARIAH/wp4-queries-hisco  - http://grlc.io/api-git/albertmeronyo/lodapi  - http://grlc.io/api-git/albertmeronyo/lsq-api  - https://grlc.io/api-git/CEDAR-project/Queries    You'll find the sources of these and many more in [GitHub](https://github.com/search?o=desc&q=endpoint+summary+language%3ASPARQL&s=indexed&type=Code&utf8=%E2%9C%93)    Use [this GitHub search](https://github.com/search?q=endpoint+summary+language%3ASPARQL&type=Code&utf8=%E2%9C%93) to see examples from other grlc users.    ## Install and run  You can use grlc in different ways:   - [Via grlc.io](#grlc.io): you can use the [grlc.io service](https://grlc.io/)   - [Via Docker](#Docker): you can use the [grlc docker image](https://hub.docker.com/r/clariah/grlc) and start your own grlc server.   - [Via pip](#Pip): you can install the [grlc Python package](https://pypi.org/project/grlc/) and start your own grlc server or use grlc as a Python library.    More details for each of these options are given below.    ### grlc.io  The easiest way to use grlc is by visiting [grlc.io](http://grlc.io/) and using this service to convert SPARQL queries into a RESTful API. Your queries can be [stored on a github repo](#from-a-github-repository) or can be [listed on a specification file](#from-a-specification-file).    ### Docker  To run grlc via [docker](https://www.docker.com/), you'll need a working installation of docker. To deploy grlc, just pull the [latest image from Docker hub](https://hub.docker.com/r/clariah/grlc/). :  ```bash  docker run -it --rm -p 8088:80 clariah/grlc  ```    The docker image allows you to setup several environment variable such as `GRLC_SERVER_NAME` `GRLC_GITHUB_ACCESS_TOKEN` and `GRLC_SPARQL_ENDPOINT`:  ```bash  docker run -it --rm -p 8088:80 -e GRLC_SERVER_NAME=grlc.io -e GRLC_GITHUB_ACCESS_TOKEN=xxx -e GRLC_SPARQL_ENDPOINT=http://dbpedia.org/sparql -e DEBUG=true clariah/grlc  ```    ### Pip  If you want to run grlc locally or use it as a library, you can install grlc on your machine. Grlc is [registered in PyPi](https://pypi.org/project/grlc/) so you can install it using pip.    #### Prerequisites  grlc has the following requirements:  - Python3  - development files (depending on your OS):  ```bash  sudo apt-get install libevent-dev python-all-dev  ```    #### pip install  Once the base requirements are satisfied, you can install grlc like this:  ```bash  pip install grlc  ```    Once grlc is installed, you have several options:   - [Stand alone server](#Standalone-server)   - [Using a WSGI server](#Using-a-WSGI-server)   - [As a python library](#Grlc-library)    #### Standalone server  grlc includes a command line tool which you can use to start your own grlc server:  ```bash  grlc-server  ```    #### Using a WSGI server  You can run grlc using a WSGI server such as gunicorn as follows:  ```bash  gunicorn grlc.server:app  ```    If you want to use your own gunicorn configuration, for example `gunicorn_config.py`:  ```python  workers = 5  worker_class = 'gevent'  bind = '0.0.0.0:8088'  ```  Then you can run it as:  ```bash  gunicorn -c gunicorn_config.py grlc.server:app  ```    **Note:** Since `gunicorn` does not work under Windows, you can use `waitress` instead:  ```bash  waitress-serve --port=8088 grlc.server:app  ```    If you want to run grlc at system boot as a service, you can find example upstart scripts at [upstart/](upstart/grlc-docker.conf)    #### grlc library  You can use grlc as a library directly from your own python script. See the [usage example](https://github.com/CLARIAH/grlc/blob/master/doc/notebooks/GrlcFromNotebook.ipynb) to find out more.    #### grlc server configuration  Regardless of how you are running your grlc server, you will need to configure it using the `config.ini` file. Have a look at the [example config file](./config.default.ini) to see how it this file is structured.    The configuration file contains the following variables:   - `github_access_token` [access token](#github-access-token) to communicate with Github API.   - `local_sparql_dir` local storage directory where [local queries](#from-local-storage) are located.   - `server_name` name of the server (e.g. grlc.io)   - `sparql_endpoint` default SPARQL endpoint   - `user` and `password` SPARQL endpoint default authentication (if required, specify `'none'` if not required)   - `debug` enable debug level logging.    ##### GitHub access token  In order for grlc to communicate with GitHub, you'll need to tell grlc what your access token is:    1. Get a GitHub personal access token. In your GitHub's profile page, go to _Settings_, then _Developer settings_, _Personal access tokens_, and _Generate new token_  2. You'll get an access token string, copy it and save it somewhere safe (GitHub won't let you see it again!)  3. Edit your `config.ini` or `docker-compose.yml` as value of the environment variable `GRLC_GITHUB_ACCESS_TOKEN`.    # Contribute!  grlc needs **you** to continue bringing Semantic Web content to developers, applications and users. No matter if you are just a curious user, a developer, or a researcher; there are many ways in which you can contribute:    - File in bug reports  - Request new features  - Set up your own environment and start hacking    Check our [contributing](CONTRIBUTING.md) guidelines for these and more, and join us today!    If you cannot code, that's no problem! There's still plenty you can contribute:    - Share your experience at using grlc in Twitter (mention the handle **@grlcldapi**)  - If you are good with HTML/CSS, [let us know](mailto:albert.meronyo@gmail.com)    ## Related tools  - [SPARQL2Git](https://github.com/albertmeronyo/SPARQL2Git) is a Web interface for editing SPARQL queries and saving them in GitHub as grlc APIs.  - [grlcR](https://github.com/CLARIAH/grlcR) is a package for R that brings Linked Data into your R environment easily through grlc.  - [Hay's tools](https://tools.wmflabs.org/hay/directory/#/showall) lists grlc as a Wikimedia-related tool :-)    ## This is what grlc users are saying  - [Flavour your Linked Data with grlc](https://blog.esciencecenter.nl/flavour-your-linked-data-with-garlic-98bfbb358e06), by Carlos Martinez  - [Converting any SPARQL endpoint to an OpenAPI](http://chem-bla-ics.blogspot.com/2018/07/converting-any-sparql-endpoint-to.html) by Egon Willighagen    Quotes from grlc users:  > A cool project that can convert a random SPARQL endpoint into an OpenAPI endpoint    > It enables us to quickly integrate any new API requirements in a matter of seconds, without having to worry about configuration or deployment of the system    > You can store your SPARQL queries on GitHub and then you can run your queries on your favourite programming language (Python, Javascript, etc.) using a Web API (including swagger documentation) just as easily as loading data from a web page    **Contributors:**	[Albert Meroño](https://github.com/albertmeronyo), [Rinke Hoekstra](https://github.com/RinkeHoekstra), [Carlos Martínez](https://github.com/c-martinez)    **Copyright:**	Albert Meroño, Rinke Hoekstra, Carlos Martínez    **License:**	MIT License (see [LICENSE.txt](LICENSE.txt))    ## Academic publications    - Albert Meroño-Peñuela, Rinke Hoekstra. “grlc Makes GitHub Taste Like Linked Data APIs”. The Semantic Web – ESWC 2016 Satellite Events, Heraklion, Crete, Greece, May 29 – June 2, 2016, Revised Selected Papers. LNCS 9989, pp. 342-353 (2016). ([PDF](https://link.springer.com/content/pdf/10.1007%2F978-3-319-47602-5_48.pdf))  - Albert Meroño-Peñuela, Rinke Hoekstra. “SPARQL2Git: Transparent SPARQL and Linked Data API Curation via Git”. In: Proceedings of the 14th Extended Semantic Web Conference (ESWC 2017), Poster and Demo Track. Portoroz, Slovenia, May 28th – June 1st, 2017 (2017). ([PDF](https://www.albertmeronyo.org/wp-content/uploads/2017/04/sparql2git-transparent-sparql-4.pdf))  - Albert Meroño-Peñuela, Rinke Hoekstra. “Automatic Query-centric API for Routine Access to Linked Data”. In: The Semantic Web – ISWC 2017, 16th International Semantic Web Conference. Lecture Notes in Computer Science, vol 10587, pp. 334-339 (2017). ([PDF](https://www.albertmeronyo.org/wp-content/uploads/2017/07/ISWC2017_paper_430.pdf))  - Pasquale Lisena, Albert Meroño-Peñuela, Tobias Kuhn, Raphaël Troncy. “Easy Web API Development with SPARQL Transformer”. In: The Semantic Web – ISWC 2019, 18th International Semantic Web Conference. Lecture Notes in Computer Science, vol 11779, pp. 454-470 (2019). ([PDF](https://www.albertmeronyo.org/wp-content/uploads/2019/06/ISWC2019_paper_237.pdf)) """
Semantic web;https://github.com/pchampin/sophia_rs;"""# Sophia    A Rust toolkit for RDF and Linked Data.    [![Actions Status](https://github.com/pchampin/sophia_rs/actions/workflows/lint_and_test.yml/badge.svg)](https://github.com/pchampin/sophia_rs/actions)  [![Latest Version](https://img.shields.io/crates/v/sophia.svg)](https://crates.io/crates/sophia)  [![Documentation](https://docs.rs/sophia/badge.svg)](https://docs.rs/sophia/)    It comprises the following crates:    * [`sophia_api`] defines a generic API for RDF and linked data,    as a set of core traits and types;    more precisely, it provides traits for describing    - terms, triples and quads,    - graphs and datasets,    - parsers and serializers  * [`sophia_iri`] provides functions, types and traits for validating and resolving IRIs.  * [`sophia_term`] defines implementations of the `TTerm` trait from `sophia_api`.  * [`sophia_inmem`] defines in-memory implementations of the `Graph` and `Dataset` traits from `sophia_api`.  * [`sophia_turtle`] provides parsers and serializers for the Turtle-family of concrete syntaxes.  * [`sophia_xml`] provides parsers and serializers for RDF/XML.  * [`sophia_jsonld`] provides preliminary support for JSON-LD.  * [`sophia_indexed`] and [`sophia_rio`] are lower-level crates, used by the ones above.     and finally:  * [`sophia`] is the “all-inclusive” crate,    re-exporting symbols from all the crates above.      ## Licence    [CECILL-B] (compatible with BSD)    ## Testing    The test suite depends on the [the [JSON-LD test-suite]  which is included as a `git` submodule.  In order to run all the tests, you need to execute the following commands:  ```bash  $ git submodule init  $ git submodule update  ```    ## Citation    When using Sophia, please use the following citation:    > Champin, P.-A. (2020) ‘Sophia: A Linked Data and Semantic Web toolkit for Rust’, in Wilde, E. and Amundsen, M. (eds). The Web Conference 2020: Developers Track, Taipei, TW. Available at: https://www2020devtrack.github.io/site/schedule.    Bibtex:  ```bibtex  @misc{champin_sophia_2020,          title = {{Sophia: A Linked Data and Semantic Web toolkit for Rust},          author = {Champin, Pierre-Antoine},          howpublished = {{The Web Conference 2020: Developers Track}},          address = {Taipei, TW},          editor = {Wilde, Erik and Amundsen, Mike},          month = apr,          year = {2020},          language = {en},          url = {https://www2020devtrack.github.io/site/schedule}  }  ```    ## History    An outdated comparison of Sophia with other RDF libraries is still available  [here](https://github.com/pchampin/sophia_benchmark/blob/master/benchmark_results.ipynb).      [`sophia_api`]: https://crates.io/crates/sophia_api  [`sophia_iri`]: https://crates.io/crates/sophia_iri  [`sophia_term`]: https://crates.io/crates/sophia_term  [`sophia_inmem`]: https://crates.io/crates/sophia_inmem  [`sophia_turtle`]: https://crates.io/crates/sophia_turtle  [`sophia_xml`]: https://crates.io/crates/sophia_xml  [`sophia_jsonld`]: https://crates.io/crates/sophia_jsonld  [`sophia_indexed`]: https://crates.io/crates/sophia_indexed  [`sophia_rio`]: https://crates.io/crates/sophia_rio  [`sophia`]: https://crates.io/crates/sophia  [CECILL-B]: https://cecill.info/licences/Licence_CeCILL-B_V1-en.html  [RDF test-suite]: https://github.com/w3c/rdf-tests/  [JSON-LD test-suite]: https://github.com/w3c/json-ld-api/ """
Semantic web;https://github.com/renespeck/fox-java;"""[1]: ./src/main/java/org/aksw/fox/binding/java/Examples.java    fox-java  ========    Java bindings for FOX - Federated Knowledge Extraction Framework      In [Examples.java][1] you can find an example.    ```Java  final IFoxApi fox = new FoxApi()       .setApiURL(new URL(""http://0.0.0.0:4444/fox""))       .setTask(FoxParameter.TASK.RE)       .setOutputFormat(FoxParameter.OUTPUT.JSONLD)       .setLang(FoxParameter.LANG.EN)       .setInput(""A. Einstein was born in Ulm."")       // .setLightVersion(FoxParameter.FOXLIGHT.ENBalie)       .send();     final String jsonld = fox.responseAsFile();   final FoxResponse response = fox.responseAsClasses();     List<Entity> entities = response.getEntities();   List<Relation> relations = response.getRelations();    ```    ### Maven      <dependencies>        <dependency>          <groupId>com.github.renespeck</groupId>          <artifactId>fox-java</artifactId>          <version>e67a2bd475</version>        </dependency>      </dependencies>        <repositories>          <repository>              <id>jitpack.io</id>              <url>https://jitpack.io</url>          </repository>      </repositories> """
Semantic web;https://github.com/stardog-union/stardog-groovy;"""Stardog Groovy  ==========    Licensed under the [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)    _Current Version **5.3.5**_     Stardog Groovy - Groovy language bindings to use to develop apps with the [Stardog Graph / RDF Database](http://stardog.com).      ![Stardog](http://stardog.com/img/stardog.png)       ## What is it? ##    This bindings provides a set of idiomatic Groovy APIs for interacting with the Stardog database, similar to the Stardog Spring project - an easy to use method for creating connection pools, and the ability run queries over them. To run the queries, Stardog Groovy uses standard Groovy patterns, such as passing in a closure to iterate over result sets.  Common use cases for Stardog-groovy are ETL scripts, command line applications, usage with Grails, or other Groovy frameworks.       ## How to use it    1. Download Stardog from stardog.com, and follow the installation instructions  2. Add the `com.complexible.stardog:stardog-groovy:<version>` dependency declaration to your build tool, such as Maven or Gradle  3. Make sure your build prioritizes your local Maven repository (i.e. `~/.m2/repository`), where the core Stardog binaries were installed by step 2  4. Enjoy!    There is also a `shadowJar` task available via the Shadow plugin to produce a fatjar with all of the Stardog dependencies.    ## Quickstart    ```  @Grab('com.complexible.stardog:stardog-groovy:5.3.5')  import com.complexible.stardog.ext.groovy.Stardog    def stardog = new Stardog(url: ""http://localhost:5820"", to:""testdb"", username: ""admin"", password:""admin"", reasoning: true)    stardog.query(""select ?s ?p ?o where { ?s ?p ?o } limit 2"", { println it })  ```    ## Examples ##    Create a new embedded database in one line  ```groovy  	def stardog = new Stardog(home:""/opt/stardog"", to:""testgroovy"", username:""admin"", password:""admin"")  ```    Collect query results via a closure  ```groovy  	def list = []  	stardog.query(""select ?x ?y ?z WHERE { ?x ?y ?z } LIMIT 2"") { list << it }   	// list has the two Sesame BindingSet's added to it, ie TupleQueryResult.next called per each run on the closure  ```    Collect query results via projected result values  ```groovy      stardog.each(""select ?x ?y ?z WHERE { ?x ?y ?z } LIMIT 2"", {         println x // whatever x is bound to in the result set         println y // ..         println z //       }  ```    Like query, this is executed over each TupleQueryResult    Insert multidimensional arrays, single triples also works  ```groovy  	stardog.insert([ [""urn:test3"", ""urn:test:predicate"", ""hello world""], [""urn:test4"", ""urn:test:predicate"", ""hello world2""] ])  ```    Remove triples via a simple groovy list  ```groovy  	stardog.remove([""urn:test3"", ""urn:test:predicate"", ""hello world""])  ```    ## Upgrading from Prior Releases    Significant changes in 2.1.3:    *    Installation now available via Maven Central and ""com.complexible.stardog:stardog-groovy:2.1.3"" dependency  *    No longer a dependency on Spring, i.e. the Stardog-Spring DataSource can no longer be passed as a constructor.  The Stardog Groovy class performs all the same operations.  *    Stardog-groovy 4.2.1 and later should be built with Gradle 2.3      ## Development ##    To get started, just clone the project. You'll need a local copy of Stardog to be able to run the build. For more information on starting the Stardog DB service and how it works, go to [Stardog's documentation](http://stardog.com/docs/), where you'll find everything you need to get up and running with Stardog.    Once you have the local project, start up a local Stardog and create a testdb with `stardog-admin db create -n testdb $STARDOG/data/examples/lumbSchema.owl $STARDOG/data/examples/University0_0.owl`.     You can then build the project        gradle build    # validate all the test pass      gradle install  # install jar into local m2    That will run all the JUnit tests and create the jar in build/libs.  The test does use a running Stardog, and if you receive error during the test it is likely you're Stardog server is not running or has an invalid license.  This usually manifests in an exit of a Gradle worker, which is the JVM running the JUnit tests.       ## Contributing ##    This framework is in continuous development, please check the [issues](https://github.com/clarkparsia/stardog-groovy/issues) page. You're welcome to contribute.    ## License    Copyright 2015 - 2018 Stardog Union  Copyright 2012 - 2015 Clark & Parsia, LLC  Copyright 2012 Al Baker    Licensed under the Apache License, Version 2.0 (the ""License"");  you may not use this file except in compliance with the License.  You may obtain a copy of the License at    * [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0)      Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License.     """
Semantic web;https://github.com/Data2Semantics/fox-ui;"""# Fox-UI  Web UI for [FoxPSL](https://github.com/uzh/fox/uzh) """
Semantic web;https://github.com/ontola/hextuples;"""# HexTuples    _Status: draft_    _Version: 0.3.0_    HexTuples is a simple datamodel for dealing with linked data.  This document both describes the model and concepts of HexTuples, as well as the (at this moment only) serialization format: HexTuples-NDJSON.  It is very **easy to parse**, can be used for **streaming parsing** and is designed to be **highly performant** in JS contexts.     ## Concepts    ### HexTuple    A single _HexTuple_ is an atomic piece of data, similar to an [RDF Triple](https://www.w3.org/TR/rdf-concepts/#section-triples) (also known as Statements or Quads).  A HexTuple cotains a small piece of information.   HexTuples consist of six fields: `subject`, `predicate`, `value`, `datatype`, `language` and `graph`.    Let's encode the following sentence in HexTuples:    _Tim Berners-Lee, the director of W3C, is born in London on the 8th of June, 1955._    | Subject    | Predicate     | Value | DataType | Language | Graph |  |---------|----------------|------------|-----|-----|----|  | [Tim](https://www.w3.org/People/Berners-Lee/)     |[birthPlace](http://schema.org/birthPlace) | [London](http://dbpedia.org/resource/London)     | | |  | [Tim](https://www.w3.org/People/Berners-Lee/)     |[birthDate](http://schema.org/birthDate) | 1955-06-08     | [xsd:date](http://www.w3.org/2001/XMLSchema#date) | |   | [Tim](https://www.w3.org/People/Berners-Lee/)     |[jobTitle](http://schema.org/jobTitle) | Director of W3C  | [xsd:string](http://www.w3.org/2001/XMLSchema#string) | en-US |     ### URI    URI stands for [Uniform Resource Identifier, specified in RDF 3986](https://tools.ietf.org/html/rfc3986).  The best known type of URI is the URL.  Although it is currently best practice to use mostly HTTPS URLs as URIs, HexTuples works with any type of URI.    ### Subject    - The _subject_ is identifier of the thing the statement is about.  - This field is required.  - It MUST be a URI.    ### Predicate    - The _predicate_ describes the abstract property of the statement.  - This field is required.  - It MUST be a URI.    ### Value    - The _value_ contains the object of the HexTuple.  - This field is required.  - It can be any datatype, specified in the `datatype` of the HexTuple.    ### Datatype    - The _datatype_ contains the object of the HexTuple.  - This field is optional.  - It MUST be a URI or an empty string.  - When the Datatype is a NamedNode, use: `globalId`  - When the Datatype is a BlankNode, use: `localId`    ### Language    - The _datatype_ contains the object of the HexTuple.  - This field is optional.  - It MUST be an [RFC 3066 language tag](https://tools.ietf.org/html/rfc3066) or an empty string.    ## Relation to RDF    The HexTuples datamodel closely resembles the RDF Data Model, which is the de-facto standard for linked data.  RDF statements are often called Triples, because they consist of a `subject`, `predicate` and `value`.  The `object` field is either a single URI (in Named Nodes), or a combination of three fields (in Literal): `value`, `datatype`, `language`.  This means that a single Triple can actually consist of _five_ fields: the `subject`, `predicate`, `value`, `datatype` and the `language`.   A Quad statement also has a `graph`, which totals to six fields, hence the name: HexTuples.  Instead of making a distinction between Literal statements and NamedNode statements (which have two different models), HexTuples uses a single model that describes both.  **Having a single model for all statements (HexTuples), makes it easier to serialize, query and store data.**    ## HexTuples-NDJSON    _This document serves as a work in progress / draft specification_    HexTuples-NDJSON is an [NDJSON](http://ndjson.org/) (Newline Delimited JSON) based HexTuples / RDF serialization format.  It is desgined to support streaming parsing and provide great performance in a JS context (i.e. the browser).    - A valid HexTuples document MUST be serialized using [NDJSON](http://ndjson.org/)  - HexTuples-NDJSON MIME type: `application/hex+x-ndjson; charset=utf-8`  - Each array MUST consist of six strings.  - Each array represents one RDF statement / quad / triple  - The six strings in each array respectively represent  `subject`, `predicate`, `value`, `datatype`, `lang` and `graph`.  - The `datatype` and `lang` fields are only used when the `value` represents a Literal value (i.e. not a URI, but a string / date / something else). In RDF, the combination of `value`, `datatype` and `lang` are known as `object`.  - When expressing an Object that is a NamedNode, use this string as the datatype: ""globalId"" ([discussion](https://github.com/ontola/hextuples/issues/1))  - When expressing an Object that is a BlankNode, use this string as the datatype: ""localId""  - If the `graph` is a blank node (i.e. anonymous), use an underscore as the URI scheme: `_:myNode`. ([discussion](https://github.com/ontola/hextuples/issues/2)). Parsers SHOULD interpret these as blank graphs, but MAY discard these if they have no support for them.  - When a field has no value, use an empty string: `""""`    ### Example    English:    _Tim Berners-Lee was born in London, on the 8th of june in 1955._    Turtle / N-Triples:    ```n-triples  <https://www.w3.org/People/Berners-Lee/> <http://schema.org/birthDate> ""1955-06-08""^^<http://www.w3.org/2001/XMLSchema#date>.  <https://www.w3.org/People/Berners-Lee/> <http://schema.org/birthPlace> <http://dbpedia.org/resource/London>.  ```    Expresed in HexTuples:    ```ndjson  [""https://www.w3.org/People/Berners-Lee/"", ""http://schema.org/birthDate"", ""1955-06-08"", ""http://www.w3.org/2001/XMLSchema#date"", """", """"]  [""https://www.w3.org/People/Berners-Lee/"", ""http://schema.org/birthPlace"", ""http://dbpedia.org/resource/London"", ""globalId"", """", """"]  ```    ## Implementations    ### Ontola TypeScript HexTuples Parser    * <https://github.com/ontola/hextuples-parser>    This Typescript code should give you some idea of how to write a parser for HexTuples.    ```ts  const object = (value: string, datatype: string, language: string): SomeTerm => {    if (language) {      return literal(value, language);    } else if (datatype === 'globalId') {      return namedNode(value);    } else if (datatype === 'localId') {      return blankNode(value);    }      return literal(value, namedNode(datatype));  };    const lineToQuad = (h: string[]) => quad(    h[0].startsWith('_:') ? blankNode(h[0]) : namedNode(h[0]),    namedNode(h[1]),    object(h[2], h[3], h[4]),    h[5] ? namedNode(h[5]) : defaultGraph(),  );  ```    ### Python RDFlib    * <https://pypi.org/project/rdflib/>  * RDFLib is a pure Python package for working with RDF.   * It supports parsing and serliazing RDF as HexTuples  * Internally (in Python objects), RDF parsed from HexTuples data is represented in a _Conjunctive Graph_, that is a multi-graph object  * HexTuples files must end in the file extension `.hext` for RDFlib to auto-recognise the format although files with any ending can be used if the format is given (`format=hext`)    An RDF format conversion tool using RDFLib that can convert from/to HexTuples is online at <http://rdftools.surroundaustralia.com/convert>.    ## Motivation for HexTuples-NDJSON    HexTuples was designed by [Thom van Kalkeren](https://github.com/fletcher91/) (CTO of Ontola) because he noticed that parsing / serialization was unnecessarily costly in our [full-RDF stack](https://ontola.io/blog/full-stack-linked-data/), even when using the relatively performant `n-quads` format.    - Since HexTuples is serialized in NDJSON, it benefits from the [highly optimised JSON parsers in browsers](https://v8.dev/blog/cost-of-javascript-2019#json).  - It uses NDJSON instead of regular JSON because it makes it easier to parse **concatenated responses** (multiple root objects in one document).  - NDJSON enables **streaming parsing** as well, which gives it another performance boost.  - Some JS RDF libraries ([link-lib](https://github.com/fletcher91/link-lib/), [link-redux](https://github.com/fletcher91/link-redux/)) have an internal RDF graph model which uses these HexTuples arrays as well, which means that there is minimal mapping cost when parsing Hex-Tuple statements.  This format is especially suitable for real front-end applications that use dynamic RDF data. """
Semantic web;https://github.com/AKSW/HiBISCuS;"""# HiBISCuS  HiBISCuS: Hypergraph-Based Source Selection for SPARQL Endpoint Federation    ## Source Code  The HiBISCuS source code along with all of the 3 extensions (SPLENDID, FedX, DARQ) can be checkout from project website https://github.com/AKSW/HiBISCuS/    ## FedBench  FedBench queries can be downloaded from project website https://code.google.com/p/fbench/    ## Datasets Availability  All the datasets and corresponding virtuoso SPARQL endpoints can be downloaded from the project old website https://code.google.com/p/hibiscusfederation/.       ## Usage Information  In the following we explain how one can setup the BigRDFBench evaluation framework and measure the performance of the federation engine.    ## SPARQL Endpoints Setup  The first step is to download the SPARQL endpoints (portable Virtuoso SAPRQL endpoints from second table above) on different machines, i.e., computers. Best would be one SPARQL endpoint per machine. Therefore, you need a total of 13 machines. However, you can start more than one SPARQL endpoints per machine.  The next step is to start the SPARQL endpoint from bin/start.bat (for windows) or bin/start_virtuoso.sh (for Linux). Make a list of the all SPARQL endpoints URL's ( required as input for index-free SPARQL query federation engines, i.e., FedX). It is important to note that index-assisted federation engines (e.g., SPLENDID, DARQ, ANAPSID) usually stores the endpoint URL's in its index. The local SPARQL endpoints URL's are given above in second table.  Running SPARQL Queries  Provides the list of SPARQL endpoints URL's, and a FedBench? query to the underlying federation engine. The query evaluation start-up files for the selected systems (which you can checkout from project website https://github.com/AKSW/HiBISCuS/) are given below.    ----------FedX-original-----------------------    package : package org.aksw.simba.start;    File:QueryEvaluation?.java    ----------FedX-HiBISCuS-----------------------    package : package org.aksw.simba.fedsum.startup;    File:QueryEvaluation?.java    ----------SPLENDID-original-----------------------    package : package de.uni_koblenz.west.evaluation;    File:QueryProcessingEval?.java    ----------SPLENDID-HiBISCuS-----------------------    package : package de.uni_koblenz.west.evaluation;    File:QueryProcessingEval?.java    ----------ANAPSID-----------------------    Follow the instructions given at https://github.com/anapsid/anapsid to configure the system and then use anapsid/ivan-scripts/runQuery.sh to run a query. """
Semantic web;https://github.com/phillord/horned-owl;"""Horned OWL  ==========    Horned-OWL is a library for manipulating OWL (Ontology Web Language)  data. While there are a number of libraries that manipulate this form  of data such as the [OWL API](https://github.com/owlcs/owlapi),  they can be quite slow. Horned-OWL is aimed at allowing ontologies  with millions of terms.    The library now implements all of OWL2, and we are working on further  parser functionality. We are testing it with real world tasks, such as  parsing the Gene Ontology, which is does in 2-3 seconds which is 20-40  times faster than the OWL API. """
Semantic web;https://github.com/esbranson/any2rdf;"""any2rdf  =======    Convert files (OpenOffice.org/LibreOffice UNO, GDAL, etc.) to RDF    shp2rdf  -------    	shp2rdf.py [-d] config.js    ### Dependencies:    * [RDFLib](https://pypi.python.org/pypi/rdflib)  * [GDAL](https://pypi.python.org/pypi/GDAL/) """
Semantic web;https://github.com/wastl/rdfdot;"""RDFDot  ======    Tools for drawing graphs from RDF files with GraphViz implemented in Java 8 using Java Native Interface calls  to GraphViz.    # Demo #    You can access the demo at [our demo server](http://demo4.newmedialab.at/rdfdot/), it should be running most of  the time. Simply cut & paste a Turtle or RDF/XML document, optionally change the configuration and press ""Render"".    # Building #    RDFDot comes with several different renderers for accessing the Graphviz layouting library. There are two different  approaches:    * call the _dot_ process as external shell command from Java; requires graphviz to be installed and accessible on your server  * use the Java Native Interface library that is provided by RDFDot; faster but requires manual compilation    ## External Shell Command ##    The first approach is simple to build using standard Maven:        mvn clean install -DskipTests    The -DskipTests is necessary at the moment, because some tests require the JNI library to work properly.    ## Java Native Interface ##    If you want to use the JNI library, please follow the following sequence:        mvn clean      cd rdfdot-core/src/main/native      make      cd ../../../..      mvn install    Calling make will download all the necessary C libraries, compile them and link them statically into a JNI library  for RDFdot. Note that this has only been tested to work on Linux.      # Installing #    RDFDot currently consists of the libraries rdfdot-api and rdfdot-core, which you can use in your own projects, and the  web application rdfdot-web, which can be deployed in any Java web container. When RDFDot has been properly installed,  simply add the approprate Maven dependencies to your project:        <dependency>          <groupId>net.wastl.rdfdot</groupId>          <artifactId>rdfdot-core</artifactId>          <version>1.0.0-SNAPSHOT</version>      </dependency>    If you want to use the (faster) JNI rendering library, it is necessary that you copy the libgraphviz.so library to  an appropriate location and call Java with -Djava.library.path=/path/to/location.      # Library Usage #    ## Configuration ##    The rdfdot-api library contains a class GraphConfiguration, which can be used for setting layouting configuration  options for the graph. It currently supports changing the node style, shape, color, and fillcolor for URI, BNode and  Literal nodes, as well as the arrow shape, style and color for edges. Furthermore, it is possible to change the layout  direction of the graph (default: left-right). All available options are defined using the enums Arrows, Layouts, Shapes  and Styles.    ## RDFHandler ##    The visualization library is implemented as a Sesame RDFHandler, so it can be used anywhere Sesame accepts an  RDFHandler, e.g.    * in a RIO parser (using RDFParser.setRDFHandler(...))  * in a repository query (using RepositoryConnection.exportStatements(...))  * in a SPARQL graph query (using GraphQuery.evaluate(...))    To initialize the RDFHandler, use e.g. the following sequence of statements:            GraphConfiguration configuration = new GraphConfiguration();          GraphvizSerializer serializer = new GraphvizSerializerNative(configuration);            RDFParser parser = Rio.createParser(RDFFormat.TURTLE);          parser.setRDFHandler(new GraphvizHandler(serializer));          parser.parse(in, ""http://localhost/"");            byte[] image = serializer.getResult();    Different GraphSerializers are available, including GraphSerializerNative (using JNI calls) and GraphSerializerCommand  (executing a shell command).   """
Semantic web;https://github.com/dice-group/CostFed;"""### CostFed: Cost-Based Query Optimization for SPARQL Endpoint Federation      CostFed is an index-assisted federation engine for federated SPARQL query processing over multiple SPARQL endpoints. CostFed makes use of statistical information collected from endpoints to perform efficient source selection and cost-based query planning. In contrast to the state of the art, it relies on a non-linear model for the estimation of the selectivity of joins. Therewith, it is able to generate better plans than the state-of-the-art federation engines. In an experimental evaluation based on FedBench benchmark, we show  that CostFed is 3 to 121 times faster than the state of the art SPARQL endpoint federation engines.   ## Live Demo  The CostFed live demo comprise the following two main applications:   * The endpoint manager is is available [here](http://manager.costfed.aksw.org). Using endpoint manager you can select the endpoints to be included in the federation. Also it allows to create/update CostFed's indexes.    * The query formulator/executer is availble [here](http://costfed.aksw.org). This is the main interface which allows executing both federated and non-federated queries.       The start CostFed-web and create your own local demo, the Dockerfile can be downloaded from [here](https://github.com/dice-group/CostFed/blob/master/Dokerfile)      To help user, we provided some federated queries [here](http://costfed.aksw.org/rdf4j-workbench/repositories/costfed/saved-queries) from FedBench and LargeRDFBench which can be directly executed.   ### How to Run CostFed?  * Checkout: the source code and import as new maven project. it will create three sub-projects, i.e, costfed, fex, and semagrow-bench.   * Create Index: Since CostFed is an index-assisted appraoch, the first step is to generate an index for all the endpoints in hand. The index generation, updation is given costfed/src/main/java/org/aksw/simba/quetsal/util/TBSSSummariesGenerator.java. Note for FedBench, LargeRDFBench, the index is already given at costfed/summaries/sum-localhost.n3.   * Configuration File: Set properties in /costfed/costfed.props or run with default  * Query Execution: costfed/src/main/java/org/aksw/simba/start/QueryEvaluation.java. Here you need to specify the URLs of the SPARQL endpoints which you want the given query to be federated and provide the configuration file, i.e., costfed.props as argument.      ### Used Benchmarks  The queries used in the evaluation can be downloaded from [FedBench](http://fedbench.fluidops.net/) and [LargeRDFBech](https://github.com/AKSW/largerdfbench) homepage.     ### Datasets Availability     All the datasets and corresponding virtuoso SPARQL endpoints can be downloaded from the links given below. You may start a SPARQL endpoint from bin/start.bat (for windows) and bin/start_virtuoso.sh (for linux).     | *Dataset*  | *Data-dump*  | *Windows Endpoint*  | *Linux Endpoint*  | *Local Endpoint Url*  | *Live Endpoint Url*|  |------------|--------------|---------------------|-------------------|-----------------------|--------------------|  | [ChEBI](https://www.ebi.ac.uk/chebi/) |[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-Vk81dGVkNVNuY1E/edit?usp=sharing/ )| [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-TUR6RF9jX2xoMFU/edit?usp=sharing/)|[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-Wk5LeHBzMUd3VHc/edit?usp=sharing )|your.system.ip.address:8890/sparql | - |  | [DBPedia-Subset](http://DBpedia.org/) |[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-QWk5MVJud3cxUXM/edit?usp=sharing/ )|  [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-WjNkZEZrTTZzbW8/edit?usp=sharing/)|[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-OEgyXzBUVmlMQlk/edit?usp=sharing )|your.system.ip.address:8891/sparql |http://dbpedia.org/sparql |  | [ DrugBank](http://www.drugbank.ca/)|[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-cVp5QV9VUWRuYkk/edit?usp=sharing/ ) | [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-QmMyOE9RWV9oNHM/edit?usp=sharing/ )| [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-U0V5Y0xDWXhzam8/edit?usp=sharing/ )|your.system.ip.address:8892/sparql | http://wifo5-04.informatik.uni-mannheim.de/drugbank/sparql|  | [Geo Names](http://www.geonames.org/) |[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-WEZZb2VwOG5vZkU/edit?usp=sharing/ ) | [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-VC1HWmhBMlFncWc/edit?usp=sharing/ ) | [Download](https://drive.google.com/file/d/0B_MUFqryVpByd3hJcHBPeHZhejA/edit?usp=sharing/ ) |your.system.ip.address:8893/sparql | http://factforge.net/sparql|  | [Jamendo](http://dbtune.org/jamendo/ ) |[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-cWpmMWxxQ3Z2eVk/edit?usp=sharing/ ) | [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-YXV6U0ZzLUF0S0k/edit?usp=sharing/ ) | [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-V3JMZjdfRkZxLUU/edit?usp=sharing/ ) |your.system.ip.address:8894/sparql  | http://dbtune.org/jamendo/sparql/|  | [KEGG](http://www.genome.jp/kegg/) |[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-TUdUcllRMGVJaHM/edit?usp=sharing/ ) | [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-c1BNQ0dVWTVkUEU/edit?usp=sharing/ )| [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-R1dKbDlHNXZ6blk/edit?usp=sharing/ ) |your.system.ip.address:8895/sparql |http://cu.kegg.bio2rdf.org/sparql |  | [Linked MDB](http://linkedmdb.org/) |[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-bU5VN25NLXZXU0U/edit?usp=sharing/ ) | [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-eXpVSjd2Y25PaVk/edit?usp=sharing/ )| [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-NjVTVERvajJUcGc/edit?usp=sharing/) |your.system.ip.address:8896/sparql |http://www.linkedmdb.org/sparql |  | [New York Times](http://data.nytimes.com/) |[ Download](https://drive.google.com/file/d/0B1tUDhWNTjO-dThoTm9DSmY4Wms/edit?usp=sharing/) | [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-VDhmNWJmZVcybm8/edit?usp=sharing/ ) | [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-RG9GeVdxbDR4YjQ/edit?usp=sharing/ ) |your.system.ip.address:8897/sparql | - |  | [Semantic Web Dog Food](http://data.semanticweb.org/) |[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-RjBWZXYyX2FDT1E/edit?usp=sharing/ )| [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-c2h4al9VREF6bDg/edit?usp=sharing/ ) | [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-UW5HaF9rekdialU/edit?usp=sharing/ ) |your.system.ip.address:8898/sparql | http://data.semanticweb.org/sparql|  | [Affymetrix](http://download.bio2rdf.org/release/2/affymetrix/affymetrix.html)| [Download](https://drive.google.com/file/d/0B1tUDhWNTjO-eHVlZ1RyVVFJQU0/edit?usp=sharing/ )| [ Download](https://drive.google.com/file/d/0B1tUDhWNTjO-RnV4SWtKelJTb0U/edit?usp=sharing/)|[Download](https://drive.google.com/file/d/0B1tUDhWNTjO-Tm9oazNUdV9Cb1k/edit?usp=sharing )|your.system.ip.address:8899/sparql |http://cu.affymetrix.bio2rdf.org/sparql |    ### Evaluation Results and Runtime Errors     We have compared 5 - [FedX](https://www.mpi-inf.mpg.de/~khose/publications/ISWC2011.pdf ), [SPLENDID](http://ceur-ws.org/Vol-782/GoerlitzAndStaab_COLD2011.pdf ), [ANAPSID](http://link.springer.com/chapter/10.1007%2F978-3-642-25073-6_2#page-1 ), [SemaGrow](http://dl.acm.org/citation.cfm?id=2814886),  [HiBISUCuS](http://svn.aksw.org/papers/2014/HiBISCuS_ESWC/public.pdf) - state-of-the-art SPARQL endpoint federation systems with CostFed. Our complete evaluation results can be downloaded from [here](https://github.com/AKSW/CostFed/blob/master/Results.xlsx).        ### Authors    * [Muhammad Saleem](https://sites.google.com/site/saleemsweb/) (AKSW, University of Leipzig)   * Alexander Potocki (AKSW, University of Leipzig)   * [Tommaso Soru](http://tommaso-soru.it/) (AKSW, University of Leipzig)  * [Olaf Hartig](http://olafhartig.de/) (Linköping University, Sweden)  * [Axel-Cyrille Ngonga Ngomo](http://aksw.org/AxelNgonga.html) (AKSW, University of Leipzig)     We are especially thankful to Andreas Schwarte (fluid Operations, Germany), Olaf Görlitz (University Koblenz, Germany), and Angelos Charalambidis	(Institute of Informatics and Telecommunication, Paraskevi, Greece) for all their email conversations, feedbacks, and explanations. 	   """
Semantic web;https://github.com/dice-group/IGUANA;"""[![GitLicense](https://gitlicense.com/badge/dice-group/IGUANA)](https://gitlicense.com/license/dice-group/IGUANA)  ![Java CI with Maven](https://github.com/dice-group/IGUANA/workflows/Java%20CI%20with%20Maven/badge.svg)[![BCH compliance](https://bettercodehub.com/edge/badge/AKSW/IGUANA?branch=master)](https://bettercodehub.com/)  [![Codacy Badge](https://api.codacy.com/project/badge/Grade/9668460dd04c411fab8bf5ee9c161124)](https://www.codacy.com/app/TortugaAttack/IGUANA?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=AKSW/IGUANA&amp;utm_campaign=Badge_Grade)  [![Project Stats](https://www.openhub.net/p/iguana-benchmark/widgets/project_thin_badge.gif)](https://www.openhub.net/p/iguana-benchmark)      # IGUANA    <img src = ""https://github.com/dice-group/IGUANA/raw/develop/images/IGUANA_logo.png"" alt = ""IGUANA Logo"" width = ""400"" align = ""center"">    ## ABOUT      Semantic Web is becoming more important and it's data is growing each day. Triple stores are the backbone here, managing these data.  Hence it is very important that the triple store must scale on the data and can handle several users.   Current Benchmark approaches could not provide a realistic scenario on realistic data and could not be adjustet for your needs very easily.  Additionally Question Answering systems and Natural Language Processing systems are becoming more and more popular and thus needs to be stresstested as well.  Further on it was impossible to compare results for different benchmarks.     Iguana is an an Integerated suite for benchmarking read/write performance of HTTP endpoints and CLI Applications.</br>  which solves all these issues.   It provides an enviroment which ...      + ... is highly configurable  + ... provides a realistic scneario benchmark  + ... works on every dataset  + ... works on SPARQL HTTP endpoints  + ... works on HTTP Get & Post endpoints  + ... works on CLI applications  + and is easily extendable      For further Information visit    [iguana-benchmark.eu](http://iguana-benchmark.eu)     [Documentation](http://iguana-benchmark.eu/docs/3.3/)      # Getting Started    # Prerequisites     You need to install Java 11 or greater.  In Ubuntu you can install these using the following commands    ```  sudo apt-get install java  ```    # Iguana Modules    Iguana consists of two modules    1. **corecontroller**: This will benchmark the systems   2. **resultprocessor**: This will calculate the Metrics and save the raw benchmark results     ## **corecontroller**    The **corecontroller** will benchmark your system. It should be started on the same machine the  is started.    ## **resultprocessor**    The **resultprocessor** will calculate the metrics.  By default it stores its result in a ntriple file. But you may configure it, to write the results directly to a Triple Store.   On the processing side, it calculates various metrics.    Per run metrics:  * Query Mixes Per Hour (QMPH)  * Number of Queries Per Hour (NoQPH)  * Number of Queries (NoQ)  * Average Queries Per Second (AvgQPS)    Per query metrics:  * Queries Per Second (QPS)      * Number of successful and failed queries      * result size      * queries per second      * sum of execution times    You can change these in the Iguana Benchmark suite config.    If you use the [basic configuration](https://github.com/dice-group/IGUANA/blob/master/example-suite.yml), it will save all mentioned metrics to a file called `results_{{DATE_RP_STARTED}}.nt`      # Setup Iguana    ## Download  Please download the release zip **iguana-x.y.z.zip** from the newest release available [here](https://github.com/dice-group/IGUANA/releases/latest):    ```  mkdir iguana  wget https://github.com/dice-group/IGUANA/releases/download/v3.3.0/iguana-3.3.0.zip  unzip iguana-3.3.0.zip  ```      It contains the following files:    * iguana.corecontroller-X.Y.Z.jar  * start-iguana.sh  * example-suite.yml    # Run Your Benchmarks    ## Create a Configuration    You can use the [basic configuration](https://github.com/dice-group/IGUANA/blob/master/example-suite.yml) we provide and modify it to your needs.  For further information please visit our [configuration](http://iguana-benchmark.eu/docs/3.2/usage/configuration/) and [Stresstest](http://iguana-benchmark.eu/docs/3.0/usage/stresstest/) wiki pages. For a detailed, step-by-step instruction please attend our [tutorial](http://iguana-benchmark.eu/docs/3.2/usage/tutorial/).        ## Execute the Benchmark    Use the start script   ```  ./start-iguana.sh example-suite.yml  ```  Now Iguana will execute the example benchmark suite configured in the example-suite.yml file      # How to Cite    ```bibtex  @InProceedings{10.1007/978-3-319-68204-4_5,  author=""Conrads, Felix  and Lehmann, Jens  and Saleem, Muhammad  and Morsey, Mohamed  and Ngonga Ngomo, Axel-Cyrille"",  editor=""d'Amato, Claudia  and Fernandez, Miriam  and Tamma, Valentina  and Lecue, Freddy  and Cudr{\'e}-Mauroux, Philippe  and Sequeda, Juan  and Lange, Christoph  and Heflin, Jeff"",  title=""Iguana: A Generic Framework for Benchmarking the Read-Write Performance of Triple Stores"",  booktitle=""The Semantic Web -- ISWC 2017"",  year=""2017"",  publisher=""Springer International Publishing"",  address=""Cham"",  pages=""48--65"",  abstract=""The performance of triples stores is crucial for applications driven by RDF. Several benchmarks have been proposed that assess the performance of triple stores. However, no integrated benchmark-independent execution framework for these benchmarks has yet been provided. We propose a novel SPARQL benchmark execution framework called Iguana. Our framework complements benchmarks by providing an execution environment which can measure the performance of triple stores during data loading, data updates as well as under different loads and parallel requests. Moreover, it allows a uniform comparison of results on different benchmarks. We execute the FEASIBLE and DBPSB benchmarks using the Iguana framework and measure the performance of popular triple stores under updates and parallel user requests. We compare our results (See https://doi.org/10.6084/m9.figshare.c.3767501.v1) with state-of-the-art benchmarking results and show that our benchmark execution framework can unveil new insights pertaining to the performance of triple stores."",  isbn=""978-3-319-68204-4""  }  ``` """
Semantic web;https://github.com/sindice/sparqled;"""# SPARQLed    Assisted SPARQL Editor    ## General Information    - Project and contact information: [http://sindice.com/sparqled/](http://sindice.com/sparqled/)  = Instructions for installing SparQLed are available in the **Downloads** section    ## Live Demos    - [SparQLed](http://hcls.sindice.com/sparql-editor/) on the Health Care Life Science datasets  - [SparQLed](http://demo.sindice.net/dbpedia-sparqled/) on the English part of DBpedia 3.7    ## Distribution Content    This distribution includes 6 modules:  - _sparql-editor-client:_ the editor User Interface;  - _recommendation-servlet:_ the servlet for providing query element recommendations;  - _sesame-sparql-queryparser:_ [Sesame](http://www.openrdf.org/) SPARQL query parser extended for the SparQLed use case;  - _sesame-backend:_ Utility classes for operating on SPARQL endpoint through Sesame;  - _sparqled-commons:_ Utility classes used in the Data Graph Summary computation;  - _sparql-summary:_ an SPARQL-based Data Graph Summary Computation.    ## Acknowledgements    This software is based upon works supported by :    * the European FP7 project [LOD2](http://lod2.eu/Welcome.html) (257943)  * the IRCSET EMPOWER 2012 Government of Ireland Postdoctoral Fellowship (Renaud Delbru)    The SparQLed User Interface is based on the [Flint SPARQL Editor](https://github.com/TSO-Openup/FlintSparqlEditor).    ========    _Copyright 2014, National University of Ireland, Galway._ """
Semantic web;https://github.com/awslabs/amazon-neptune-tools;"""## Amazon Neptune Tools    Utilities to enable loading data and building graph applications with Amazon Neptune.    ### Examples    You may also be interested in the [Neptune Samples github repository](https://github.com/aws-samples/amazon-neptune-samples), which includes samples and example code.    ### GraphML 2 CSV  This is a [utility](graphml2csv/README.md) to convert graphml files into the Neptune CSV format.    ### Neptune Export  Exports Amazon Neptune data to CSV for Property Graph or Turtle for RDF graphs.    You can use [neptune-export](neptune-export/) to export an Amazon Neptune database to the bulk load CSV format used by the Amazon Neptune bulk loader for Property Graph or Turtle for RDF graphs. Alternatively, you can supply your own queries to neptune-export and unload the results to CSV or Turtle.    ### Export Neptune to Elasticsearch  Backfills Elasticsearch with data from an existing Amazon Neptune database.    The [Neptune Full-text Search](https://docs.aws.amazon.com/neptune/latest/userguide/full-text-search-cfn-create.html) CloudFormation templates provide a mechanism for indexing all _new_ data that is added to an Amazon Neptune database in Elasticsearch. However, there are situations in which you may want to index _existing_ data in a Neptune database prior to enabling the full-text search integration.    You can use this [export Neptune to Elasticsearch solution](export-neptune-to-elasticsearch/) to index existing data in an Amazon Neptune database in Elasticsearch.    ### Neo4j to Neptune  A [command-line utility](neo4j-to-neptune/readme.md) for migrating data to Neptune from Neo4j.    ### Glue Neptune    [glue-neptune](glue-neptune/) is a Python library for AWS Glue that helps writing data to Amazon Neptune from Glue jobs. With glue-neptune you can:  * Get Neptune connection information from the Glue Data Catalog  * Create label and node and edge ID columns in DynamicFrames, named in accordance with the Neptune CSV bulk load format for property graphs  * Write from DynamicFrames directly to Neptune    ### Neptune CSV to RDF    If you're interested in converting Neptune's CSV format to RDF, see [amazon-neptune-csv-to-rdf-converter](https://github.com/aws/amazon-neptune-csv-to-rdf-converter).    ### Neptune CSV to Gremlin    [csv-gremlin](csv-gremlin/README.md) is a tool that can turn Amazon Neptune format CSV files into Gremlin steps allowing them to be loaded into different Apache TinkerPop compliant stores (including Amazon Neptune) using Gremlin queries. The tool also tries to validate that the CSV files do not contain errors and can be use to inspect CSV files prior to starting a bulk load.    ### CSV to Neptune Bulk Format CSV    [csv-to-neptune-bulk-format](csv-to-neptune-bulk-format/README.md) is a utility to identify nodes and edges in the source CSV data file(s) and generate the Amazon Neptune gremlin load data format files. A configuration file (JSON) defines the source and target files, nodes/edges definition, and selection logic. The script interprets one or more configuration files and generates Amazon Neptune gremlin load data format files. The generated files can be loaded into the Neptune database.    ## License    This library is licensed under the Apache 2.0 License.  """
Semantic web;https://github.com/zazuko/d3-sparql;"""# d3-sparql    This module lets you request data from [SPARQL](https://www.w3.org/TR/sparql11-query/) [endpoints](https://www.w3.org/wiki/SparqlEndpoints) in the vein of [d3-csv](https://github.com/d3/d3-dsv) and friends. It is generating a JSON structure from SPARQL query results, you can use that in any way you like in your code, with or without D3.    The access through a SPARQL endpoint allows a faster and more efficient data preparation (once you got [the hang of SPARQL and the RDF data model](https://www.youtube.com/watch?v=FvGndkpa4K0)). Ultimately it keeps visualizations up to date. Think of SPARQL endpoints as the most flexible API imaginable.    Define the SPARQL query and endpoint:    ```js  // Author of Q3011087 (D3.js)  var mikeQuery = `SELECT ?developerName WHERE {    wd:Q3011087 wdt:P178 ?developer.    ?developer rdfs:label ?developerName.    FILTER(LANG(?developerName) = 'en')  }`    wikidataUrl = 'https://query.wikidata.org/bigdata/namespace/wdq/sparql'  ```    To query the endpoint and get the result:    ```js  d3.sparql(wikidataUrl, mikeQuery).then((data) => {    console.log(data); // [{'developerName': 'Mike Bostock'}]  })  ```    More [examples](https://github.com/zazuko/d3-sparql/tree/master/examples) are provided in the [repository](https://github.com/zazuko/d3-sparql).    ## Features    - Transformation of [XSD Datatypes](https://www.w3.org/2011/rdf-wg/wiki/XSD_Datatypes) (e.g. `xsd:dateTime`, `xsd:boolean`, ...) to native JavaScript types.  - Reformatting of the JSON Structure to a d3 style layout while using the provided variables names of the SPARQL Query.    ## Limitations    - Only `SELECT` queries are supported. (This provides a projection of the graph data onto a table structure used by d3.)  - Currently only supports endpoints which are able to respond with `application/sparql-results+json`.    ## Installing    Using NPM: `npm install d3-sparql`. You can also use a CDN, for instance <https://www.jsdelivr.com>.    See [CHANGELOG](CHANGELOG.md) for details about available versions.    ## API Reference    This package adds a `sparql` function to the global `d3` object: `d3.sparql(endpoint, query, options = {})`.    <a name=""request"" href=""#sparql"">#</a> d3.<b> sparql </b>(<i>endpoint</i>, <i>query</i>[, <i>options = {}</i>]) [<>](https://github.com/zazuko/d3-sparql/blob/master/src/sparql.js#L8 ""Source"")    `options` is an optional object that will get merged with the second argument of [`fetch()`](https://developer.mozilla.org/en-US/docs/Web/API/WindowOrWorkerGlobalScope/fetch).    ```js  d3.sparql(endpoint, query)    .then((data) => …);  ```    ## Acknowledgement  The initial development of this library by [Zazuko](http://www.zazuko.com) was supported by the [City of Zürich](https://www.stadt-zuerich.ch/). """
Semantic web;https://github.com/owlcs/ont-api;"""# ONT-API (ver. 2.1.0)    ## Summary  ONT-API is a RDF-centric Java library to work with OWL.    For more info see [wiki](https://github.com/owlcs/ont-api/wiki).     ## Dependencies  - **[Apache Jena](https://github.com/apache/jena)** (**3.x.x**)  - **[OWL-API](https://github.com/owlcs/owlapi)** (**5.x.x**)    ## Requirements:    - Java **8+**    ## License  * Apache License Version 2.0  * GNU LGPL Version 3.0   """
Semantic web;https://github.com/SemWebCentral/parliament;"""# News    April 17, 2019:  Released Parliament™ version 2.7.12.  Changes of note:    * Parliament was recently moved to this GitHub project from its long-time home on [SemWebCentral](http://parliament.semwebcentral.org/)    * Added a SWRL rules engine    * Added a script to install Parliament as a service/daemon on systemd-based Linux distributions, including CentOS and Ubuntu    * Long-running queries now time out    * Added linear growth settings for Parliament's resource and statement tables    * The inference engine now recognizes classes to be subclasses of themselves    * Decreased the likelihood that an ungraceful shutdown will corrupt the data files    * Fixed numerous bugs        # Parliament™ Introduction    Parliament™ is a high-performance triple store and reasoner designed for the [Semantic Web](http://www.w3.org/2001/sw/).  Parliament's initial development was funded by DARPA through the DAML program under the name [DAML DB](http://www.daml.org/2001/09/damldb/) and was extended by Raytheon BBN Technologies (BBN) for internal use in its R&D programs.  BBN released Parliament as an open source project under the [BSD license](http://opensource.org/licenses/bsd-license.php) on [SemWebCentral](http://parliament.semwebcentral.org/) in 2009.  In 2018, BBN migrated the Parliament open source project to [GitHub](https://github.com/SemWebCentral/parliament) under the same license.    Parliament™ is a trademark of Raytheon BBN Technologies.  It is so named because a group of owls is properly called a _parliament_ of owls. """
Semantic web;https://github.com/protegeproject/snap-sparql-query;"""# snap-sparql-api  An API for parsing SPARQL queries """
Semantic web;github.com/semantalytics/awesome-ontologies;"""# awesome-ontologies    - xsd  - [rdf](http://www.w3.org/1999/02/22-rdf-syntax-ns) http://www.w3.org/1999/02/22-rdf-syntax-ns#   - [rdfs](http://www.w3.org/2000/01/rdf-schema) http://www.w3.org/2000/01/rdf-schema#  - [owl](http://www.w3.org/2002/07/owl) http://www.w3.org/2002/07/owl#  - [skos](https://www.w3.org/2004/02/skos/core.rdf) http://www.w3.org/2004/02/skos/core#  - [skosxl]() http://www.w3.org/2008/05/skos-xl#  - [dcat]() http://http://www.w3.org/ns/org#www.w3.org/ns/dcat#  - [org]() http://www.w3.org/ns/org#  - [vcard]() http://www.w3.org/2006/vcard/ns#  - [geonames]() http://www.geonames.org/ontology#  - [gr]() http://purl.org/goodrelations/v1#  - [prov]() http://www.w3.org/ns/prov#  - [dc]()  http://purl.org/dc/elements/1.1/  - [dcterm]() http://purl.org/dc/terms/  - [sioc]() http://rdfs.org/sioc/ns#  - [mo]() http://musicontology.com/  - [MarineTLO]()  http://ics.forth.gr/Ontology/MarineTLO/icore#  http://ics.forth.gr/Ontology/MarineTLO/imarine#  - [schema]() http://schema.org/  - [Spar](http://www.sparontologies.net/ontologies)  - yago  - dbpedia  - http://aber-owl.net/ontology/#/    - [ontology list](http://info.slis.indiana.edu/~dingying/Teaching/S604/OntologyList.html)  - [ontology design patterns](http://ontologydesignpatterns.org/wiki/Ontology:Main)  - https://dase.cs.wright.edu/content/modl-modular-ontology-design-library  - http://www.cropontology.org    ## Cyber  - [UCO](https://github.com/ucoProject/UCO)    ## SKOS    - [AGROVOC](http://aims.fao.org/vest-registry/vocabularies/agrovoc)  - [STW Thesaurus](http://zbw.eu/stw/)  - [UNESCO Thesaurus](http://skos.um.es/unescothes/)  - https://www.w3.org/2001/sw/wiki/SKOS/Datasets """
Semantic web;https://github.com/ucbl/HyLAR-Reasoner;"""# HyLAR-Reasoner ![HyLAR icon](https://raw.githubusercontent.com/ucbl/HyLAR-Reasoner/master/hylar-icon.png)     A rule-based incremental reasoner for the Web.    To cite HyLAR: [HyLAR+: improving Hybrid Location-Agnostic Reasoning  with Incremental Rule-based Update](https://hal.archives-ouvertes.fr/hal-01276558/file/Demo_www2016.pdf)    ## Table of contents    - [Description](#description)  - [Use HyLAR locally](#use-hylar-locally)  - [Use HyLAR in a browser](#use-hylar-in-a-browser)  - [Use HyLAR as a server](#use-hylar-as-a-server)  - [Supported Inferences](#supported-inferences)  - [Publications](#publications)      ## Description    HyLAR is a **Hy**brid **L**ocation-**A**gnostic incremental **R**easoner that uses known rdf-based librairies such as rdfstore.js, sparqljs and rdf-ext while providing an additional incremental reasoning engine. HyLAR can be either used locally as a npm module or globally as a server, and comes with a browserified version.    HyLAR relies on the rdfstore.js triplestore and therefore supports JSON-LD, N3 and Turtle serializations.  SPARQL support is detailed [here](https://github.com/antoniogarrote/rdfstore-js#sparql-support). The inferences initially supported by HyLAR are described [at the bottom of this page](#supported-inferences). HyLAR supports custom business rules.    ## Use HyLAR locally    ### Installation    To use HyLAR locally, just launch  `npm install --save hylar`    ### Loading an ontology    Import HyLAR, then classify your ontology and query it using `load()`,  which takes three parameters:  - rawOntology: A string, the raw ontology.  - mimeType: A string, either `text/turtle`, `text/n3` or `application/ld+json`.  - keepOldValues: A boolean: true to keep old values while classfying, false to overwrite the KB. Default is **false**.    ```javascript  const Hylar = require('hylar');  const h = new Hylar();        // async function  h.load(rawOntology, mimeType, keepOldValues);  ```    ### Querying an ontology    Once loaded, HyLAR is able to process SPARQL queries using `query()`, with the following parameters:    - query: A string, the SPARQL query    ```javascript  let results = await h.query(query);  ```    ### Create your own rules    HyLAR supports insertion of custom forward-chaining conjunctive rules in the form:  ```  triple_head_1 ^ ... ^ triple_head_n -> triple_body_3  ```  Where `triple_head_x` and `triple_body_x` are respectively ""cause"" triples (*i.e.* the input) and ""consequence"" triples (*i.e.* the inferred output) in the form:  ```  (subject predicate object)  ```  Each subject/predicate/object can be one of the following:  - A variable, *e.g.* `?x`  - An URI, *e.g.* `http://www.w3.org/2000/01/rdf-schema#subClassOf`  - A literal, *e.g.* `""0.5""`, `""Hello world!""`    A predicate can also be any of these comparison operators: `<`, `>`, `=`, `<=`, `=>`.    **Rule add example (first param: the 'raw' rule, second param: the rule name)**    ```javascript  h.parseAndAddRule('(?p1 http://www.w3.org/2002/07/owl#inverseOf ?p2) ^ (?x ?p1 ?y) -> (?y ?p2 ?x)', 'inverse-1');  ```  **Rule removal example (first and only param: either the rule name or the raw rule)**    ```javascript  h.removeRule('inverse-1');  // Outputs ""[HyLAR] Removed rule (?p1 inverseOf ?p2) ^ (?x ?p1 ?y) -> (?y ?p2 ?x)"" if succeeded.  ```    ## Use HyLAR in a browser    Run `npm run clientize`, which will generate the file `hylar-client.js`.  Include this script in your page with this line:  ```html  <script src=""path-to/hylar-client.js""></script>  ```  As in the node module version, you can instantiate HyLAR with `const h = new Hylar();` and call the same methods `query()`, `load()` and `parseAndAddRule()`.    ## Use HyLAR as a server    ### Installation    `npm install -g hylar`    ### Run the server    Command `hylar` with the following optional parameters    - `--port <port_number>` (port 3000 by default)  - `--no-persist` deactivates database persistence (activated by default)  - `--graph-directory </your/base/graph/directory/>` where local datasets are saved  - `--entailment` either ```OWL2RL``` (default) or ```RDFS```  - `--reasoning-method` either `incremental` (default) or `tag-based` (provides *reasoning proofs*)    ### Hylar server API    - `/classify/{FILE_NAME}` (GET)    Loads, parses and classify the file `{FILE_NAME}` from the ontology directory.  > **Note:** You don't have to specify the ontology file's mimetype as it is detected automatically using its extension.    - `/classify/` (GET)    Allows classifying an ontology as a string, which requires its original serialization type.  > **Body parameters**   >`filename` the absolute path of the ontology file to be processed.  > `mimetype` the serialization of the ontology (mimetype, one of text/turtle, text/n3 or application/ld+json).    - `/query`(GET)    SPARQL queries your loaded ontology as does `Hylar.query()`.    > **Body parameters**  > `query` the SPARQL query string.    - `/rule` (PUT)    Puts an list of custom rules and adds it to the reasoner.    > **Body parameters**  > `rules` the array of conjunctive rules.    ## Supported inferences    HyLAR supports a subset of OWL 2 RL and RDFS.  - [RDFS](https://www.w3.org/TR/rdf-mt/#RDFSRules)      - Rules:  `rdf1, rdfs2, rdfs3, rdfs4a, rdfs4b, rdfs5, dfs6, rdfs7, rdfs8, rdfs9, rdfs10, rdfs11, rdfs12, rdfs13`.      - Supports all RDFS axiomatic triples, except axioms related to `rdf:Seq` and `rdf:Bag`.      - [OWL 2 RL](https://www.w3.org/TR/owl2-profiles/#Reasoning_in_OWL_2_RL_and_RDF_Graphs_using_Rules)      - Rules: `prp-dom, prp-rng, prp-fp, prp-ifp, prp-irp, prp-symp, prp-asyp, prp-trp, prp-spo1, prp-spo2, prp-eqp1, prp-eqp2, prp-pdw, prp-inv1, prp-inv2, prp-npa1, prp-npa2, cls-nothing2, cls-com, cls-svf1, cls-svf2, cls-avf, cls-hv1, cls-hv2, cls-maxc1, cls-maxc2, cls-maxqc1, cls-maxqc2, cls-maxqc3, cls-maxqc4, cax-sco, cax-eqc1, cax-eqc2, cax-dw, scm-cls, scm-sco, scm-eqc1, scm-eqc2, scm-op, scm-dp, scm-spo, scm-eqp1, scm-eqp2, scm-dom1, scm-dom2, scm-rng1, scm-rng2, scm-hv, scm-svf1, scm-svf2, scm-avf1, scm-avf2`      - Axiomatic triples are *not yet* supported.    ## Publications    ### Location-agnostic mechanism    Terdjimi, M., Médini, L., & Mrissa, M. (2015, May). [Hylar: Hybrid location-agnostic reasoning 📚](https://hal.archives-ouvertes.fr/hal-01154549/file/hylar.pdf) In ESWC Developers Workshop 2015 (p. 1).    ### Incremental reasoning on the Web with HyLAR    Terdjimi, M., Médini, L., & Mrissa, M. (2016, April). [HyLAR+: improving hybrid location-agnostic reasoning with incremental rule-based update 📚](https://hal.archives-ouvertes.fr/hal-01276558/file/Demo_www2016.pdf) In Proceedings of the 25th International Conference Companion on World Wide Web (pp. 259-262). International World Wide Web Conferences Steering Committee.     ### Tag-based maintenance    Terdjimi, M., Médini, L., & Mrissa, M. (2018, April). [Web Reasoning Using Fact Tagging 📚](http://mmrissa.perso.univ-pau.fr/pub/Accepted-papers/2018-TheWebConf-RoD.pdf) In Companion of the The Web Conference 2018 on The Web Conference 2018 (pp. 1587-1594). International World Wide Web Conferences Steering Committee.   """
Semantic web;https://github.com/comunica/comunica;"""<p align=""center"">    <a href=""https://comunica.dev/"">      <img alt=""Comunica"" src=""https://comunica.dev/img/comunica_red.svg"" width=""200"">    </a>  </p>    <p align=""center"">    <strong>A knowledge graph querying framework for JavaScript</strong>    <br />    <i>Flexible SPARQL and GraphQL over decentralized RDF on the Web.</i>  </p>    <p align=""center"">  <a href=""https://github.com/comunica/comunica/actions?query=workflow%3ACI""><img src=""https://github.com/comunica/comunica/workflows/CI/badge.svg"" alt=""Build Status""></a>  <a href=""https://coveralls.io/github/comunica/comunica?branch=master""><img src=""https://coveralls.io/repos/github/comunica/comunica/badge.svg?branch=master"" alt=""Coverage Status""></a>  <a href=""https://zenodo.org/badge/latestdoi/107345960""><img src=""https://zenodo.org/badge/107345960.svg"" alt=""DOI""></a>  <a href=""https://gitter.im/comunica/Lobby""><img src=""https://badges.gitter.im/comunica.png"" alt=""Gitter chat""></a>  </p>    **[Learn more about Comunica on our website](https://comunica.dev/).**    Comunica is an open-source project that is used by [many other projects](https://github.com/comunica/comunica/network/dependents),  and is being maintained by a [group of volunteers](https://github.com/comunica/comunica/graphs/contributors).  If you would like to support this project, you may consider:    * Contributing directly by [writing code or documentation](https://comunica.dev/contribute/); or  * Contributing indirectly by funding this project via [Open Collective](https://opencollective.com/comunica-association).    ## Supported by    Comunica is a community-driven project, sustained by the [Comunica Association](https://comunica.dev/association/).  If you are using Comunica, [becoming a sponsor or member](https://opencollective.com/comunica-association) is a way to make Comunica sustainable in the long-term.    Our top sponsors are shown below!    <a href=""https://opencollective.com/comunica-association/sponsor/0/website"" target=""_blank""><img src=""https://opencollective.com/comunica-association/sponsor/0/avatar.svg""></a>  <a href=""https://opencollective.com/comunica-association/sponsor/1/website"" target=""_blank""><img src=""https://opencollective.com/comunica-association/sponsor/1/avatar.svg""></a>  <a href=""https://opencollective.com/comunica-association/sponsor/2/website"" target=""_blank""><img src=""https://opencollective.com/comunica-association/sponsor/2/avatar.svg""></a>  <a href=""https://opencollective.com/comunica-association/sponsor/3/website"" target=""_blank""><img src=""https://opencollective.com/comunica-association/sponsor/3/avatar.svg""></a>    ## Query with Comunica    Read one of our [guides to **get started** with querying](https://comunica.dev/docs/query/getting_started/):    * [Querying from the command line](https://comunica.dev/docs/query/getting_started/query_cli/)  * [Updating from the command line](https://comunica.dev/docs/query/getting_started/update_cli/)  * [Querying local files from the command line](https://comunica.dev/docs/query/getting_started/query_cli_file/)  * [Querying in a JavaScript app](https://comunica.dev/docs/query/getting_started/query_app/)  * [Updating in a JavaScript app](https://comunica.dev/docs/query/getting_started/update_app/)  * [Querying in a JavaScript browser app](https://comunica.dev/docs/query/getting_started/query_browser_app/)  * [Setting up a SPARQL endpoint](https://comunica.dev/docs/query/getting_started/setup_endpoint/)  * [Querying from a Docker container](https://comunica.dev/docs/query/getting_started/query_docker/)  * [Setting up a Web client](https://comunica.dev/docs/query/getting_started/setup_web_client/)  * [Query using the latest development version](https://comunica.dev/docs/query/getting_started/query_dev_version/)    Or jump right into one of the available query engines:  * [Comunica SPARQL](https://github.com/comunica/comunica/tree/master/engines/query-sparql#readme): SPARQL/GraphQL querying from JavaScript applications or the CLI ([Browser-ready via a CDN](https://github.com/rdfjs/comunica-browser))  * [Comunica SPARQL File](https://github.com/comunica/comunica/tree/master/engines/query-sparql-file#readme): Engine to query over local RDF files  * [Comunica SPARQL RDFJS](https://github.com/comunica/comunica/tree/master/engines/query-sparql-rdfjs#readme): Engine to query over in-memory [RDFJS-compliant sources](https://rdf.js.org/stream-spec/#source-interface).  * [Comunica SPARQL HDT](https://github.com/comunica/comunica-actor-init-sparql-hdt#readme): Library to query over local [HDT](https://www.rdfhdt.org/) files  * [Comunica SPARQL Solid](https://github.com/comunica/comunica-feature-solid/tree/master/engines/query-sparql-solid#readme): Engine to query over files behind [Solid access control](https://solidproject.org/).  * [Comunica SPARQL Link Traversal](https://github.com/comunica/comunica-feature-link-traversal/tree/master/engines/query-sparql-link-traversal#readme): Engine to query over multiple files by following links between them.  * [Comunica SPARQL Link Traversal Solid](https://github.com/comunica/comunica-feature-link-traversal/tree/master/engines/query-sparql-link-traversal-solid#readme): Engine to query within [Solid data vaults](https://solidproject.org/) by following links between documents.    ## Modify or Extending Comunica    [Read one of our guides to **get started** with modifying Comunica](https://comunica.dev/docs/modify/),  or have a look at some [examples](https://github.com/comunica/examples):    * [Querying with a custom configuration from the command line](https://comunica.dev/docs/modify/getting_started/custom_config_cli/)  * [Querying with a custom configuration in a JavaScript app](https://comunica.dev/docs/modify/getting_started/custom_config_app/)  * [Exposing your custom config as an npm package](https://comunica.dev/docs/modify/getting_started/custom_init/)  * [Exposing your custom config in a Web client](https://comunica.dev/docs/modify/getting_started/custom_web_client/)  * [Contributing a new query operation actor to the Comunica repository](https://comunica.dev/docs/modify/getting_started/contribute_actor/)  * [Adding a config parameter to an actor](https://comunica.dev/docs/modify/getting_started/actor_parameter/)    ## Contribute    Interested in contributing? Have a look at our [contribution guide](https://comunica.dev/contribute/).    ## Development Setup    _(JSDoc: https://comunica.github.io/comunica/)_    This repository should be used by Comunica module **developers** as it contains multiple Comunica modules that can be composed.  This repository is managed as a [monorepo](https://github.com/babel/babel/blob/master/doc/design/monorepo.md)  using [Lerna](https://lernajs.io/).    If you want to develop new features  or use the (potentially unstable) in-development version,  you can set up a development environment for Comunica.    Comunica requires [Node.JS](http://nodejs.org/) 8.0 or higher and the [Yarn](https://yarnpkg.com/en/) package manager.  Comunica is tested on OSX, Linux and Windows.    This project can be setup by cloning and installing it as follows:    ```bash  $ git clone https://github.com/comunica/comunica.git  $ cd comunica  $ yarn install  ```    **Note: `npm install` is not supported at the moment, as this project makes use of Yarn's [workspaces](https://yarnpkg.com/lang/en/docs/workspaces/) functionality**    This will install the dependencies of all modules, and bootstrap the Lerna monorepo.  After that, all [Comunica packages](https://github.com/comunica/comunica/tree/master/packages) are available in the `packages/` folder  and can be used in a development environment, such as querying with [Comunica SPARQL (`@comunica/query-sparql`)](https://github.com/comunica/comunica/tree/master/engines/query-sparql).    Furthermore, this will add [pre-commit hooks](https://www.npmjs.com/package/pre-commit)  to build, lint and test.  These hooks can temporarily be disabled at your own risk by adding the `-n` flag to the commit command.    ## Benchmarking    If you want to do benchmarking with Comunica in Node.js,  make sure to run Node.js in production mode as follows:    ```bash  > NODE_ENV=production node packages/some-package/bin/some-bin.js  ```    The reason for this is that Comunica extensively generates  internal `Error` objects.  In non-production mode, these also produce long stacktraces,  which may in some cases impact performance.    ## Cite    If you are using or extending Comunica as part of a scientific publication,  we would appreciate a citation of our [article](https://comunica.github.io/Article-ISWC2018-Resource/).    ```bibtex  @inproceedings{taelman_iswc_resources_comunica_2018,    author    = {Taelman, Ruben and Van Herwegen, Joachim and Vander Sande, Miel and Verborgh, Ruben},    title     = {Comunica: a Modular SPARQL Query Engine for the Web},    booktitle = {Proceedings of the 17th International Semantic Web Conference},    year      = {2018},    month     = oct,    url       = {https://comunica.github.io/Article-ISWC2018-Resource/}  }  ```    ## License  This code is copyrighted by [the Comunica Association](https://comunica.dev/association/) and [Ghent University – imec](http://idlab.ugent.be/)  and released under the [MIT license](http://opensource.org/licenses/MIT). """
Semantic web;https://github.com/lanthaler/Hydra;"""Hydra: Hypermedia-Driven Web APIs  =====================================================================    Hydra is an effort to simplify the development of interoperable,  hypermedia-driven Web APIs. The two fundamental building blocks of  Hydra are [JSON-LD][1] and the [Hydra Core Vocabulary][2].    JSON-LD is the serialization format used in the communication between  the server and its clients. The Hydra Core Vocabulary represents the  shared vocabulary between them. By specifying a number of concepts  which are commonly used in Web APIs it can be used as the foundation  to build Web services that share REST's benefits in terms of loose  coupling, maintainability, evolvability, and scalability. Furthermore  it enables the creation of generic API clients instead of requiring  specialized clients for every single API.    To participate in the development of this specification please join  the [Hydra W3C Community Group][3].    More information about Hydra is currently available at:  http://www.markus-lanthaler.com/hydra      [1]: http://www.w3.org/TR/json-ld/  [2]: http://www.markus-lanthaler.com/hydra/spec/latest/core/  [3]: http://m.lanthi.com/HydraCG """
Semantic web;https://github.com/lyrasis/docker-blazegraph;"""# Docker Blazegraph    Run Blazegraph in Docker.    ## Quickstart    ```bash  docker run --name blazegraph -d -p 8889:8080 lyrasis/blazegraph:2.1.5  docker logs -f blazegraph  ```    ## Local builds    ```bash  docker build -t blazegraph:2.1.5 2.1.5/  docker run --name blazegraph -d -p 8889:8080 blazegraph:2.1.5  docker logs -f blazegraph  ```    ## Loading data (example)    Files or directories need to be made available to the container:    ```bash  # using a host volume mount to make files available  mkdir -p /tmp/blazegraph/data/  cp data/authoritieschildrensSubjects.nt /tmp/blazegraph/data/    # set uid / gid for container, example is ubuntu primary user compatible  BLAZEGRAPH_UID=$UID  BLAZEGRAPH_GID=$GROUPS    # start container making files available under /data  docker run --name blazegraph -d \    -e BLAZEGRAPH_UID=$BLAZEGRAPH_UID \    -e BLAZEGRAPH_GID=$BLAZEGRAPH_GID \    -p 8889:8080 \    -v $PWD/data/RWStore.properties:/RWStore.properties \    -v /tmp/blazegraph/data/:/data \    lyrasis/blazegraph:2.1.5    # create payload config  cp data/dataloader.txt.example dataloader.txt    # trigger data import  curl -X POST \    --data-binary @dataloader.txt \    --header 'Content-Type:text/plain' \    http://localhost:8889/bigdata/dataloader  ```    Sample query:    ```sparql  prefix bds: <http://www.bigdata.com/rdf/search#>  select ?identifier ?value  where {    ?value bds:search ""Women"" .    ?value bds:matchAllTerms ""true"" .    ?identifier <http://www.loc.gov/mads/rdf/v1#authoritativeLabel> ?value .  }  ```    --- """
Semantic web;https://github.com/newres/aesopica;"""# Aesopica    A Clojure library designed to help create Semantic Web, and in particular Linked Data/RDF based applications.   It allows the user to create Linked Data using idiomatic Clojure datastructures, and translate them to various RDF formats.    ## Example Usage      ```clojure  (ns example     (:require [aesopica.core :as aes]               [aesopica.converter :as conv]))    (def fox-and-stork-edn    {::aes/context     {nil ""http://www.newresalhaider.com/ontologies/aesop/foxstork/""      :rdf ""http://www.w3.org/1999/02/22-rdf-syntax-ns#""}     ::aes/facts     #{[:fox :rdf/type :animal]       [:stork :rdf/type :animal]       [:fox :gives-invitation :invitation1]       [:invitation1 :has-invited :stork]       [:invitation1 :has-food :soup]       [:invitation1 :serves-using :shallow-plate]       [:stork :gives-invitation :invitation2]       [:invitation2 :has-invited :fox]       [:invitation2 :has-food :crumbled-food]       [:invitation2 :serves-using :narrow-mouthed-jug]       [:fox :can-eat-food-served-using :shallow-plate]       [:fox :can-not-eat-food-served-using :narrow-mouthed-jug]       [:stork :can-eat-food-served-using :narrow-mouthed-jug]       [:stork :can-not-eat-food-served-using :shallow-plate]}})      (conv/convert-to-turtle fox-and-stork-edn)  ```  ## Features    ### String, Integer, Boolean, Long and Custom Datatypes    ```clojure  (def fox-and-stork-literals-edn    {::aes/context     {nil ""http://www.newresalhaider.com/ontologies/aesop/foxstork/""      :rdf ""http://www.w3.org/1999/02/22-rdf-syntax-ns#""      :foaf ""http://xmlns.com/foaf/0.1/""      :xsd ""http://www.w3.org/2001/XMLSchema#""}     ::aes/facts     #{[:fox :rdf/type :animal]       [:fox :foaf/name ""vo""]       [:fox :foaf/age 2]       [:fox :is-cunning true]       [:fox :has-weight 6.8]       [:stork :rdf/type :animal]       [:stork :foaf/name ""ooi""]       [:stork :foaf/age 13]       [:stork :is-cunning true]       [:dinner1 :has-date {::aes/value ""2002-05-30T18:00:00"" ::aes/type :xsd/dateTime}]}})  ```  ### Quads/Named Graphs      ```clojure  (def fox-and-stork-reif-edn    {::aes/context     {nil ""http://www.newresalhaider.com/ontologies/aesop/foxstork/""      :rdf ""http://www.w3.org/1999/02/22-rdf-syntax-ns#""}     ::aes/facts     #{[:fox :rdf/type :animal :dinner1]       [:stork :rdf/type :animal :dinner1]       [:fox :gives-invitation :invitation1 :dinner1]       [:invitation1 :has-invited :stork :dinner1]       [:invitation1 :has-food :soup :dinner1]       [:invitation1 :serves-using :shallow-plate :dinner1]       [:stork :gives-invitation :invitation2 :dinner2]       [:invitation2 :has-invited :fox :dinner2]       [:invitation2 :has-food :crumbled-food :dinner2]       [:invitation2 :serves-using :narrow-mouthed-jug :dinner2]       [:fox :can-eat-food-served-using :shallow-plate]       [:fox :can-not-eat-food-served-using :narrow-mouthed-jug]       [:stork :can-eat-food-served-using :narrow-mouthed-jug]       [:stork :can-not-eat-food-served-using :shallow-plate]}})  ```    ### Blank Nodes       ```clojure  (def fox-and-stork-blank-node-edn    {::aes/context     {nil ""http://www.newresalhaider.com/ontologies/aesop/foxstork/""      :rdf ""http://www.w3.org/1999/02/22-rdf-syntax-ns#""}     ::aes/facts     #{[:fox :rdf/type :animal]       [:stork :rdf/type :animal]       [:fox :gives-invitation 'invitation1]       ['invitation1 :has-invited :stork]       ['invitation1 :has-food :soup]       ['invitation1 :serves-using :shallow-plate]       [:stork :gives-invitation 'invitation2]       ['invitation2 :has-invited :fox]       ['invitation2 :has-food :crumbled-food]       ['invitation2 :serves-using :narrow-mouthed-jug]       [:fox :can-eat-food-served-using :shallow-plate]       [:fox :can-not-eat-food-served-using :narrow-mouthed-jug]       [:stork :can-eat-food-served-using :narrow-mouthed-jug]       [:stork :can-not-eat-food-served-using :shallow-plate]}})  ```    ### Conversion to Common Formats such as Turtle, Trig, N-Quads, JSON-LD    The conversion utilizes the [Apache Jena](https://jena.apache.org/) library for conversion.   First the Clojure EDN representation needs to be converted to a [Jena DataSetGraph](http://jena.apache.org/documentation/javadoc/arq/org/apache/jena/sparql/core/DatasetGraph.html) (a Jena representation of a set of graphs).  Afterwards the Clojure functions that utilize and wrap Jena's [RDF I/O technology (RIOT)](https://jena.apache.org/documentation/io/) can be called.     Assuming `fox-and-stork-edn` is a Clojure EDN representation of RDF, and `conv` the shorthand for the `aesopica.converter` namespace, a conversion to Turtle can be written as:    ```clojure  (conv/convert-to-turtle fox-and-stork-edn)  ```  See the `aesopica.converter` namespace and related tests for more examples.     Note that certain formats, such as Turtle, are not designed with quads/named graphs in mind.  In cases such as these, a converter to a format that supports quads need to be used (e.g.: TriG, N-Quads) to not lose information.    ## Design Decisions and Tutorial    I have been writing a number of articles about the use of Clojure for creating Linked Data, that is interlinked with the creation of this library:    1. [General Introduction](https://www.newresalhaider.com/post/aesopica-1/)  2. [Datatypes](https://www.newresalhaider.com/post/aesopica-2/)  2. [Named Graphs](https://www.newresalhaider.com/post/aesopica-3/)    ## License    Copyright © 2018 Newres Al Haider    Distributed under the Eclipse Public License (see the LICENSE file).  """
Semantic web;https://github.com/AKSW/OntoWiki;"""# OntoWiki    [![Build Status](http://owdev.ontowiki.net/job/OntoWiki/badge/icon)](http://owdev.ontowiki.net/job/OntoWiki/)  [API Documentation](http://api.ontowiki.net/)    ![](https://raw.github.com/wiki/AKSW/OntoWiki/images/owHeader.png)    ## Introduction    is a tool providing support for agile, distributed knowledge engineering scenarios.  OntoWiki facilitates the visual presentation of a knowledge base as an information map, with different views on instance data.  It enables intuitive authoring of semantic content.  It fosters social collaboration aspects by keeping track of changes, allowing to comment and discuss every single part of a knowledge base.    Other remarkable features are:    * OntoWiki is a Linked Data Server for you data as well as a Linked Data client to fetch additional data from the web  * OntoWiki is a Semantic Pingback Client in order to receive and send back-linking request as known from the blogosphere.  * OntoWiki is backend independent, which means you can save your data on a MySQL database as well as on a Virtuoso Triple Store.  * OntoWiki is easily extendible by you, since it features a sophisticated Extension System.    ## Installation/Update    If you are updating OntoWiki, please don't forget to run `make deploy`.  If `make deploy` fails, you might also have to run `make getcomposer` once before run `make deploy` again.    For further installation instructions please have a look at our [wiki](https://github.com/AKSW/OntoWiki/wiki/GetOntowikiUsers) (might be outdated in some parts).    ## Screenshot / Webinar  Below is a screenshot showing OntoWiki in editing mode.    For a longer visual presentation you can watch our [webinar@youtube](http://www.youtube.com/watch?v=vP1UDKeZsQk)  (thanks to Phil and the Semantic Web company).    ![Screenshot](http://lh4.ggpht.com/-kXpKMqBBCIU/Tpx45SUaItI/AAAAAAAAA9w/aPYaNQjcpvo/s800/ontowiki.png)    ## License    OntoWiki is licensed under the [GNU General Public License Version 2, June 1991](http://www.gnu.org/licenses/gpl-2.0.txt) (license document is in the application subfolder). """
Semantic web;https://github.com/Swirrl/csv2rdf;"""# csv2rdf    [![CircleCI](https://circleci.com/gh/Swirrl/csv2rdf/tree/master.svg?style=svg)](https://circleci.com/gh/Swirrl/csv2rdf/tree/master)    Command line application (and clojure library) for converting [CSV to RDF](https://www.w3.org/TR/2015/REC-csv2rdf-20151217/) according to the specifications for [CSV on the web](https://w3c.github.io/csvw/).    ## Native Builds    We have some experimental native builds of the command line app here:    - For [linux x86](https://github.com/Swirrl/csv2rdf/releases/tag/graal-linux-0.4.7-SNAPSHOT-c8fe70c)  - For [mac os](https://github.com/Swirrl/csv2rdf/releases/tag/graal-0.4.7-SNAPSHOT-8839f3f)    ## Running    csv2rdf can be run from the command line given the location of either a tabular data file or metadata file referencing the described tabular file. The location  can be either a path on the local machine or URI for the document on the web.    To run from a tabular file:        java -jar csv2rdf-standalone.jar -t /path/to/tabular/file.csv    The resulting RDF is written to standard output in [turtle](https://www.w3.org/TR/turtle/) format. The output can instead be written to file with the -o option:        java -jar csv2rdf-standalone.jar -t /path/to/tabular/file.csv -o output.ttl    The extension of the output file is used to determine the output format. The full list of supported formats is defined by [rdf4j](http://docs.rdf4j.org/programming/#_detecting_the_file_format),  some common formats are listed below:    | Extension | Format                                               |  | --------- | -----------------------------------------------------|  | .ttl      | [turtle](https://www.w3.org/TR/turtle/)              |  | .nt       | [n-triples](https://www.w3.org/TR/n-triples/)        |  | .xml      | [rdf-xml](https://www.w3.org/TR/rdf-syntax-grammar/) |  | .trig     | [trig](https://www.w3.org/TR/trig/)                  |  | .nq       | [n-quads](https://www.w3.org/TR/n-quads/)            |    Note that for quad formats like trig and n-quads the graph will be nil.    The triples are generated according to CSVW standard mode by default. The mode to use can be specified by the -m parameter:        java -jar csv2rdf-standalone.jar -t /path/to/tabular/file.csv -m minimal    The supported values for the mode are `standard` and `minimal` and `annotated`. `annotated` mode is a non-standard mode which behaves like  `minimal` mode with the addition that any notes or non-standard annotations defined for table groups and tables will be output if the  corresponding metadata element specifies an `@id`.    The recommended way to start processing a tabular file is from a metadata document that describes the structure of a referenced tabular file. The tabular file does not  need to be provided when processing from a metadata file since the metadata should contain a reference to the tabular file(s).        java -jar csv2rdf-standalone.jar -u /path/to/metadata/file.json -o output.ttl    ## Using as a library    csv2rdf also exposes its functionality as a library - please see [the csv2rdf library](doc/library.md) for a description of the library and its interface.    - See [overview of the code](doc/code.md) for an overview of the codebase.  - See [Developing csv2rdf itself](doc/developing.md) for a quickstart guide on how to work on the library and application itself.    ## License    Copyright © 2018 Swirrl IT Ltd.    Distributed under the Eclipse Public License either version 1.0 or (at  your option) any later version. """
Semantic web;https://github.com/knakk/fenster;"""## Fenster  Fenster is a fronted for RDF quad-stores.    It is inspired by, and similar to [Pubby](http://wifo5-03.informatik.uni-mannheim.de/pubby/), but differs in that it shows triples from all public graphs, not just the default graph.    Example of how a resolvable URI, [http://data.deichman.no/resource/tnr_1140686](http://data.deichman.no/resource/tnr_1140686), is presented in Fenster:  ![screenshot](https://dl.dropboxusercontent.com/u/27551242/azur.png)    ### Status  Fenster is stable and has been in production since November 2013 as a frontend for the [RDF-catalogue](http://data.deichman.no) of [Oslo public library](http://www.deichman.no). Currently it's only been tested against Virtuoso, but presumably any compliant SPARQL endpoint should work. Please let us know if you run into any issues.    ### Deployment  Fenster is written in Go, so you'll need the [Go toolchain](http://golang.org/doc/install) in order to build. It compiles to a statically linked binary, so deployment couldn't be simpler:    1. `git clone https://github.com/knakk/fenster`  2. `cd fenster`  3. `make package`*  4. copy `fenster.tar.gz` to your server  5. unpack, adjust settings in `config.ini` and run `fenster`    *You have to do the build step manually if your target platform is of a different architecture than your compilation platform.  [See this guide](http://dave.cheney.net/2012/09/08/an-introduction-to-cross-compilation-with-go) if you don't know where to start.    If you're on Ubuntu, you might want to deploy Fenster as an Upstart service. Example config:  ```upstart  description ""Fenster""    start on filesystem or runlevel [2345]  stop on runlevel [!2345]    respawn    chdir /path/to/fenster  exec ./fenster  ```    #### Apache routing  If Fenster is running on same server as the RDF-store, you'll have to proxy the requests to the SPARQL endpoint.    Here is an example Apache config, given Fenster running on localhost:8080 and SPARQL endpoint running on localhost:8890/sparql:    ```apache  <VirtualHost *:80>        ServerAdmin serveradmin@example.no      DocumentRoot /var/www/example.com      ServerName example.com        ProxyRequests off      ProxyPreserveHost on      ProxyTimeout        300      # Proxy ACL      <Proxy *>          Order allow,deny          Allow from all      </Proxy>      RewriteEngine on      RewriteRule ^/sparql(.*)$ http://localhost:8890/sparql$1 [P]        # default proxy if not handled above      ProxyPass / http://example.com:8080/ timeout=300      ProxyPassReverse / http://example.com:8080/    </VirtualHost>  ```      ### License  GPLv3 """
Semantic web;https://github.com/castagna/EARQ;"""EARQ = ElasticSearch + ARQ   ==========================    EARQ is a combination of ARQ and ElasticSearch. It gives ARQ the ability to   perform free text searches using an ElasticSearch cluster. ElasticSearch   indexes are additional information for accessing the RDF graph, not storage   for the graph itself.    This is *experimental* (and unsupported).      How to use it  -------------    This is how you build an index from a Jena Model:        ModelIndexerString indexer = new ModelIndexerString(""earq_index"");      indexer.indexStatements(model.listStatements());      indexer.close();    This is how you configure ARQ to use ElasticSearch:                IndexSearcher searcher = IndexSearcherFactory.create(Type.ELASTICSEARCH, ""earq_index"") ;      EARQ.setDefaultIndex(searcher) ;    This is an example of a SPARQL query using the sarq:search property function:         PREFIX earq: <http://openjena.org/EARQ/property#>      SELECT * WHERE {          ?doc ?p ?lit .          (?lit ?score ) earq:search ""+text"" .      }      Acknowledgement  ---------------            The design and part of the code has been inspired from LARQ, see:     * [http://openjena.org/ARQ/lucene-arq.html](http://openjena.org/ARQ/lucene-arq.html)      Todo  ----     * ... still broken   * Remove LARQ reference from ModelIndexerSubject """
Semantic web;https://github.com/wikibus/rdf.vocabularies;"""# Rdf.Vocabularies [![Build status](https://ci.appveyor.com/api/projects/status/8uepsle8g54v101m/branch/master?svg=true)](https://ci.appveyor.com/project/tpluscode78631/rdf-vocabularies/branch/master) [![NuGet version](https://badge.fury.io/nu/rdf.vocabularies.svg)](https://badge.fury.io/nu/rdf.vocabularies) [![codefactor][codefactor-badge]][codefactor-link]     ## What     Statically typed RDF Vocabularies for .NET. Most generated automatically from respective RDF/OWL files:    * `bibo` - [Bibliographic Ontology](http://lov.okfn.org/dataset/lov/vocabs/bibo)  * `dc` - [Dublin Core Metadata Element Set](http://lov.okfn.org/dataset/lov/vocabs/dce)  * `dcterms` - [DCMI Metadata Terms](http://lov.okfn.org/dataset/lov/vocabs/dcterms)  * `foaf` - [Friend of a Friend Vocabulary](http://lov.okfn.org/dataset/lov/vocabs/foaf)  * `Hydra` - [A lightweight vocabulary for hypermedia-driven Web APIs](http://www.hydra-cg.com/)  * `opus` - [Ontology of Computer Science Publications](http://lov.okfn.org/dataset/lov/vocabs/opus)  * `owl` - [Ontology Web Language](http://lov.okfn.org/dataset/lov/vocabs/owl)  * `rdf` - [The RDF Concepts Vocabulary](http://lov.okfn.org/dataset/lov/vocabs/rdf)  * `rdfs` - [The RDF Schema vocabulary](http://lov.okfn.org/dataset/lov/vocabs/rdfs)  * `schema` - [Schema.org vocabulary](http://lov.okfn.org/dataset/lov/vocabs/schema)  * `sioc` - [Semantically-Interlinked Online Communities](http://lov.okfn.org/dataset/lov/vocabs/sioc)  * `skos` - [Simple Knowledge Organization System](http://lov.okfn.org/dataset/lov/vocabs/skos)    ## How    Install NuGet    ```  install-package Rdf.Vocabularies  ```    Use    ``` csharp  using Vocab;    var rdfsLabel = Rdfs.label;  ```    [codefactor-badge]: https://www.codefactor.io/repository/github/wikibus/Rdf.Vocabularies/badge/master  [codefactor-link]: https://www.codefactor.io/repository/github/wikibus/Rdf.Vocabularies/overview/master """
Semantic web;https://github.com/cosminbasca/cysparql;"""CySparql  ========    CySparql is a python wrapper over the excellent heavy-duty C [Rasqal RDF](http://librdf.org/rasqal/) v0.9.33+ [SPARQL](http://www.w3.org/TR/rdf-sparql-query/) parser. The library is intended to give a pythonic feel to parsing SPARQL queries. There are several goodies included as well like:  * simple and fast star-pattern extraction from SPARQL queries  * node-edge (graph) visualizations of SPARQL queries  * simple descriptive command line utility to describe a given SPARQL query (from a file or read from stdin)  * auto pretty formatter for SPARQL queries (slower, than just parsing)    Important Notes  ---------------  This software is the product of research carried out at the [University of Zurich](http://www.ifi.uzh.ch/ddis.html) and comes with no warranty whatsoever. Have fun!    TODO's  ------  * The *librasqal* is not fully supported (e.g. filters, etc)  * The project is not documented (yet)    How to Compile the Project  --------------------------  Ensure that *librasqal* v0.9.33+ and *libraptor2* v2.0.13+ are installed on your system (either using the package manager of the OS or compiled from source).    To install **CySparql** you have two options: 1) manual installation (install requirements first) or 2) automatic with **pip**    **Manual** installation:  ```sh  $ git clone https://github.com/cosminbasca/cysparql  $ cd cysparql  $ python setup.py install  ```    Install the project with **pip**:  ```sh  $ pip install https://github.com/cosminbasca/cysparql  ```    Also have a look at the build.sh, clean.sh, test.sh scripts included in the codebase     Basic Example  -------------  ```python  from cysparql import *  q_string = """"""  PREFIX example: <http://www.example.org/rdf#>  SELECT * WHERE {      ?a example:p ?b1.      ?a example:p ?b2.      ?a example:p ?b3.      ?a example:p ?b4.      ?a example:p ?b5.      ?a example:p ?b6.      ?a example:q ?b6.      ?b5 example:p ?x .      ?b6 example:p ?y .  }  """"""    query = Query(q_string, pretty=True)  # should print:  #[[ 0.  0.  0.  0.  1.  0.  0.  0.  0.]  # [ 0.  0.  0.  0.  0.  1.  0.  0.  0.]  # [ 0.  0.  0.  1.  1.  1.  1.  1.  1.]  # [ 0.  0.  1.  0.  0.  0.  0.  0.  0.]  # [ 1.  0.  1.  0.  0.  0.  0.  0.  0.]  # [ 0.  1.  1.  0.  0.  0.  0.  0.  0.]  # [ 0.  0.  1.  0.  0.  0.  0.  0.  0.]  # [ 0.  0.  1.  0.  0.  0.  0.  0.  0.]  # [ 0.  0.  1.  0.  0.  0.  0.  0.  0.]]  print query.adjacency_matrix    # sould print: triple_patterns =  <cysparql.pattern.TriplePatternSequence object at 0x1049a4730>  print 'triple_patterns = ',query.triple_patterns    # should print:  #STAR (0):   # [< ?a, http://www.example.org/rdf#p, ?b1 ,None>, < ?a, http://www.example.org/rdf#p, ?b2 ,None>, < ?a, http://www.example.org/rdf#p, ?b3 ,None>, < ?a, http://www.example.org/rdf#p, ?b4 ,None>, < ?a, http://www.example.org/rdf#p, ?b5 ,None>, < ?a, http://www.example.org/rdf#p, ?b6 ,None>, < ?a, http://www.example.org/rdf#q, ?b6 ,None>]  #  #STAR (1):   # [< ?a, http://www.example.org/rdf#p, ?b5 ,None>, < ?b5, http://www.example.org/rdf#p, ?x ,None>]  #  #STAR (2):   # [< ?a, http://www.example.org/rdf#p, ?b6 ,None>, < ?a, http://www.example.org/rdf#q, ?b6 ,None>, < ?b6, http://www.example.org/rdf#p, ?y ,None>]  stars = get_stars(query.triple_patterns)  for i,s in enumerate(stars):      print '\nSTAR (%s): \n %s'%(i,s)    # if asciinet is installed  # should print:  #ASCII:   #          ┌─────────────┐             #          │     ?a      │             #          └┬┬──┬─────┬┬┬┘             #           ││  │     │││              #           ││  │     ││└───┐          #    ┌──────┼┘  │     ││    │          #    │      │   │     ││    │          #    v      v   │     ││    │          #  ┌───┐  ┌───┐ │     ││    │          #  │?b6│  │?b5│ │     ││    │          #  └┬──┘  └─┬─┘ │     ││    │          #   │       │   │     ││    │          #   │     ┌─┘   │     │└────┼─────┐    #   │     │     │     │     │     │    #   v     v     v     v     v     v    # ┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐  # │?y │ │?x │ │?b3│ │?b2│ │?b1│ │?b4│  # └───┘ └───┘ └───┘ └───┘ └───┘ └───┘  print 'ASCII: \n',query.ascii    # print the librasqal debug information  # should print:  #query verb: SELECT  #data graphs: []  #named variables: [variable(a), variable(b1), variable(b2), variable(b3), variable(b4), variable(b5), variable(b6), variable(x), variable(y)]  #anonymous variables: []  #projected variable names: a, b1, b2, b3, b4, b5, b6, x, y  #  #bound variables: [variable(a), variable(b1), variable(b2), variable(b3), variable(b4), variable(b5), variable(b6), variable(x), variable(y)]  #triples: [triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b1)), triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b2)), triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b3)), triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b4)), triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b5)), triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b6)), triple(variable(a), uri<http://www.example.org/rdf#q>, variable(b6)), triple(variable(b5), uri<http://www.example.org/rdf#p>, variable(x)), triple(variable(b6), uri<http://www.example.org/rdf#p>, variable(y))]  #prefixes: [prefix(example as http://www.example.org/rdf#)]  #query graph pattern: graph pattern[0] Basic(over 9 triples[triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b1)) ,triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b2)) ,triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b3)) ,triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b4)) ,triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b5)) ,triple(variable(a), uri<http://www.example.org/rdf#p>, variable(b6)) ,triple(variable(a), uri<http://www.example.org/rdf#q>, variable(b6)) ,triple(variable(b5), uri<http://www.example.org/rdf#p>, variable(x)) ,triple(variable(b6), uri<http://www.example.org/rdf#p>, variable(y))])  query.debug()  ```    Thanks a lot to  ---------------  * [University of Zurich](http://www.ifi.uzh.ch/ddis.html) and the [Swiss National Science Foundation](http://www.snf.ch/en/Pages/default.aspx) for generously funding the research that led to this software. """
Semantic web;https://github.com/AKSW/SparqlAnalytics;"""## Welcome to the SPARQL Analytics project.    This project aims to develop a java based middleware/proxy framework for live analysis of SPARQL queries. Publish live SPARQL endpoint metrics using embeddable HTML/JavaScript widgets.    ### Live Query Usage Stats  Although the goal of this project is more amibitious than ""just"" providing a live chart of SPARQL endpoint activity, this is still a pretty neat ""by-product"", which we intend to develop further.    #### Demo    A demo can be seen here:  * [FP7-ICT project partners dataset landing page](http://fp7-pp.publicdata.eu) shows the live chart (unfortunately requires IPv6 - if you know how to proxy websockets, please tell me :)  * [SNORQL-SPARQL explorer](http://fp7-pp.publicdata.eu/snorql) lets you do the queries (at the moment only Select queries are handled in the live chart)    ![Screenshot](https://raw.github.com/AKSW/SparqlAnalytics/master/images/2013-04-04-sparql-analytics-screenshot-fp7-pp.publicdata.eu.png)      #### Server Setup  A note in advance: currently the server is [CORS](http://enable-cors.org) enabled on all paths, so you *and anyone else* should be able to do cross site requests from JavaScript.    Clone the project and run        maven clean install    First, you need a postgres database. All query activity will be written to it.        sudo apt-get install postgres      # ... further configuration is up to you            # Create a DB called 'sparql_analytics'      createdb sparql_analytics            # Load the core schema      psql -d sparql_analytics -f sparql-analytics-core/schema.sql    An example server configuration is located under `sparql-analytics-server/config/example/sparql-analytics/platform.properties`. Either modify it directly, or better: create a copy of it and edit the copy:          mkdir sparql-analytics-server/config/myconf      cp -rf sparql-analytics-server/config/example/* sparql-analytics-server/config/myconf    Note that the `sparqlify-analytics` directory under your config directory (i.e. `example` or `myconf`) corresponds to the context path under which the server will run. So this is not optional!    Under `bin` you find the script to run the server:        cd bin      ./run-platform sparql-analytics-server/config/myconf    By default, the server will start on port 5522. Try your browser or curl to test:    [http://localhost:5522/sparql-analytics/api/sparql?query=Select { ?s ?p ?o } Limit 1](http://localhost:5522/sparql-analytics/api/sparql?query=Select%20%2A%20%7B%20%3Fs%20%3Fp%20%3Fo%20%7D%20Limit%201)        curl 'http://localhost:5522/sparql-analytics/api/sparql?query=Select%20%2A%20%7B%20%3Fs%20%3Fp%20%3Fo%20%7D%20Limit%201'    #### Client Setup    The client chart widget is in the `sparql-analytics-client` module. To build the minimized .js file, run        cd sparql-analytics-client      mvn package            # Link the built js file to the webapp js directory, because our HTML file in the next step references it      # CARE: Note the {version} placeholder in the next line :)            ln -s target/sparql-analytics-client-{version}/webapp/js/sparql-analytics-client.min.js src/main/webapp/js/sparql-analytics-client.min.js      Link the client HTML/JavaScript code to your webserver directory (requires you to allow your webserver to follow symlinks)        ln -s /path/to/repo/sparql-analytics-client/src/main/webapp /var/www/sparql-analytics-client    Now visit the following file [index-sparql-analytics-minimal.html](https://github.com/AKSW/SparqlAnalytics/blob/master/sparql-analytics-client/src/main/webapp/index-sparql-analytics-minimal.html) for a minimal example:     [http://localhost/sparql-analytics-client/index-sparql-analytics-minimal.html](http://localhost/sparql-analytics-client/index-sparql-analytics-minimal.html)    You can embad the chart widget by only integrating the following snippet (with properly adjusted paths) into your web page:        <html>      <body>          <div id=""histogram""></div>            <script type=""text/javascript"" src=""js/lib/jquery/1.9.1/jquery-1.9.1.js""></script>          <script type=""text/javascript"" src=""js/lib/jquery-atmosphere/jquery.atmosphere.js""></script>          <script type=""text/javascript"" src=""js/bootstrap.min.js""></script>          <script type=""text/javascript"" src=""js/lib/underscore/1.4.4/underscore.js""></script>          <script type=""text/javascript"" src=""js/lib/highcharts/2.2.5/js/highcharts.js""></script>          <script type=""text/javascript"" src=""js/lib/namespacedotjs/a28da387ce/Namespace.js""></script>            <script type=""text/javascript"" src=""js/sparql-analytics-client.min.js""></script>            <script type=""text/javascript"">              $(document).ready(function() {			                  new SparqlAnalytics.WidgetChartQueryLoad({                      el: '#histogram',                      apiUrl: 'http://localhost:5522/sparql-analytics/api/live'                  });              });          </script>        </body>      </html>      ### License  Will be clarified shortly.     """
Semantic web;https://github.com/specgen/specgen;"""SpecGen v6  ==========    About  -----    This is an experimental new codebase for [specgen](http://forge.morfeo-project.org/wiki_en/index.php/SpecGen) tools based on danbri's [specgen5 version](http://svn.foaf-project.org/foaftown/specgen/),   which was heavily updated by [Bo Ferri](http://github.com/zazi/) in summer 2010.    <b>Features (incl. modifications + extensions)</b>:    * multiple property and class types  * muttiple restrictions modelling  * rdfs:label, rdfs:comment  * classes and properties from other namespaces  * inverse properties (explicit and anonymous)  * sub properties  * union ranges and domains (appear only in the property descriptions, not on the class descriptions)  * equivalent properties  * simple individuals as optional feature    Dependencies  ------------  		  It depends utterly upon     * [rdflib](http://rdflib.net/)  * [rdfextras](http://code.google.com/p/rdfextras/) (`easy_install rdfextras`)  * [pyparsing](http://pyparsing.wikispaces.com/) (`easy_install pyparsing`)  * [igraph](http://igraph.org/python/) (`easy_install python-igraph`)  	  (at least I had to install these packages additionally ;) )    If you're lucky, typing this is enough:    	easy_install rdflib python-igraph    and if you have problems there, update easy_install etc with:    	easy_install -U setuptools    Ubuntu, you can install the dependencies with pip after installing the relevant libraries  ```  sudo apt-get install python-dev build-essential libxml2-dev libxslt python-igraph  sudo pip install -r requirements.txt  ```  	  Purpose  -------  	  <b>Inputs</b>: [RDF](http://www.w3.org/TR/rdf-primer/), HTML and [OWL](http://www.w3.org/TR/owl-semantics/) description(s) of an RDF vocabulary<br/>  <b>Output</b>: an [XHTML+RDFa](http://www.w3.org/TR/rdfa-syntax/) specification designed for human users    Example  -------    	specgen6.py --indir=onto/olo/ --ns=http://purl.org/ontology/olo/core#  --prefix=olo --ontofile=orderedlistontology.owl --outdir=spec/olo/ --templatedir=onto/olo/ --outfile=orderedlistontology.html    * the template of this example can also be found in the folder: onto/olo  * the output of this example can also be found in the folder: spec/olo    See [libvocab.py](https://github.com/specgen/specgen/blob/master/libvocab.py) and [specgen6.py](https://github.com/specgen/specgen/blob/master/specgen6.py) for details.    Status  ------    * we load up and interpret the core RDFS/OWL   * we populate Vocab, Term (Class, Property or Individual) instances  * able to generate a XHTML/RDFa ontology specification with common concepts and properties from OWL, [RDFS](http://www.w3.org/TR/rdf-schema/), RDF    Known Forks  -----------    * [WebID fork](http://dvcs.w3.org/hg/WebID/file/029f115c08a5/ontologies/specgen) (note this link is only a reference to a specific revision of that fork; to ensure that you'll utilise the most recent one, go to summary and walk that path to the specgen directory again from the most recent revision ;) )    TODO  ----    * enable more OWL features, especially an automated construction of owl:Ontology (currently this must be done manually in the template)  * enable more support for other namespaces (super classes and super properties from other namespaces already possible)  * restructure the code !!!  * write a cool parser for the ""\n""'s and ""\t""'s etc. in the parsed comments (e.g. ""\n"" to \<br\/\> ...)    Known Issues  ------------    <ol>  	<li>librdf doesn't seem to like abbreviations in FILTER clauses.    <ul>  	<li>this worked:  <pre><code>q= 'SELECT ?x ?l ?c ?type WHERE { ?x rdfs:label ?l . ?x rdfs:comment ?c . ?x a ?type .  FILTER (?type = &lt;http://www.w3.org/2002/07/owl#ObjectProperty\&gt;)  } '</code></pre></li>  <li>while this failed:  <pre><code>q= 'PREFIX owl: <http://www.w3.org/2002/07/owl#> SELECT ?x ?l ?c ?type WHERE { ?x rdfs:label ?l . ?x rdfs:comment ?c . ?x a ?type .  FILTER (?type = owl:ObjectProperty)  } '</pre></code>  (even when passing in bindings)</li>  <li>This forces us to be verbose, ie.  <pre><code>q= 'SELECT distinct ?x ?l ?c WHERE { ?x rdfs:label ?l . ?x rdfs:comment ?c . ?x a ?type . FILTER (?type = &lt;http://www.w3.org/2002/07/owl#ObjectProperty&gt; || ?type = &lt;http://www.w3.org/2002/07/owl#DatatypeProperty&gt; || ?type = &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#Property&gt; || ?type = &lt;http://www.w3.org/2002/07/owl#FunctionalProperty&gt; || ?type = &lt;http://www.w3.org/2002/07/owl#InverseFunctionalProperty&gt;) } '</pre></code></li>  </ul></li>	  <li>TODO: work out how to do "".encode('UTF-8')"" everywhere</li>  <li>Be more explicit and careful re defaulting to English, and more robust when multilingual labels are found.</li></ol>    PS  --    The [old project repository location at SourceForge](http://smiy.svn.sourceforge.net/viewvc/smiy/specgen/) is now deprecated. All new developments will be pushed to this repository location here at GitHub. """
Semantic web;https://github.com/Claudenw/jdbc4sparql;"""Complete functionality has not been tested.  However, most code is functional and testing should   be completed in the near future.    # JDBC 4 SPARQL    JDBC 4 SPARQL is a JDBC Driver that uses a SPARQL endpoint (or Jena Model) as the data store.    ## Licenses    JDBC 4 SPARQL is licensed under Apache License V2.  However, there is one component that is Licensed under the GNU LGPL V3.0  this means that if you want to use this product in an environment that is not permitted under GNU LGPL V3.0 you will need to   find another implementation of the SparqlParser interface.  How to do this is covered in the Extensions Points section of this   document.  	  ## Maven    Group ID: org.xenei    Artifact ID: jdbc4sparql    ## Usage    ### URL  The URL for the J4SDriver is       jdbc:j4s:[?arg=value>[&arg2=value2[&arg3=value3...]]]:url    For current runtime configuration options and defaults execute the J4SDriver class as an application.    #### Valid arguments  * catalog The name of the catalogue to restrict queries for.  * type The type of the input.  * builder The builder to build the SQL Schema with.  Either the name of a registered builder or a class name.  See ""Registered Schema Builders"" below.  * parser The parser class.  A fully qualified SparqlParser implementation.  Defaults to org.xenei.jdbc4sparql.sparql.parser.jsqlparser.SparqlParserImpl    #### Valid Types:  * (Default) config - URL is a J4S configuration file (NOT YET IMPLEMENTED)  * sparql - URL is a sparql endpoint  * RDFXML or RDF/XML - URL is a RDF/XML formatted RDF file  * NTRIPLES or N-Triples - URL is a N-Triples formatted RDF file  * N3 or N3 - URL is a N3 formatted RDF file  * TURTLE or Turtle - URL is a Turtle formatted RDF file  * RDFJSON or RDF/JSON - URL is a RDF/JSON formatted RDF file  * NQUADS or N-Quads - URL is a N-Quads formatted RDF file  * TRIG or TriG - URL is a TriG formatted RDF file    #### Registered Schema Builders  (Default) Simple_Builder: A simple schema builder that builds tables based on RDFS Class names    #### Notes ####    Currently the catalogue is built at runtime by the builder, however future improvements should include a mechanism to store an entire configuration of multiple catalogues.    the Catalogue contains the URL for the SPARQL endpoint or RDF file so it will be possible to configure the driver to access multiple endpoints.    ## A Conflict Of Nomenclatures    SQL and SPARQL use nomenclatures that conflict with each other.  For the purposes of this documentation, unless otherwise specified,  SQL nomenclatures will be used.    ## How This Works    ### Mapping The Graph  In general an RDF Graph (and thus a SPARQL endpoint) is an instance of a navigational database, SQL is generally designed to work  against a relational model so there this application attempts to map a navigational database onto relational tables and keys.  It   does this through the use of a ""SchemaBuilder"" implementation to map the RDF Graph to tables and columns.  The default implementation looks for RDF resources that have a rdf:type of rdfs:Class.  It considers   each of these resources as tables and uses the resource name as the table name, we will call these ""Table Resources"".  It then scans the   RDF store looking for distinct properties of objects of the ""Table Resource"" types.  The properties are the columns in the table.  All tables generated by the ""SchemaBuilder"" are   stored in a SQL Schema, so multiple schemas may be created from a single RDF Schema, Vocabulary or datastore.    It is possible to override the default implementation by implementing a new SchemaBuilder and adding it to the /META-INF/services/org.xenei.jdbc4sparql.sparql.builders.SchemaBuilder file.  The first entry in the file is considered the default builder.    ### Generating A Query  A SQL query is parsed by a ""SparqlParser"" implementation the and converted into a SPARQL query.    The default implementation of SparqlParser is LGPL v3 and uses an LGPL v2.1 based package.  Other implementations may be used.  To register a different implementation place it as the first entry in the /META-INF/services/org.xenei.jdbc4sparql.sparql.parser.SparqlParser file.  The returned query is then executed against the SPARQL endpoint or the   Jena Model.  The SPARQL ResultSet is retrieved and the QuerySolutions stored in a local list.  A SQL ResultSet is   created against that list and returned.     """
Semantic web;https://github.com/Wimmics/corese;"""<!-- markdownlint-configure-file { ""MD004"": { ""style"": ""consistent"" } } -->  <!-- markdownlint-disable MD033 -->    #    <p align=""center"">      <a href=""https://project.inria.fr/corese/"">          <img src=""https://user-images.githubusercontent.com/5692787/151987397-316a61f0-8098-4d37-a4e8-69180e33261a.svg"" width=""300"" height=""149"" alt=""Corese-logo"">      </a>      <br>      <strong>Software platform for the Semantic Web of Linked Data</strong>  </p>  <!-- markdownlint-enable MD033 -->    Corese is a software platform implementing and extending the standards of the Semantic Web. It allows to create, manipulate, parse, serialize, query, reason and validate RDF data.    Corese implement W3C standarts [RDF](https://www.w3.org/RDF/), [RDFS](https://www.w3.org/2001/sw/wiki/RDFS), [SPARQL1.1 Query & Update](https://www.w3.org/2001/sw/wiki/SPARQL), [OWL RL](https://www.w3.org/2005/rules/wiki/OWLRL), [SHACL](https://www.w3.org/TR/shacl/) …  It also implements extensions like [STTL SPARQL](https://files.inria.fr/corese/doc/sttl.html), [SPARQL Rule](https://files.inria.fr/corese/doc/rule.html) and [LDScript](https://files.inria.fr/corese/doc/ldscript.html).    There are three versions of Corese:    - **Corese-library:** Java library to process RDF data and use Corese features via an API.  - **Corese-server:** Tool to easily create, configure and manage SPARQL endpoints.  - **Corese-gui:** Graphical interface that allows an easy and visual use of Corese features.    ## Download and install    ### Corese-library    - Download from [maven-central](https://search.maven.org/search?q=g:fr.inria.corese)    ```xml  <dependency>      <groupId>fr.inria.corese</groupId>      <artifactId>corese-core</artifactId>      <version>4.3.0</version>  </dependency>    <dependency>      <groupId>fr.inria.corese</groupId>      <artifactId>corese-rdf4j</artifactId>      <version>4.3.0</version>  </dependency>  ```    Documentation: [Getting Started With Corese](https://notes.inria.fr/s/hiiedLfVe#)    ### Corese-server    - Download from [Docker-hub](https://hub.docker.com/r/wimmics/corese)    ```sh  docker run --name my-corese \      -p 8080:8080 \      -d wimmics/corese  ```    - Download [Corese-server jar file](https://project.inria.fr/corese/download/).    ```sh  # Replace ${VERSION} with the desired version number (e.g: 4.3.0)  wget ""files.inria.fr/corese/distrib/corese-server-${VERSION}.jar""  java -jar ""corese-server-${VERSION}.jar""  ```    ### Corese-GUI    - Download [Corese-gui jar file](https://project.inria.fr/corese/download/).    ```sh  # Replace ${VERSION} with the desired version number (e.g: 4.3.0)  wget ""files.inria.fr/corese/distrib/corese-gui-${VERSION}.jar""  java -jar ""corese-gui-${VERSION}.jar""  ```    ## Compilation from source    Download source code and compile.    ```shell  git clone ""https://github.com/Wimmics/corese.git""  cd corese  mvn package  ```    ## Contributions and discussions    For support questions, comments, and any ideas for improvements you'd like to discuss, please use our [discussion forum](https://github.com/Wimmics/corese/discussions/).  We welcome everyone to contribute to [issue reports](https://github.com/Wimmics/corese/issues), suggest new features, and create [pull requests](https://github.com/Wimmics/corese/pulls).    ## General informations    - [Corese website](https://project.inria.fr/corese)  - [Source code](https://github.com/Wimmics/corese)  - [Corese server demo](http://corese.inria.fr/)  - [Changelog](https://notes.inria.fr/s/TjriAbX14#)  - **Mailing list:** corese-users at inria.fr  - **Subscribe to mailing list:** corese-users-request at inria.fr **subject:** subscribe """
Semantic web;https://github.com/stain/profilechecker;"""# OWL API profile checker    (c) 2012-2017 The University of Manchester    License: [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0)     (see `LICENSE` and `NOTICE` for required notices)    Author: Stian Soiland-Reyes <soiland-reyes@manchester.ac.uk>      ## Requirements    * [Java](https://java.com/en/download/) 8 or [OpenJDK](http://openjdk.java.net/) 8  * [Apache Maven](https://maven.apache.org/download.cgi) 3.3 or later       ## Building    Note: you don't need to compile from source code, you can also use one of the   [releases](https://github.com/stain/profilechecker/releases).          stain@ralph-ubuntu:~/src/profilechecker$ mvn clean package      [INFO] Scanning for projects...      [INFO]                                                                               [INFO] ------------------------------------------------------------------------      [INFO] Building OWL API profile checker 1.1.0      [INFO]       (..)      [INFO] Replacing /home/stain/src/profilechecker/target/profilechecker-1.1.0.jar with /home/stain/src/profilechecker/target/profilechecker-1.0-shaded.jar      [INFO] Dependency-reduced POM written at: /home/stain/src/profilechecker/dependency-reduced-pom.xml      [INFO] ------------------------------------------------------------------------      [INFO] BUILD SUCCESS      [INFO] ------------------------------------------------------------------------      [INFO] Total time: 29.912s      [INFO] Finished at: Thu Feb 07 15:34:12 GMT 2013      [INFO] Final Memory: 22M/169M      [INFO] ------------------------------------------------------------------------        ## Usage    ### Help        $ java -jar target/profilechecker-1.1.0.jar -h      Usage: profilechecker.jar <ontology.owl> [profile]            Available profiles:      OWL2_DL (OWL 2 DL)      OWL2_QL (OWL 2 QL)      OWL2_EL (OWL 2 EL)      OWL2_RL (OWL 2 RL)      OWL2_FULL (OWL 2 DL) -default-      --all    (Modify the version number `1.1.0` above to correspond to the output of your build)    The `<ontology.owl>` parameter can be given as a local file name or an  absolute IRI.    ### Default profile    With only ontology IRI or file name, will check against default profile  (OWL 2 Full):        $ java -jar target/profilechecker-1.1.0.jar https://cdn.rawgit.com/owlcs/pizza-ontology/v1.5.0/pizza.owl    Exit code is 0 if the ontology conforms to OWL 2 Full, in which case there should be no output on STDERR.          ### Specify OWL2 profile    Checking against a specific profile:            $ java -jar target/profilechecker-1.1.0.jar https://cdn.rawgit.com/owlcs/pizza-ontology/v1.5.0/pizza.owl OWL2QLProfile        Use of non-superclass expression in position that requires a        superclass expression:        ObjectAllValuesFrom(<http://www.co-ode.org/ontologies/pizza/pizza.owl#hasTopping>        ObjectUnionOf(<http://www.co-ode.org/ontologies/pizza/pizza.owl#MozzarellaTopping>        <http://www.co-ode.org/ontologies/pizza/pizza.owl#TomatoTopping>))        [SubClassOf(<http://www.co-ode.org/ontologies/pizza/pizza.owl#Margherita>        ObjectAllValuesFrom(<http://www.co-ode.org/ontologies/pizza/pizza.owl#hasTopping>        ObjectUnionOf(<http://www.co-ode.org/ontologies/pizza/pizza.owl#MozzarellaTopping>        <http://www.co-ode.org/ontologies/pizza/pizza.owl#TomatoTopping>)))        in <http://www.co-ode.org/ontologies/pizza/pizza.owl>]       (..)    Exit code is 0 if the ontology conforms to the specified profile, with errors logged to STDERR.    The ontology profile can be specified in any of these forms (in order of preference):    * `OWL2_DL` ([`Profiles`](http://owlcs.github.io/owlapi/apidocs_5/org/semanticweb/owlapi/profiles/Profiles.html) enum value)  * `http://www.w3.org/ns/owl-profile/DL` (IRI of OWL profile)  * `DL` (relative IRI of OWL profile)  * `OWL2DLProfile` (classname)      ### All profiles    Checking against all profiles:          $ java -jar target/profilechecker-1.1.0.jar https://cdn.rawgit.com/owlcs/pizza-ontology/v1.5.0/pizza.owl --all      OWL2_DL: OK      OWL2_QL: 52 violations      OWL2_EL: 66 violations      OWL2_RL: 188 violations      OWL2_FULL: OK      Exit code is 0 if the ontology conforms to all profiles.   The violation count per profile is output to STDOUT.      ### Warnings    Note that any warnings or errors logged from the OWLAPI (prefix `[main]`)  during ontology loading do not necessarily mean violation against the profile:        $ java -jar target/profilechecker-1.1.0.jar ~/Desktop/annotated.ttl --all      [main] ERROR uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl - Illegal redeclarations of entities: reuse of entity http://example.com/annotatedOntology#property1 in punning not allowed [Declaration(AnnotationProperty(<http://example.com/annotatedOntology#property1>)), Declaration(ObjectProperty(<http://example.com/annotatedOntology#property1>))]      OWL2_DL: 1 violations      OWL2_QL: 1 violations      OWL2_EL: 1 violations      OWL2_RL: 1 violations      OWL2_FULL: OK     """
Semantic web;https://github.com/phenoscape/scowl;"""# Scowl    [![status](http://joss.theoj.org/papers/1b9d09aab8754997884a04c081cfc019/status.svg)](http://joss.theoj.org/papers/1b9d09aab8754997884a04c081cfc019)    Scowl provides a Scala DSL allowing a declarative approach to composing OWL expressions and axioms using the [OWL API](http://owlapi.sourceforge.net).    ## Usage    Since version 1.2.1, Scowl is available via Maven Central. Add the dependency to your `build.sbt`:    ```scala  libraryDependencies += ""org.phenoscape"" %% ""scowl"" % ""1.4.0""  ```    Import `org.phenoscape.scowl._`, and Scowl implicit conversions will add pseudo Manchester syntax methods to native OWL API objects. Additionally, functional syntax-style constructors and extractors will be in scope.    Scowl 1.2+ is built with OWL API 4.x. For OWL API 3.5, use Scowl 1.0.2. Scowl is cross-compiled to support Scala 2.13 and Scala 3.    ## Examples  The easiest way to get started is to see how the DSL can be used to implement all the examples from the [OWL 2 Web Ontology Language   Primer](https://www.w3.org/TR/owl2-primer/):    * [Manchester syntax style](https://github.com/phenoscape/scowl/blob/master/src/main/scala/org/phenoscape/scowl/example/OWL2PrimerManchester.scala)  * [Functional syntax style](https://github.com/phenoscape/scowl/blob/master/src/main/scala/org/phenoscape/scowl/example/OWL2PrimerFunctional.scala)    The examples below are also available in   [code](https://github.com/phenoscape/scowl/blob/master/src/main/scala/org/phenoscape/scowl/example/ReadMeExamples.scala).    ### Scowl expressions use and return native OWL API objects  ```scala  import org.phenoscape.scowl._  // import org.phenoscape.scowl._    val hasParent = ObjectProperty(""http://www.co-ode.org/roberts/family-tree.owl#hasParent"")  // hasParent: org.semanticweb.owlapi.model.OWLObjectProperty = <http://www.co-ode.org/roberts/family-tree.owl#hasParent>    val isParentOf = ObjectProperty(""http://www.co-ode.org/roberts/family-tree.owl#isParentOf"")  // isParentOf: org.semanticweb.owlapi.model.OWLObjectProperty = <http://www.co-ode.org/roberts/family-tree.owl#isParentOf>    val isSiblingOf = ObjectProperty(""http://www.co-ode.org/roberts/family-tree.owl#isSiblingOf"")  // isSiblingOf: org.semanticweb.owlapi.model.OWLObjectProperty = <http://www.co-ode.org/roberts/family-tree.owl#isSiblingOf>    val Person = Class(""http://www.co-ode.org/roberts/family-tree.owl#Person"")  // Person: org.semanticweb.owlapi.model.OWLClass = <http://www.co-ode.org/roberts/family-tree.owl#Person>    val FirstCousin = Class(""http://www.co-ode.org/roberts/family-tree.owl#FirstCousin"")  // FirstCousin: org.semanticweb.owlapi.model.OWLClass = <http://www.co-ode.org/roberts/family-tree.owl#FirstCousin>    val axiom = FirstCousin EquivalentTo (Person and (hasParent some (Person and (isSiblingOf some (Person and (isParentOf some Person))))))  // axiom: org.semanticweb.owlapi.model.OWLEquivalentClassesAxiom = EquivalentClasses(<http://www.co-ode.org/roberts/family-tree.owl#FirstCousin> ObjectIntersectionOf(<http://www.co-ode.org/roberts/family-tree.owl#Person> ObjectSomeValuesFrom(<http://www.co-ode.org/roberts/family-tree.owl#hasParent> ObjectIntersectionOf(<http://www.co-ode.org/roberts/family-tree.owl#Person> ObjectSomeValuesFrom(<http://www.co-ode.org/roberts/family-tree.owl#isSiblingOf> ObjectIntersectionOf(<http://www.co-ode.org/roberts/family-tree.owl#Person> ObjectSomeValuesFrom(<http://www.co-ode.org/roberts/family-tree.owl#isParentOf> <http://www.co-ode.org/roberts/family-tree.owl#Person>)))))) )  ```  ### Add some axioms and programmatically generated GCIs to an ontology  ```scala  val manager = OWLManager.createOWLOntologyManager()  val ontology = manager.createOntology()  val PartOf = ObjectProperty(""http://example.org/part_of"")  val HasPart = ObjectProperty(""http://example.org/has_part"")  val DevelopsFrom = ObjectProperty(""http://example.org/develops_from"")  val Eye = Class(""http://example.org/eye"")  val Head = Class(""http://example.org/head"")  val Tail = Class(""http://example.org/tail"")    manager.addAxiom(ontology, Eye SubClassOf (PartOf some Head))  manager.addAxiom(ontology, Eye SubClassOf (not(PartOf some Tail)))    val gcis = for {    term <- ontology.getClassesInSignature(true)  } yield {    (not(HasPart some term)) SubClassOf (not(HasPart some (DevelopsFrom some term)))  }  manager.addAxioms(ontology, gcis)  ```    ### Using pattern matching extractors to implement negation normal form  ```scala  def nnf(expression: OWLClassExpression): OWLClassExpression = expression match {    case Class(_)                                                          => expression    case ObjectComplementOf(Class(_))                                      => expression    case ObjectComplementOf(ObjectComplementOf(expression))                => nnf(expression)    case ObjectUnionOf(operands)                                           => ObjectUnionOf(operands.map(nnf))    case ObjectIntersectionOf(operands)                                    => ObjectIntersectionOf(operands.map(nnf))    case ObjectComplementOf(ObjectUnionOf(operands))                       => ObjectIntersectionOf(operands.map(c => nnf(ObjectComplementOf(c))))    case ObjectComplementOf(ObjectIntersectionOf(operands))                => ObjectUnionOf(operands.map(c => nnf(ObjectComplementOf(c))))    case ObjectAllValuesFrom(property, filler)                             => ObjectAllValuesFrom(property, nnf(filler))    case ObjectSomeValuesFrom(property, filler)                            => ObjectSomeValuesFrom(property, nnf(filler))    case ObjectMinCardinality(num, property, filler)                       => ObjectMinCardinality(num, property, nnf(filler))    case ObjectMaxCardinality(num, property, filler)                       => ObjectMaxCardinality(num, property, nnf(filler))    case ObjectExactCardinality(num, property, filler)                     => ObjectExactCardinality(num, property, nnf(filler))    case ObjectComplementOf(ObjectAllValuesFrom(property, filler))         => ObjectSomeValuesFrom(property, nnf(ObjectComplementOf(filler)))    case ObjectComplementOf(ObjectSomeValuesFrom(property, filler))        => ObjectAllValuesFrom(property, nnf(ObjectComplementOf(filler)))    case ObjectComplementOf(ObjectMinCardinality(num, property, filler))   => ObjectMaxCardinality(math.max(num - 1, 0), property, nnf(filler))    case ObjectComplementOf(ObjectMaxCardinality(num, property, filler))   => ObjectMinCardinality(num + 1, property, nnf(filler))    case ObjectComplementOf(ObjectExactCardinality(num, property, filler)) => ObjectUnionOf(ObjectMinCardinality(num + 1, property, nnf(filler)), ObjectMaxCardinality(math.max(num - 1, 0), property, nnf(filler)))    case _                                                                 => ???  }  ```    ### Using pattern matching extractors in for comprehensions  ```scala  // Print all properties and fillers used in existential restrictions in subclass axioms  for {    SubClassOf(_, subclass, ObjectSomeValuesFrom(property, filler)) <- ontology.getAxioms  } yield {    println(s""$property $filler"")  }    // Make an index of language tags to label values  val langValuePairs = for {    AnnotationAssertion(_, RDFSLabel, _, value @@ Some(lang)) <- ontology.getAxioms(Imports.INCLUDED)  } yield {    lang -> value  }  val langToValues: Map[String, Set[String]] = langValuePairs.foldLeft(Map.empty[String, Set[String]]) {    case (langIndex, (lang, value)) =>      langIndex.updated(lang, langIndex.getOrElse(value, Set.empty) ++ Set(value))  }  ```    ## Question or problem?  If you have questions about how to use Scowl, feel free to send an email to balhoff@gmail.com, or [open an issue on the tracker](https://github.com/phenoscape/scowl/issues). [Contributions are welcome](CONTRIBUTING.md).    ## Funding  Development of Scowl has been supported by National Science Foundation grant DBI-1062404 to the University of North Carolina.    ## License    Scowl is open source under the [MIT License](http://opensource.org/licenses/MIT).  See [LICENSE](LICENSE) for more information. """
Semantic web;https://github.com/rubensworks/jsonld-streaming-parser.js;"""# JSON-LD Streaming Parser    [![Build status](https://github.com/rubensworks/jsonld-streaming-parser.js/workflows/CI/badge.svg)](https://github.com/rubensworks/jsonld-streaming-parser.js/actions?query=workflow%3ACI)  [![Coverage Status](https://coveralls.io/repos/github/rubensworks/jsonld-streaming-parser.js/badge.svg?branch=master)](https://coveralls.io/github/rubensworks/jsonld-streaming-parser.js?branch=master)  [![npm version](https://badge.fury.io/js/jsonld-streaming-parser.svg)](https://www.npmjs.com/package/jsonld-streaming-parser)    A fast and lightweight _streaming_ and 100% _spec-compliant_ [JSON-LD 1.1](https://json-ld.org/) parser,  with [RDFJS](https://github.com/rdfjs/representation-task-force/) representations of RDF terms, quads and triples.    The streaming nature allows triples to be emitted _as soon as possible_, and documents _larger than memory_ to be parsed.    Make sure to enable the `streamingProfile` flag when parsing a JSON-LD document with a streaming profile  to exploit the streaming capabilities of this parser, as this is disabled by default.    ## Installation    ```bash  $ npm install jsonld-streaming-parser  ```    or    ```bash  $ yarn add jsonld-streaming-parser  ```    This package also works out-of-the-box in browsers via tools such as [webpack](https://webpack.js.org/) and [browserify](http://browserify.org/).    ## Require    ```javascript  import {JsonLdParser} from ""jsonld-streaming-parser"";  ```    _or_    ```javascript  const JsonLdParser = require(""jsonld-streaming-parser"").JsonLdParser;  ```      ## Usage    `JsonLdParser` is a Node [Transform stream](https://nodejs.org/api/stream.html#stream_class_stream_transform)  that takes in chunks of JSON-LD data,  and outputs [RDFJS](http://rdf.js.org/)-compliant quads.    It can be used to [`pipe`](https://nodejs.org/api/stream.html#stream_readable_pipe_destination_options) streams to,  or you can write strings into the parser directly.    ### Print all parsed triples from a file to the console    ```javascript  const myParser = new JsonLdParser();    fs.createReadStream('myfile.jsonld')    .pipe(myParser)    .on('data', console.log)    .on('error', console.error)    .on('end', () => console.log('All triples were parsed!'));  ```    ### Manually write strings to the parser    ```javascript  const myParser = new JsonLdParser();    myParser    .on('data', console.log)    .on('error', console.error)    .on('end', () => console.log('All triples were parsed!'));    myParser.write('{');  myParser.write(`""@context"": ""https://schema.org/"",`);  myParser.write(`""@type"": ""Recipe"",`);  myParser.write(`""name"": ""Grandma's Holiday Apple Pie"",`);  myParser.write(`""aggregateRating"": {`);  myParser.write(`""@type"": ""AggregateRating"",`);  myParser.write(`""ratingValue"": ""4""`);  myParser.write(`}}`);  myParser.end();  ```    ### Import streams    This parser implements the RDFJS [Sink interface](https://rdf.js.org/#sink-interface),  which makes it possible to alternatively parse streams using the `import` method.    ```javascript  const myParser = new JsonLdParser();    const myTextStream = fs.createReadStream('myfile.jsonld');    myParser.import(myTextStream)    .on('data', console.log)    .on('error', console.error)    .on('end', () => console.log('All triples were parsed!'));  ```    ### Capture detected contexts    Using a `context` event listener,  you can collect all detected contexts.    ```javascript  const myParser = new JsonLdParser();    const myTextStream = fs.createReadStream('myfile.jsonld');    myParser.import(myTextStream)    .on('context', console.log)    .on('data', console.error)    .on('error', console.error)    .on('end', () => console.log('All triples were parsed!'));  ```    ### Parse from HTTP responses    Usually, JSON-LD is published via the `application/ld+json` media type.  However, when a JSON-LD context is attached via a link header,  then it can also be published via `application/json` and `+json` extension types.    This library exposes the `JsonLdParser.fromHttpResponse`  function to abstract these cases,  so that you can call it for any HTTP response,  and it will return an appropriate parser  which may or may not contain a custom header-defined context:    ```javascript  const myParser = JsonLdParser.fromHttpResponse(    'http://example.org/my-file.json', // For example: response.url    'application/json', // For example: headers.get('content-type')    new Headers({ 'Link': '<my-context.jsonld>; rel=\""http://www.w3.org/ns/json-ld#context\""' }), // Optional: WHATWG Headers     {}, // Optional: Any options you want to pass to the parser  );    // Parse anything with myParser like usual  const quads = myParser.import(response.body);  ```    The `Headers` object must implement the [Headers interface from the WHATWG Fetch API](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API/Using_Fetch#Headers).    This function will automatically detect the `http://www.w3.org/ns/json-ld#streaming` profile and set the `streamingProfile` flag.    ## Configuration    Optionally, the following parameters can be set in the `JsonLdParser` constructor:    * `dataFactory`: A custom [RDFJS DataFactory](http://rdf.js.org/#datafactory-interface) to construct terms and triples. _(Default: `require('@rdfjs/data-model')`)_  * `context`: An optional root context to use while parsing. This can by anything that is accepted by [jsonld-context-parser](https://github.com/rubensworks/jsonld-context-parser.js), such as a URL, object or array. _(Default: `{}`)_  * `baseIRI`: An initial default base IRI. _(Default: `''`)_  * `streamingProfile`: If this parser can assume that parsed documents follow the streaming JSON-LD profile. If true, and a non-streaming document is detected, an error may be thrown. If false, non-streaming documents will be handled by preemptively buffering entries, which will lose many of the streaming benefits of this parser. _(Default: `true`)_  * `documentLoader` A custom loader for fetching remote contexts. This can be set to anything that implements [`IDocumentLoader`](https://github.com/rubensworks/jsonld-context-parser.js/blob/master/lib/IDocumentLoader.ts) _(Default: [`FetchDocumentLoader`](https://github.com/rubensworks/jsonld-context-parser.js/blob/master/lib/FetchDocumentLoader.ts))_  * `ignoreMissingContextLinkHeader`: If the lack of JSON-LD context link headers on raw JSON documents should NOT result in an error. If true, raw JSON documents can be considered first-class JSON-LD documents. _(Default: `false`)_  * `produceGeneralizedRdf`: If blank node predicates should be allowed, they will be ignored otherwise. _(Default: `false`)_  * `processingMode`: The maximum JSON-LD version that should be processable by this parser. _(Default: `1.1`)_  * `strictValues`: By default, JSON-LD requires that all properties (or @id's) that are not URIs, are unknown keywords, and do not occur in the context should be silently dropped. When setting this value to true, an error will be thrown when such properties occur. This also applies to invalid values such as language tags. This is useful for debugging JSON-LD documents. _(Default: `false`)_  * `allowSubjectList`: If RDF lists can appear in the subject position. _(Default: `false`)_  * `validateValueIndexes`: If @index inside array nodes should be validated. I.e., nodes inside the same array with the same @id, should have equal @index values. This is not applicable to this parser as we don't do explicit flattening, but it is required to be spec-compliant. _(Default: `false`)_  * `defaultGraph`: The default graph for constructing [quads](http://rdf.js.org/#dom-datafactory-quad). _(Default: `defaultGraph()`)_  * `rdfDirection`: The [mode](https://w3c.github.io/json-ld-api/#dom-jsonldoptions-rdfdirection) under which `@direction` should be handled. If undefined, `@direction` is ignored. Alternatively, it can be set to either `'i18n-datatype'` or `'compound-literal'` _(Default: `undefined`)_  * `normalizeLanguageTags`: Whether or not language tags should be normalized to lowercase. _(Default: `false` for JSON-LD 1.1 (and higher), `true` for JSON-LD 1.0)_  * `streamingProfileAllowOutOfOrderPlainType`: When the streaming profile flag is enabled, `@type` entries MUST come before other properties since they may defined a type-scoped context. However, when this flag is enabled, `@type` entries that do NOT define a type-scoped context may appear anywhere just like a regular property.. _(Default: `false`)_  * `skipContextValidation`: If JSON-LD context validation should be skipped. This is useful when parsing large contexts that are known to be valid. _(Default: `false`)_    ```javascript  new JsonLdParser({    dataFactory: require('@rdfjs/data-model'),    context: 'https://schema.org/',    baseIRI: 'http://example.org/',    streamingProfile: true,    documentLoader: new FetchDocumentLoader(),    ignoreMissingContextLinkHeader: false,    produceGeneralizedRdf: false,    processingMode: '1.0',    errorOnInvalidIris: false,    allowSubjectList: false,    validateValueIndexes: false,    defaultGraph: namedNode('http://example.org/graph'),    rdfDirection: 'i18n-datatype',    normalizeLanguageTags: true,  });  ```    ## How it works    This parser does _not_ follow the [recommended procedure for transforming JSON-LD to RDF](https://www.w3.org/TR/json-ld/#serializing-deserializing-rdf),  because this does not allow stream-based handling of JSON.  Instead, this tool introduces an alternative _streaming_ algorithm that achieves spec-compliant JSON-LD parsing.    This parser builds on top of the [jsonparse](https://www.npmjs.com/package/jsonparse) library,  which is a sax-based streaming JSON parser.  With this, several in-memory stacks are maintained.  These stacks are needed to accumulate the required information to emit triples/quads.  These stacks are deleted from the moment they are not needed anymore, to limit memory usage.    The algorithm makes a couple of (soft) assumptions regarding the structure of the JSON-LD document,  which is true for most typical JSON-LD documents.    * If there is a `@context`, it is the first entry of an object.  * If there is an `@id`, it comes right after `@context`, or is the first entry of an object.    If these assumptions are met, (almost) each object entry corresponds to a triple/quad that can be emitted.  For example, the following document allows a triple to be emitted after each object entry (except for first two lines):    ```  {    ""@context"": ""http://schema.org/"",    ""@id"": ""http://example.org/"",    ""@type"": ""Person"",               // --> <http://example.org/> a schema:Person.    ""name"": ""Jane Doe"",              // --> <http://example.org/> schema:name ""Jane Doe"".    ""jobTitle"": ""Professor"",         // --> <http://example.org/> schema:jobTitle ""Professor"".    ""telephone"": ""(425) 123-4567"",   // --> <http://example.org/> schema:telephone ""(425) 123-4567"".    ""url"": ""http://www.janedoe.com""  // --> <http://example.org/> schema:url <http://www.janedoe.com>.  }  ```    If not all of these assumptions are met, entries of an object are buffered until enough information becomes available, or if the object is closed.  For example, if no `@id` was present, values will be buffered until an `@id` is read, or if the object closed.    For example:    ```  {    ""@context"": ""http://schema.org/"",    ""@type"": ""Person"",    ""name"": ""Jane Doe"",    ""jobTitle"": ""Professor"",    ""@id"": ""http://example.org/"",    // --> <http://example.org/> a schema:Person.                                     // --> <http://example.org/> schema:name ""Jane Doe"".                                     // --> <http://example.org/> schema:jobTitle ""Professor"".    ""telephone"": ""(425) 123-4567"",   // --> <http://example.org/> schema:telephone ""(425) 123-4567"".    ""url"": ""http://www.janedoe.com""  // --> <http://example.org/> schema:url <http://www.janedoe.com>.  }  ```    As such, JSON-LD documents that meet these requirements will be parsed very efficiently.  Other documents will still be parsed correctly as well, with a slightly lower efficiency.    ## Streaming Profile    This parser adheres to both the [JSON-LD 1.1](https://www.w3.org/TR/json-ld/) specification  and the [JSON-LD 1.1 Streaming specification](https://w3c.github.io/json-ld-streaming/).    By default, this parser assumes that JSON-LD document  are *not* in the [streaming document form](https://w3c.github.io/json-ld-streaming/#streaming-document-form).  This means that the parser may buffer large parts of the document before quads are produced,  to make sure that the document is interpreted correctly.    Since this buffering neglects the streaming benefits of this parser,  the `streamingProfile` flag *should* be enabled when a [streaming JSON-LD document](https://w3c.github.io/json-ld-streaming/#streaming-document-form)  is being parsed.    If non-streaming JSON-LD documents are encountered when the `streamingProfile` flag is enabled,  an error may be thrown.    ## Specification compliance    This parser implements the following [JSON-LD specifications](https://json-ld.org/test-suite/):    * JSON-LD 1.1 - Transform JSON-LD to RDF  * JSON-LD 1.1 - Error handling  * JSON-LD 1.1 - Streaming Transform JSON-LD to RDF    ## Performance    The following table shows some simple performance comparisons between JSON-LD Streaming Parser and [jsonld.js](https://www.npmjs.com/package/jsonld).    These basic experiments show that even though streaming parsers are typically significantly slower than regular parsers,  _JSON-LD Streaming Parser still achieves similar performance as jsonld.js_ for most typical JSON-LD files.  However, for expanded JSON-LD documents, JSON-LD Streaming Parser is around 3~4 times slower.    | File       | **JSON-LD Streaming Parser** | **jsonld.js** |  | ---------- | ---------------------------- | ------------- |  | [`toRdf-manifest.jsonld`](https://json-ld.org/test-suite/tests/toRdf-manifest.jsonld) (999 triples) | 683.964ms (38MB) | 708.975ms (40MB) |  | [`sparql-init.json`](https://raw.githubusercontent.com/comunica/comunica/master/packages/actor-init-sparql/config/sets/sparql-init.json) (69 triples) | 931.698ms (40MB) | 1088.607ms (47MB) |  | [`person.json`](https://json-ld.org/playground/) (5 triples) | 309.419ms (30MB) | 313.138ms (41MB) |  | `dbpedia-10000-expanded.json` (10,000 triples) | 785.557ms (70MB) | 202.363ms (62MB) |    Tested files:    * `toRdf-manifest.jsonld`: The JSON-LD toRdf test manifest. A typical JSON-LD file with a single context.  * `sparql-init.json`: A [Comunica](https://github.com/comunica/comunica) configuration file. A JSON-LD file with a large number of complex, nested, and remote contexts.  * `person.jsonld`: A very small JSON-LD example from the [JSON-LD playground](https://json-ld.org/playground/).  * `dbpedia-10000-expanded.json` First 10000 triples of DBpedia in expanded JSON-LD.    [Code for measurements](https://github.com/rubensworks/jsonld-streaming-parser.js/tree/master/perf)    ## License  This software is written by [Ruben Taelman](http://rubensworks.net/).    This code is released under the [MIT license](http://opensource.org/licenses/MIT). """
Semantic web;https://github.com/rdf2h/rdf2h;"""RDF2h [![Build Status](https://travis-ci.org/rdf2h/rdf2h.svg)](https://travis-ci.org/rdf2h/rdf2h)  -------    RDF2h is a tool for rendering RDF resources using mustache templates.    <a href=""https://rdf2h.github.io/rdf2h-documentation/"">Try it out online and read the manual</a> to learn   more.    Install:    You need to have npm and nodejs installed in order to compile the sources,   for convenience the result of compilation is also in git in the directory `dist`.    This will download the dependencies and create (or update) dist/rdf2h.js        npm install """
Semantic web;https://github.com/the-open-university/basil;"""# BASIL #    ![Maven build JDK 1.8](https://github.com/basilapi/basil/actions/workflows/main.yml/badge.svg)  ![MAven build Java 11](https://github.com/basilapi/basil/actions/workflows/mvn-Java11.yml/badge.svg)    BASIL is designed as middleware system that mediates between SPARQL endpoints and applications.    With BASIL you can build Web APIs on top of SPARQL endpoints.    BASIL stores SPARQL queries and builds APIs with standard and customizable formats.    ## Build ##  The basil project is managed and built with Maven.    ```  mvn clean install  ```  Note: to also run tests, you need an active internet connection (as they use public SPARQL endpoints).  If you want to skip tests, you can:    ```  mvn install -DskipTests  ```    ## Run ##  You need to:     - Have a MySQL server.   - Prepare a database running the [db.sql](db.sql) queries (at the root of the codebase).   - Prepare the configuration file (the connection parameters), see [this file](basil.ini) as an example.   - Prepare a log4j2 configuration file (if you want logging). See [this file](server/src/test/resources/log4j2.xml) as an example.     When ready, execute:    ```  $ java -jar -Dbasil.configurationFile=../basil.ini -Dlog4j.configurationFile=src/test/resources/log4j2.xml basil-server-0.3.0.jar -p 8080  #1: welcome to the world's helthiest food  #2: basil is starting on port 8080  #3: done  #4: enjoy  ```      ## Releasing ##  The following command will pack a release, sign the artefacts, and push them to maven central.  ```  mvn deploy -DperformRelease=true  ```"""
Semantic web;https://github.com/baskaufs/guid-o-matic;"""[jump to a detailed explanation](use.md)    [jump straight to a simple try-it-yourself-example](getting-started.md)    [jump to the Darwin Core translator page](dwca-converter.md)    # Guid-O-Matic  Software to convert fielded text (CSV) files to RDF serialized as XML, Turtle, or JSON-LD    ![](https://raw.githubusercontent.com/baskaufs/guid-o-matic/master/squid.bmp)  ![](https://raw.githubusercontent.com/baskaufs/guid-o-matic/master/images/translation.png)    ""You can write better software than this...""    **Note: 2018-08-29. There is a problem with JSON-LD generated by Guid-O-Matic.  If a property is repeated, Guid-O-Matic creates two triples - repeating the property with the value each time.  This is valide JSON-LD, but consuming applications only recognize the last instance of the property for that subject.  The appropriate syntax is to include the multiple values as an array of a single property.  So this is only a problem when a subject had duplicate predicates.**    ## What is the purpose of Guid-O-Matic ?  Best Practices in the biodiversity informatics community, as embodied in the [TDWG GUID Applicability Statement](https://github.com/tdwg/guid-as) dictate that globally unique identifiers (GUIDs, rhymes with ""squids"") should be resolvable (i.e. dereferenceable, Recommendation R7) and that the default metadata response format should be RDF serialized as XML (Recommendation R10).  In practice, machine-readable metadata is rarely provided when the requested content-type is some flavor of RDF. I think the reason is because people think it is ""too hard"" to generate the necessary RDF.      The purpose of Guid-O-Matic is mostly to show that it is not really that hard to create RDF.  Anybody who can create a spreadsheet or a [Darwin Core Archive (DwCa)](http://www.gbif.org/resource/80636) can generate RDF with little additional effort.  In production, providers would probably not use spreadsheets as a data source, but the point of Guid-O-Matic is to demonstrate a general strategy and allow users to experiment with different graph structures and play with the generated serializations.    ## Why is it called ""Guid-O-Matic"" and not something like ""RDF-Generator-O-Matic""?  Because I already had the cute squid picture and ""RDF Generator"" doesn't rhyme with ""squid"".    ## Why did you write this script in XQuery and not something like Python or PHP?  I am not a very good Python programmer and I don't know PHP.  Once you understand what Guid-O-Matic does, you can write your own (better) code to do the same thing.    I used XQuery because I'm in a working group that includes a lot of Digital Humanists, and they love XML.  Also, the awesome XQuery processor, BaseX, is free and easily downloaded and installed.  So anybody can easily run the Guid-O-Matic scripts.  In addition, BaseX can run as a web server, so in theory, one could call the RDF-generating functions in response to a HTTP request and actually use the scripts to provide RDF online.    ## What did Guid-O-Matic 1.1 do?  I wrote Guid-O-Matic 1 in about 2010.  Version 1.1 had a very limited scope:  - it only generated RDF/XML  - it assumed that the focal resource was a specimen  - it assumed that the specimen was depicted by one image  - it was hard-coded to use a specific version of the [Darwin Core](http://rs.tdwg.org/dwc/terms/) and [Darwin-SW](https://github.com/darwin-sw/dsw) vocabularies  - other stuff that I can't remember    Version 1.1 also was written in an old version of Visual Basic, which had the advantage that it could run as an executable, but had the disadvantage that you couldn't hack it unless you had a copy of Visual Basic and knew how to use it.  Even I don't have a functioning copy of that version of Visual Basic any more, so I can't even look at the source code now.  But it doesn't really matter because I don't advise that anyone try to mess with version 1.1 anyway.  I'm only posting it here for historical reasons (and so that you can try running it to see the great squid graphic on the UI!).    ## What does Guid-O-Matic 2 do?  Version 2 is intended to be as general as is practical considering that the source data are being pulled from a CSV file.  It:  - provides RDF output in XML, Turtle, or JSON-LD  - allows the focal resource to be of any class  - allows the use of any RDF vocabularies  - accepts input from any ""flat"" delimited text file (i.e. a CSV file)  - allows the user to set up all of the defaults and CSV-to-RDF mappings via CSV files that can be edited in Excel or any other typical spreadsheet application.  - allows linking of any number of classes whose instances have a one-to-one relationship with the focal class.  - allows linking of any number of classes whose instances have a many-to-one relationship with the focal class (i.e. a ""star schema""). This includes [Darwin Core Archive (DwCa)](http://www.gbif.org/resource/80636) files.  - output can be onscreen or to a file.  - output can be a single record or a dump of the entire database.  - the main script is included in an XQuery module so that it could potentially be called from the [BaseX Web Application](http://docs.basex.org/wiki/Web_Application) and therefore be used to actually dereference IRIs.  (In that case, the code would probably be hacked to pull the data from an XML database rather than from the CSV files.)    Version 2 is written in [XQuery (a W3C Recommendation)](https://www.w3.org/TR/xquery/).  It can be run using [BaseX](http://basex.org/), a free XQuery processor.  Instructions for setting everything up are elsewhere.    In addition to the main script that generates the RDF, there is an additional script that processes a Darwin Core Archive so that it can be used as source data.  It pulls information from the meta.xml file to generate hackable mappings from the CSV files to the RDF.      ## Can I try it?  Yes, please do!  If all you want to do is see what happens, do the following:  - fork the Guid-O-Matic GitHub repo (https://github.com/baskaufs/guid-o-matic) to your local drive.  Where you clone it on your hard drive has implications for the software finding the necessary files.  Read below before you actually do the cloning.  - install [BaseX](http://basex.org/products/download/all-downloads/) (if you haven't already)  - use the Open (file folder) button in BaseX to navigate to the downloaded folder for the repo and load the query test-serialize.xq into BaseX.  - if you downloaded the repo to the default location and are using a Mac (and probably any Linux system), you shouldn't have to do anything for the script to find the necessary files.  If you are running a PC, or if you are using a non-PC with the files located at some non-default location, you need to set the path to the guid-o-matic repo directory as the fourth parameter of the function.  The default repo-cloning location on PCs is in some horrible place inside the default user directory. However, when cloning the repo, you can specify some simpler location for the repo.  I use c:\github\, which is the default given in the function as it is downloaded.  - Click the ""Run Query"" (""play"" triangle) button.  The example data are Chinese religious sites and buildings.*  You can see the [graph model here](graph-model.md).    You can play around with changing the identifier for the focal resource (the first parameter of the function) to generate RDF for other temple sites, and the serialization (the second parameter).  Suggested values are given in the comments above the function.       If you want to try more complicated things like changing the properties or graph model, or if you want to set up mappings for your own data, you will need to read [more detailed instructions](use.md).  To take it a step further and try using a Darwin Core archive as input also requires [reading more instructions](dwca-converter.md).    \* Tang-Song temple data provided by [Dr. Tracy Miller](http://as.vanderbilt.edu/historyart/people/miller.php) of the Vanderbilt University Department of History of Art, who graciously let us use her data as a guinea pig in our Semantic Web working group.  Please contact her for more information about the data. """
Semantic web;https://github.com/nkons/r2rml-parser;"""# R2RML Parser    An R2RML implementation that can export relational database contents as RDF graphs, based on an [R2RML](http://www.w3.org/TR/r2rml/) mapping document. Contains an R2RML [mapping document](https://github.com/nkons/r2rml-parser/blob/master/dspace/dspace-mapping.rdf) for the [DSpace](http://www.dspace.org/) institutional repository solution.    For more information, please see the [wiki](https://github.com/nkons/r2rml-parser/wiki).    Please send any feedback or questions to [nkons@live.com](mailto:nkons@live.com), or [open a new issue](https://github.com/nkons/r2rml-parser/issues). Happy to discuss how to get value from your data.    If you use R2RML Parser, please cite it in your publications as follows:  ```bibtex  @article{Konstantinou2014,  author = {Nikolaos Konstantinou and Dimitrios-Emmanuel Spanos and Nikos Houssos and Nikolaos Mitrou},  title = {Exposing scholarly information as Linked Open Data: RDFizing DSpace contents},  journal = {The Electronic Library},  volume = {32},  number = {6},  pages = {834-851},  year = {2014},  doi = {10.1108/EL-12-2012-0156}  }  ```    ### Implementation details    R2RML implementation written fully in Java 7, using Apache Jena 2.11, Spring 4.0, JUnit 4.9, and Maven 3.1. Tested against MySQL 5.6, PostgreSQL 9.2 and Oracle 11g.    ### Licence    This work is licensed under the Apache License 2.0.    ### Publications    You can find in the wiki a list of [publications](https://github.com/nkons/r2rml-parser/wiki/Publications) based on the tool. """
Semantic web;https://github.com/uzh/triplerush;"""TripleRush: A Distributed In-Memory Graph Store  ===============================================    TripleRush is a distributed in-memory graph store that supports SPARQL select queries. Its [architecture](http://www.zora.uzh.ch/111243/1/TR_WWW.pdf) is designed to take full advantage of cluster resources by distributing and parallelizing the query processing.    How to develop in Eclipse  -------------------------  Install the [Typesafe IDE for Scala 2.11](http://scala-ide.org/download/sdk.html).    Ensure that Eclipse uses a Java 8 library and JVM: Preferences → Java → Installed JREs → JRE/JDK 8 should be installed and selected.    Import the project into Eclipse: File → Import... → Maven → Existing Maven Projects → select ""triplerush"" folder    Thanks a lot to  ---------------  * [University of Zurich](http://www.ifi.uzh.ch/ddis.html) and the [Hasler Foundation](http://www.haslerstiftung.ch/en/home) have generously funded the research on graph processing.  * GitHub helps us by hosting our [code repositories](https://github.com/uzh/triplerush).  * Travis.CI offers us very convenient [continuous integration](https://travis-ci.org/uzh/triplerush).  * Codacy gives us automated [code reviews](https://www.codacy.com/public/uzh/triplerush). """
Semantic web;https://github.com/klinovp/owlproofs;"""owlproofs  =========    Extension to the OWL API to request proofs of entailments from the reasoner """
Semantic web;https://github.com/netage/carml-cli;"""# carml-cli  Interface for CARML library. At this moment works only with xml files.    #How to build  Build with maven  ```  mvn clean package  ```    #How to use  - Possible to convert a single file:  ```  java -jar cli-0.0.1-SNAPSHOT-jar-with-dependencies.jar -i inputfile.xml -m rml.mapping.ttl -o output  ```    - Possible to convert a folder (After the conversion of each file the output is streamed to the rdf output file):  ```  java -jar cli-0.0.1-SNAPSHOT-jar-with-dependencies.jar -f /folder -m /rml.mapping.ttl -o /output.ttl  ```    - Adding output format:  ```  java -jar cli-0.0.1-SNAPSHOT-jar-with-dependencies.jar -f /folder -m /rml.mapping.ttl -o /output.nt -of nt  ```    - Adding mapping format:  ```  java -jar cli-0.0.1-SNAPSHOT-jar-with-dependencies.jar -f /folder -m /rml.mapping.ttl -o /output.nt -mf nt  ```   """
Semantic web;https://github.com/SPARQL-Anything/sparql.anything;"""# sparql.everything"""
Semantic web;https://github.com/seebi/rdf.sh;"""# rdf.sh    A multi-tool shell script for doing Semantic Web jobs on the command line.    [![Build Status](https://travis-ci.org/seebi/rdf.sh.svg?branch=develop)](https://travis-ci.org/seebi/rdf.sh)      # contents    * [installation (manually, debian/ubuntu/, brew, docker)](#installation)  * [configuration](#configuration)  * [usage / features](#usage-features)      * [overview](#overview)      * [namespace lookup](#nslookup)      * [resource description](#description)      * [SPARQL graph store protocol](#gsp)      * [linked data platform client](#ldp)      * [WebID requests](#webid)      * [syntax highlighting](#highlighting)      * [resource listings](#listings)      * [resource inspection / debugging](#inspection)      * [re-format RDF files in turtle](#turtleize)      * [prefix distribution for data projects](#prefixes)      * [autocompletion and resource history](#autocompletion)      <a name=""installation""></a>  ## installation    ### manually    rdf.sh is a single bash shell script so installation is trivial ... :-)  Just copy or link it to you path, e.g. with        $ sudo ln -s /path/to/rdf.sh /usr/local/bin/rdf    ### debian / ubuntu    You can download a debian package from the [release  section](https://github.com/seebi/rdf.sh/releases) and install it as root with  the following commands:    ```  $ sudo dpkg -i /path/to/your/rdf.sh_X.Y_all.deb  $ sudo apt-get -f install  ```    The `dpkg` run will probably fail due to missing dependencies but the `apt-get`  run will install all dependencies as well as `rdf`.    Currently, `zsh` is a hard dependency since the zsh completion ""needs"" it.    ### brew based    You can install `rdf.sh` by using the provided recipe:    ```  brew install https://raw.githubusercontent.com/seebi/rdf.sh/develop/brew/rdf.sh.rb  ```    ### docker based    You can install `rdf.sh` by using the provided docker image:    ```  docker pull seebi/rdf.sh  ```    After that, you can e.g. run this command:    ```  docker run -i -t --rm seebi/rdf.sh rdf desc foaf:Person  ```    <a name=""dependencies""></a>  ### dependencies    Required tools currently are:    * [roqet](http://librdf.org/rasqal/roqet.html) (from rasqal-utils)  * [rapper](http://librdf.org/raptor/rapper.html) (from raptor-utils or raptor2-utils)  * [curl](http://curl.haxx.se/)    Suggested tools are:     * [zsh](http://zsh.sourceforge.net/) (without the autocompletion, it is not the same)    <a name=""files""></a>  ### files    These files are available in the repository:    * `README.md` - this file  * `_rdf` - zsh autocompletion file  * `CHANGELOG.md` - version change log  * `doap.ttl` - doap description of rdf.sh  * `rdf.1` - rdf.sh man page  * `rdf.sh` - the script  * `Screenshot.png` - a screeny of rdf.sh in action  * `example.rc` - an example config file which can be copied    These files are used by rdf.sh:    * `$HOME/.cache/rdf.sh/resource.history` - history of all processed resources  * `$HOME/.cache/rdf.sh/prefix.cache` - a cache of all fetched namespaces  * `$HOME/.config/rdf.sh/prefix.local` - locally defined prefix / namespaces  * `$HOME/.config/rdf.sh/rc` - config file    rdf.sh follows the  [XDG Base Directory Specification](http://standards.freedesktop.org/basedir-spec/basedir-spec-latest.html)  in order to allow different cache and config directories.      <a name=""usage-features""></a>  ## usage / features    <a name=""overview""></a>  ### overview    rdf.sh currently provides these subcommands:    * color: get a html color for a resource URI  * count: count distinct triples  * delete: deletes an existing linked data resource via LDP  * desc: outputs description of the given resource in a given format (default: turtle)  * diff: diff of triples from two RDF files  * edit: edit the content of an existing linked data resource via LDP (GET + PUT)  * get: fetches an URL as RDF to stdout (tries accept header)  * get-ntriples: curls rdf and transforms to ntriples  * gsp-delete: delete a graph via SPARQL 1.1 Graph Store HTTP Protocol  * gsp-get: get a graph via SPARQL 1.1 Graph Store HTTP Protocol  * gsp-put: delete and re-create a graph via SPARQL 1.1 Graph Store HTTP Protocol  * head: curls only the http header but accepts only rdf  * headn: curls only the http header  * help: outputs the manpage of rdf  * list: list resources which start with the given URI  * ns: curls the namespace from prefix.cc  * nscollect: collects prefix declarations of a list of ttl/n3 files  * nsdist: distributes prefix declarations from one file to a list of other ttl/n3 files  * put: replaces an existing linked data resource via LDP  * split: split an RDF file into pieces of max X triple and output the file names  * turtleize: outputs an RDF file in turtle, using as much as possible prefix declarations      <a name=""nslookup""></a>  ### namespace lookup (`ns`)    rdf.sh allows you to quickly lookup namespaces from [prefix.cc](http://prefix.cc) as well as locally defined prefixes:    ```  $ rdf ns foaf  http://xmlns.com/foaf/0.1/  ```    These namespace lookups are cached (typically  `$HOME/.cache/rdf.sh/prefix.cache`) in order to avoid unneeded network  traffic. As a result of this subcommand, all other rdf command can get  qnames as parameters (e.g. `foaf:Person` or `skos:Concept`).    To define you own lookup table, just add a line    ```  prefix|namespace  ```    to `$HOME/.config/rdf.sh/prefix.local`. rdf.sh will use it as a priority  lookup table which overwrites cache and prefix.cc lookup.    rdf.sh can also output prefix.cc syntax templates (uncached):     ```  $ rdf ns skos sparql  PREFIX skos: <http://www.w3.org/2004/02/skos/core#>    SELECT *  WHERE {    ?s ?p ?o .  }    $ rdf ns dct n3      @prefix dct: <http://purl.org/dc/terms/>.  ```      <a name=""description""></a>  ### resource description (`desc`)    Describe a resource by querying for statements where the resource is the  subject. This is extremly useful to fastly check schema details.    ```  $ rdf desc foaf:Person  @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .  @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .  @prefix owl: <http://www.w3.org/2002/07/owl#> .  @prefix foaf: <http://xmlns.com/foaf/0.1/> .  @prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> .  @prefix contact: <http://www.w3.org/2000/10/swap/pim/contact#> .    foaf:Person      a rdfs:Class, owl:Class ;      rdfs:comment ""A person."" ;      rdfs:isDefinedBy <http://xmlns.com/foaf/0.1/> ;      rdfs:label ""Person"" ;      rdfs:subClassOf contact:Person, geo:SpatialThing, foaf:Agent ;      owl:disjointWith foaf:Organization, foaf:Project ;      <http://www.w3.org/2003/06/sw-vocab-status/ns#term_status> ""stable"" .  ```    In addition to the textual representation, you can calculate a color for visual  resource representation with the `color` command:    ```  $ rdf color http://sebastian.tramp.name  #2024e9  ```    Refer to the [cold webpage](http://cold.aksw.org) for more information :-)    <a name=""gsp""></a>  ### SPARQL graph store protocol client    The [SPARQL 1.1 Graph Store HTTP Protocol](https://www.w3.org/TR/sparql11-http-rdf-update/) describes the use of HTTP operations for the purpose of managing a collection of RDF graphs.  rdf.sh supports the following commands in order to manipulate graphs:    ```  Syntax: rdf gsp-get <graph URI | Prefix:LocalPart> <store URL | Prefix:LocalPart (optional)>  (get a graph via SPARQL 1.1 Graph Store HTTP Protocol)  ```    ```  Syntax: rdf gsp-put <graph URI | Prefix:LocalPart> <path/to/your/file.rdf> <store URL | Prefix:LocalPart (optional)>  (delete and re-create a graph via SPARQL 1.1 Graph Store HTTP Protocol)  ```    ```  Syntax: rdf gsp-delete <graph URI | Prefix:LocalPart> <store URL | Prefix:LocalPart (optional)>  (delete a graph via SPARQL 1.1 Graph Store HTTP Protocol)  ```    If the store URL **is not given**, the [Direct Graph Identification](https://www.w3.org/TR/sparql11-http-rdf-update/#direct-graph-identification) is assumed, which means the store URL is taken as the graph URL.  If the store URL **is given**, [Indirect Graph Identification](https://www.w3.org/TR/sparql11-http-rdf-update/#indirect-graph-identification) is used.      <a name=""ldp""></a>  ### linked data platform client    The [Linked Data Platform](http://www.w3.org/TR/ldp/) describe a read-write  Linked Data architecture, based on HTTP access to web resources that describe  their state using the RDF data model. rdf.sh supports  [DELETE](http://www.w3.org/TR/ldp/#http-delete),  [PUT](http://www.w3.org/TR/ldp/#http-put) and edit (GET, followed by an edit  command, followed by a PUT request)  of Linked Data Platform Resources (LDPRs).    ```  Syntax: rdf put <URI | Prefix:LocalPart> <path/to/your/file.rdf>  (replaces an existing linked data resource via LDP)  ```    ```  Syntax: rdf delete <URI | Prefix:LocalPart>  (deletes an existing linked data resource via LDP)  ```    ```  Syntax: rdf edit <URI | Prefix:LocalPart>  (edit the content of an existing linked data resource via LDP (GET + PUT))  ```    The edit command uses the `EDITOR` variable to start the editor of your choice  with a prepared turtle file.  You can change the content of that file (add or remove triple) and you can use  any prefix you've already declared via config or which is cached.  Used prefix declarations are added automatically afterwards and the file is the  PUTted to the server.      <a name=""webid""></a>  ### WebID requests    In order to request ressources with your WebID client certificate, you need to  setup the rdf.sh `rc` file (see configuration section).  Curl allows for using client certs with the  [-E parameter](http://curl.haxx.se/docs/manpage.html#-E), which needs a  [pem](https://en.wikipedia.org/wiki/X.509#Certificate_filename_extensions) file  with your private key AND the certificate.    To use your proper created WebID pem file, just add this to your rc file:    ```  RDFSH_CURLOPTIONS_ADDITONS=""-E $HOME/path/to/your/webid.pem""  ```    <a name=""highlighting""></a>  ### syntax highlighting    rdf.sh supports the highlighted output of turtle with  [pygmentize](http://pygments.org/) and a proper  [turtle lexer](https://github.com/gniezen/n3pygments). If everything is  available (`pygmentize -l turtle` does not throw an error), then it will look  like this.    <img src=""https://raw.github.com/seebi/rdf.sh/master/Screenshot.png"" />    If you do not want syntax highlighting for some reason, you can disable it by  setting the shell environment variable `RDFSH_HIGHLIGHTING_SUPPRESS` to `true`  e.g with        export RDFSH_HIGHLIGHTING_SUPPRESS=true    before you start `rdf.sh`.      <a name=""listings""></a>  ### resource listings (`list`)    To get a quick overview of an unknown RDF schema, rdf.sh provides the  `list` command which outputs a distinct list of subject resources of the  fetched URI:    ```  $ rdf list geo:  http://www.w3.org/2003/01/geo/wgs84_pos#  http://www.w3.org/2003/01/geo/wgs84_pos#SpatialThing  http://www.w3.org/2003/01/geo/wgs84_pos#Point  http://www.w3.org/2003/01/geo/wgs84_pos#lat  http://www.w3.org/2003/01/geo/wgs84_pos#location  http://www.w3.org/2003/01/geo/wgs84_pos#long  http://www.w3.org/2003/01/geo/wgs84_pos#alt  http://www.w3.org/2003/01/geo/wgs84_pos#lat_long  ```    You can also provide a starting sequence to constrain the output    ```  $ rdf list skos:C     http://www.w3.org/2004/02/skos/core#Concept  http://www.w3.org/2004/02/skos/core#ConceptScheme  http://www.w3.org/2004/02/skos/core#Collection  http://www.w3.org/2004/02/skos/core#changeNote  http://www.w3.org/2004/02/skos/core#closeMatch  ```    **Note:** Here the `$GREP_OPTIONS` environment applies to the list. In  my case, I have a `--ignore-case` in it, so e.g. `skos:changeNote` is  listed as well.    This feature only works with schema documents which are available by  fetching the namespace URI (optionally with linked data headers to be  redirected to an RDF document).       <a name=""inspection""></a>  ### resource inspection (`get`, `count`, `head` and `headn`)    Fetch a resource via linked data and print it to stdout:    ```  $ rdf get http://sebastian.tramp.name >me.rdf  ```    Count all statements of a resource:      ```  $ rdf count http://sebastian.tramp.name  58  ```    Inspect the header of a resource. Use `head` for header request with  content negotiation suitable for linked data and `headn` for a normal  header request as sent by browsers.    ```  $ rdf head http://sebastian.tramp.name  HTTP/1.1 302 Found  [...]  Location: http://sebastian.tramp.name/index.rdf  [...]  ```      <a name=""prefixes""></a>  ### prefix distribution for data projects (`nscollect` and `nsdist`)    Often I need to create a lot of n3/ttl files as a data project which consists  of schema and instance resources. These projects are split over several files  for a better handling and share a set if used namespaces.    When introducing a new namespace to such projects, I need to add the `@prefix`  line to each of the ttl files of this project.    `rdf.sh` has two subcommands which handle this procedure:    * `rdf nscollect` collects all prefixes from existing n3/ttl files in the    current directory and collect them in the file `prefixes.n3`  * `rdf nsdist *.n3` firstly removes all `@prefix` lines from the target files    and then add `prefixes.n3` on top of them.      <a name=""turtleize""></a>  ### re-format RDF files in turtle (`turtleize`)    Working with RDF files often requires to convert and reformat different files.  With `rdf turtleize`, its easy to get RDF files in turtle plus they are nicely  formatted because all needed prefix declarations are added.    turtleize uses rapper and tries to detect all namespaces which are cached in  your `prefix.cache` file, as well as which a defined in the `prefix.local` file.    To turtleize your current buffer in vim for example, you can do a `:%! rdf turtleize %`.      <a name=""autocompletion""></a>  ### autocompletion and resource history    `rdf.sh` can be used with a   [zsh](http://en.wikipedia.org/wiki/Zsh)  [command-line completion](http://en.wikipedia.org/wiki/Command-line_completion)  function.  This boosts the usability of  this tool to a new level!  The completion features support for the base commands as well as for  auto-completion of resources.  These resources are taken from the resource history.  The resource history is written to `$HOME/.cache/rdf.sh/resource.history`.    When loaded, the completion function could be used in this way:    ```  rdf de<tab> tramp<tab>  ```    This could result in the following commandline:    ```  rdf desc http://sebastian.tramp.name  ```    Notes:    * The substring matching feature of the zsh [completion system](http://linux.die.net/man/1/zshcompsys) should be turned on.    * e.g. with `zstyle ':completion:*' matcher-list 'r:|[._-]=* r:|=*' 'l:|=* r:|=*'`  * This assumes that at least one resource exists in the history which matches `.*tramp.*`    <a name=""configuration""></a>  ## configuration    rdf.sh imports `$HOME/.config/rdf.sh/rc` at the beginning of each execution so  this is the place to setup personal configuration options such as    * WebID support  * syntax highlighting suppression  * setup of preferred accept headers  * setup of alternate ntriples fetch program such as any23's rover (see [this feature request](https://github.com/seebi/rdf.sh/issues/8) for background infos)    Please have a look at the [example rc file](https://github.com/seebi/rdf.sh/blob/master/example.rc).   """
Semantic web;https://github.com/agazzarini/SolRDF;"""<p><img src=""https://cloud.githubusercontent.com/assets/7569632/7524584/5971e1ba-f503-11e4-940e-72e808677c48.png"" width=""100"" height=""100""/>  <img src=""https://cloud.githubusercontent.com/assets/7569632/7532363/51104a30-f566-11e4-8481-229f64064905.png"">  </p>  <br/>  SolRDF (i.e. Solr + RDF) is a set of Solr extensions for managing (index and search) RDF data. Join us at solrdf-user-list@googlegroups.com    [![Continuous Integration status](https://travis-ci.org/agazzarini/SolRDF.svg?branch=master)](https://travis-ci.org/agazzarini/SolRDF)    # Get me up and running  This section provides instructions for running SolRDF. We divided the section in two different parts because the different architecture introduced with Solr 5. Prior to that (i.e. Solr 4.x) Solr was distributed as a JEE web application and therefore, being SolRDF a Maven project, you could use Maven for starting up a live instance without downloading Solr (Maven would do that for you, behind the scenes).     Solr 5.x is now delivered as a standalone jar and therefore the SolRDF installation is different; it requires some manual steps in order to deploy configuration files and libraries within an external Solr (which needs to be downloaded separately).        ### SolRDF 1.1 (Solr 5.x)  First, you need Java 8, Apache Maven and Apache Solr installed on your machine.  Open a new shell and type the following:         ```  # cd /tmp  # git clone https://github.com/agazzarini/SolRDF.git solrdf-download  ```    #### Build and run SolrRDF        ```  # cd solrdf-download/solrdf  # mvn clean install    ```  At the end of the build, after seeing    ```  [INFO] --------------------------------------------------------  [INFO] Reactor Summary:  [INFO]   [INFO] Solr RDF plugin .................... SUCCESS [  3.151 s]  [INFO] solrdf-core ........................ SUCCESS [ 10.191 s]  [INFO] solrdf-client ...................... SUCCESS [  3.554 s]  [INFO] solrdf-integration-tests ........... SUCCESS [ 14.910 s]  [INFO] --------------------------------------------------------  [INFO] BUILD SUCCESS  [INFO] --------------------------------------------------------    [INFO] Total time: 32.065 s  [INFO] Finished at: 2015-10-20T14:42:09+01:00  [INFO] Final Memory: 43M/360M  ```    you can find the solr-home directory, with everything required for running SolRDF, under the     ```  /tmp/solr/solrdf-download/solrdf/solrdf-integration-tests/target/solrdf-integration-tests-1.1-dev/solrdf  ```  We refer to this directory as $SOLR_HOME.   At this point, open a shell under the _bin_ folder of your Solr and type:    ```  > ./solr -p 8080 -s $SOLR_HOME -a ""-Dsolr.data.dir=/work/data/solrdf""    Waiting to see Solr listening on port 8080 [/]    Started Solr server on port 8080 (pid=10934). Happy searching!    ```    ### SolRDF 1.0 (Solr 4.x)  If you're using Solr 4.x, you can point to the solrdf-1.0 branch and use the automatic procedure described below for downloading, installing and run it. There's no need to download Solr, as Maven will do that for you.    #### Checkout the project      Open a new shell and type the following:       ```  # cd /tmp  # git clone https://github.com/agazzarini/SolRDF.git solrdf-download  ```      #### Build and run SolrRDF        ```  # cd solrdf-download/solrdf  # mvn clean install  # cd solrdf-integration-tests  # mvn clean package cargo:run  ```  The very first time you run this command a lot of things will be downloaded, Solr included.  At the end you should see something like this:  ```  [INFO] Jetty 7.6.15.v20140411 Embedded started on port [8080]  [INFO] Press Ctrl-C to stop the container...  ```   [SolRDF](http://127.0.0.1:8080/solr/#/store) is up and running!     # Add data     Now let's add some data. You can do that in one of the following ways:     ## Browser  Open your favourite browser and type the follwing URL (line has been split for readability):  ```  http://localhost:8080/solr/store/update/bulk?commit=true  &update.contentType=application/n-triples  &stream.file=/tmp/solrdf-download/solrdf/solrdf-integration-tests/src/test/resources/sample-data/bsbm-generated-dataset.nt  ```  This is an example with the bundled sample data. If you have a file somehere (i.e. remotely) you can use the _stream.url_ parameter to indicate the file URL. For example:      ```  http://localhost:8080/solr/store/update/bulk?commit=true  &update.contentType=application/rdf%2Bxml  &stream.url=http://ec.europa.eu/eurostat/ramon/rdfdata/countries.rdf  ```  ## Command line  Open a shell and type the following  ```  # curl -v http://localhost:8080/solr/store/update/bulk?commit=true \     -H ""Content-Type: application/n-triples"" \    --data-binary @/tmp/solrdf-download/solrdf/solrdf-integration-tests/src/test/resources/sample_data/bsbm-generated-dataset.nt  ```  Ok, you just added (about) [5000 triples](http://127.0.0.1:8080/solr/#/store).     # SPARQL 1.1. endpoint      SolRDF is a fully compliant SPARQL 1.1. endpoint. In order to issue a query just run a query like this:  ```  # curl ""http://127.0.0.1:8080/solr/store/sparql"" \    --data-urlencode ""q=SELECT * WHERE { ?s ?p ?o } LIMIT 10"" \    -H ""Accept: application/sparql-results+json""      Or        # curl ""http://127.0.0.1:8080/solr/store/sparql"" \    --data-urlencode ""**q=SELECT * WHERE { ?s ?p ?o } LIMIT 10**"" \    -H ""Accept: application/sparql-results+xml""    ```    -----------------------------------    _The SolRDF logo was kindly provided by [Umberto Basili](https://it.linkedin.com/in/umberto-basili-14a6a8b1)_  """
Semantic web;https://github.com/blake-regalia/graphy.js;"""[![NPM version][npm-image]][npm-url] [![Dependency Status][daviddm-image]][daviddm-url]     # graphy.js 🍌  `graphy` is a collection of *high-performance* RDF libraries for JavaScript developers with a focus on usability. API works in both the browser and Node.js. Expressive CLI tool also available for Node.js.    [https://graphy.link/](https://graphy.link/)      ## Performance Benchmarks  🚀 [See how well `graphy` outperforms all others](https://github.com/blake-regalia/graphy.js/blob/master/perf/README.md).      ## Command Line Interface  📑 [See documentation for CLI here](https://graphy.link/cli).    ### Install the `graphy` bin CLI   - npm:     ```console     $ npm install --global graphy     $ graphy --help     ```     - yarn:     ```console     $ yarn global add graphy     $ graphy --help     ```      ## [Features](https://graphy.link/)   - [Read RDF documents](https://graphy.link/content.textual#verb_read) using streams. Includes support for N-Triples (.nt), N-Quads (.nq), Turtle (.ttl), and TriG (.trig).   - [Write RDF data](https://graphy.link/content.textual#verb_write) using streaming transforms with the awesome and intuitive [concise triples and concise quads language](https://graphy.link/concise).   - [Construct RDF data](https://graphy.link/concise#hash_c3) using ES object literals that reflect the tree-like structure of quads, `graph -> subject -> predicate -> object`, including nested blank nodes and RDF collections.   - [Compute the union, intersection, difference or subtraction](https://graphy.link/memory.dataset.fast) between multiple RDF graphs analagous to [Set Algebra](https://en.wikipedia.org/wiki/Algebra_of_sets).   - [Compare two RDF graphs](https://graphy.link/memory.dataset.fast#method_canonicalize) for isomoprhic equivalence, containment, and disjointness by first canonicalizing them with the [RDF Dataset Normalization Algorithm](https://json-ld.github.io/normalization/spec/).   - [Transform RDF data from the command-line](https://graphy.link/cli) by piping them through a series of sub-commands.   - [Scan RDF documents](https://graphy.link/content.textual#verb_scan) and run custom code using multiple threads for maximum throughput.      ## [See API Documentation](https://graphy.link/api)  🔎 Find the package you need _or_ install the super-package `npm install --save graphy` .    ### Core   - [DataFactory](https://graphy.link/core.data.factory)    ### Memory   - [FastDataset](https://graphy.link/memory.dataset.fast)    ### Content   - **N-Triples**: [NTriplesReader](https://graphy.link/content.textual#verb_read), [NTriplesScanner](https://graphy.link/content.textual#verb_scan), [NTriplesWriter](https://graphy.link/content.textual#verb_write), [NTriplesScriber](https://graphy.link/content.textual#verb_scribe)   - **N-Quads**: [NQuadsReader](https://graphy.link/content.textual#verb_read), [NQuadsScanner](https://graphy.link/content.textual#verb_scan), [NQuadsWriter](https://graphy.link/content.textual#verb_write), [NQuadsScriber](https://graphy.link/content.textual#verb_scribe)   - **Turtle**: [TurtleReader](https://graphy.link/content.textual#verb_read), [TurtleWriter](https://graphy.link/content.textual#verb_write), [TurtleScriber](https://graphy.link/content.textual#verb_scribe)   - **TriG**: [TriGReader](https://graphy.link/content.textual#verb_read), [TriGWriter](https://graphy.link/content.textual#verb_write), [TriGScriber](https://graphy.link/content.textual#verb_scribe)   - **RDF/XML**: [RdfXmlScriber](https://graphy.link/content.textual#verb_scribe)    ## Changelog  🍭⚡︎🔧  [See history of changes here](https://github.com/blake-regalia/graphy.js/blob/master/CHANGELOG.md).    ## Roadmap  🚧  [See the list of planned features](https://github.com/blake-regalia/graphy.js/blob/master/ROADMAP.md).    ## License    ISC © [Blake Regalia]()      [npm-image]: https://badge.fury.io/js/graphy.svg  [npm-url]: https://npmjs.org/package/graphy  <!-- [travis-image]: https://travis-ci.org/blake-regalia/graphy.js.svg?branch=master -->  <!-- [travis-url]: https://travis-ci.org/blake-regalia/graphy.js -->  [daviddm-image]: https://david-dm.org/blake-regalia/graphy.js.svg?theme=shields.io  [daviddm-url]: https://david-dm.org/blake-regalia/graphy.js """
Semantic web;https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark;"""# GeoSPARQL Compliance Benchmark    This is the GeoSPARQL Compliance Benchmark, integrated into the [HOBBIT Platform](https://github.com/hobbit-project/platform).    The GeoSPARQL Compliance Benchmark aims to evaluate the GeoSPARQL compliance of RDF storage systems. The benchmark uses  206 SPARQL queries to test the extent to which the benchmarked system supports the 30 requirements defined in the [GeoSPARQL standard](https://www.ogc.org/standards/geosparql).    As a result, the benchmark provides two metrics:   * **Correct answers**: The number of correct answers out of all GeoSPARQL queries, i.e. tests.   * **GeoSPARQL compliance percentage**: The percentage of compliance with the requirements of the GeoSPARQL standard.    ## Results    You can find a set of results from the [latest experiments on the hosted instance of the HOBBIT Platform](https://master.project-hobbit.eu/experiments/1612476122572,1612477003063,1612476116049,1625421291667,1612477500164,1612661614510,1612637531673,1612828110551,1612477849872)  (log in as Guest). [last update: 07.07.2021]    If you want your RDF triplestore tested, you can [add it as a system to the HOBBIT Platform](https://hobbit-project.github.io/system_integration.html),  and then [run an experiment](https://hobbit-project.github.io/benchmarking.html) using the [hosted instance of the HOBBIT Platform](https://hobbit-project.github.io/master.html).    ## Publications     * Milos Jovanovik, Timo Homburg, Mirko Spasić. ""[A GeoSPARQL Compliance Benchmark](https://www.mdpi.com/2220-9964/10/7/487)"". ISPRS International Journal of Geo-Information 10(7):487, 2021.   * Milos Jovanovik, Timo Homburg, Mirko Spasić. ""[Software for the GeoSPARQL Compliance Benchmark](https://doi.org/10.1016/j.simpa.2021.100071)"". Software Impacts 8:100071, 2021.   * (preprint) Milos Jovanovik, Timo Homburg, Mirko Spasić. ""[A GeoSPARQL Compliance Benchmark](https://arxiv.org/abs/2102.06139)"". arXiv:2102.06139.    ## Mapping Requirements to Queries    | Req. | Set of corresponding queries | Description  | :--: | :--- | :---   | <tr><th colspan=""3"">Core component (CORE)</th></tr>  | [R1](http://www.opengis.net/spec/geosparql/1.0/req/core/sparql-protocol) | [Q01.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r01.rq) | Selecting of the first triple where geometry A is the subject  | [R2](http://www.opengis.net/spec/geosparql/1.0/req/core/spatial-object-class ) | [Q02.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r02.rq) | Selecting of the first entity of type [geo:SpatialObject](http://www.opengis.net/ont/geosparql#SpatialObject)  | [R3](http://www.opengis.net/spec/geosparql/1.0/req/core/feature-class) | [Q03.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r03.rq) | Selecting of the first entity of type [geo:Feature](http://www.opengis.net/ont/geosparql#Feature)  | <tr><th colspan=""3"">Topology vocabulary extension (TOP)</th></tr>  | [R4](http://www.opengis.net/spec/geosparql/1.0/req/topology-vocab-extension/sf-spatial-relations) | [Q04-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r04-1.rq), [Q04-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r04-2.rq),  [Q04-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r04-3.rq), [Q04-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r04-4.rq), <br /> [Q04-5.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r04-5.rq), [Q04-6.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r04-6.rq),  [Q04-7.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r04-7.rq), [Q04-8.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r04-8.rq) | Testing the properties [geo:sfEquals](http://www.opengis.net/ont/geosparql#sfEquals), [geo:sfDisjoint](http://www.opengis.net/ont/geosparql#sfDisjoint), [geo:sfIntersects](http://www.opengis.net/ont/geosparql#sfIntersects),  [geo:sfTouches](http://www.opengis.net/ont/geosparql#sfTouches), [geo:sfCrosses](http://www.opengis.net/ont/geosparql#sfCrosses), [geo:sfWithin](http://www.opengis.net/ont/geosparql#sfWithin), [geo:sfContains](http://www.opengis.net/ont/geosparql#sfContains) and  [geo:sfOverlaps](http://www.opengis.net/ont/geosparql#sfOverlaps)  | [R5](http://www.opengis.net/spec/geosparql/1.0/req/topology-vocab-extension/eh-spatial-relations) | [Q05-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r05-1.rq), [Q05-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r05-2.rq),  [Q05-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r05-3.rq), [Q05-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r05-4.rq), <br /> [Q05-5.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r05-5.rq), [Q05-6.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r05-6.rq),  [Q05-7.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r05-7.rq), [Q05-8.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r05-8.rq) | Testing the properties [geo:ehEquals](http://www.opengis.net/ont/geosparql#ehEquals), [geo:ehDisjoint](http://www.opengis.net/ont/geosparql#ehDisjoint), [geo:ehMeet](http://www.opengis.net/ont/geosparql#ehMeet),  [geo:ehOverlap](http://www.opengis.net/ont/geosparql#ehOverlap), [geo:ehCovers](http://www.opengis.net/ont/geosparql#ehCovers), [geo:ehCoveredBy](http://www.opengis.net/ont/geosparql#ehCoveredBy), [geo:ehInside](http://www.opengis.net/ont/geosparql#ehInside) and  [geo:ehContains](http://www.opengis.net/ont/geosparql#ehContains)  | [R6](http://www.opengis.net/spec/geosparql/1.0/req/topology-vocab-extension/rcc8-spatial-relations) | [Q06-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r06-1.rq), [Q06-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r06-2.rq),  [Q06-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r06-3.rq), [Q06-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r06-4.rq), <br /> [Q06-5.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r06-5.rq), [Q06-6.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r06-6.rq),  [Q06-7.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r06-7.rq), [Q06-8.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r06-8.rq) | Testing the properties [geo:rcc8eq](http://www.opengis.net/ont/geosparql#rcc8eq), [geo:rcc8dc](http://www.opengis.net/ont/geosparql#rcc8dc), [geo:rcc8ec](http://www.opengis.net/ont/geosparql#rcc8ec), [geo:rcc8po](http://www.opengis.net/ont/geosparql#rcc8po),  [geo:rcc8tppi](http://www.opengis.net/ont/geosparql#rcc8tppi), [geo:rcc8tpp](http://www.opengis.net/ont/geosparql#rcc8tpp), [geo:rcc8ntpp](http://www.opengis.net/ont/geosparql#rcc8ntpp) and [geo:rcc8ntppi](http://www.opengis.net/ont/geosparql#rcc8ntppi)  | <tr><th colspan=""3"">Geometry extension (GEOEXT)</th></tr>  | [R7](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/geometry-class) | [Q07.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r07.rq) | Selecting of all entities of type [geo:Geometry](http://www.opengis.net/ont/geosparql#Geometry)  | [R8](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/feature-properties) | [Q08-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r08-1.rq), [Q08-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r08-2.rq) | Selecting of the value of geometry A denoted by the properties  [geo:hasGeometry](http://www.opengis.net/ont/geosparql#hasGeometry) and [geo:hasDefaultGeometry](http://www.opengis.net/ont/geosparql#hasDefaultGeometry)  | [R9](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/geometry-properties) | [Q09-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r09-1.rq), [Q09-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r09-2.rq),  [Q09-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r09-3.rq), [Q09-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r09-4.rq), <br /> [Q09-5.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r09-5.rq), [Q09-6.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r09-6.rq) | Selecting of the value of geometry A denoted by the properties  [geo:dimension](http://www.opengis.net/ont/geosparql#dimension), [geo:coordinateDimension](http://www.opengis.net/ont/geosparql#coordinateDimension), [geo:spatialDimension](http://www.opengis.net/ont/geosparql#spatialDimension),  [geo:isEmpty](http://www.opengis.net/ont/geosparql#isEmpty), [geo:isSimple](http://www.opengis.net/ont/geosparql#isSimple) and [geo:hasSerialization](http://www.opengis.net/ont/geosparql#hasSerialization)|   | [R10](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/wkt-literal) | [Q10.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r10.rq) | Checking of the datatype of a correctly defined WKT literal from the dataset  | [R11](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/wkt-literal-default-srs) | [Q11.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r11.rq) | Checking of the equality of two geometries from the dataset  | [R12](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/wkt-axis-order) | [Q12.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r12.rq) | Checking if the system interprets the axis order within a point geometry  according to the spatial reference system being used  | [R13](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/wkt-literal-empty) | [Q13-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r13-1.rq), [Q13-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r13-2.rq) | Checking if an empty RDFS Literal of type [geo:wktLiteral](http://www.opengis.net/ont/geosparql#wktLiteral) is interpreted as  an empty geometry  | [R14](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/geometry-as-wkt-literal) | [Q14.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r14.rq) | Checking of the [geo:asWKT](http://www.opengis.net/ont/geosparql#asWKT) value of geometry A against the expected  literal value  | [R15](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/gml-literal) | [Q15.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r15.rq) | Checking whether all the values of the [geo:asGML](http://www.opengis.net/ont/geosparql#asGML) property contain a valid  GM_Object subtype in it and whether its datatype is [geo:gmlLiteral](http://www.opengis.net/ont/geosparql#gmlLiteral)  | [R16](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/gml-literal-empty) | [Q16-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r16-1.rq), [Q16-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r16-2.rq) | Checking if an empty [geo:gmlLiteral](http://www.opengis.net/ont/geosparql#gmlLiteral) is interpreted as an empty geometry  | [R17](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/gml-profile) | --- | ---  | [R18](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/geometry-as-gml-literal) | [Q18.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r18.rq) | Checking the [geo:asGML](http://www.opengis.net/ont/geosparql#asGML) value of geometry A against the expected literal value  | [R19](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/query-functions) | [Q19-1-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-1-1.rq), [Q19-1-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-1-2.rq),  [Q19-1-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-1-3.rq), [Q19-1-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-1-4.rq), <br /> [Q19-2-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-2-1.rq), [Q19-2-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-2-2.rq),  [Q19-3-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-3-1.rq), [Q19-3-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-3-2.rq), <br /> [Q19-4-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-4-1.rq), [Q19-4-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-4-2.rq),  [Q19-4-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-4-3.rq), [Q19-4-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-4-4.rq), <br /> [Q19-5-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-5-1.rq), [Q19-5-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-5-2.rq),  [Q19-5-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-5-3.rq), [Q19-5-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-5-4.rq), <br /> [Q19-6-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-6-1.rq), [Q19-6-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-6-2.rq),  [Q19-6-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-6-3.rq), [Q19-6-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-6-4.rq), <br /> [Q19-7-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-7-1.rq), [Q19-7-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-7-2.rq),  [Q19-7-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-7-3.rq), [Q19-7-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-7-4.rq), <br /> [Q19-8-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-8-1.rq), [Q19-8-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-8-2.rq),  [Q19-9-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-9-1.rq), [Q19-9-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r19-9-2.rq) | Checking a support of the geospatial functions [geof:distance](http://www.opengis.net/def/function/geosparql/distance), [geof:buffer](http://www.opengis.net/def/function/geosparql/buffer),  [geof:convexHull](http://www.opengis.net/def/function/geosparql/convexHull), [geof:intersection](http://www.opengis.net/def/function/geosparql/intersection), [geof:union](http://www.opengis.net/def/function/geosparql/union), [geof:difference](http://www.opengis.net/def/function/geosparql/difference),  [geof:symDifference](http://www.opengis.net/def/function/geosparql/symDifference), [geof:envelope](http://www.opengis.net/def/function/geosparql/envelope) and [geof:boundary](http://www.opengis.net/def/function/geosparql/boundary)  | [R20](http://www.opengis.net/spec/geosparql/1.0/req/geometry-extension/srid-function) | [Q20-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r20-1.rq), [Q20-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r20-2.rq) | Checking a support of the geospatial function [geof:getSRID](http://www.opengis.net/def/function/geosparql/getSRID)  | <tr><th colspan=""3"">Geometry topology extension (GTOP)</th></tr>  | [R21](http://www.opengis.net/spec/geosparql/1.0/req/geometry-topology-extension/relate-query-function) | [Q21-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r21-1.rq), [Q21-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r21-2.rq),  [Q21-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r21-3.rq), [Q21-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r21-4.rq) | Checking a support of the geospatial operator [geof:relate](http://www.opengis.net/def/function/geosparql/relate)  | [R22](http://www.opengis.net/spec/geosparql/1.0/req/geometry-topology-extension/sf-query-functions) | [Q22-1-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-1-1.rq), [Q22-1-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-1-2.rq),  [Q22-1-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-1-3.rq), [Q22-1-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-1-4.rq), <br /> [Q22-2-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-2-1.rq), [Q22-2-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-2-2.rq),  [Q22-2-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-2-3.rq), [Q22-2-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-2-4.rq), <br /> [Q22-3-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-3-1.rq), [Q22-3-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-3-2.rq),  [Q22-3-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-3-3.rq), [Q22-3-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-3-4.rq), <br /> [Q22-4-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-4-1.rq), [Q22-4-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-4-2.rq),  [Q22-4-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-4-3.rq), [Q22-4-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-4-4.rq), <br /> [Q22-5-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-5-1.rq), [Q22-5-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-5-2.rq),  [Q22-5-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-5-3.rq), [Q22-5-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-5-4.rq), <br /> [Q22-6-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-6-1.rq), [Q22-6-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-6-2.rq),  [Q22-6-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-6-3.rq), [Q22-6-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-6-4.rq), <br /> [Q22-7-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-7-1.rq), [Q22-7-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-7-2.rq),  [Q22-7-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-7-3.rq), [Q22-7-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-7-4.rq), <br /> [Q22-8-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-8-1.rq), [Q22-8-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-8-2.rq),  [Q22-8-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-8-3.rq), [Q22-8-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r22-8-4.rq) | Checking a support of the geospatial functions [geof:sfEquals](http://www.opengis.net/def/function/geosparql/sfEquals), [geof:sfDisjoint](http://www.opengis.net/def/function/geosparql/sfDisjoint),  [geof:sfIntersects](http://www.opengis.net/def/function/geosparql/sfIntersects), [geof:sfTouches](http://www.opengis.net/def/function/geosparql/sfTouches), [geof:sfCrosses](http://www.opengis.net/def/function/geosparql/sfCrosses), [geof:sfWithin](http://www.opengis.net/def/function/geosparql/sfWithin), [geof:sfContains](http://www.opengis.net/def/function/geosparql/sfContains)  and [geof:sfOverlaps](http://www.opengis.net/def/function/geosparql/sfOverlaps)  | [R23](http://www.opengis.net/spec/geosparql/1.0/geometry-topology-extension/eh-query-functions) | [Q23-1-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-1-1.rq), [Q23-1-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-1-2.rq),  [Q23-1-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-1-3.rq), [Q23-1-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-1-4.rq), <br /> [Q23-2-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-2-1.rq), [Q23-2-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-2-2.rq),  [Q23-2-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-2-3.rq), [Q23-2-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-2-4.rq), <br /> [Q23-3-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-3-1.rq), [Q23-3-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-3-2.rq),  [Q23-3-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-3-3.rq), [Q23-3-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-3-4.rq), <br /> [Q23-4-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-4-1.rq), [Q23-4-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-4-2.rq),  [Q23-4-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-4-3.rq), [Q23-4-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-4-4.rq), <br /> [Q23-5-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-5-1.rq), [Q23-5-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-5-2.rq),  [Q23-5-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-5-3.rq), [Q23-5-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-5-4.rq), <br /> [Q23-6-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-6-1.rq), [Q23-6-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-6-2.rq),  [Q23-6-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-6-3.rq), [Q23-6-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-6-4.rq), <br /> [Q23-7-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-7-1.rq), [Q23-7-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-7-2.rq),  [Q23-7-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-7-3.rq), [Q23-7-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-7-4.rq), <br /> [Q23-8-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-8-1.rq), [Q23-8-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-8-2.rq),  [Q23-8-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-8-3.rq), [Q23-8-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r23-8-4.rq) | Checking a support of the geospatial functions [geof:ehEquals](http://www.opengis.net/def/function/geosparql/ehEquals), [geof:ehDisjoint](http://www.opengis.net/def/function/geosparql/ehDisjoint),  [geof:ehMeet](http://www.opengis.net/def/function/geosparql/ehMeet), [geof:ehOverlap](http://www.opengis.net/def/function/geosparql/ehOverlap), [geof:ehCovers](http://www.opengis.net/def/function/geosparql/ehCovers), [geof:ehCoveredBy](http://www.opengis.net/def/function/geosparql/ehCoveredBy), [geof:ehInside](http://www.opengis.net/def/function/geosparql/ehInside)  and [geof:ehContains](http://www.opengis.net/def/function/geosparql/ehContains)  | [R24](http://www.opengis.net/spec/geosparql/1.0/req/geometry-topology-extension/rcc8-query-functions) | [Q24-1-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-1-1.rq), [Q24-1-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-1-2.rq),  [Q24-1-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-1-3.rq), [Q24-1-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-1-4.rq), <br /> [Q24-2-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-2-1.rq), [Q24-2-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-2-2.rq),  [Q24-2-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-2-3.rq), [Q24-2-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-2-4.rq), <br /> [Q24-3-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-3-1.rq), [Q24-3-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-3-2.rq),  [Q24-3-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-3-3.rq), [Q24-3-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-3-4.rq), <br /> [Q24-4-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-4-1.rq), [Q24-4-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-4-2.rq),  [Q24-4-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-4-3.rq), [Q24-4-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-4-4.rq), <br /> [Q24-5-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-5-1.rq), [Q24-5-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-5-2.rq),  [Q24-5-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-5-3.rq), [Q24-5-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-5-4.rq), <br /> [Q24-6-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-6-1.rq), [Q24-6-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-6-2.rq),  [Q24-6-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-6-3.rq), [Q24-6-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-6-4.rq), <br /> [Q24-7-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-7-1.rq), [Q24-7-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-7-2.rq),  [Q24-7-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-7-3.rq), [Q24-7-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-7-4.rq), <br /> [Q24-8-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-8-1.rq), [Q24-8-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-8-2.rq),  [Q24-8-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-8-3.rq), [Q24-8-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r24-8-4.rq) | Checking a support of the geospatial functions [geof:rcc8eq](http://www.opengis.net/def/function/geosparql/rcc8eq), [geof:rcc8dc](http://www.opengis.net/def/function/geosparql/rcc8dc), [geof:rcc8ec](http://www.opengis.net/def/function/geosparql/rcc8ec),  [geof:rcc8po](http://www.opengis.net/def/function/geosparql/rcc8po), [geof:rcc8tppi](http://www.opengis.net/def/function/geosparql/rcc8tppi), [geof:rcc8tpp](http://www.opengis.net/def/function/geosparql/rcc8tpp), [geof:rcc8ntpp](http://www.opengis.net/def/function/geosparql/rcc8ntpp) and [geof:rcc8ntppi](http://www.opengis.net/def/function/geosparql/rcc8ntppi)  | <tr><th colspan=""3"">RDFS entailment extension (RDFSE)</th></tr>  | [R25](http://www.opengis.net/spec/geosparql/1.0/req/rdfs-entailment-extension/bgp-rdfs-ent) | [Q25-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r25-1.rq), [Q25-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r25-2.rq),  [Q25-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r25-3.rq) | Checking if the system supports selecting both materialized RDF triples, as well as  inferred RDF triples based on the RDFS Entailment Regime  | [R26](http://www.opengis.net/spec/geosparql/1.0/req/rdfs-entailment-extension/wkt-geometry-types) | [Q26-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r26-1.rq), [Q26-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r26-2.rq) | Checking if the system supports selecting both materialized RDF triples, as well as  inferred RDF triples based on the RDFS/OWL class hierarchy of geometry types from  Simple Features  | [R27](http://www.opengis.net/spec/geosparql/1.0/req/rdfs-entailment-extension/gml-geometry-types) | [Q27.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r27.rq) | Checking if the system supports selecting both materialized RDF triples, as well as  inferred RDF triples based on the RDFS/OWL class hierarchy of geometry types of  the GML schema   | <tr><th colspan=""3"">Query rewrite extension (QRW)</th></tr>  | [R28](http://www.opengis.net/spec/geosparql/1.0/req/query-rewrite-extension/sf-query-rewrite) | [Q28-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r28-1.rq), [Q28-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r28-2.rq),  [Q28-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r28-3.rq), [Q28-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r28-4.rq), <br /> [Q28-5.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r28-5.rq), [Q28-6.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r28-6.rq),  [Q28-7.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r28-7.rq), [Q28-8.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r28-8.rq) | Testing the properties [geor:sfEquals](http://www.opengis.net/def/rule/geosparql/sfEquals), [geor:sfDisjoint](http://www.opengis.net/def/rule/geosparql/sfDisjoint), [geor:sfIntersects](http://www.opengis.net/def/rule/geosparql/sfIntersects), [geor:sfTouches](http://www.opengis.net/def/rule/geosparql/sfTouches),  [geor:sfCrosses](http://www.opengis.net/def/rule/geosparql/sfCrosses), [geor:sfWithin](http://www.opengis.net/def/rule/geosparql/sfWithin), [geor:sfContains](http://www.opengis.net/def/rule/geosparql/sfContains) and [geor:sfOverlaps](http://www.opengis.net/def/rule/geosparql/sfOverlaps) using both  materialized RDF triples and inferred RDF triples   | [R29](http://www.opengis.net/spec/geosparql/1.0/req/query-rewrite-extension/eh-query-rewrite) | [Q29-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r29-1.rq), [Q29-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r29-2.rq),  [Q29-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r29-3.rq), [Q29-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r29-4.rq), <br /> [Q29-5.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r29-5.rq), [Q29-6.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r29-6.rq),  [Q29-7.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r29-7.rq), [Q29-8.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r29-8.rq) | Testing the properties [geor:ehEquals](http://www.opengis.net/def/rule/geosparql/ehEquals), [geor:ehDisjoint](http://www.opengis.net/def/rule/geosparql/ehDisjoint), [geor:ehMeet](http://www.opengis.net/def/rule/geosparql/ehMeet), [geor:ehOverlap](http://www.opengis.net/def/rule/geosparql/ehOverlap),  [geor:ehCovers](http://www.opengis.net/def/rule/geosparql/ehCovers), [geor:ehCoveredBy](http://www.opengis.net/def/rule/geosparql/ehCoveredBy), [geor:ehInside](http://www.opengis.net/def/rule/geosparql/ehInside) and [geor:ehContains](http://www.opengis.net/def/rule/geosparql/ehContains) using both  materialized RDF triples and inferred RDF triples   | [R30](http://www.opengis.net/spec/geosparql/1.0/req/query-rewrite-extension/rcc8-query-rewrite) | [Q30-1.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r30-1.rq), [Q30-2.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r30-2.rq),  [Q30-3.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r30-3.rq), [Q30-4.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r30-4.rq), <br /> [Q30-5.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r30-5.rq), [Q30-6.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r30-6.rq),  [Q30-7.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r30-7.rq), [Q30-8.rq](https://github.com/OpenLinkSoftware/GeoSPARQLBenchmark/blob/master/src/main/resources/gsb_queries/query-r30-8.rq) | Testing the properties [geor:rcc8eq](http://www.opengis.net/def/rule/geosparql/rcc8eq), [geor:rcc8dc](http://www.opengis.net/def/rule/geosparql/rcc8dc), [geor:rcc8ec](http://www.opengis.net/def/rule/geosparql/rcc8ec), [geor:rcc8po](http://www.opengis.net/def/rule/geosparql/rcc8po), [geor:rcc8tppi](http://www.opengis.net/def/rule/geosparql/rcc8tppi),  [geor:rcc8tpp](http://www.opengis.net/def/rule/geosparql/rcc8tpp), [geor:rcc8ntpp](http://www.opengis.net/def/rule/geosparql/rcc8ntpp) and [geor:rcc8ntppi](http://www.opengis.net/def/rule/geosparql/rcc8ntppi) using both materialized RDF triples  and inferred RDF triples       ## Acknowledgement    The benchmark has been developed as part of the [HOBBIT](https://project-hobbit.eu) and [SAGE](https://sage.cs.uni-paderborn.de/sage/) research projects. """
Semantic web;https://github.com/tomatophantastico/sparqlmap;"""#SparqlMap - Client      SparqlMap - A SPARQL to SQL rewriter based on [R2RML](http://www.w3.org/TR/r2rml/) specification.    It can be used in allows both extracting RDF from an relational database and rewrite SPARQL queries into SQL.    The SparqlMap Client provides command line and web access to the SparqlMap core.    ## Download    Get the latest release on the [release page](https://github.com/tomatophantastico/sparqlmap/releases).  Please note, that no MySQL driver is included. You will need to get it from the [MySQL page](https://dev.mysql.com/downloads/connector/j/)  and copy it into the ./lib folder.      ## Building    We use a [patched version](https://github.com/tomatophantastico/metamodel) of [Apache Metamode](http://metamodel.apache.org/), which improves quotation mark handling.    To include it into your SparqlMap-build please install it locally (using [maven](http://maven.apache.org))  ```  git clone --branch feature/quoteColumns https://github.com/tomatophantastico/metamodel && cd metamodel && mvn install  ```    SparqlMap utilizes gradle, so you can just run  ```  ./gradle installDist  ```  which will create a distribution in `build/install/sparqlmap`.            ## Overview over the mapping process    ![SparqlMap overview](https://raw.github.com/tomatophantastico/sparqlmap/doc/doc/sparqlMap.png)        ##Quick Start    Most of the time, dump creation will take place on the command line.  In the binary distributions use the sparqlmap command.   Calling sparqlmap without or with a wrong combination of options will present all options available.    Let's have a look at some samples:    ### RDF Dump    ```shell  ./bin/sparqlmap --action=dump --ds.type=JDBC --ds.url=""jdbc:mysql://192.168.59.103:3306/sparqlmaptest"" --ds.username=sparqlmap --ds.password=sparqlmap  --r2rmlFile=src/test/resources/hsql-bsbm/mapping.ttl     ```  Or if you do not have a R2RML mapping, you can create a dump based on a Direct Mapping    ```shell  ./bin/sparqlmap --action=dump --ds.type=JDBC --ds.url=""jdbc:mysql://192.168.59.103:3306/sparqlmaptest"" --ds.username=sparqlmap --ds.password=sparqlmap   ```      ## R2RML Mappings    ### Re-use an Existing Mapping  Is quite simple, just provide the ```-r2rml.file```parameter:   ```  -r2rml.file=<fileLocation>  ```    ### Creation of a Mapping    Creating a R2RML representation of a default mapping is as easy as this, just change the action:    ```shell  ./bin/sparqlmap --action=directmapping --ds.type=JDBC --ds.url=""jdbc:mysql://192.168.59.103:3306/sparqlmaptest"" -ds.username=sparqlmap  --ds.password=sparqlmap    ```    ## Direct Mapping options  With the following options, the generation of the direct mapping can be controlled.  These options are meant to ease manual editing of the generated R2RML file, or when the     * a given baseuriprefix is suffixed by either mapping, vocab or instance to produce the corresponding uri prefixes.  * the mappinguriprefix will only show up in the resulting r2rml-file for name the R2RML Resources, such as TriplesMaps  * vocaburiprefix helps constructing useful identifiers for predicates and class generated from the schema of the data source. It will therefore show up in the extraced data.  * Resources generated in the RDB-to-RDF translation process use the instanceuriprefix for IRI generation.    ```  --dm.baseuriprefix=<baseuri/>  --dm.mappinguriprefix=<baseuri/mapping/>  --dm.vocaburiprefix=<basuri/vocab/>  --dm.instanceuriprefix=<baseuri/instance>  --dm.separatorchar=#  ```              ## Rewrite SPARQL queries into SQL    For rewriting SPARQL queries into SQL SparqlMap can expose a SPARQL endpoint by an embedded tomcat.  The enpoint is started by     This will expose an SPARQL endpoint with a little snorql interface.        ## R2RML conformance    SparqlMap conforms with the R2RML specification and was tested with PostgreSQL, MySQL and HSQL.      ## Adding additional database drivers    Simply copy them into the lib folder.    ## Building and installation    SparqlMap requires the following dependencies, which are not present in the main repos.    2. Metamodel with a patch  ```shell  git clone https://github.com/tomatophantastico/metamodel.git  cd metamodel  mvn install  ```      # Actions    ## CLI  SparqlMap allows the following command line interactions, selected by the `--action=<action>´ parameter    ### dump    This creates a dump of the mapped database. You can specify the outputformat, using the `--format=<FORMAT>` parameter.  Supported is TURTLE,  TRIG, NTRIPLES and NQUADS.  Triple serializations simply ignore the graph definitions of the mappings. A triple that occurs in multiple graphs will consequently appear multiple times in the result.    ### directmapping    A R2RML mapping is created, based on the concept of a direct mapping, always yields TURTLE output.    ### query    A query is executed and the result is returned on the command line.    ## web  SparqlMap can be used as a SPARQL enndpoint, for example for exposing a ACCESS database:    ```shell  bin/sparqlmap --action=web --ds.type=ACCESS --ds.url=mydata.accdb  ```     The endpoint will be accessible on:  ```  localhost:8080/sparql  ```  Currently, a number of limitations apply:  * Some the query processing is executed in-memory, which degrades performance  * There is no further UI for entering and viewing queries        # Mapping Options  Existing mapping files can be provided via the r2rmlfile-parameter:  ```  --r2rmlFile=<filename>  ```  If this parameter is omitted, a direct mapping will be created.    The direct mapping generation can be modified by following attributes.  If you just define the  ```<baseUriPrefix>``` you should be fine in most cases.  ```  --dm.baseUriPrefix=http://localhost/baseiri/  --dm.mappingUriPrefix=<baseUriPrefix>/mapping/  --dm.vocabUriPrefix=<baseUriPrefix>/instance/  --dm.instanceUriPrefix=<baseUriPrefix>/vocab/  --dm.separatorChar=+  ```  # Data Sources      ## Using with MongoDB (```--ds.type=MONGODB2``` or ```--ds.type=MONGODB3```)  ```  --ds.type=MONGODB2 or --ds.type=MONGODB3  --ds.url  the host and port of the mongodb server, e.g.: localhost:11111  --ds.dbname the database name  --ds.username  username  --ds.password  password  ```    ## Using with a JDBC database  First, make sure that a JDBC4 driver is in the classpath. SparqlMap already contains drivers for the most important FOSS RDBMs, for closed source RDBMs, they have to be added manually.    ```  --ds.url  the full jdbc url, e.g. jdbc:mysql://localhost:3306/test  --ds.username  username of the RDBMS  --ds.password  password of the RDBMS  --ds.maxPoolSize max number of connections to the RDBMs, defaults to 10  ```    ## Using with CSV files (--ds.type=CSV)  For more details check the [Apache MetaModel CSV adapter wiki page](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=65875503)    Required parameters  ```  --ds.type=CSV If it is not CSV, you will have to look at your other options  --ds.url=<path> required  The path to the file  ```  Optional parameters and their defaults  ```  --ds.quoteChar="" Encloses values  --ds.separatorChar=, default to , values in a row are split according to this value  --ds.escapeChar=\ for escaping special characters  --ds.encoding=UTF-8   --ds.columnNameLineNumber=1 Starting from 1,   --ds.failOnInconsistentRowLength=true defaults to true if the column count varies in the file, this pushes the parser further.  --ds.multilineValues=false allows multiline values  ```    ##Excel-Files  ```  --ds.type=EXCEL  Mandatory  --ds.url=<path> required  The path to the file  ```  Optional  ```  --ds.columnNameLineNumber=1  --ds.SkipEmptyLines=true  --ds.SkipEmptyColumns=true  ```    ## Access-Files  Besides the type, only the file location needs to be provided  ```  --ds.type=ACCESS  Mandatory  --ds.url=<path>  ```  ##CouchDB  ```  --ds.type=COUCHDB  Mandatory  --ds.url=<httpurl> required  The url of the couchdbserver (using ektorp)  ```  Optionally:  ```  --ds.username=<username>  --ds.password=<password>  ``` """
Semantic web;https://github.com/isl/XML2RDF-DataTransformation-MappingTool;"""XML2RDF-DataTransformation-MappingTool  ======================================       XML2RDF Data Transformation Tool (Mapping Tool): This generic data transformation tool maps XML data    files to RDF files, given a schema matching definition, based on this Mapping Language Schema.    This distribution contains:  The XML2RDF-DataTransformation-Mapping Tool version 3.3 bundle   	includes: source code (NetBeans project), binary, required libraries and javadocs.     COPYRIGHT (c) 2010-2012 by Institute of Computer Science,   Foundation for Research and Technology - Hellas  Contact:       POBox 1385, Heraklio Crete, GR-700 13 GREECE      Tel:+30-2810-391632      Fax: +30-2810-391638      E-mail: isl@ics.forth.gr      http://www.ics.forth.gr/isl     Authors :  Maria Koutraki, Dimitra Zografistou, Evangelia Daskalaki, Elias Zabetakis     This file is part of XML2RDF Data Transformation Tool (Mapping Tool).        XML2RDF Data Transformation Tool (Mapping Tool) is free software: you can redistribute it and/or modify      it under the terms of the GNU Lesser General Public License as published by      the Free Software Foundation, either version 3 of the License, or      (at your option) any later version.     XML2RDF Data Transformation Tool (Mapping Tool) is distributed in the hope that it will be useful,   but WITHOUT ANY WARRANTY; without even the implied warranty of   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the   GNU Lesser General Public License for more details.     You should have received a copy of the GNU Lesser General Public License   along with XML2RDF Data Transformation Tool (Mapping Tool).  If not, see <http://www.gnu.org/licenses/>.     """
Semantic web;https://github.com/arc-lasalle/AutoMap4OBDA;"""# AutoMap4OBDA  AutoMap4OBDA is a system which automatically generates R2RML mappings based on the intensive use of relational source contents and features of the target ontology. AutoMap4OBDA takes as inputs a relational database (i.e., PostgreSQL) and an ontology in OWL to produce a putative ontology from a relational database which is used as an intermediate element in the relational-to-ontology process. Moreover, AutoMap4OBDA has been designed to be used in OBDA scenarios and is able to generate fully compliant R2RML mappings without user intervention.    In AutoMap4OBDA, the database content and features of the target ontology are taken into account during the mapping generation process. We have developed three techniques that make the mapping process strongly dependent on the input database and the features of the target ontology to increase the performance of the relational-to-ontology mappings.    - Ontology learning technique to infer class hierarchies for development of a putative ontology  - String similarity metric selection based on target ontology labels for ontology alignment  - Short path strategy for R2RML mapping generation based on alignments    This is the first version which will require some code cleansing and refactoring. Currently it is only supporting PostgreSQL.    Use:  ```  java -jar automap4obda.jar -db <databaseURL> -schema <schemaname> -driver <databaseDriver> -u <username> - p ""<password>""   -n <ontologyname> -d <path-to-domainontology> -o <outputfiles>   [-attrasclass <0/1>] [-ol <0/1>] [-olclasstable <0/1>] [-olclassnamealone <0/1>] [-extendedmappings <0/1>]  ```    Example:  ```  java -jar automap4obda.jar -db jdbc:postgresql:postgres -schema sigkdd_structured -driver org.postgresql.Driver -u postgres -p ""postgres""   -n sigkdd_structured_putative -d ""c:\...\sigkdd_structured.ttl"" -o sigkdd_structured_putative   -attrasclass 1 -ol 1 -olclasstable 1 -olclassnamealone 1  -extendedmappings 1  ```      - Password can be empty  - attrasclass: Attributes as classes option (default 1)  - ol: Ontology learning technique option (default 1)  - olclasstable: New classes in ontology learning technique belongs to the table class (1) or to the column class (0) (default 1)  - olclassnamealone: New classes in ontology learning technique is taken as it is in the DB (1) or the name of the table/column is attached (0) (default 1)  - extendedmappings: Short path technique option (default 1)    http://arc.salleurl.edu/automap4obda    Copyright (C) 2016 ARC Engineering and Architecture La Salle, Ramon Llull University.     for comments please contact Alvaro Sicilia (ascilia@salleurl.edu)   """
Semantic web;https://github.com/protegeproject/swrlapi;"""SWRLAPI  =======    [![Build Status](https://travis-ci.org/protegeproject/swrlapi.svg?branch=master)](https://travis-ci.org/protegeproject/swrlapi)  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/edu.stanford.swrl/swrlapi/badge.svg)](https://maven-badges.herokuapp.com/maven-central/edu.stanford.swrl/swrlapi)    The SWRLAPI is a Java API for working with the [OWL](http://en.wikipedia.org/wiki/Web_Ontology_Language)-based [SWRL](http://www.w3.org/Submission/SWRL/) rule and [SQWRL](https://github.com/protegeproject/swrlapi/wiki/SQWRL) query languages.   It includes graphical tools for editing and executing rules and queries.    See the [SWRLAPI Wiki](https://github.com/protegeproject/swrlapi/wiki) for documentation.    A standalone [SWRLTab](https://github.com/protegeproject/swrltab) application and a [Protégé-based](http://protege.stanford.edu/)   [SWRLTab Plugin](https://github.com/protegeproject/swrltab-plugin), both built using this API, are also available.     #### Getting Started    The following examples can be used to quickly get started with the API. A sample SWRLAPI-based project can also be found [here](https://github.com/protegeproject/swrlapi-example).    The library's dependency information can be found here:    [![Maven Central](https://maven-badges.herokuapp.com/maven-central/edu.stanford.swrl/swrlapi/badge.svg)](https://maven-badges.herokuapp.com/maven-central/edu.stanford.swrl/swrlapi)    If you'd like to be able to execute SWRL rules or SQWRL queries you will need a SWRLAPI-based rule engine implementation. Currently, a [Drools-based SWRL rule engine implementation](https://github.com/protegeproject/swrlapi-drools-engine) is provided. This implementation is also hosted on Maven Central. Its dependency information can be found here:    [![Maven Central](https://maven-badges.herokuapp.com/maven-central/edu.stanford.swrl/swrlapi-drools-engine/badge.svg)](https://maven-badges.herokuapp.com/maven-central/edu.stanford.swrl/swrlapi-drools-engine)    The SWRLAPI uses the [OWLAPI](https://github.com/owlcs/owlapi) to manage OWL ontologies.  The following example illustrates how the library can be used to create a SWRL query engine using an ontology   created by the OWLAPI and then execute rules in that ontology.    ```java   // Create OWLOntology instance using the OWLAPI   OWLOntologyManager ontologyManager = OWLManager.createOWLOntologyManager();   OWLOntology ontology = ontologyManager.loadOntologyFromOntologyDocument(new File(""/ont/Ont1.owl""));     // Create a SWRL rule engine using the SWRLAPI   SWRLRuleEngine swrlRuleEngine = SWRLAPIFactory.createSWRLRuleEngine(ontology);     // Run the SWRL rules in the ontology   swrlRuleEngine.infer();  ```    This example shows how the API can be used to create a SQWRL query engine, create and execute a SQWRL query using  this engine, and then process the results.    ```java   // Create OWLOntology instance using the OWLAPI   OWLOntologyManager ontologyManager = OWLManager.createOWLOntologyManager();   OWLOntology ontology = ontologyManager.loadOntologyFromOntologyDocument(new File(""/ont/Ont1.owl""));     // Create SQWRL query engine using the SWRLAPI   SQWRLQueryEngine queryEngine = SWRLAPIFactory.createSQWRLQueryEngine(ontology);     // Create and execute a SQWRL query using the SWRLAPI   SQWRLResult result = queryEngine.runSQWRLQuery(""q1"",""swrlb:add(?x, 2, 2) -> sqwrl:select(?x)"");     // Process the SQWRL result   if (result.next())      System.out.println(""Name: "" + result.getLiteral(""x"").getInteger());  ```    Extensive documentation on the SWRLAPI can be found on the [SWRLAPI Wiki](https://github.com/protegeproject/swrlapi/wiki).    #### Building from Source    To build this library you must have the following items installed:    + [Java 8](http://www.oracle.com/technetwork/java/javase/downloads/index.html)  + A tool for checking out a [Git](http://git-scm.com/) repository  + Apache's [Maven](http://maven.apache.org/index.html)    Get a copy of the latest code:        git clone https://github.com/protegeproject/swrlapi.git     Change into the swrlapi directory:        cd swrlapi    Then build it with Maven:        mvn clean install    On build completion your local Maven repository will contain the generated swrlapi-${version}.jar file.    This JAR is used by the [Protégé](http://protege.stanford.edu/) [SWRLTab Plugin](https://github.com/protegeproject/swrltab-plugin)  and by the standalone [SWRLTab](https://github.com/protegeproject/swrltab) tool.    A [Build Project](https://github.com/protegeproject/swrlapi-project) is provided to build core SWRLAPI-related components.  A project containing a [library of integration tests](https://github.com/protegeproject/swrlapi-integration-tests) is also provided.    #### License    This software is licensed under the [BSD 2-clause License](https://github.com/protegeproject/swrlapi/blob/master/license.txt).    #### Questions    If you have questions about this library, please go to the main  Protégé website and subscribe to the [Protégé Developer Support  mailing list](http://protege.stanford.edu/support.php#mailingListSupport).  After subscribing, send messages to protege-dev at lists.stanford.edu. """
Semantic web;https://github.com/fluentdesk/FRESCA;"""The FRESH Resume Schema  ======  *A rational schema for your résumé or CV. Based on [FRESH][f].*    The [FRESH résumé schema][fresh] is an open source, standards-friendly,  JSON/YAML-driven format for résumé / CVs and other employment artifacts.    - [**View the official FRESH schema document.**][schema]  - [**View a sample FRESH resume.**][exemplar]    FRESH is supported natively by [HackMyResume][hmr] and can be trivially  converted to and from the [JSON Resume][jrs] format.    ## What It Does    FRESCA establishes an optimized, human-readable, computer-friendly  representation for your résumé and career data based on JSON or equivalent  YAML...    ```js  // Pared-down FRESH/FRESCA resume representation (JSON)  {    ""name"": ""Jane Doe"",    ""info"": { /* Basic info */ },    ""contact"": { /* Contact information */ },    ""location"": { /* Location / address */ },    ""meta"": { /* Resume metadata */ },    ""employment"": { /* Employment history */ },    ""projects"": [ /* Project history */ ],    ""skills"": [ /* Skills and technologies */ ],    ""education"": { /* Schools, training, certifications */ },    ""affiliation"": { /* Clubs, groups, and associations */ },    ""service"": { /* Volunteer, military, civilian service */ },    ""disposition"": { /* Disposition towards work, relocation, schedule */ },    ""writing"": [ /* Writing, blogging, and publications */ ],    ""reading"": [ /* Books and publication a la StackOverflow Careers */ ],    ""speaking"": [ /* Writing, blogging, and publications */ ],    ""governance"": [ /* Board memberships, committees, standards groups */ ],    ""recognition"": [ /* Awards and commendations */ ],    ""samples"": [ /* Work samples and portfolio pieces */ ],    ""social"": [ /* Social networking & engagement */ ],    ""references"": [ /* Candidate references */ ],    ""testimonials"": [ /* Public candidate testimonials */ ],      ""extracurricular"": [ /* Interests & hobbies */ ],    ""interests"": [ /* Interests & hobbies */ ],    ""languages"": [ /* languages spoken */ ]  }  ```    ...which you can use to generate resumes and other career artifacts in specific  concrete formats (HTML, LaTeX, Markdown, PDF, etc.) as well as enable  21st-century analysis of your resume and career data in a way that's not  possible with traditional, 20th-century resume tools and formats.    ## Anatomy of a FRESH Resume    FRESH resumes are:    - Text-based.  - Versionable.  - Standards-compliant.  - Human-readable/editable.  - Computer-friendly / easily parsable by tools.  - Built from JSON or equivalent YAML.  - Used to generate specific formats like HTML, PDF, or LaTeX.  - Free from proprietary structures or site- and/or tool-specific lock-in.    ## License    The FRESH resume schema is licensed under MIT. Go crazy.    [f]: https://freshstandard.org  [hmr]: https://fluentdesk.com/hackmyresume  [fresh]: https://resume.freshstandard.org  [schema]: schema/fresh-resume-schema.json  [cli]: https://www.npmjs.com/package/fluentcv  [fluentcv]: https://fluentdesk.com/fluentcv  [jrs]: http://jsonresume.org  [exemplar]: https://github.com/fluentdesk/jane-q-fullstacker/blob/master/resume/jane-resume.json  [npm]: https://www.npmjs.com/package/fluentcv """
Semantic web;https://github.com/AtomGraph/Processor;"""AtomGraph Processor is a server of declarative, read-write Linked Data applications. If you have a triplestore with RDF data that you want to serve Linked Data from, or write RDF over a RESTful HTTP interface, AtomGraph Processor is the only component you need.    What AtomGraph Processor provides for users as out-of-the-box generic features:  * API logic in a single [Linked Data Templates](https://atomgraph.github.io/Linked-Data-Templates/) ontology  * control of RDF input quality with SPARQL-based constraints  * SPARQL endpoint and Graph Store Protocol endpoint  * HTTP content negotiation and caching support    AtomGraph's direct use of semantic technologies results in extemely extensible and flexible design and leads the way towards declarative Web development. You can forget all about broken hyperlinks and concentrate on building great apps on quality data. For more details, see [articles and presentations](https://github.com/AtomGraph/Processor/wiki/Articles-and-presentations) about AtomGraph.    For a compatible frontend framework for end-user applications, see [AtomGraph Web-Client](https://github.com/AtomGraph/Web-Client).    # Getting started    * [how AtomGraph Processor works](https://github.com/AtomGraph/Processor/wiki/How-Processor-works)  * [Linked Data Templates](https://github.com/AtomGraph/Processor/wiki/Linked-Data-Templates)  * [installing AtomGraph Processor](https://github.com/AtomGraph/Processor/wiki/Installation)    For full documentation, see the [wiki index](https://github.com/AtomGraph/Processor/wiki).    # Usage    ## Docker    Processor is available from Docker Hub as [`atomgraph/processor`](https://hub.docker.com/r/atomgraph/processor/) image.  It accepts the following environment variables (that become webapp context parameters):    <dl>      <dt><code>ENDPOINT</code></dt>      <dd><a href=""https://www.w3.org/TR/sparql11-protocol/"">SPARQL 1.1 Protocol</a> endpoint</dd>      <dd>URI</dd>      <dt><code>GRAPH_STORE</code></dt>      <dd><a href=""https://www.w3.org/TR/sparql11-http-rdf-update/"">SPARQL 1.1 Graph Store Protocol</a> endpoint</dd>      <dd>URI</dd>      <dt><code>ONTOLOGY</code></dt>      <dd><a href=""https://atomgraph.github.io/Linked-Data-Templates/"">Linked Data Templates</a> ontology</dd>      <dd>URI</dd>      <dt><code>AUTH_USER</code></dt>      <dd>SPARQL service HTTP Basic auth username</dd>      <dd>string, optional</dd>      <dt><code>AUTH_PWD</code></dt>      <dd>SPARQL service HTTP Basic auth password</dd>      <dd>string, optional</dd>      <dt><code>PREEMPTIVE_AUTH</code></dt>      <dd>use premptive HTTP Basic auth?</dd>      <dd><code>true</code>/<code>false</code>, optional</dd>  </dl>    If you want to have your ontologies read from a local file rather than their URIs, you can define a custom [location mapping](https://jena.apache.org/documentation/notes/file-manager.html#the-locationmapper-configuration-file) that will be appended to the system location mapping.  The mapping has to be a file in N3 format and mounted to the `/usr/local/tomcat/webapps/ROOT/WEB-INF/classes/custom-mapping.n3` path. Validate the file syntax beforehand to avoid errors.    To enable logging, mount `log4j.properties` file to `/usr/local/tomcat/webapps/ROOT/WEB-INF/classes/log4j.properties`.    ### Examples    The examples show Processor running with combinations of  * default and custom LDT ontologies  * local and remote SPARQL services  * Docker commands    However different combinations are supported as well.    #### Default ontology and a local SPARQL service    The [Fuseki example](https://github.com/AtomGraph/Processor/tree/master/examples/fuseki) shows how to run a local [Fuseki](https://jena.apache.org/documentation/fuseki2/) SPARQL service together with Processor and how to setup [nginx](https://www.nginx.com) as a reverse proxy in front of Processor. Fuseki loads RDF dataset from a file. Processor uses a built-in LDT ontology.  It uses the [`docker-compose`](https://docs.docker.com/compose/) command.    Run the Processor container together with [Fuseki](https://hub.docker.com/r/atomgraph/fuseki) and [nginx](https://hub.docker.com/_/nginx) container:        cd examples/fuseki            docker-compose up    After that, open one of the following URLs in the browser and you will retrieve RDF descriptions:  * [`http://localhost:8080/`](http://localhost:8080/) - root resource  * [`http://localhost/`](http://localhost/) - root resource where the hostname of the Processor's base URI is rewritten to `example.org`    Alternatively you can run `curl http://localhost:8080/` etc. from shell.    In this setup Processor is also available on `http://localhost/` which is the nginx host.  The internal hostname rewriting is done by nginx and useful in situations when the Processor hostname is different from the application's dataset base URI and SPARQL queries do not match any triples.  The [dataset](https://github.com/AtomGraph/Processor/blob/master/examples/fuseki/dataset.ttl) for this example contains a second `http://example.org/` base URI, which works with the rewritten `example.org` hostname.    #### Custom ontology and a remote SPARQL service    The [Wikidata example](https://github.com/AtomGraph/Processor/tree/master/examples/wikidata) example shows to run Processor with a custom LDT ontology and a remote SPARQL service.  It uses the [`docker run`](https://docs.docker.com/engine/reference/run/) command.    Run the Processor container with the Wikidata example:        cd examples/wikidata            docker-compose up    After that, open one of the following URLs in the browser and you will retrieve RDF descriptions:  * [`http://localhost:8080/`](http://localhost:8080/) - root resource  * [`http://localhost:8080/birthdays`](http://localhost:8080/birthdays) - 100 people born today  * [`http://localhost:8080/birthdays?sex=http%3A%2F%2Fwww.wikidata.org%2Fentity%2FQ6581072`](http://localhost:8080/birthdays?sex=http%3A%2F%2Fwww.wikidata.org%2Fentity%2FQ6581072) - 100 females born today  * [`http://localhost:8080/birthdays?sex=http%3A%2F%2Fwww.wikidata.org%2Fentity%2FQ6581097`](http://localhost:8080/birthdays?sex=http%3A%2F%2Fwww.wikidata.org%2Fentity%2FQ6581097) - 100 males born today    Alternatively you can run `curl http://localhost:8080/` etc. from shell.    _Note that Wikidata's SPARQL endpoint [`https://query.wikidata.org/bigdata/namespace/wdq/sparql`](https://query.wikidata.org/bigdata/namespace/wdq/sparql) is very popular and therefore often overloaded. An error response received by the SPARQL client from Wikidata will result in `500 Internal Server Error` response by the Processor._    ## Maven    Processor is released on Maven central as [`com.atomgraph:processor`](https://search.maven.org/artifact/com.atomgraph/processor/).    # Datasource    AtomGraph Processor does *not* include an RDF datasource. It queries RDF data on the fly from a SPARQL endpoint using [SPARQL 1.1 Protocol](https://www.w3.org/TR/sparql11-protocol/) over HTTP. SPARQL endpoints are provided by most RDF [triplestores](http://en.wikipedia.org/wiki/Triplestore).    The easiest way to set up a SPARQL endpoint on an RDF dataset is Apache Jena [Fuseki](https://jena.apache.org/documentation/fuseki2/) as a Docker container using our [fuseki](https://hub.docker.com/r/atomgraph/fuseki) image. There is also a number of of [public SPARQL endpoints](http://sparqles.ai.wu.ac.at).    For a commercial triplestore with SPARQL 1.1 support see [Dydra](https://dydra.com).    # Test suite    Processor includes a basic HTTP [test suite](https://github.com/AtomGraph/Processor/tree/master/http-tests) for Linked Data Templates, SPARQL Protocol and the Graph Store Protocol.    ![master](https://github.com/AtomGraph/Processor/workflows/HTTP-tests/badge.svg?branch=master)  ![develop](https://github.com/AtomGraph/Processor/workflows/HTTP-tests/badge.svg?branch=develop)    # Support    Please [report issues](https://github.com/AtomGraph/Processor/issues) if you've encountered a bug or have a feature request.    Commercial consulting, development, and support are available from [AtomGraph](https://atomgraph.com).    # Community    Please join the W3C [Declarative Linked Data Apps Community Group](http://www.w3.org/community/declarative-apps/) to discuss  and develop AtomGraph and declarative Linked Data architecture in general. """
Semantic web;https://github.com/mro/librdf.sqlite;"""  [![Build Status](https://travis-ci.org/mro/librdf.sqlite.svg)](https://travis-ci.org/mro/librdf.sqlite)    Improved [SQLite](http://sqlite.org) RDF triple [storage module](http://librdf.org/docs/api/redland-storage-modules.html)  for [librdf](http://librdf.org/).    Cross platform, plain C source file. Comes with a [![Version](https://img.shields.io/cocoapods/v/librdf.sqlite.svg)](https://github.com/CocoaPods/Specs/tree/master/Specs/librdf.sqlite/) for those targeting iOS.    Inspired by the [official sqlite store](https://github.com/dajobe/librdf/blob/master/src/rdf_storage_sqlite.c).    ## Usage    ```c  #include ""rdf_storage_sqlite_mro.h""  ....  librdf_world *world = librdf_new_world();  librdf_init_storage_sqlite_mro(world);  // register storage factory  ....  const char* options = ""new='yes', contexts='no'"";  librdf_storage *newStorage = librdf_new_storage(world, LIBRDF_STORAGE_SQLITE_MRO, file_path, options);  ```    See e.g. in (my) <http://purl.mro.name/ios/librdf.objc>.    ## License    - `test/minunit.h`, Copyright (C) 2002 [John Brewer](http://jera.com), NO WARRANTY,  - *all others*, Copyright (C) 2014-2015 [Marcus Rohrmoser mobile Software](http://mro.name/~me), [Human Rights License](LICENSE)    ## Design Goals    | Quality         | very good | good | normal | irrelevant |  |-----------------|:---------:|:----:|:------:|:----------:|  | Functionality   |           |      |    ×   |            |  | Reliability     |           |  ×   |        |            |  | Usability       |           |      |        |     ×      |  | Efficiency      |     ×     |      |        |            |  | Changeability   |           |  ×   |        |            |  | Portability     |           |      |    ×   |            |    Currently 50% code and 99% runtime saving (for 100k triples).    - intense use of [SQLite prepared statements](https://www.sqlite.org/c3ref/stmt.html) and    [bound values](https://www.sqlite.org/c3ref/bind_blob.html):    - no stringbuffers    - no strcpy/memcpy,    - no SQL escaping,  - re-use compiled statements where possible (at the cost of thread safety),  - as few SQL statements as possible (at the cost of some non-trivial ones),  - SQLite indexes (at the cost of larger DB files). """
Semantic web;https://github.com/AKSW/RDFUnit;"""RDFUnit - RDF Unit Testing Suite  ==========    [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.aksw.rdfunit/rdfunit-parent/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.aksw.rdfunit/rdfunit-parent)  [![Build Status](https://travis-ci.org/AKSW/RDFUnit.svg?branch=master)](https://travis-ci.org/AKSW/RDFUnit)  [![Coverity Scan Build Status](https://scan.coverity.com/projects/2650/badge.svg?flat=1)](https://scan.coverity.com/projects/2650)  [![Coverage Status](https://coveralls.io/repos/AKSW/RDFUnit/badge.svg?branch=master&service=github)](https://coveralls.io/github/AKSW/RDFUnit?branch=master)  [![Codacy Badge](https://api.codacy.com/project/badge/grade/02907c27b76141709e5a6e9682fc836c)](https://www.codacy.com/app/jimkont/RDFUnit)  [![codebeat badge](https://codebeat.co/badges/fc781acc-0a9f-4796-9d33-28d1ffb3b019)](https://codebeat.co/projects/github-com-aksw-rdfunit)  [![Project Stats](https://www.openhub.net/p/RDFUnit/widgets/project_thin_badge.gif)](https://www.ohloh.net/p/RDFUnit)        **Homepage**: http://rdfunit.aksw.org <br/>  **Documentation**: https://github.com/AKSW/RDFUnit/wiki  <br/>  **Slack #rdfunit**: https://dbpedia-slack.herokuapp.com/ <br/>  **Mailing list**: https://groups.google.com/d/forum/rdfunit (rdfunit [at] googlegroups.com)  <br/>  **Presentations**: http://www.slideshare.net/jimkont  <br/>  **Brief Overview**: https://github.com/AKSW/RDFUnit/wiki/Overview      RDFUnit is implemented on top of the [Test-Driven Data Validation Ontology](http://rdfunit.aksw.org/ns/core#) and designed to read and produce RDF that complies to that ontology only.  The main components that RDFUnit reads are  [TestCases (manual & automatic), TestSuites](https://github.com/AKSW/RDFUnit/wiki/TestCases),  [Patterns & TestAutoGenerators](https://github.com/AKSW/RDFUnit/wiki/Patterns-Generators).  RDFUnit also strictly defines the results of a TestSuite execution along with [different levels of result granularity](https://github.com/AKSW/RDFUnit/wiki/Results).    ### Contents   * [Basic Usage](#basic-usage)   * [Using Docker](#using-docker)   * [Supported Schemas](#supported-schemas)   * [Acknowledgements](#acknowledgements)      ### Basic usage    See [RDFUnit from Command Line](https://github.com/AKSW/RDFUnit/wiki/CLI) or `bin/rdfunit -h` for (a lot) more options but the simplest setting is as follows:    ```console  $ bin/rdfunit -d <local-or-remote-location-URI>  ```    What RDFUnit will do is:    1. Get statistics about all properties & classes in the dataset  1. Get the namespaces out of them and try to dereference all that exist in [LOV](http://lov.okfn.org)  1. Run our [Test Generators](https://github.com/AKSW/RDFUnit/wiki/Patterns-Generators) on the schemas and generate RDFUnit Test cases  1. Run the RDFUnit test cases on the dataset  1. You get a results report in html (by default) but you can request it in [RDF](http://rdfunit.aksw.org/ns/core#) or even multiple serializations with e.g.  `-o html,turtle,jsonld`    * The results are by default aggregated with counts, you can request different levels of result details using `-r {status|aggregate|shacl|shacllite}`. See [here](https://github.com/AKSW/RDFUnit/wiki/Results) for more details.    You can also run:  ```console  $ bin/rdfunit -d <dataset-uri> -s <schema1,schema2,schema3,...>  ```    Where you define your own schemas and we pick up from step 3. You can also use prefixes directly (e.g. `-s foaf,skos`) we can get everything that is defined in [LOV](http://lov.okfn.org).    ### Using Docker    A Dockerfile is provided to create a Docker image of the CLI of RDFUnit.    To create the Docker image:    ```console  $ docker build -t rdfunit .  ```    It is meant to execute a rdfunit command and then shutdown the container. If the output of rdfunit on stdout is not enough or you want to include files in the container, a directory could be mounted via Docker in order to create the output/result there or include files.    Here an example of usage:    ```console  $ docker run --rm -it rdfunit -d https://awesome.url/file -r aggregate  ```    This creates a temporary Docker container which runs the command, prints the results on stdout and stops plus removes itself. For further usage of CLI visit https://github.com/AKSW/RDFUnit/wiki/CLI.    ### Supported Schemas    RDFUnit supports the following types of schemas    1. **OWL** (using CWA): We pick the most commons OWL axioms as well as schema.org. (see [[1]](https://github.com/AKSW/RDFUnit/labels/OWL),[[2]](https://github.com/AKSW/RDFUnit/issues/20) for details)  1. **SHACL**: Full SHACL is almost available except for [a few SHACL constructs](https://github.com/AKSW/RDFUnit/issues/62). Whatever constructs we support can also run directly on SPARQL Endpoints  1. IBM **Resource Shapes**: The progress is tracked [here](https://github.com/AKSW/RDFUnit/issues/23) but as soon as SHACL becomes stable we will drop support for RS  1. **DSP** (Dublin Core Set Profiles): The progress is tracked [here](https://github.com/AKSW/RDFUnit/issues/22) but as soon as SHACL becomes stable we will drop support for DSP    Note that you can mix all of these constraints together and RDFUnit will validate the dataset against all of them.    ### Acknowledgements    The first version  of RDFUnit (formely known as Databugger) was developed by AKSW as part of the PhD thesis of Dimitris Kontokostas.   A lot of additional work for improvement, requirements & refactoring was performed through the [EU funded project ALIGNED](http://aligned-project.eu). Through the project, a lot of project partners provided feedback and contributed code like e.g.  Wolters Kluwers Germany and Semantic Web Company that are also users of RDFUnit.    There are also many [code contributors](https://github.com/AKSW/RDFUnit/graphs/contributors) as well as people submitted bug reports or provided constructive feedback.    In addition, RDFUnit used [Java profiler (JProfiler)](http://www.ej-technologies.com/products/jprofiler/overview.html) for optimizations """
Semantic web;https://github.com/DataFabricRus/datastudio-sparql-connector;"""# SPARQL Connector for Google Data Studio    It allows to load data from a SPARQL endpoint using SELECT queries.    ![Connector in the gallery](screenshot_ingallery.png)    ## Example report    ![Example report on Google Datastudio](screenshot_examplereport.png)    Here is [link](https://datastudio.google.com/open/1Mbwa4VUue5Z9Ke4TsN6FcsQMiQX2uDPC) to the example report.    ## Getting started    1. Open the [link](https://datastudio.google.com/datasources/create?connectorId=AKfycbzDHEBN9qHXPni4xO4P2cIZtyQ3rnYmzkCnVsnh9oEJrnhGe4MntBF-t1zAu2Lm-Vjc) to create a new Data Source.  1. Once authorization has successfully completed, you're ready to configure the parameters. You should see the form:        ![Screenshot of the configuration page](screenshot_parameters.png)    1. Enter the SPARQL endpoint URL, e.g. http://dbpedia.org/sparql  1. Enter your SELECT query, e.g.        ```      PREFIX dbr: <http://dbpedia.org/resource/>      PREFIX dbo: <http://dbpedia.org/ontology/>      PREFIX foaf: <http://xmlns.com/foaf/0.1/>      PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>        SELECT ?name ?gender ?birthDate WHERE {        ?person dbo:birthPlace dbr:Berlin ;                dbo:birthDate ?birthDate ;                foaf:name ?name ;                foaf:gender ?gender .           FILTER (?birthDate > ""{dateRange.startDate}""^^xsd:date && ?birthDate < ""{dateRange.endDate}""^^xsd:date) .      }      ```        The following variables are supported:        * `dateRange.startDate` - format `YYYY-MM-DD`, e.g. 2018-10-01,      * `dateRange.endDate` - format `YYYY-MM-DD`, e.g. 2018-10-01,      * `dateRange.numDays` - it's a positive integer value.        If you don't use `dateRange.startDate` or `dateRange.endDate`, then **Date range** filter won't be able to control the date range.    1. Enter the schema of projections, e.g.        ```      [          {""name"": ""name"", ""dataType"": ""STRING""},          {""name"": ""gender"", ""dataType"": ""STRING""},          {""name"": ""birtDate"", ""dataType"": ""STRING""}      ]      ```        At this step is enough to set only data types for each projection, at the next step you'll be able to refine it. More about the schema elements, data types you can read in https://developers.google.com/datastudio/connector/semantics.    1. Press **Connect** button and the next page is the same for all connectors.    ## Supported data type conversions    Google Data Studio uses it's own formats for some of the data types. Therefore the connector automatically converts them. The following data types are supported:    * `xsd:date` is converted to `YYYYMMDD`,  * `xsd:dateTime` is converted to `YYYYMMDDHH`,  * `xsd:duration` is converted to an integer corresponding to the number of seconds.    ## Default values    The connector may apply default values in query results which don't a value for a requested field. The default values:    Datatype   | Default value  -----------|--------------  `NUMBER`   | `0`  `BOOLEAN`  | `false`  `STRING`   | `""""`    If you don't like these defaults, then write your query in a way when it can't have missing/empty values, especially in case of `GROUP BY`.    ## License    MIT License   """
Semantic web;https://github.com/davideceolin/rcsvw;"""# rcsvw    Package that implements the candidate recommendations from the W3C CSV on the Web Working Group.    # Copyright / License    ## rcsvw package    Copyright (C) 2011-2015  Davide Ceolin    ## Authors / Contributors    Author: Davide Ceolin    This package relies on an extended version of the rrdf package. The instructions below allow installing this version of rrdf, before installing csvw.    # Install from R        > install.packages(""rJava"") # if not present already      > install.packages(""devtools"") # if not present already      > library(devtools)      > install_github(""davideceolin/rrdf"", ref=""extended"", subdir=""rrdflibs"")      > install_github(""davideceolin/rrdf"", ref=""extended"", subdir=""rrdf"", build_vignettes = FALSE)      > install_github(""davideceolin/rcsvw"") """
Semantic web;https://github.com/cygri/pubby;"""This is a Linked Data server that adds an HTML interface and  dereferenceable URLs on top of RDF data that sits in a SPARQL  store.    See [the Pubby website](http://www4.wiwiss.fu-berlin.de/pubby/)  for details and instructions. """
Semantic web;https://github.com/dotnetrdf/dotnetrdf;"""# Welcome    [![Build status](https://ci.appveyor.com/api/projects/status/f8wtq0qh4k6620sl/branch/master?svg=true)](https://ci.appveyor.com/project/dotNetRDFadmin/dotnetrdf/branch/master)      dotNetRDF is a powerful and flexible API for working with RDF and SPARQL in .NET environments.    dotNetRDF is licensed under the MIT License, see the LICENSE.txt file in this repository    ## Getting Started    The easiest way to get dotNetRDF is via NuGet. We provide the following packages:    - **dotNetRDF** - contains the core libraries. This includes support for reading and writing RDF; and for managing and querying RDF data in-memory.  - **dotNetRDF.Data.DataTables** - a package which integrates RDF data with System.Data.DataTable  - **dotNetRDF.Data.Virtuoso** - provides support for using OpenLink Virtuoso as a backend store with dotNetRDF.  - **dotNetRDF.Query.FullText** - provides a full-text query plugin for dotNetRDF's Leviathan SPARQL query engine. The text indexing is provided by Lucene.  - **dotNetRDF.Query.Spin** - provides an implementation of [SPIN](http://spinrdf.org/) using dotNetRDF's Leviathan SPARQL query engine.  - **dotNetRDF.Web** - provides a framework for hosting RDF data in an IIS web application. This includes implementations of the SPARQL Protocol and SPARQL Graph Store Protocol.    We currently provide support for the following .NET frameworks:    - .NET 4.0  - .NET 4.0 Client Profile  - .NET Standard 2.0  	  ## Read The Docs!    To get started with using dotNetRDF you may want to check out the following resources:     - [User Guide](https://www.dotnetrdf.org/docs/stable/user_guide/index.html) - Series of articles detailing how to use various features of the library   - [Developer Guide](https://www.dotnetrdf.org/docs/stable/developer_guide/index.html) - Some advanced documentation    ## Asking Questions and Reporting Bugs    If you have a question about using dotNetRDF, please post it on [StackOverflow using the tag `dotnetrdf`](https://stackoverflow.com/questions/tagged/dotnetrdf).    Bugs and feature requests can be submitted to our [issues list on GitHub](https://github.com/dotnetrdf/dotnetrdf/issues). When submitting a bug report, please  include as much detail as possible. Code and/or data that reproduces the problem you are reporting will make it much more likely that your issue gets addressed   quickly.    ## Developers    dotNetRDF is developed by the following people:     - Rob Vesse   - Tomasz Pluskiewicz   - Ron Michael Zettlemoyer   - Max   - Khalil Ahmed   - Samu Lang   - Giacomo Citi    dotNetRDF also benefits from many community contributors who contribute in the form of bug reports, patches, suggestions and other feedback,   please see the [Acknowledgements](https://github.com/dotnetrdf/dotnetrdf/blob/master/Acknowledgments.txt) file for a full list.    ## Pull Requests    We are always pleased to receive pull requests that fix bugs or add features.   When fixing a bug, please make sure that it has been reported on the [issues list](https://github.com/dotnetrdf/dotnetrdf/issues) first.  If you plan to work on a new feature for dotNetRDF, it would be good to raise that on the issues list before you commit too much time to it.   """
Semantic web;https://github.com/lukostaz/prissma;"""PRISSMA  ===========  ### Context-Aware Adaptation for Linked Data    [PRISSMA](http://wimmics.inria.fr/projects/prissma) is a presentation-level framework for [Linked Data](http://linkeddata.org) adaptation.    It is a Java rendering engine for [RDF](http://www.w3.org/TR/rdf11-primer/) that selects the most appropriate presentation of RDF triples according to [mobile context](http://en.wikipedia.org/wiki/Context_awareness).    PRISSMA is compatible with the [Fresnel vocabulary](http://www.w3.org/2005/04/fresnel-info/manual/) and is based on a graph edit distance algorithm that finds optimal error-tolerant subgraph isomorphisms between RDF context graphs.    PRISSMA is optimized for Android platforms, but can be used in regular Java Projects as well.      # Installation    PRISSMA is a Java library, optimized for Android applications.    ## Minimum Requirements    + Java 1.6  + Android 4.2.2    ## Download  The latest PRISSMA release is [v1.0.0](https://github.com/lukostaz/prissma/releases/tag/v1.0.0). Download it and add it to your build path.    Make sure `config.properties` is included in your build path and that it contains the  parameter values needed by the search algorithm (e.g. [similarity threshold, cost of edit distance operations, etc](http://2014.eswc-conferences.org/sites/default/files/papers/paper_81.pdf)).      ## Build from Source  Check out sources:    	$ git clone https://github.com/lukostaz/prissma.git    PRISSMA depends on the [Simmetrics](https://github.com/Simmetrics/simmetrics) and [WS4J](https://code.google.com/p/ws4j/) libraries, that are not available in Maven central. Simmetrics 1.6.2 and WS4J 1.0.1 are provided under `libs/`. You can install it in your local Maven repository:  	      $ cd prissma            $ mvn install:install-file \            -Dfile=libs/simmetrics-1.6.2.jar \            -DgroupId=simmetrics \            -DartifactId=simmetrics \            -Dversion=1.6.2 \            -Dpackaging=jar        $ mvn install:install-file \            -Dfile=libs/ws4j-1.0.1.jar \            -DgroupId=ws4j \            -DartifactId=ws4j \            -Dversion=1.0.1 \            -Dpackaging=jar    PRISSMA depends on the `xerces-impl` Android-optimized version available [available here](http://elite.polito.it/index.php/research/downloads/182-jena-on-android-download). A copy of the library is available under `libs/`.  Add it to local Maven repository:    	$ mvn install:install-file \            -Dfile=libs/xercesImpl-repack.jar \            -DgroupId=xerces \            -DartifactId=xercesImpl-repack \            -Dversion=1.0.0 \            -Dpackaging=jar      Build and install the library in local Maven repository:  	      $ mvn install    Add the following dependency in your `pom.xml`:    	<dependency>          <groupId>fr.inria.wimmics</groupId>          <artifactId>PRISSMA</artifactId>          <version>1.0.0</version>      </dependency>      # Designing Prisms    Prisms are context-aware presentation metadata for Linked Data visualization based on Fresnel and PRISSMA.    Prisms can be created manually, or with [PRISSMA Studio](http://luca.costabello.info/prissma-studio/).    ## Example    Prism to style a `dbpedia:Museum` when a user is walking in Paris.    First, define the Prism general information:	  ```turtle  :museumPrism a prissma:Prism ;     fresnel:purpose :walkingInParis ;     fresnel:stylesheetLink  <style.css>.  ```    Add some Fresnel Lenses:  ```turtle  :museumlens a fresnel:Lens;     fresnel:group :museumPrism;     fresnel:classLensDomain dbpedia:Museum;     fresnel:showProperties (                         dbpprop:location                        dbpprop:publictransit                        ex:telephone                       ex:openingHours                       ex:ticketPrice ) .  ```    Add Fresnel styling metadata:    ```turtle  :addressFormat a fresnel:Format ;     fresnel:group :museumPrism ;     fresnel:propertyFormatDomain                        dbpprop:location ;     fresnel:label ""Address"" ;     fresnel:labelStyle          ""css-class1""^^fresnel:styleClass ;     fresnel:valueStyle          ""css-class2""^^fresnel:styleClass .  ```    Finally, define a `prissma:Context` entity with the [PRISSMA vocabulary](http://ns.inria.fr/prissma/v2/prissma_v2.html):  ```turtle  # PRISSMA context description  :walkingInParisArtLover a prissma:Context ;     prissma:user :artLover ;      prissma:environment :parisWalking .        :artLover a prissma:User ;     foaf:interest ""art"".    :parisWalking a prissma:Environment ;     prissma:poi :paris ;     prissma:motion ""walking"" .  	  :paris geo:lat ""48.8567"" ;     geo:long ""2.3508"" ;     prissma:radius ""5000"" .  ```  Save the Prism locally, and store it in the Decomposition structure as explained below.        # API Overview    Make sure `config.properties` is included in your build path and that it contains the  parameters values needed by the search algorithm (e.g. [similarity threshold, cost of edit distance operations, etc](http://2014.eswc-conferences.org/sites/default/files/papers/paper_81.pdf)).    ## Step 1: Decomposing Prisms    ```java  // Instantiate a decomposer  Decomposer decomposer = new Decomposer();  // The Decomposition is the shared data structure for Prisms  Decomposition decomp = new Decomposition();    // The inputPrism Jena Model is the Prism read from local repository  Model inputPrism = ModelFactory.createDefaultModel();    // Get the path of the Prism local repository on Android devices.  // If executed on desktop environment p is a regular file path string.  String p = Environment.getExternalStorageDirectory().getAbsolutePath();  InputStream in = FileManager.get().open( p + ""/PRISSMA/prisms/prism.ttl"" );    // Decompose the Prism  if (in != null) {      inputPrism.read(in, null,  ""TURTLE"");      decomp = decomposer.decompose(inputPrism, decomp);  }    ```    ## Step 2: Running the search algorithm    ```java  // Read input context  Model actualCtx = ModelFactory.createDefaultModel();  InputStream inCtx = FileManager.get().open( p + ""/PRISSMA/ctx/ctx1.ttl"" );  if (inCtx != null) {      actualCtx.read(inCtx, null,  ""TURTLE"");  }    // Instantiate an error-tolerant matcher with a decomposition  Matcher matcher = new Matcher(decomp);  // get the prissma:Context element, i.e. the root element of input context  RDFNode ctxRoot = ContextUnitConverter.getRootCtxNode(actualCtx);  // Covnert core PRISSMA entities to their PRISSMA classes  ctxRoot = ContextUnitConverter.switchToClasses(ctxRoot, decomp);  // Execute error-tolerant match against Prisms in the decomposition  matcher.search(ctxRoot);    ```    ## Step 3: Rendering resources    ```java  // inputResource is the desired RDF resource to display.  Model prism = readPrismFromFS(matcher.results);  Renderer r = new Renderer();  String html = r.renderHTML(prism, inputResource, true);    ```      # Publications      + L. Costabello. [Error-Tolerant RDF Subgraph Matching for Adaptive Presentation of Linked Data on Mobile](http://2014.eswc-conferences.org/sites/default/files/papers/paper_81.pdf). 11th Extended Semantic Web Conference (ESWC), 2014.  + L. Costabello. [PRISSMA, Towards Mobile Adaptive Presentation of the Web of Data](http://iswc2011.semanticweb.org/fileadmin/iswc/Papers/DC_Proposals/70320273.pdf). ISWC 2011 Doctoral Consortium, Bonn, Germany    # Licence  	      Copyright (C) 2013-2015 Luca Costabello, v1.0.0        This program is free software; you can redistribute it and/or modify it      under the terms of the GNU General Public License as published by the      Free Software Foundation; either version 2 of the License, or (at your      option) any later version.        This program is distributed in the hope that it will be useful, but      WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY      or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License      for more details.        You should have received a copy of the GNU General Public License along      with this program; if not, see <http://www.gnu.org/licenses/>.    # Contacts  Further details on the [PRISSMA Project Page](http://wimmics.inria.fr/projects/prissma/), or contact [Luca Costabello](http://luca.costabello.info).   """
Semantic web;https://github.com/joshsh/sesametools;"""<!-- This README can be viewed at https://github.com/joshsh/sesametools/wiki -->    ![SesameTools logo|width=322px|height=60px](https://github.com/joshsh/sesametools/wiki/graphics/SesameTools-logo.png)    Welcome to the SesameTools wiki!  SesameTools is a collection of general-purpose components for use with the [Sesame](http://rdf4j.org) RDF framework.  It includes:    * **SesameTools common utilities**: miscellaneous useful classes  * **CachingSail**: an in-memory cache for RDF data  * **ConstrainedSail**: a Sail implementation which interacts only with given named graphs.  Useful for simple access control.  * **DeduplicationSail**: a Sail implementation which avoids duplicate statements.  For use with triple stores such as AllegroGraph which otherwise allow duplicates.  * [LinkedDataServer](https://github.com/joshsh/sesametools/wiki/LinkedDataServer): a RESTful web service to publish a Sesame data store as Linked Data  * **MappingSail**: a Sail which translates between two URI spaces.  Used by LinkedDataServer.  * **RDFTransactionSail**: a write-only Sail which generates RDF transactions instead of performing per-call updates.  Useful for streaming RDF data as described [here](http://arxiv.org/abs/1011.3595)  * **ReadOnlySail**: a read-only Sail implementation  * **ReplaySail**: a pair of Sail implementations which allow Sail operations to be first recorded to a log file, then reproduced from the log file  * **RepositorySail**: a Sail implementation which wraps a Repository object.  This is essentially the inverse of Sesame's [SailRepository](http://rdf4j.org/sesame/2.7/apidocs/org/openrdf/repository/sail/SailRepository.html)  * **Sesamize**: command-line tools for Sesame  * **URI Translator**: a utility which runs SPARQL-1.1 Update queries against a Repository to convert URIs between different prefixes  * **WriteOnlySail**: a write-only Sail implementation    See also the [Sesametools API](http://fortytwo.net/projects/sesametools/api/latest/index.html).    For projects which use Maven, SesameTools snapshots and release packages can be imported by adding configuration like the following to the project's POM:    ```xml          <dependency>              <groupId>net.fortytwo.sesametools</groupId>              <artifactId>linked-data-server</artifactId>              <version>1.10</version>          </dependency>  ```    The latest Maven packages can be browsed [here](http://search.maven.org/#search%7Cga%7C1%7Csesametools).    **Credits**: SesameTools is written and maintained by [Joshua Shinavier](https://github.com/joshsh) and [Peter Ansell](https://github.com/ansell). Patches have been contributed by [Faisal Hameed](https://github.com/faisal-hameed), [Florian Kleedorfer](https://github.com/fkleedorfer), [Olaf Görlitz](https://github.com/goerlitz), and [Michal Klempa](https://github.com/michalklempa). An RDF/JSON parser and writer present in releases 1.7 and earlier contain code by [Hannes Ebner](http://ebner.wordpress.com/). """
Semantic web;https://github.com/pebbie/triplify;"""triplify  ========    Continuing the triplify.org PHP project by AKSW, and merging with triplify-python by rgeorgy     """
Semantic web;https://github.com/dvcama/LodLive;"""LodLive  =======    browse the web of data - a SPARQL navigator    LodLive is an experimental project that was set-up to spread and promote the linked-open-data philosophy and to create a tool that can be used for connecting RDF browser capabilities with the effectiveness of data graph representation.     LodLive is the first navigator to use RDF resources based solely on SPARQL endpoints.     LodLive aims to demonstrate how resources published by W3C standards for the semantic web can be made easily accessible and legible with a few viable tools.     LodLive is capable of connecting the resources existing in its configured endpoints, allowing the user to pass from one endpoint to another by making use of LOD interconnection capacities.    ## About us  LodLive is ideated and maintained by Diego Valerio Camarda and Alessandro Antonuccio with the support of Silvia Mazzini.    Diego is the RDF guy behind the technology (see https://www.linkedin.com/in/dvcama)     Alessandro is the designer responsible for the interface and the UX (see http://hstudio.it)   """
Semantic web;https://github.com/camwebb/owlconvert;"""owlconvert  ==========    > A very simple OWL format converter based on [OWLAPI](http://owlapi.sourceforge.net/2.x.x/index.html).    I wanted a local tool to handle format conversion similar to the  online converter at  [cs.Manchester](http://owl.cs.manchester.ac.uk/converter/), but  couldn’t find anything handy, ready made.  Before this hack, I knew  close to zero java, so please be lenient!    # Usage          $ owlconvert  manchester|functional|turtle|rdfxml  <owl infile>    sends to standard out.    # Installation    To compile, place in the same dir as `owlapi-bin.jar`          $ javac -classpath owlapi-bin.jar owlconvert.java    Run with (replacing with correct paths):          $ java -classpath ""owlapi-bin.jar:."" owlconvert    Make a shell script or alias to run from anywhere:          $ alias owlconvert 'java -classpath ""$OWLAPI:$OWLCONVERTPATH"" owlconvert'   """
Semantic web;https://github.com/cyberborean/rdfbeans;"""# RDFBeans framework    RDFBeans is an object-RDF mapping framework for Java. It provides ORM-like databinding functionality for RDF databases (""triplestores"") with two basic techniques:      * Classic object persistence: storing and retrieving the state of POJO objects      to/from a RDF model      * Dynamic proxy mechanism to access RDF data with Java interfaces mapped directly       to the underlying model    RDFBeans is built upon [Eclipse RDF4J](http://rdf4j.org/) (Sesame) API  to provide object persistence for a number of state-of-the-art   RDF triplestore implementations.    RDFBeans is based on Java Annotations mechanism.   No special interfaces and superclasses is required, that guarantees minimum   modifications of existing codebase and compatibility with other POJO-oriented   frameworks.         ##Features      * Cascade databinding to reduce development time and ensure referential integrity of complex object models      * Inheritance of RDFBeans annotations from superclasses and/or interfaces          * No external specifications (RDF-schemas) are required: everything is declared by the annotations      * Extensible mechanism of mapping Java types to RDF literals: you can define your own algorithms to represent your data structures with RDF      * Support of basic Java Collections, optionally represented with RDF containers        * Transactions support (triplestore-specific)          * Support of indexed JavaBean properties      * Support of RDF namespaces     """
Semantic web;https://github.com/linkeddata/rdflib.js;"""# rdflib.js  [![NPM Version](https://img.shields.io/npm/v/rdflib.svg?style=flat)](https://npm.im/rdflib)  [![Join the chat at https://gitter.im/linkeddata/rdflib.js](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/linkeddata/rdflib.js?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    Javascript RDF library for browsers and Node.js.    - Reads and writes RDF/XML, Turtle and N3; Reads RDFa and JSON-LD  - Read/Write Linked Data client, using WebDav or SPARQL/Update  - Real-Time Collaborative editing with web sockets and PATCHes  - Local API for querying a store  - Compatible with [RDFJS task force spec](https://github.com/rdfjs/representation-task-force/blob/master/interface-spec.md)  - SPARQL queries (not full SPARQL - just graph match and optional)  - Smushing of nodes from `owl:sameAs`, and `owl:{f,inverseF}unctionProperty`  - Tracks provenance of triples keeps metadata (in RDF) from HTTP accesses    ## Documentation    See:    * The [API documentation](https://linkeddata.github.io/rdflib.js/doc/) is partial but useful  * [Tutorial: Using rdflib in a Solid web app](https://linkeddata.github.io/rdflib.js/Documentation/webapp-intro.html)  * [Tutorial: Using rdflib.js](https://github.com/solid/solid-tutorial-rdflib.js)  * [Tutorial: Using Turtle](https://linkeddata.github.io/rdflib.js/Documentation/turtle-intro.html)  * [Using authenticated & alternate fetch methods](https://linkeddata.github.io/rdflib.js/Documentation/alternate-fetches.md)  * [Block diagram: rdflib modules](https://linkeddata.github.io/rdflib.js/Documentation/diagrams/rdflib-block-diagram.svg)  * [Block diagram: The Fetcher](https://linkeddata.github.io/rdflib.js/Documentation/diagrams/fetcher-block-diagram.svg)  * [Block diagram: The Fetcher - handling retries](https://linkeddata.github.io/rdflib.js/Documentation/diagrams/fetcher-block-diagram-2.svg)  * [Block diagram: The Update Manager](https://linkeddata.github.io/rdflib.js/Documentation/diagrams/update-manager-diagram.svg)      * [The Solid developer portal at Inrupt](https://solid.inrupt.com/)    for more information.    ## Install    #### Browser (using a bundler like Webpack)    ```bash  npm install rdflib  ```    #### Browser (generating a `<script>` file to include)    ```bash  git clone git@github.com:linkeddata/rdflib.js.git;  cd rdflib.js;  npm install;  ```    Generate the dist directory    ```bash  npm run build:browser  ```    #### Node.js    Make sure you have Node.js and Node Package Manager ([npm](https://npmjs.org/))  installed.    ```bash  npm install --save rdflib  ```    ## Contribute    #### Subdirectories    - `dist`: Where the bundled libraries are built. Run `npm run build` to generate them.  - `test`: Tests are here.  - `lib`: Transpiled, non-bundled library is built here when the library is    published to npm.    #### Dependencies        - XMLHTTPRequest (Node.js version)    ## Thanks    Thanks to the many contributors who have been involved along the way.  LinkedData team & TimBL    ## LICENSE  MIT """
Semantic web;https://github.com/balhoff/whelk;"""<img align=""right"" src=""https://farm7.staticflickr.com/6205/6045158767_e70d43139d_m_d.jpg"">    # Whelk    Whelk is an OWL reasoner based on the algorithm implemented in [ELK](https://github.com/liveontologies/elk-reasoner), as described in [The Incredible ELK](https://doi.org/10.1007/s10817-013-9296-3).    Whelk is implemented as an [immutable functional data structure](https://en.wikipedia.org/wiki/Purely_functional_data_structure), so that each time axioms are added, a new reasoner state is created. References to the previous state remain unchanged. This allows Whelk to answer queries concurrently, and also allows rapid classification of multiple datasets which build upon the same ontology Tbox, which can be classified ahead of time and stored. However, for basic classification of a single ontology, Whelk is much slower than ELK.    In addition to OWL EL classification, Whelk provides OWL RL and a subset of SWRL for reasoning on individuals.    ## Use cases    Whelk is based on ELK, and ELK is much faster at classifying an ontology. Some reasons to use Whelk include:  - object property assertion materialization  - SWRL rules (class and object property atoms)  - OWL RL features for Abox  - some classification for unions in superclass position (e.g., infer least common subsumer of operands)    - Example:       ```      C SubClassOf B      D SubClassOf B      E EquivalentTo C or D      then      E SubClassOf B      ```    - *this feature is useful, but not guaranteed to be complete*  - extended support for Self restrictions, supporting rolification  - in application code, submitting many DL queries programmatically (Whelk is much faster at this)  - in application code, performing many DL queries in parallel, non-blocking  - in application code, storing a reasoning state (e.g., Tbox classification) and simultaneously extending it with multiple independent new axiom sets (e.g., Aboxes); or, quickly rolling back to a previous, saved reasoning state    ## Status    Whelk is under development. It works, but its Scala API is in flux. A basic implementation of the OWL API OWLReasoner interface is included, but to make use of its immutability features, Whelk is best used from Scala. """
Semantic web;https://github.com/AtomGraph/Web-Client;"""AtomGraph Web-Client is a Linked Data web client. If you have a triplestore with RDF data that you want to publish  and/or build an end-user application on it, or would like to explore Linked Open Data, Web-Client provides the components you need.    Web-Client renders (X)HTML user interface by transforming [""plain"" RDF/XML](https://jena.apache.org/documentation/io/rdf-output.html#rdfxml) (without nested resource descriptions)  using [XSLT 2.0](https://www.w3.org/TR/xslt20/) stylesheets.    ![AtomGraph Web-Client screenshot](https://raw.github.com/AtomGraph/Web-Client/master/screenshot.png)    Features  ========    What AWC provides for users as out-of-the-box generic features:  * loading RDF data from remote Linked Data sources  * multilingual, responsive user interface built with Twitter Bootstrap (currently [2.3.2](https://getbootstrap.com/2.3.2/))  * multiple RDF rendering modes (currently item/list/table/map)  * RDF editing mode based on [RDF/POST](http://www.lsrn.org/semweb/rdfpost.html) encoding  * SPARQL endpoint with interactive results    Getting started  ===============    * [what is Linked Data](https://github.com/AtomGraph/Web-Client/wiki/What-is-Linked-Data)  * [installing Web-Client](https://github.com/AtomGraph/Web-Client/wiki/Installation)  * [extending Web-Client](https://github.com/AtomGraph/Web-Client/wiki/Extending-Web-Client)    For full documentation, see the [wiki index](https://github.com/AtomGraph/Web-Client/wiki).    Usage  =====    Docker  ------    Processor is available from Docker Hub as [`atomgraph/web-client`](https://hub.docker.com/r/atomgraph/web-client/) image.  It accepts the following environment variables (that become webapp context parameters):    <dl>      <dt><code>STYLESHEET</code></dt>      <dd>Custom XSLT stylesheet</dd>      <dd>URI, optional</dd>      <dt><code>RESOLVING_UNCACHED</code></dt>      <dd>If <code>true</code>, the stylesheet will attempt to resolve (dereference) URI resources in the rendered RDF data to improve the UX</dd>      <dd><code>true</code>/<code>false</code>, optional</dd>  </dl>    Run Web-Client with the [default XSLT stylesheet](https://github.com/AtomGraph/Web-Client/blob/master/src/main/webapp/static/com/atomgraph/client/xsl/bootstrap/2.3.2/layout.xsl) like this:        docker run -p 8080:8080 atomgraph/web-client    Web-Client will be available on [http://localhost:8080](http://localhost:8080).    Maven  -----    Web-Client is released on Maven central as [`com.atomgraph:client`](https://search.maven.org/artifact/com.atomgraph/client/).    Support  =======    Please [report issues](https://github.com/AtomGraph/Web-Client/issues) if you've encountered a bug or have a feature request.    Commercial AtomGraph consulting, development, and support are available from [AtomGraph](https://atomgraph.com).    Community  =========    Please join the W3C [Declarative Linked Data Apps Community Group](http://www.w3.org/community/declarative-apps/) to discuss  and develop AtomGraph and declarative Linked Data architecture in general. """
Semantic web;https://github.com/kbss-cvut/jb4jsonld-jackson;"""# Java Binding for JSON-LD - Jackson    [![Build Status](https://kbss.felk.cvut.cz/jenkins/buildStatus/icon?job=jaxb-jsonld-jackson)](https://kbss.felk.cvut.cz/jenkins/job/jaxb-jsonld-jackson)    Java Binding for JSON-LD - Jackson (JB4JSON-LD-Jackson) is a binding of JB4JSON-LD for [Jackson](https://github.com/FasterXML/jackson).    The core implementation of JB4JSON-LD with a mapping example can be found at [https://github.com/kbss-cvut/jb4jsonld](https://github.com/kbss-cvut/jb4jsonld).    More info can be found at [https://kbss.felk.cvut.cz/web/kbss/jb4json-ld](https://kbss.felk.cvut.cz/web/kbss/jb4json-ld).    ## Usage    JB4JSON-LD is based on annotations from [JOPA](https://github.com/kbss-cvut/jopa), which enable POJO attributes  to be mapped to ontological constructs (i.e. to object, data or annotation properties) and Java classes to ontological  classes.    Use `@OWLDataProperty` to annotate data fields and `@OWLObjectProperty` to annotate fields referencing other mapped entities.    To integrate the library with Jackson, register a `cz.cvut.kbss.jsonld.jackson.JsonLdModule` in Jackson's `ObjectMapper` like this:    `objectMapper.registerModule(new JsonLdModule())`    and you should be good to go. See the `JsonLdSerializionTest` for examples.    See [https://github.com/kbss-cvut/jopa-examples/tree/master/jsonld](https://github.com/kbss-cvut/jopa-examples/tree/master/jsonld) for  an executable example of JB4JSON-LD-Jackson in action.    ## Serialization    The serializer's output has been verified to be a valid JSON-LD and is parseable by Java's JSON-LD reference implementation   [jsonld-java](https://github.com/jsonld-java/jsonld-java).    The output is basically a context-less compacted JSON-LD, which uses full IRIs for attribute names.    ## Deserialization    Since we are using jsonld-java to first process the incoming JSON-LD, it does not matter in which form (expanded, framed, flattened) the  input is.    ## Getting JB4JSON-LD-Jackson    There are two ways to get JB4JSON-LD-Jackson:    * Clone repository/download zip and build it with Maven,  * Use a [Maven dependency](http://search.maven.org/#search%7Cga%7C1%7Ccz.cvut.kbss.jsonld):    ```XML  <dependency>      <groupId>cz.cvut.kbss.jsonld</groupId>      <artifactId>jb4jsonld-jackson</artifactId>  </dependency>  ``` """
Semantic web;https://github.com/nxparser/nxparser;"""# Welcome to NxParser #    NxParser is a Java open source, streaming, non-validating parser for the Nx format, where x = Triples, Quads, or any other number. For more details see the specification for the [NQuads format](http://sw.deri.org/2008/07/n-quads/), a extension for the [N-Triples](http://www.w3.org/TR/rdf-testcases/#ntriples) RDF format. Note that the parser handles any combination (cf. [generalised triples](http://www.w3.org/TR/rdf11-concepts/#section-generalized-rdf)) or number of N-Triples syntax terms on each line (the number of terms per line can also vary).    It ate 2 mil. quads (~4GB, (~240MB GZIPped)) on a T60p (Win7, 2.16 GHz)  in ~1 min 35 s (1:18min). Overall, it's more than twice as fast as the previous version when it comes to reading Nx.    The NxParser is non-validating, meaning that, e.g., it will happily eat non-conformant N-Triples. Also, the NxParser will not parse certain valid N-Triples files where the RDF terms are not separated by whitespace. We pass all positive W3C N-Triples test cases except one, where the RDF terms are not separated by whitespace (surprise!).    ## Other formats ##  The NxParser Parser family also includes a [RDF/XML](http://www.w3.org/TR/rdf-syntax-grammar/) and a [Turtle](http://www.w3.org/TR/turtle/) parser. Moreover, we attached a [JSON-LD](http://json-ld.org/) parser ([jsonld-java](https://github.com/jsonld-java/jsonld-java)) and a [RDFa](http://www.w3.org/TR/rdfa-core/) parser ([semargl](https://github.com/levkhomich/semargl)) such that they emit Triples in the NxParser API.    ## Binaries ##  Compiles are available on Maven Central. The groupId is `org.semanticweb.yars`. Depending on what part you need, you have to choose the artifactId accordingly: For example, if you only want to use the data model, use `nxparser-model`. If you want to make use of the parsers, use `nxparser-parsers`. If you want to use the RDF support for JAX-RS, use `nxparser-jax-rs`. The modules are linked as required.  ```xml  <dependency>    <groupId>org.semanticweb.yars</groupId>    <artifactId>nxparser-parsers</artifactId>    <version>2.3.3</version>  </dependency>    ```    ### Legacy binaries ###  Find old compiles in the repository on Google Code, which we do not maintain any more. To use it nevertheless, add  ```xml  <repository>   <id>nxparser-repo</id>   <url>    http://nxparser.googlecode.com/svn/repository   </url>  </repository>  <repository>   <id>nxparser-snapshots</id>   <url>    http://nxparser.googlecode.com/svn/snapshots   </url>  </repository>  ```  to your pom.xml.    ## Code Examples ##  ### Read Nx from a file ###  ```java  FileInputStream is = new FileInputStream(""path/to/file.nq"");    NxParser nxp = new NxParser();  nxp.parse(is);    for (Node[] nx : nxp)    // prints the subject, eg. <http://example.org/>    System.out.println(nx[0]);  ```    ### Use a blank node ###  ```java  // true means you are supplying proper N-Triples RDF terms that do not need to be processed  Resource subjRes = new Resource(""<http://example.org/123>"", true);  Resource predRes = new Resource(""<http://example.org/123>"", true);  BNode bn = new BNode(""_:bnodeId"", true);    Node[] triple = new Node[]{subjRes, predRes, bn};  // yields <http://example.org/123> <http://example.org/123> _:bnodeId  System.out.println(Arrays.toString(triple));  ```    ### Use Unicode-characters ###  ```java  String japaneseString = (""祝福は、チーズのメーカーです。"");  Literal japaneseLiteral = new Literal(japaneseString, ""ja"");    // yields ""\u795D\u798F\u306F\u3001\u30C1\u30FC\u30BA\u306E\u30E1\u30FC\u30AB\u30FC\u3067\u3059\u3002""@ja  System.out.println(japaneseLiteral);    // yields 祝福は、チーズのメーカーです。  System.out.println(japaneseLiteral.getLabel());  ```    ### Use datatyped literals ###  Example: Get a Calendar object from an `xsd:dateTime`-typed Literal  ```java  Literal dtl; // parser-generated  XSDDateTime dt = (XSDDateTime)DatatypeFactory.getDatatype(dtl);   GregorianCalendar cal = dt.getValue();  ```    ### Use from Python ###  Provided you use the Jython implementation (thanks to Uldis Bojars, this is saved from his now offline blog).    ```python  import sys  sys.path.append(""./nxparser.jar"")  	   from org.semanticweb.yars.nx.parser import *  from java.io import FileInputStream  from java.util.zip import GZIPInputStream  	   def all_triples(fname, use_gzip=False):    in_file = FileInputStream(fname)    if use_gzip:        in_file = GZIPInputStream(in_file)  	     nxp = NxParser()    nxp.parse(in_file)  	     while nxp.hasNext():      triple = nxp.next()      n3 = ([i.toString() for i in triple])      yield n3  ```  The code above defines a generator function which will yield a stream of NQuad records. We can now add some demo code in order to see it in action:  ```python  def main():    gzfname = ""sioc-btc-2009.gz""       for line in all_triples(gzfname, use_gzip=True):      print line  	     if __name__ == ""__main__"":      main()  ```  results in:  ```python  [u'<http://2008.blogtalk.net/node/29>', u'<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>', u'<http://rdfs.org/sioc/ns#Post>', u'<http://2008.blogtalk.net/sioc/node/29>']  [u'<http://2008.blogtalk.net/node/65>', u'<http://rdfs.org/sioc/ns#content>', u'""We\'ve created a map showing the main places of interest (event locations, restaurants, pubs, shopping locations and tourist sights) during BlogTalk 2008.  The conference venue is shown on the left-hand side of the map.  We will also have a hardcopy for all attendees. View Larger Map""', u'<http://2008.blogtalk.net/sioc/node/65>']  ```  	  #### issues with Eclipse ####  we had an issue with eclipse not being able to create his folder structure for nxparser-parsers, ```` mvn eclipse:eclipse ```` did the trick. """
Semantic web;https://github.com/albertmeronyo/LSD-Dimensions;"""LSD Dimensions  ==============    All dimensions and Data Structure Definitions (DSDs) of Linked  Statistical Data, codes and concept schemes associated to them, and  endpoints using them.    ## What is this?    LSD Dimensions is an aggregator of all `qb:DataStructureDefinition`  and `qb:DimensionProperty` (and their associated triples) that can be  currently found in the Linked Data Cloud (read: the SPARQL endpoints  in Datahub.io). Its purpose is to improve the reusability of  statistical dimensions, codes and concept schemes in the Web of Data,  providing an interface for users (future work: to programs) to search  for resources commonly used to describe open statistical datasets. It  also looks for ways of using semantic descriptions of these resources  to enhance comparability of statistical datasets in Linked Data.    ## How does it work?    1. Querying the Datahub.io API for a list of endpoints  2. Querying these endpoints for DSDs, dimensions, codes and concept schemes  3. Storing all in a messy MongoDB instance  4. Displaying it using Boostrap Table    ## Online instance    See http://lsd-dimensions.org    ## Dependencies    - Python 2.7.5  - SPARQL Wrapper  - pymongo  - Bottle  - Bootstrap  - Bootstrap Table """
Semantic web;https://github.com/cosminbasca/rdftools;"""rdftools  ========    rdftools is a python wrapper over a number of RDF related tools  * rdf parsers / serializers  * void utilities  * lubm generator  * etc    Important Notes  ---------------  This software is the product of research carried out at the [University of Zurich](http://www.ifi.uzh.ch/ddis.html) and comes with no warranty whatsoever. Have fun!    TODO's  ------  * The project is not documented (yet)    How to Compile/Install the Project  ----------------------------------  Ensure that *libraptor2* v2.0.13+ and *cityhash* are installed on your system (either using the package manager of the OS or compiled from source).    To install **rdftools** you have two options: 1) manual installation (install requirements first) or 2) automatic with **pip**    **Manual** installation:  ```sh  $ git clone https://github.com/cosminbasca/rdftools  $ cd rdftools  $ python setup.py install  ```    Install the project with **pip**:  ```sh  $ pip install https://github.com/cosminbasca/rdftools  ```    Also have a look at the build.sh, clean.sh, test.sh scripts included in the codebase     To include the latest JVM RDF tools update to the latest of [jvmrdftools](https://github.com/cosminbasca/jvmrdftools) and create an assembly:    ```sh  $ sbt compile assembly  ```    copy the resulting jar from the target folder to the *lib* folder inside the *rdftools.tools.jvmrdftools* module and reinstall the python package.      The tools  ---------    To find out what a tool does, simply supply the *--help* comand line argument to any of the tools  Available tools:    * rdfconvert, convert RDF files from source format to a destination format using the *libraptor2* C RDF parser    ```sh  usage: rdfconvert [-h] [--clear] [--dst_format DST_FORMAT]                    [--buffer_size BUFFER_SIZE] [--version]                    SOURCE    rdftools v0.9.2, rdf converter, based on libraptor2    positional arguments:    SOURCE                the source file or location (of files) to be converted    optional arguments:    -h, --help            show this help message and exit    --clear               clear the original files (delete) - this action is                          permanent, use with caution!    --dst_format DST_FORMAT                          the destination format to convert to. Supported                          parsers: ['rdfxml', 'ntriples', 'turtle', 'trig',                          'guess', 'rss-tag-soup', 'rdfa', 'nquads', 'grddl'].                          Supported serializers ['rdfxml', 'rdfxml-abbrev',                          'turtle', 'ntriples', 'rss-1.0', 'dot', 'html',                          'json', 'atom', 'nquads'].    --buffer_size BUFFER_SIZE                          the buffer size in Mb of the input buffer (the parser                          will only parse XX Mb at a time)    --version             the current version  ```    * rdfconvert2 convert RDF files from source format to a destination format using the *rdf2rdf* java RDF parser    ```sh  usage: rdfconvert2 [-h] [--clear] [--dst_format DST_FORMAT]                     [--workers WORKERS] [--version]                     SOURCE    rdftools v0.9.2, rdf converter (2), makes use of rdf2rdf bundled - requires  java    positional arguments:    SOURCE                the source file or location (of files) to be converted    optional arguments:    -h, --help            show this help message and exit    --clear               clear the original files (delete) - this action is                          permanent, use with caution!    --dst_format DST_FORMAT                          the destination format to convert to    --workers WORKERS     the number of workers (default -1 : all cpus)    --version             the current version  ```    * rdfencode, endode an ntriples file to a binary format (each S, P, O string is hashed with *cityhash* 64 bit)    ```sh  usage: rdfencode [-h] [--version] SOURCE    rdftools v0.9.2, encode the RDF file(s)    positional arguments:    SOURCE      the source file or location (of files) to be encoded    optional arguments:    -h, --help  show this help message and exit    --version   the current version  ```    * genlubm, generate a **LUBM** dataset (in parallel)    ```sh  usage: genlubm [-h] [--univ UNIV] [--index INDEX] [--seed SEED]                 [--ontology ONTOLOGY] [--workers WORKERS] [--version]                 OUTPUT    rdftools v0.9.2, lubm dataset generator wrapper (bundled) - requires java    positional arguments:    OUTPUT               the location in which to save the generated                         distributions    optional arguments:    -h, --help           show this help message and exit    --univ UNIV          number of universities to generate    --index INDEX        start university    --seed SEED          the seed    --ontology ONTOLOGY  the lubm ontology    --workers WORKERS    the number of workers (default -1 : all cpus)    --version            the current version  ```    * genlubmdistro generate a **LUBM** dataset (in parallel) and mix the universities to *N* sites with the specified distribution    ```sh  usage: genlubmdistro [-h] [--distro DISTRO] [--univ UNIV] [--index INDEX]                       [--seed SEED] [--ontology ONTOLOGY] [--pdist PDIST]                       [--sites SITES] [--clean] [--workers WORKERS] [--version]                       OUTPUT    rdftools v0.9.4, lubm dataset generator wrapper (bundled) - requires java    positional arguments:    OUTPUT               the location in which to save the generated                         distributions    optional arguments:    -h, --help           show this help message and exit    --distro DISTRO      the distibution to use, valid values are ['seedprop',                         'uni2many', 'horizontal', 'uni2one']    --univ UNIV          number of universities to generate    --index INDEX        start university    --seed SEED          the seed    --ontology ONTOLOGY  the lubm ontology    --pdist PDIST        the probabilities used for the uni2many distribution,                         valid choices are ['3S', '7S', '5S'] or file with                         probabilities split by line    --sites SITES        the number of sites    --clean              delete the generated universities    --workers WORKERS    the number of workers (default -1 : all cpus)    --version            the current version  ```  * genvoid, generate [VoID](http://www.w3.org/TR/void/) statistics from the source file    ```sh  usage: genvoid [-h] [--version] SOURCE    rdftools v0.9.2, generate void statistics for RDF source file    positional arguments:    SOURCE      the source file to be analized    optional arguments:    -h, --help  show this help message and exit    --version   the current version  ```    * genvoid2, generate [VoID](http://www.w3.org/TR/void/) statistics from the RDF source file, using the *nxparser VoID* exporter    ```sh  usage: genvoid2 [-h] [--dataset_id DATASET_ID] [--use_nx] [--version] SOURCE    rdftools v0.9.2, generate a VoiD descriptor using the nxparser java package    positional arguments:    SOURCE                the source file to be analized    optional arguments:    -h, --help            show this help message and exit    --dataset_id DATASET_ID                          dataset id    --use_nx              if true (default false) use the nx parser builtin void                          generator    --version             the current version  ```    * ntround, round all numeric literals (typed or untyped) in an ntriples files with the given precision    ```sh  usage: ntround [-h] [--prefix PREFIX] [--precision PRECISION] [--version] PATH    rdftools v0.9.2, rounds ntriple files in a folder, (rounds the floating point literals)    positional arguments:    PATH                  location of the indexes    optional arguments:    -h, --help            show this help message and exit    --prefix PREFIX       the prefix used for files that are transformed, cannot                          be the enpty string!    --precision PRECISION                          the precision to round to, if 0, floating point                          numbers are rounded to long    --version             the current version  ```    Thanks a lot to  ---------------  * [University of Zurich](http://www.ifi.uzh.ch/ddis.html) and the [Swiss National Science Foundation](http://www.snf.ch/en/Pages/default.aspx) for generously funding the research that led to this software. """
Semantic web;https://github.com/vocol/vocol;"""  VoCol - Vocabulary collaboration and build environment.  =====    Inspired by agile software and content development methodologies, the VoCol methodology and tool environment allows building leight-weight ontologies using version control systems such as Git and repository hosting platforms such as Github.   VoCol is implemented without dependencies on complex software components, it provides collaborators with comprehensible feedback on syntax and semantics errors in a tight loop, and gives access to a human-readable presentation of the vocabulary.   The VoCol environment is employing loose coupling of validation and documentation generation components on top of a standard Git repository.   All VoCol components, even the repository engine, can be exchanged with little effort.       ## Installation on a local machine or on a Web Server    The following steps are needed to setup the VoCol Environment either on a local machine or a web server. These steps are valid in the Linux-based operating systems and with slight modifications can be used in Windows-based as well.    1. You should have the following libraries installed: Java JDK, NodeJS, NPM, and Git packages with their respective versions or higher. For more info see in **[Required libraries and tools](https://github.com/vocol/vocol/wiki/Required-libraries-and-tools)**.     2. Create a new directory e.g. ""newFolder"", clone the VoCol repository, and give the execution permissions as follows:  ```  mkdir newFolder  cd newFolder  git clone https://github.com/vocol/vocol.git  chmod u+x  .  ```  4. Enter inside the ""VoCol"" folder and run the following script to clean up any not necessary file:  ```  cd vocol  ./helper/scripts/resetApp.sh  ```  5. Install the dependent packages (assuming that node package manager is installed already):  ```  sudo npm install  ```  Semantic-Ui framework is used in VoCol development, a couple of selections need to be given while installing it.   Select ""Skip install"" as follows:   ```  ? It looks like you have a semantic.json file already.    Yes, extend my current settings.  > Skip install  ```  Then ""Yes"" for that Vocol is using ""NPM Nice"".  ```  ? We detected you are using NPM Nice! Is this your project folder? D:\vocolrepo\vocol  > Yes    No, let me specify  ```  Finally, give ""public/semantic"" as the location of Sematic-Ui in VoCol Project.  ```  ? Where should we put Semantic UI inside your project? (semantic/) public/semantic/  ```  6. The last step is to start VoCol with **npm start [VocolPortNumber] [SparqlEndPointPortNumber]**. In the following command, we are going to start Vocol on port 3000 where Fuseki Server is runing at port 3030    ```  npm start 3000 3030  ```  8. You can access VoCol start page with http://localhost:3000 , if the port number was not changed. If you clear old data as step 4 describes, then the configuration page will be displayed. Otherwise, you can use http://localhost:3000/config URL for configuring of the VoCol. Sometimes, the port number is also changed during our project's development, for that, you have a possibility to look-up the vocol access's port number and as well change it, by opening **bin/www** file if you are on the root path of VoCol.    9. To keep your repository synchronized with VoCol instance (for example when you push something), you should configure **a webhook path** on the repository hosting platform such as Github, GitLab and BitBucket to point with the VoCol API: **http(s)://hostname(:port or /vocolInstancePath)/listener**. The connection between both hosting server and VoCol instance should be available in such a way that hosting platform can send the notification to the VoCol instance. Please the fundamental explanations of WebHooks in the following link: [https://developer.github.com/webhooks/](https://developer.github.com/webhooks/).    For more details about VoCol repository, please have a look on our [VoColWiki](https://github.com/vocol/vocol/wiki).        Check out a list of projects that are currently using [VoCol](https://vocol.iais.fraunhofer.de/).    Moreover, you can use the **docker image** of VoCol [here](https://hub.docker.com/r/ahemid/newvocol/) or use the included Dockerfile to build the docker image.    ## License    VoCol is licensed under the MIT License. See LICENSE.txt for more details. For respective licenses of individual components and libraries please refer to the **[Required libraries and tools](https://github.com/vocol/vocol/wiki/Required-libraries-and-tools)** section.     ## Current Work   We have extened this VoCol version to work As A Service in VoCoREG. Please, visit us on the following link: https://www.vocoreg.org or https://www.vocoreg.com   """
Semantic web;https://github.com/seebi/semweb.vim;"""# vim bundle for working with RDF knowledge bases    ## Installation    This bundle is created to work with `pathogen` so just copy the folder  into your bundle directory.    ## Features     * namespace completion   * syntax highlightning   * insert xsd:dateTime current time literal with abbrevation `xsdnow`    ## namespace completion    You can use ctrl-x+u (autocomplete user function) for completing  rdf prefixes (as `owl`, `skos` and `foaf`) to namespace URIs (e.g.  `http://xmlns.com/foaf/0.1/` for the `foaf` prefix).    This done with the multifunctional shell script  [rdf.sh](https://github.com/seebi/rdf.sh) which is a dependency and must  be available as `rdf` in the path.    ## syntax highlightning    ### N3 Syntax    I just pull the syntax file from [Niklas  Lindström](git://github.com/vim-scripts/n3.vim.git) here.    ### SPARQL Syntax    I just pull the syntax file from [Jeroen  Pulles](https://github.com/vim-scripts/sparql.vim) here.   """
Semantic web;https://github.com/linkeddata/rabel;"""# rabel - linked data format converter    Program for reading and writing linked data in various formats.    To install,        npm install -g rabel    ## Command line    Commands look like unix options are executed *in order* from left to right. They  include:  ```  -base=rrrr    Set the current base URI (relative URI, default is file:///$PWD)  -clear        Clear the current store  -dump         Serialize the current store in current content type  -format=cccc  Set the current content-type  -help         This message  -in=uri       Load a web resource or file  -out=filename Output in the current content type  -report=file  set the report file destination for future validation  -size         Give the current store  -spray=base   Write out linked data to lots of different linked files CAREFUL!  -test=manifest   Run tests as described in the test manifest  -validate=shapeFile   Run a SHACL validator on the data loaded by previous in=x  -version      Give the version of this program  ```    Formats cccc are given as MIME types. These can be used for input or output:     * text/turtle   *(default)*   * application/rdf+xml    whereas these can only input:     * application/rdfa   * application/xml     #### Examples    ```    rabel -format=application/xml -in=foo.xml -format=text/turtle -out=foo.ttl    rabel part*.ttl -out=whole.ttl  ```  ## Details  Currently rabel can read from the web or files, and write only to files.  Filenames are deemed to be relative URIs just taken relative to file:///{pwd}/ where {pwd} is the  current working directory.    One use case is testing all the parsers. Another is providing a stable serialization. The output serialization is designed to be stable under small changes of the the data, to allow data files to be checked into source code control systems.    The name comes from RDF and Babel.    ### XML    When loading XML, elements are mapped to arcs, and text content to trimmed RDF strings. For the XML namespace used for IANA registry documents, custom mapping is done, both of properties and datatypes, and local identifier generation.  (See the source for details!) """
Semantic web;https://github.com/GeoKnow/TripleGeo;"""<html>  <HEAD>  </head>  <body>    <div id=""readme"" class=""clearfix announce instapaper_body md"">  <article class=""markdown-body entry-content"" itemprop=""mainContentOfPage"">    <h1><a name=""welcome-to-triplegeo"" class=""anchor"" href=""#welcome-to-triplegeo""><span class=""octicon octicon-link""></span></a>TripleGeo: An open-source tool for extracting geospatial features into RDF triples</h1>    <p>TripleGeo is a utility developed by the <a href=""http://www.ipsyp.gr/"">Institute for the Management of Information Systems</a> at <a href=""http://www.athena-innovation.gr/en.html"">Athena Research Center</a> under the EU/FP7 project <a href=""http://geoknow.eu"">GeoKnow: Making the Web an Exploratory for Geospatial Knowledge</a>. This generic purpose, open-source tool can be used for integrating features from geospatial databases into RDF triples.</p>    <p>TripleGeo is based on open-source utility <a href=""https://github.com/boricles/geometry2rdf/tree/master/Geometry2RDF"">geometry2rdf</a>. TripleGeo is written in Java and is still under development; more enhancements will be included in future releases. However, all supported features have been tested and work smoothly in both MS Windows and Linux platforms.</p>    <p>The web site for <a href=""https://web.imis.athena-innovation.gr/redmine/projects/geoknow_public/wiki/TripleGeo"">TripleGeo</a> provides more details about the project, its architecture, usage tips, and foreseen extensions.</p>    <h2>  <a name=""quick-start"" class=""anchor"" href=""#Quick start""><span class=""octicon octicon-link""></span></a>Quick start</h2>    How to use TripleGeo:    You have 2 options: either build from source (using Apache Ant) or use the prepackaged binaries (JARs) shipped with this code.    <h4>1.a Build from source</h4>    <ul>  <li>Build (with ant):<br/>  <code>mkdir build</code><br/>  <code>ant compile</code>  </li>  <li>Package as a jar (with ant):<br/>  <code>ant package</code><br/>  If build finishes successfully, generated JARs will be placed under <code>build/jars</code>.  </li>  </ul>    <h4>1.b Use prepackaged JARs</h4>    In order to use TripleGeo for extracting triples from a spatial dataset, the user should follow these steps (in a Windows platform, but these are similar in Linux as well):  <ul>    <li>  Download the current software bundle from https://github.com/GeoKnow/TripleGeo/archive/master.zip</li>  <li>  Extract the downloaded .zip file into a separate folder, e.g., <code>c:\temp</code>.</li>  <li>  Open a terminal window (in DOS or in Linux) and navigate to the directory where TripleGeo has been extracted, e.g.,  <code>cd c:\temp\TripleGeo-master</code>. This directory must be the one that holds the LICENSE file. For convenience, this is where you can place your configuration file (e.g., options.conf), although you can specify another path for your configuration if you like.</li>  <li>Normally, under this same folder there must be a lib/ subdirectory with the required libraries. Make sure that the actual TripleGeo.jar is under the bin/ subdirectory.</li>  <li>Verify that Java JRE (or SDK) ver 1.7 or later is installed. Currently installed version of Java can be checked using <code>java –version</code> from the command line.</li>  <li>Next, specify all properties in the required configuration file, e.g., options.conf. You must specify correct paths to files (i.e., in[parameters inputFile, outputFile, and tmpDir), which are RELATIVE to the executable.</li>  <li>In case that triples will be extracted from ESRI shapefiles, give the following command (in one line):<br/>      <code>java -cp lib/*;bin/TripleGeo.jar eu.geoknow.athenarc.triplegeo.ShpToRdf options.conf</code><br/>  Make sure that the specified paths to .jar files are correct. You must modify these paths to the libraries and/or the configuration file, if you run this command from a path other than the one containing the LICENSE file, as specified in step (3).</li>  <li>While conversion is running, it periodically issues notifications about its progress. Note that for large datasets (i.e., hundreds of thousands of records), conversion may take several minutes. As soon as processing is finished and all triples are written into a file, the user is notified about the total amount of extracted triples and the overall execution time.</li>    </ul>    <h4>2. Usage and examples</h4>    <p>The current distribution comes with a dummy configuration file <code>options.conf</code>. This file contains indicative values for the most important properties when accessing data from ESRI shapefiles or a spatial DBMS. Self-contained brief instructions can guide you into the extraction process.</p>  <p>Run the jar file from the command line in several alternative modes, depending on the input data source (of course, you should change the directory separator to the one your OS understands, e.g. "":"" in the case of *nix systems):</p>    <p>In case that triples will be extracted from ESRI shapefiles, and assuming that binaries are bundled together in <code>triplegeo.jar</code>, give a command like this:</br>  <code>java -cp ""./lib/*;./build/jars/triplegeo.jar"" eu.geoknow.athenarc.triplegeo.ShpToRdf options.conf</code></p>  <p>Alternatively, if triples will be extracted from a geospatially-enabled DBMS (e.g., Oracle Spatial), give a command like this:</br>  <code>java -cp ""./lib/*;./build/jars/triplegeo.jar"" eu.geoknow.athenarc.triplegeo.wkt.RdbToRdf options.conf</code></p>    <p>Wait until the process gets finished, and verify that the resulting output file is according to your specifications.</p>    The current distribution also offers transformations from other geographical formats, and it also supports GML datasets aligned to EU INSPIRE Directive. More specifically, TripleGeo can transform into RDF triples geometries available in GML (Geography Markup Language) and KML (Keyhole Markup Language). It can also handle INSPIRE-aligned GML data for seven Data Themes (Annex I). Assuming that binaries are bundled together in <code>triplegeo.jar</code>, you may transform such datasets as follows:  <ul>  <li>In case that triples will be extracted from a GML file, give a command like this:</br>  <code>java -cp ""./lib/*;./build/jars/triplegeo.jar"" eu.geoknow.athenarc.triplegeo.GmlToRdf <input.gml> <output.rdf> </code></li>  <li>In case that triples will be extracted from a KML file, give a command like this:</br>  <code>java -cp ""./lib/*;./build/jars/triplegeo.jar"" eu.geoknow.athenarc.triplegeo.KmlToRdf <input.kml> <output.rdf> </code></li>  <li>In case that triples will be extracted from an INSPIRE-aligned GML file, you must first configure XSL stylesheet <code>Inspire_main.xsl</code> with specific parameters and then give a command like this:</br>  <code>java -cp ""./lib/*;./build/jars/triplegeo.jar"" eu.geoknow.athenarc.triplegeo.InspireToRdf <input.gml> <output.rdf> </code></li>  </ul>    An alternative way to run the TripleGeo utility (the jar file) is provided via ant targets:<br/>  in the case of a shapefile input:<br/>  <code>ant run-on-shp -Dconfig=options.conf</code><br/>  in the case of the relational database:<br/>  <code>ant run-on-rdb -Dconfig=options.conf</code><br/>  in the case of a GML input:<br/>  <code>ant run-on-gml -Dinput=sample.gml -Doutput=sample.rdf</code><br/>  in the case of a KML input:<br/>  <code>ant run-on-kml -Dinput=sample.kml -Doutput=sample.rdf</code><br/>  in the case of an INSPIRE-aligned XML input:<br/>  <code>ant run-on-inspire -Dinput=sample.xml -Doutput=sample.rdf</code><br/>    <p>Indicative configuration files for several cases are available <a href=""https://github.com/GeoKnow/TripleGeo/tree/master/test/conf/"">here</a> in order to assist you when preparing your own.    <h3>  <a name=""input"" class=""anchor"" href=""#Input""><span class=""octicon octicon-link""></span></a>Input</h3>    <p>The current version of TripleGeo utility can access geometries from:</p>  <ul>  <li>ESRI shapefiles, a widely used file-based format for storing geospatial features.</li>  <li>Geographical data stored in GML (Geography Markup Language) and KML (Keyhole Markup Language).</li>  <li>INSPIRE-aligned datasets for seven Data Themes (Annex I) in GML format: Addresses, Administrative Units, Cadastral Parcels, GeographicalNames, Hydrography, Protected Sites, and Transport Networks (Roads).</li>  <li>Several geospatially-enabled DBMSs, including: Oracle Spatial, PostGIS, MySQL, and IBM DB2 with Spatial extender.</li>  </ul>  </ul>    <p>Sample geographic <a href=""https://github.com/GeoKnow/TripleGeo/tree/master/test/data/"">datasets</a> for testing are available in ESRI shapefile format.</p>    <h3>  <a name=""output"" class=""anchor"" href=""#Output""><span class=""octicon octicon-link""></span></a>Output</h3>    <p>In terms of <i>output serializations</i>, triples can be obtained in one of the following formats: RDF/XML (<i>default</i>), RDF/XML-ABBREV, N-TRIPLES, N3, TURTLE (TTL).</p>  <p>Concerning <i>geospatial representations</i>, triples can be exported according to:</p>  <ul>  <li>the <a href=""https://portal.opengeospatial.org/files/?artifact_id=47664"">GeoSPARQL standard</a> for several geometric types (including points, linestrings, and polygons)</li>  <li>the <a href=""http://www.w3.org/2003/01/geo/"">WGS84 RDF Geoposition vocabulary</a> for point features</li>  <li>the <a href=""http://docs.openlinksw.com/virtuoso/rdfsparqlgeospat.html"">Virtuoso RDF vocabulary</a> for point features.</li>  </ul>    <p>Resulting triples are written into a local file, so that they can be readily imported into a triple store.</p>      <h2>  <a name=""license"" class=""anchor"" href=""#license""><span class=""octicon octicon-link""></span></a>License</h2>    <p>The contents of this project are licensed under the <a href=""https://github.com/GeoKnow/TripleGeo/blob/master/LICENSE"">GPL v3 License</a>.</p></article>    </body>  </html> """
Semantic web;https://github.com/pkumod/gStore;"""# gStore    gStore is a graph database engine for managing large graph-structured data, which is open-source and targets at Linux. This project is written in C++, with the help of some libraries such as readline, antlr, and so on. Only source tarballs are provided currently, which means you have to compile the source code if you want to use gStore.    **The formal help document is in [English(EN)](docs/help/gStore_help.pdf) and [中文(ZH)](docs/help/gStore_help_ZH.pdf).**    **The formal experiment result is in [Experiment](docs/test/formal_experiment.pdf).**    **We have built an IRC channel named #gStore on freenode, and you can visit [the homepage of gStore](http://gstore-pku.com).**    <!--**You can write your information in [survey](http://59.108.48.38/survey) if you like.**-->    ## Getting Started  ### Compile from Source  This system is really user-friendly and you can pick it up in several minutes. Remember to check your platform where you want to run this system by viewing [System Requirements](docs/DEMAND.md). After all are verified, please get this project's source code. There are several ways to do this:    - (suggested)type `git clone https://github.com/pkumod/gStore.git` in your terminal or use git GUI to acquire it    - download the zip from this repository and extract it    - fork this repository in your github account    Then you need to compile the project, for the first time you need to type `make pre` to prepare the `ANTLR` library and some Lexer/Parser programs.  Later you do not need to type this command again, just use the `make` command in the home directory of gStore, then all executables will be generated.  (For faster compiling speed, use `make -j4` instead, using how many threads is up to your machine)  To check the correctness of the program, please type `make test` command.    The first strategy is suggested to get the source code because you can easily acquire the updates of the code by typing `git pull` in the home directory of gStore repository.   In addition, you can directly check the version of the code by typing `git log` to see the commit logs.  If you want to use code from other branches instead of master branch, like 'dev' branch, then:    - clone the master branch and type `git checkout dev` in your terminal    - clone the dev branch directly by typing `git clone -b dev`    ### Deploy via Docker  You can easily deploy gStore via Docker. We provide both of Dockerfile and docker image. Please see our [Docker Deployment Doc(EN)](docs/DOCKER_DEPLOY_EN.md) or [Docker部署文档(中文)](docs/DOCKER_DEPLOY_CN.md) for details.    ### Run  To run gStore, please type `bin/gbuild database_name dataset_path` to build a database named by yourself. And you can use `bin/gquery database_name` command to query an existing database. What is more, `bin/ghttp` is a wonderful tool designed for you, as a database server which can be accessed via HTTP protocol. Notice that all commands should be typed in the root directory of gStore, and your database name should not end with "".db"".    - - -    ## Advanced Help    If you want to understand the details of the gStore system, or you want to try some advanced operations(for example, using the API, server/client), please see the chapters below.    - [Basic Introduction](docs/INTRO.md): introduce the theory and features of gStore    - [Install Guide](docs/INSTALL.md): instructions on how to install this system    - [How To Use](docs/USAGE.md): detailed information about using the gStore system    - [API Explanation](docs/API.md): guide you to develop applications based on our API    - [Project Structure](docs/STRUCT.md): show the whole structure and process of this project    - [Related Essays](docs/ESSAY.md): contain essays and publications related with gStore    - [Update Logs](docs/CHANGELOG.md): keep the logs of the system updates    - [Test Results](docs/TEST.md): present the test results of a series of experiments    - - -    ## Other Business    Bugs are recorded in [BUG REPORT](docs/BUGS.md).  You are welcomed to submit the bugs you discover if they do not exist in this file.    We have written a series of short essays addressing recurring challenges in using gStore to realize applications, which are placed in [Recipe Book](docs/TIPS.md).    You are welcome to report any advice or errors in the github Issues part of this repository, if not requiring in-time reply. However, if you want to urgent on us to deal with your reports, please email to <gjsjdbgroup@pku.edu.cn> to submit your suggestions and report bugs. A full list of our whole team is in [Mailing List](docs/MAIL.md).    There are some restrictions when you use the current gStore project, you can see them on [Limit Description](docs/LIMIT.md).    Sometimes you may find some strange phenomena(but not wrong case), or something hard to understand/solve(don't know how to do next), then do not hesitate to visit the [Frequently Asked Questions](docs/FAQ.md) page.    Graph database engine is a new area and we are still trying to go further. Things we plan to do next is in [Future Plan](docs/PLAN.md) chapter, and we hope more and more people will support or even join us. You can support in many ways:    - watch/star our project    - fork this repository and submit pull requests to us    - download and use this system, report bugs or suggestions    - ...    People who inspire us or contribute to this project will be listed in the [Thanks List](docs/THANK.md) chapter.    <!--This whole document is divided into different pieces, and each them is stored in a markdown file. You can see/download the combined markdown file in [help_markdown](docs/gStore_help.md), and for html file, please go to [help_html](docs/gStore_help.html). What is more, we also provide help file in pdf format, and you can visit it in [help_pdf](docs/latex/gStore_help.pdf).-->   """
Semantic web;https://github.com/oxigraph/rio;"""Rio  ===    [![actions status](https://github.com/oxigraph/rio/workflows/build/badge.svg)](https://github.com/oxigraph/rio/actions)    Rio is a low level library which provides conformant and fast parsers and formatters for RDF related file formats.    It currently provides [N-Triples](https://docs.rs/rio_turtle/latest/rio_turtle/struct.NTriplesParser.html), [N-Quads](https://docs.rs/rio_turtle/latest/rio_turtle/struct.NQuadsParser.html), [Turtle](https://docs.rs/rio_turtle/latest/rio_turtle/struct.TurtleParser.html), [TriG](https://docs.rs/rio_turtle/latest/rio_turtle/struct.TrigParser.html) and [RDF/XML](https://docs.rs/rio_xml/latest/rio_xml/struct.RdfXmlParser.html) parsers and formatters.    It is split into multiple crates:  * `rio_api` provides common traits and data structures to be used in Rio parsers (`Triple`, `TriplesParser`, `Iri`...).    [![Latest Version](https://img.shields.io/crates/v/rio_api.svg)](https://crates.io/crates/rio_api)     [![Released API docs](https://docs.rs/rio_api/badge.svg)](https://docs.rs/rio_api)  * `rio_turtle` provides conformant streaming parsers and formatters for [Turtle](https://www.w3.org/TR/turtle/), [TriG](https://www.w3.org/TR/trig/), [N-Triples](https://www.w3.org/TR/n-triples/) and [N-Quads](https://www.w3.org/TR/n-quads/).    [RDF-star](https://w3c.github.io/rdf-star/cg-spec/) syntaxes are also supported: [Turtle-star](https://w3c.github.io/rdf-star/cg-spec/#turtle-star), [TriG-star](https://w3c.github.io/rdf-star/cg-spec/#trig-star), [N-Triples-star](https://w3c.github.io/rdf-star/cg-spec/#n-triples-star) and [N-Quads-star](https://w3c.github.io/rdf-star/cg-spec/#n-quads-star).    [![Latest Version](https://img.shields.io/crates/v/rio_turtle.svg)](https://crates.io/crates/rio_turtle)    [![Released API docs](https://docs.rs/rio_turtle/badge.svg)](https://docs.rs/rio_turtle)  * `rio_xml` provides a conformant streaming parser and a formatter for [RDF/XML](https://www.w3.org/TR/rdf-syntax-grammar/).    [![Latest Version](https://img.shields.io/crates/v/rio_xml.svg)](https://crates.io/crates/rio_xml)    [![Released API docs](https://docs.rs/rio_xml/badge.svg)](https://docs.rs/rio_xml)    There is also the `rio_testsuite` crate that is used for testing Rio parsers against the [W3C RDF tests](http://w3c.github.io/rdf-tests/) to ensure their conformance.  It provides both an executable for building implementation reports and integration test to quickly ensure that the parsers stay conformant.  It is not designed to be used outside of Rio.      ## License    Copyright 2019-2021 The Rio developers.    Licensed under the Apache License, Version 2.0 (the ""License"");  you may not use this file except in compliance with the License.  You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License. """
Semantic web;https://github.com/solid/solid-spec;"""# Solid Specification Draft  [![](https://img.shields.io/badge/project-Solid-7C4DFF.svg?style=flat-square)](https://github.com/solid/solid)  [![Join the chat at https://gitter.im/solid/solid-spec](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/solid/solid-spec)    **Latest version:** [`v.0.7.0`](https://github.com/solid/solid-spec/tree/v0.7.0) (see [CHANGELOG.md](CHANGELOG.md))    **Publication status**: Unofficial Draft    **Current development version:** `v.0.7.0-next` (evolving)    **This document contains an informal description of implementation guidelines for Solid servers and clients.  A normative specification is in the making at https://github.com/solid/specification/.  For the time being, the present document contains the best approximation of expected server and client behavior.**    ## Table of Contents    1. [Overview](#overview)  2. [Identity](#identity)  3. [Profiles](#profiles)      * [WebID Profile Documents](#webid-profile-documents)  4. [Authentication](#authentication)      * [Primary Authentication](#primary-authentication)        * [WebID-TLS](#webid-tls)        * [Alternative Authentication            Mechanisms](#alternative-authentication-mechanisms)      * [Secondary Authentication: Account          Recovery](#secondary-authentication-account-recovery)  5. [Authorization and Access Control](#authorization-and-access-control)      * [Web Access Control](#web-access-control)  6. [Content Representation](#content-representation)  7. [Reading and Writing Resources](#reading-and-writing-resources)      * [HTTPS REST API](#https-rest-api)      * [WebSockets API](#websockets-api)  8. [Social Web App Protocols](#social-web-app-protocols)      * [Notifications](#notifications)      * [Friends Lists, Followers and          Following](#friends-lists-followers-and-following)  9. [Recommendations for Server        Implementation](#recommendations-for-server-implementations)  10. [Recommendations for Client App        Implementation](#recommendations-for-client-app-implementations)  11. [Examples](#examples)  12. [Current Implementations](#current-implementations)    ## Overview    [Solid](https://github.com/solid/solid)  is a proposed set of conventions and tools for building  *decentralized applications* based on [Linked  Data](https://www.w3.org/DesignIssues/LinkedData) principles. Solid is  modular and extensible. It relies as much as possible on existing  [W3C](http://www.w3.org/) standards and protocols.    See Also:    * [About Solid](https://github.com/solid/solid#about-solid)  * [Contributing to Solid](https://github.com/solid/solid#contributing-to-solid)    * [Pre-Requisites](https://github.com/solid/solid#pre-requisites)    * [Solid Project        Workflow](https://github.com/solid/solid#solid-project-workflow)  * [Standards Used](https://github.com/solid/solid#standards-used)  * [Platform Notes](https://github.com/solid/solid#solid-platform-notes)  * [Solid Project directory](https://github.com/solid/solid#project-directory)    ## Identity    Solid uses [WebID](http://www.w3.org/2005/Incubator/webid/spec/identity/) URIs  as universal usernames or actor identifiers. Frequently referred to simply as    *WebIDs*, these URIs form the basis of most other Solid-related technologies,  such as authentication, authorization, access control, user profiles, discovery  of user preferences and server capabilities, and more.    WebIDs provide globally unique decentralized identifiers, enable cross-service  federated signin, prevent service provider lock-in, and give users control over  their own identity. *The WebID URI's primary function is to point to the  location of a public [WebID Profile document](#profiles) (see below).*    **Example WebIDs:** `https://alice.databox.com/profile/card#me` or  `http://somepersonalsite.com/#webid`    ## Profiles    Solid uses WebID Profile Documents for management of user identity and security  credentials (such as public keys), and user preferences discovery.    Although here we mostly refer to them in the context of user profiles,  other types of actors use these profiles as well, such as groups, organizations,  devices, and software applications.    ### WebID Profile Documents    A WebID URI, when dereferenced, yields a WebID Profile Document in a  Linked Data format ([Turtle](http://www.w3.org/TR/turtle/) by default, but  often available as JSON-LD or HTML+RDFa). Parsing this document provides a  client application with useful information, such as the user's name and  profile image, links to user preferences and related documents, and lists of  public key certificates or other relevant identity credentials.    **See component spec:    [Solid WebID Profiles Specification](solid-webid-profiles.md)**    ## Authentication    Authentication is the process of determining a user’s identity, of asking the  question “How do I know you are who you say?”.    How do web applications typically authenticate users (that is, how do they  verify identity)? The most common method is usernames and passwords. A  *username* uniquely identifies a user (and ties them to a user profile), and a  *password* verifies that the user is who they say they are. Many applications or  services also have a *secondary authentication mechanism* (usually an external  email address) that they use for account recovery (in case the user forgets or  loses their primary authentication tokens, username and password).    Solid currently uses WebID-TLS as its primary authentication mechanism.  Alternative complementary mechanisms are also being actively investigated.  In addition, Solid recommends that server implementations also offer secondary  authentication available for users for Account Recovery (via email or some  other out-of-band mechanism).    ### Primary Authentication    Solid, being a decentralized web application platform, has a set of requirements  for its authentication mechanisms that are not commonly encountered by most  platforms and ecosystems. Specifically, it requires *cross-domain*,  de-centralized authentication mechanisms not tied to any particular identity  provider or certificate authority.    #### WebID-TLS    **Note:** Several browser vendors (Chrome, Firefox) have removed support  for the `KEYGEN` element, on which WebID-TLS relied for in-browser certificate  generation.    Solid uses the [WebID-TLS  protocol](http://www.w3.org/2005/Incubator/webid/spec/tls/) as one of its  primary authentication mechanism. Instead of usernames, it uses WebIDs as unique  identifiers, as previously mentioned. And instead of using passwords as bearer  tokens, it uses cryptographic certificates (stored and managed by the user's web  browser) to prove a user's identity.    When accessing a Solid server using WebID-TLS, a user is presented by their  web browsers with a popup asking them to select an appropriate security  certificate for that site. After a user makes their selection, the server  securely matches the private key stored by the browser with the public key  stored in that user's [WebID Profile Document](#webid-profile-documents), and  authenticates them.    **See component spec:    [Solid WebID-TLS Specification](authn-webid-tls.md)**    #### WebID-OIDC    The Solid team is currently implementing support for WebID-OIDC as another  primary authentication mechanism. It is based on the OAuth2/OpenID Connect  protocols, adapted for WebID based decentralized use cases.    **See component spec:    [WebID-OIDC Specification](https://github.com/solid/webid-oidc-spec)**    #### Alternative Authentication Mechanisms    There are several other authentication mechanisms that are  currently being investigated, such as combinations of traditional  username-and-password authentication and WebID-TLS Delegation).    ### Secondary Authentication: Account Recovery    Regardless of the primary authentication mechanism, bearer tokens and other  proofs of identity tend to get lost by users. Passwords can be forgotten,  browser certificates can be lost to hardware failure, and so on. Solid  recommends that secondary Account Recovery mechanisms are provided by server  implementers, to aid in these scenarios.    ## Authorization and Access Control    Authorization is the process of deciding whether a user has *access* to a  particular resource. If authentication asks ""who is the user?"", authorization  is concerned with ""what is the user allowed to do?"".    Solid currently uses the Web Access Control (WAC) mechanism for cross-domain  authorization for all its resources.    ### Web Access Control    [Web Access Control (WAC)](https://github.com/solid/web-access-control-spec) is  a decentralized system that allows different users and groups various forms of  access to resources where users and groups are identified by HTTP URIs. The  system is similar to the access control system used within many file systems  except that the documents controlled, the users, and the groups, are all  identified by URIs. Users are identified by WebIDs. Groups of users are  identified by the URI of a class of users which, if you look it up, returns a  list of users in the class. This means a WebID hosted by any server can be a  member of a group hosted some other server.    Users do not need to have an account (i.e. WebID) on a given server to have  access to documents on it.    **See component spec:  [Solid WAC Specification](https://github.com/solid/web-access-control-spec)**    ## Content Representation    Solid deals with reading and writing two kinds of resources:    1. Linked Data resources (RDF in the form of JSON-LD, Turtle, HTML+RDFa, etc)  2. Everything else (binary data and non-linked-data structured text)    While you can build Solid applications with non-linked data resources, using  actual RDF-based Linked Data provides you with considerable benefits in terms  of interoperability with the rest of the Solid app ecosystem.    Resources are grouped in directory-like **Containers** (currently conforming  to the [LDP Basic Container spec](https://www.w3.org/TR/ldp/#ldpbc)).    **See component spec: [Solid Content    Representation](content-representation.md)**    ## Reading and Writing Resources    ### HTTPS REST API    Solid extends the [Linked Data Platform spec](https://www.w3.org/TR/ldp/) to  provide a simple REST API for CRUD operations on resources and containers.    **See component spec: [HTTPS REST API](api-rest.md)**    ### WebSockets API    Solid also provides a WebSockets based API for a PubSub (Publish/Subscribe)  mechanism, through which clients can be notified in real time of  changes affecting a give resource.    **See component spec: [WebSockets API](api-websockets.md)**    ## Social Web App Protocols    In addition to read/write operations on resources, Solid provides a number of  specs and recommendations to help developers achieve interoperability between  various social web applications that are part of the ecosystem.    ### Notifications    **See component spec: [Linked Data Notifications](https://www.w3.org/TR/ldn/)**    ### Friends Lists, Followers and Following    API recommendations for managing subscriptions and friends lists are still  being discussed. TBD.    ## Recommendations for Server Implementations    **See component spec: [Recommendations for Server    Implementations](recommendations-server.md)**    ## Recommendations for Client App Implementations    **See component spec: [Recommendations for Client    Implementations](recommendations-client.md)**    ## Examples    * [User Posts a Note](examples/user-posts-note.md)    ## Current Implementations    **Server Implementations:** See  [solid/solid-platform](https://github.com/solid/solid-platform#servers) for a  list of Solid servers and developer tools.  Note: The Solid team uses  [`node-solid-server`](https://github.com/solid/node-solid-server) as  its main server implementation.    **Client App Implementations:** See  [`solid-client`](https://github.com/solid/solid-client) for the main client  library, and [solid/solid-apps](https://github.com/solid/solid-apps) for an  example list of Apps built using Solid. """
Semantic web;https://github.com/fafalios/sparql-ld;"""# SPARQL-LD: A SPARQL Extension for Fetching and Querying Linked Data    This Jena ARQ SERVICE extension allows to fetch, query and integrate in the same SPARQL query:  - *data stored in the (local) endpoint*  - *data coming from online RDF files (in any standard format)*  - *data embedded in Web pages as RDFa*  - *data coming from JSON-LD files*  - *data coming from dereferenceable URIs*  - *data (in RDF) dynamically created by a Web Service (Web API)*  - *data coming by querying other SPARQL endpoints*    by simply using the SERVICE operator of [SPARQL 1.1 Federated Query](http://www.w3.org/TR/sparql11-federated-query/).    A distinctive characteristic of SPARQL-LD is that it enables to   fetch and query even data in datasets returned by a portion of the query,  i.e. discovered at query-execution time.     SPARQL-LD is actually a generalization of SPARQL  in the sense that every query that can be answered by the original SPARQL can  be also answered by SPARQL-LD. Specifically, if the IRI given to the service  operator corresponds to a SPARQL endpoint, then it works exactly as the original  SPARQL (the remote endpoint evaluates the query and returns the result).  Otherwise, instead of returning an error (and no bindings), it tries to fetch and  query the triples that may exist in the given resource.      SPARQL-LD has been tested with Jena 2.13.0 ARQ (nevertheless, it may also work with other Jena ARQ releases).       ### Demo    A prototype implemention of a SPARQL endpoint that support SPARQL-LD is available at:   * https://demos.isl.ics.forth.gr/sparql-ld-endpoint/    ### Related publications    Full Paper describing SPARQL-LD:  ```  P. Fafalios, T. Yannakis and Y. Tzitzikas,  ""Querying the Web of Data with SPARQL-LD"",   20th International Conference on Theory and Practice of Digital Libraries (TPDL'16),   Hannover, Germany, September 5-9, 2016  ```   [PDF](http://l3s.de/~fafalios/files/pubs/fafalios_2016_tpdl.pdf) | [BIB](http://l3s.de/~fafalios/files/bibs/fafalios2016sparql-ld.bib)     Demo Paper describing SPARQL-LD:  ```  P. Fafalios and Y. Tzitzikas,  ""SPARQL-LD: A SPARQL Extension for Fetching and Querying Linked Data"",   14th International Semantic Web Conference (Demo Paper) - ISWC'15,   Bethlehem, Pennsylvania, USA, October 11-15, 2015   ```   [PDF](http://users.ics.forth.gr/~fafalios/files/pubs/fafalios_2015_sparql-ld.pdf) | [BIB](http://users.ics.forth.gr/~fafalios/files/bibs/fafalios2015sparql.bib)    Paper on optimizing the execution of SPARQL-LD queries through query reordering:   ```  T. Yannakis, P. Fafalios, and Y. Tzitzikas,  ""Heuristics-based Query Reordering for Federated Queries in SPARQL 1.1 and SPARQL-LD"",   2nd Workshop on Querying the Web of Data (QuWeDa), in conjunction with the 15th Extended Semantic Web Conference (ESWC'18),   Heraklion, Greece, June 3-7, 2018  ```   [PDF](http://l3s.de/~fafalios/files/pubs/fafalios2018_QuWeDa.pdf) | [BIB](http://l3s.de/~fafalios/files/bibs/fafalios2018_QuWeDa.bib) | [SOURCE CODE](https://github.com/TYannakis/SPARQL-LD-Query-Optimizer)    Paper on how a SPARQL query (to be evaluated on a SPARQL endpoint) can be transformed to a SPARQL-LD query that is answered through   link traversal, without accessing the endpoint:  ```  P. Fafalios, and Y. Tzitzikas,  ""How Many and What Types of SPARQL Queries can be Answered through Zero-Knowledge Link Traversal?"",   34th ACM/SIGAPP Symposium On Applied Computing (Semantic Web and Applications track),   Limassol, Cyprus, April 8-12, 2019  ```   [PDF](http://users.ics.forth.gr/~fafalios/files/pubs/SAC2019_ZeroKnowledgeLinkTraversal.pdf) | [BIB](http://users.ics.forth.gr/~fafalios/files/bibs/fafalios2019_SAC_LinkTraversal.bib) | [SOURCE CODE](https://github.com/fafalios/LDaQ)     ## Example Query    The following query   can be answered by an implementation of SPARQL-LD.  The query returns all co-authors of Pavlos Fafalios (main contributor of this repository)  together with the number of their publications and the number of different conferences  in which they have a publication.  Notice that this query combines and integrates:  i) data embedded in the HTML Web page http://users.ics.forth.gr/~fafalios as RDFa (lines 3-4),  ii) data coming from dereferenceable URIs derived at *query-execution* time (lines 5-6), and  iii) data coming by querying another endpoint (lines 7-9).  Note also that this query can be answered by any endpoint that implements  this extension (independently of its ""local"" contents).    ```  1.  SELECT DISTINCT ?authorURI (count(distinct ?paper)  AS ?numOfPapers)  2.                             (count(distinct ?series) AS ?numOfDiffConfs) WHERE {  3.    SERVICE <http://users.ics.forth.gr/~fafalios/> {  4.      ?p <http://purl.org/dc/terms/creator> ?authorURI }  5.    SERVICE ?authorURI {   6.      ?paper <http://purl.org/dc/elements/1.1/creator> ?authorURI }  7.    SERVICE <http://dblp.l3s.de/d2r/sparql> {  8.      ?p2 <http://purl.org/dc/elements/1.1/creator> ?authorURI .  9.      ?p2 <http://swrc.ontoware.org/ontology#series> ?series  }  10. } GROUP BY ?authorURI ORDER BY ?numOfPapers  ```     ## Source code    For implementing SPARQL-LD, we have created the following 4 classes:    - com.hp.hpl.jena.sparql.engine.http.**ReadRDFFromIRI**  - com.hp.hpl.jena.sparql.engine.http.**ResourcesCache**  - com.hp.hpl.jena.sparql.engine.http.**EndpointsIndex**  - arq.**SPARQL_LD_QueryExamples**    We have also updated the following 2 classes of Jena 2.13.0 ARQ:    - com.hp.hpl.jena.sparql.engine.**QueryExecutionBase**  - com.hp.hpl.jena.sparql.engine.http.**Service**      This repository contains only the above 6 classes.   We also provide a zip containing the *original* Jena 2.13.0 ARQ source code  (as downloaded from [https://jena.apache.org/download](https://jena.apache.org/download) in April 17, 2015)  as well as the extended, already built, Jena ARQ JAR file (**jena-arq-2.13.0_SPARQL-LD-1.1.jar**) and the corresponding extended Jena sources (**jena-arq-2.13.0-sources_SPARQL-LD-1.1.jar**).     ## Installation    - Directly use the provided (already built) extended Jena ARQ jar:        **jena-arq-2.13.0_SPARQL-LD-1.1.jar**    OR    - Download the original Jena 2.13.0 ARQ source code  - Add the 4 new classes  - Replace the 2 updated classes  - Add the *endpoints.lst* file to the project folder (same level as pom.xml)  - Add the following dependency to pom.xml (which allows to load and query RDFa data):  ```   <dependency>     <groupId>org.semarglproject</groupId>     <artifactId>semargl-jena</artifactId>     <version>0.6.1</version>     <exclusions>       <exclusion>         <groupId>org.apache.jena</groupId>         <artifactId>jena-core</artifactId>       </exclusion>     </exclusions>   </dependency>  ```	  - Build the sources  - Try to run the main class ""arq.SPARQL_LD_QueryExamples""   """
Semantic web;https://github.com/eBay/akutan;"""# Akutan    [![Build Status](https://travis-ci.com/eBay/akutan.svg?branch=master)](https://travis-ci.com/eBay/akutan)  [![GoDoc](https://godoc.org/github.com/ebay/akutan/src/github.com/ebay/akutan?status.svg)](https://godoc.org/github.com/ebay/akutan/src/github.com/ebay/akutan)    There's a blog post that's a [good introduction to Akutan](https://www.ebayinc.com/stories/blogs/tech/beam-a-distributed-knowledge-graph-store/).    Akutan is a distributed knowledge graph store, sometimes called an RDF store or a  triple store. Knowledge graphs are suitable for modeling data that is highly  interconnected by many types of relationships, like encyclopedic information  about the world. A knowledge graph store enables rich queries on its data, which  can be used to power real-time interfaces, to complement machine learning  applications, and to make sense of new, unstructured information in the context  of the existing knowledge.    How to model your data as a knowledge graph and how to query it will feel a bit  different for people coming from SQL, NoSQL, and property graph stores. In a  knowledge graph, data is represented as a single table of *facts*, where each  fact has a *subject*, *predicate*, and *object*. This representation enables the  store to sift through the data for complex queries and to apply inference rules  that raise the level of abstraction. Here's an example of a tiny graph:    subject         | predicate | object  ----------------|-----------|-----------------  `<John_Scalzi>` | `<born>`  | `<Fairfield>`  `<John_Scalzi>` | `<lives>` | `<Bradford>`  `<John_Scalzi>` | `<wrote>` | `<Old_Mans_War>`    To learn about how to represent and query data in Akutan, see  [docs/query.md](docs/query.md).    Akutan is designed to store large graphs that cannot fit on a single server. It's  scalable in how much data it can store and the rate of queries it can execute.  However, Akutan serializes all changes to the graph through a central log, which  fundamentally limits the total rate of change. The rate of change won't improve  with a larger number of servers, but a typical deployment should be able to  handle tens of thousands of changes per second. In exchange for this limitation,  Akutan's architecture is a relatively simple one that enables many features. For  example, Akutan supports transactional updates and historical global snapshots. We  believe this trade-off is suitable for most knowledge graph use cases, which  accumulate large amounts of data but do so at a modest pace. To learn more about  Akutan's architecture and this trade-off, see  [docs/central_log_arch.md](docs/central_log_arch.md).    Akutan isn't ready for production-critical deployments, but it's useful today for  some use cases. We've run a 20-server deployment of Akutan for development  purposes and off-line use cases for about a year, which we've most commonly  loaded with a dataset of about 2.5 billion facts. We believe Akutan's current  capabilities exceed this capacity and scale; we haven't yet pushed Akutan to its  limits. The project has a good architectural foundation on which additional  features can be built and higher performance could be achieved.    Akutan needs more love before it can be used for production-critical deployments.  Much of Akutan's code consists of high-quality, documented, unit-tested modules,  but some areas of the code base are inherited from Akutan's earlier prototype days  and still need attention. In other places, some functionality is lacking before  Akutan could be used as a critical production data store, including deletion of  facts, backup/restore, and automated cluster management. We have filed  GitHub issues for these and a few other things. There are also areas where Akutan  could be improved that wouldn't necessarily block production usage. For example,  Akutan's query language is not quite compatible with Sparql, and its inference  engine is limited.    So, Akutan has a nice foundation and may be useful to some people, but it also  needs additional love. If that's not for you, here are a few alternative  open-source knowledge and property graph stores that you may want to consider  (we have no affiliation with these projects):    - [Blazegraph](https://github.com/blazegraph/database): an RDF store. Supports    several query languages, including SPARQL and Gremlin. Disk-based,    single-master, scales out for reads only. Seems unmaintained. Powers    <https://query.wikidata.org/>.  - [Dgraph](https://github.com/dgraph-io/dgraph): a triple-oriented property    graph store. GraphQL-like query language, no support for SPARQL. Disk-based,    scales out.  - [Neo4j](https://github.com/neo4j/neo4j): a property graph store. Cypher query    language, no support for SPARQL. Single-master, scales out for reads only.  - See also Wikipedia's    [Comparison of Triplestores](https://en.wikipedia.org/wiki/Comparison_of_triplestores)    page.    The remainder of this README describes how to get Akutan up and running. Several  documents under the `docs/` directory describe aspects of Akutan in more  detail; see [docs/README.md](docs/README.md) for an overview.    ## Installing dependencies and building Akutan    Akutan has the following system dependencies:   - It's written in [Go](https://golang.org/). You'll need v1.11.5 or newer.   - Akutan uses [Protocol Buffers](https://developers.google.com/protocol-buffers/)     extensively to encode messages for [gRPC](https://grpc.io/), the log of data     changes, and storage on disk. You'll need protobuf version 3. We reccomend     3.5.2 or later. Note that 3.0.x is the default in many Linux distributions, but     doesn't work with the Akutan build.   - Akutan's Disk Views store their facts in [RocksDB](https://rocksdb.org/).    On Mac OS X, these can all be installed via [Homebrew](https://brew.sh/):    	$ brew install golang protobuf rocksdb zstd    On Ubuntu, refer to the files within the [docker/](docker/) directory for  package names to use with `apt-get`.    After cloning the Akutan repository, pull down several Go libraries and additional  Go tools:    	$ make get    Finally, build the project:    	$ make build    ## Running Akutan locally    The fastest way to run Akutan locally is to launch the in-memory log store:    	$ bin/plank    Then open another terminal and run:    	$ make run    This will bring up several Akutan servers locally. It starts an API server that  listens on localhost for gRPC requests on port 9987 and for HTTP requests on  port 9988, such as <http://localhost:9988/stats.txt>.    The easiest way to interact with the API server is using `bin/akutan-client`. See  [docs/query.md](docs/query.md) for examples. The API server exposes the  `FactStore` gRPC service defined in  [proto/api/akutan_api.proto](proto/api/akutan_api.proto).    ## Deployment concerns    ### The log    Earlier, we used `bin/plank` as a log store, but this is unsuitable for real  usage! Plank is in-memory only, isn't replicated, and by default, it only  keeps 1000 entries at a time. It's only meant for development.    Akutan also supports using [Apache Kafka](https://kafka.apache.org/) as its log  store. This is recommended over Plank for any deployment. To use Kafka, follow the  [Kafka quick start](https://kafka.apache.org/quickstart) guide to install  Kafka, start ZooKeeper, and start Kafka. Then create a topic called ""akutan""  (not ""test"" as in the Kafka guide) with `partitions` set to 1. You'll want to  configure Kafka to synchronously write entries to disk.    To use Kafka with Akutan, set the `akutanLog`'s `type` to `kafka` in your Akutan  configuration (default: `local/config.json`), and update the `locator`'s  `addresses` accordingly (Kafka uses port 9092 by default). You'll need to clear  out Akutan's Disk Views' data before restarting the cluster. The Disk Views  by default store their data in $TMPDIR/rocksdb-akutan-diskview-{space}-{partition}  so you can delete them all with `rm -rf $TMPDIR/rocksdb-akutan-diskview*`    ### Docker and Kubernetes    This repository includes support for running Akutan inside  [Docker](https://www.docker.com/) and  [Minikube](https://kubernetes.io/docs/setup/minikube/). These environments can  be tedious for development purposes, but they're useful as a step towards a  modern and robust production deployment.    See `cluster/k8s/Minikube.md` file for the steps to build and deploy Akutan  services in `Minikube`. It also includes the steps to build the Docker images.    ### Distributed tracing    Akutan generates distributed [OpenTracing](https://opentracing.io/) traces for use  with [Jaeger](https://www.jaegertracing.io/). To try it, follow the  [Jaeger Getting Started Guide](https://www.jaegertracing.io/docs/getting-started/#all-in-one-docker-image)  for running the all-in-one Docker image. The default `make run` is configured to  send traces there, which you can query at <http://localhost:16686>. The Minikube  cluster also includes a Jaeger all-in-one instance.    ## Development    ### VS Code    You can use whichever editor you'd like, but this repository contains some  configuration for [VS Code](https://code.visualstudio.com/Download). We  suggest the following extensions:   - [Go](https://marketplace.visualstudio.com/items?itemName=ms-vscode.Go)   - [Code Spell Checker](https://marketplace.visualstudio.com/items?itemName=streetsidesoftware.code-spell-checker)   - [Rewrap](https://marketplace.visualstudio.com/items?itemName=stkb.rewrap)   - [vscode-proto3](https://marketplace.visualstudio.com/items?itemName=zxh404.vscode-proto3)   - [Docker](https://marketplace.visualstudio.com/items?itemName=PeterJausovec.vscode-docker)    Override the default settings in `.vscode/settings.json` with  [./vscode-settings.json5](./vscode-settings.json5).    ### Test targets    The `Makefile` contains various targets related to running tests:    Target       | Description  ------------ | -----------  `make test`  | run all the akutan unit tests  `make cover` | run all the akutan unit tests and open the web-based coverage viewer  `make lint`  | run basic code linting  `make vet`   | run all static analysis tests including linting and formatting    ## License Information    Copyright 2019 eBay Inc.    Primary authors: Simon Fell, Diego Ongaro, Raymond Kroeker, Sathish Kandasamy    Licensed under the Apache License, Version 2.0 (the ""License""); you may not use  this file except in compliance with the License. You may obtain a copy of the  License at <https://www.apache.org/licenses/LICENSE-2.0>.    Unless required by applicable law or agreed to in writing, software distributed  under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR  CONDITIONS OF ANY KIND, either express or implied. See the License for the  specific language governing permissions and limitations under the License.      ----  **Note** the project was renamed to Akutan in July 2019. """
Semantic web;https://github.com/castagna/SARQ;"""SARQ - Free Text Indexing for SPARQL  ====================================    SARQ is a combination of ARQ and Solr. It gives ARQ the ability to perform  free text searches using a remote Solr server. Lucene indexes in Solr are   additional information for accessing the RDF graph, not storage for the   graph itself.    This is *experimental* (and unsupported).      How to use it  -------------    This is how you build an index from a Jena Model:        IndexBuilderModel builder = new IndexBuilderString(""http://127.0.0.1:8983/solr/sarq"");      builder.indexStatements(model.listStatements());      builder.commit();    This is how you configure ARQ to use Solr:                SARQ.setDefaultIndex(builder.getSolrServer());    This is an example of a SPARQL query using the sarq:search property function:         PREFIX sarq:     <http://openjena.org/SARQ/property#>      SELECT * WHERE {          ?doc ?p ?lit .          (?lit ?score ) sarq:search ""+text"" .      }      Acknowledgement  ---------------            The design and part of the code has been taken from LARQ, see:     * [http://openjena.org/ARQ/lucene-arq.html](http://openjena.org/ARQ/lucene-arq.html)      TODO  ----     * Fix the failing test and add more tests. [DONE]   * Double check the id as unique key, does it make sense? [DONE]   * Consider using multiValue=""true"" when index by subject, is it possible/useful?   * Test with Solr, add id=x title=foo then add id=x title=bar... if title is multi value field, what happens?   * Add required=""true"" in the schema.xml where appropriate?   * Add custom rank field and how to combined it with existing rank?   * Investigate allowDups=true|false, when is it appropriate/safe to use?   * Use [EmbeddedSolrServer](http://lucene.apache.org/solr/api/org/apache/solr/client/solrj/embedded/EmbeddedSolrServer.html) for testing   * ... """
Semantic web;https://github.com/nick-manasys/zeppelin-sparql;"""# zeppelin-sparql  Zeppelin sparql interpreter """
Semantic web;https://github.com/CLARIAH/COW;"""## CoW: Integrated CSV to RDF Converter    > CoW (Csv on the Web) is an integrated CSV to RDF converter that uses the W3C standard [CSVW](https://www.w3.org/TR/tabular-data-primer/) for rich semantic table specificatons, and [nanopublications](http://nanopub.org/) as an output RDF model        ### What is CoW    CoW is a command-line utility to convert any CSV file into an RDF dataset. Its distinctive features are:    - Expressive CSVW-compatible schemas based on the [Jinja](https://github.com/pallets/jinja) template enginge  - Highly efficient implementation leveraging multithreaded and multicore architectures  - Available as a pythonic [CLI tool](#cli), [library](#library), and [web service](#web-service)  - Supports Python 3    ### Install (requires Python to be installed)    `pip3` is the recommended method of installing COW in your system:    ```  pip3 install cow-csvw  ```    You can upgrade your currently installed version with:    ```  pip3 install cow-csvw --upgrade  ```    Possible issues:    - Permission issues. You can get around them by installing CoW in user space: `pip3 install cow-csvw --user`. Make sure your binary user directory (typically something like `/Users/user/Library/Python/3.7/bin` in MacOS or `/home/user/.local/bin` in Linux) is in your PATH. For Windows/MacOS we recommend to install Python via the [official distribution page](https://www.python.org/downloads/). You can also use [virtualenv](https://virtualenv.pypa.io/en/latest/) to avoid conflicts with your system libraries  - Please [report your unlisted issue](https://github.com/CLARIAH/CoW/issues/new)    If you can't/don't want to deal with installing CoW, you can use the [cattle](http://cattle.datalegend.net/) [web service version](#web-service) (deprecated).    ### Usage    #### CLI    The CLI (command line interface) is the recommended way of using CoW for most users. The straightforward CSV to RDF conversion is done in two steps. First:    ```  cow_tool build myfile.csv  ```    This will create a file named `myfile.csv-metadata.json` (from now on: JSON schema file or just JSF). You don't need to worry about this file if you only want a syntactic conversion. Then:    ```  cow_tool convert myfile.csv  ```    Will output a `myfile.csv.nq` RDF file (nquads by default; you can control the output RDF serialization with e.g. ``--format turtle``). That's it!    If you want to control the base URI namespace, URIs used in predicates, virtual columns, and the many other features of CoW, you'll need to edit the `myfile.csv-metadata.json` JSF and/or use CoW arguments. Have a look at the [CLI options](#options) below, the examples in the [wiki](https://github.com/CLARIAH/CoW/wiki), and the [technical documentation](http://csvw-converter.readthedocs.io/en/latest/).    ##### Options    Check the ``--help`` for a complete list of options:    ```  usage: cow_tool [-h] [--dataset DATASET] [--delimiter DELIMITER]                  [--quotechar QUOTECHAR] [--encoding ENCODING] [--processes PROCESSES]                  [--chunksize CHUNKSIZE] [--base BASE]                  [--format [{xml,n3,turtle,nt,pretty-xml,trix,trig,nquads}]]  				[--gzip] [--version]                  {convert,build} file [file ...]    Not nearly CSVW compliant schema builder and RDF converter    positional arguments:    {convert,build}       Use the schema of the `file` specified to convert it                          to RDF, or build a schema from scratch.    file                  Path(s) of the file(s) that should be used for                          building or converting. Must be a CSV file.    optional arguments:    -h, --help            show this help message and exit    --dataset DATASET     A short name (slug) for the name of the dataset (will                          use input file name if not specified)    --delimiter DELIMITER                          The delimiter used in the CSV file(s)    --quotechar QUOTECHAR                          The character used as quotation character in the CSV                          file(s)    --encoding ENCODING   The character encoding used in the CSV file(s)      --processes PROCESSES                          The number of processes the converter should use    --chunksize CHUNKSIZE                          The number of rows processed at each time    --base BASE           The base for URIs generated with the schema (only                          relevant when `build`ing a schema)    --gzip 				Compress the output file using gzip    --format [{xml,n3,turtle,nt,pretty-xml,trix,trig,nquads}], -f [{xml,n3,turtle,nt,pretty-xml,trix,trig,nquads}]                          RDF serialization format    --version             show program's version number and exit  ```    #### Web service    There is web service and interface running CoW, called [cattle](http://cattle.datalegend.net/). Two public instances are running at:    - http://cattle.datalegend.net/ - runs CoW in Python3  - http://legacy.cattle.datalegend.net/ - runs CoW in Python2 for legacy reasons    Beware of the web service limitations:    - There's a limit to the size of the CSVs you can upload  - It's a public instance, so your conversion could take longer  - Cattle is no longer being maintained and these public instances will eventually be taken offline    #### Library    Once installed, CoW can be used as a library as follows:    ```  from cow_csvw.csvw_tool import CoW  import os    COW(mode='build', files=[os.path.join(path, filename)], dataset='My dataset', delimiter=';', quotechar='\""')    COW(mode='convert', files=[os.path.join(path, filename)], dataset='My dataset', delimiter=';', quotechar='\""', processes=4, chunksize=100, base='http://example.org/my-dataset', format='turtle', gzipped=False)  ```    ### Documentation    Technical documentation for CoW are maintained in this GitHub repository (under <docs>), and published through [Read the Docs](http://readthedocs.org) at <http://csvw-converter.readthedocs.io/en/latest/>.    To build the documentation from source, change into the `docs` directory, and run `make html`. This should produce an HTML version of the documentation in the `_build/html` directory.    ### Examples    The [wiki](https://github.com/CLARIAH/COW/wiki) provides more hands-on examples of transposing CSVs into Linked Data    ### License    MIT License (see [license.txt](license.txt))    ### Acknowledgements    **Authors:**    Albert Meroño-Peñuela, Roderick van der Weerdt, Rinke Hoekstra, Kathrin Dentler, Auke Rijpma, Richard Zijdeman, Melvin Roest, Xander Wilcke    **Copyright:**  Vrije Universiteit Amsterdam, Utrecht University, International Institute of Social History      CoW is developed and maintained by the CLARIAH project and funded by NWO. """
Semantic web;https://github.com/correndo/mediation;"""# Mediation toolkit    It's a lightweight toolkit to implement ontological mediation over RDF.  It uses ontology mappings in order to rewrite SPARQL SELECT queries and to generate SPARQL CONSTRUCT queries to import an external data set.     API  --------     The tool is divided in the following packages:    * [uk.soton.service.dataset](https://github.com/correndo/mediation/tree/master/src/uk/soton/service/dataset) Provides the classes and interfaces necessaries to manages distributed datasets.  * [uk.soton.service.mediation](https://github.com/correndo/mediation/tree/master/src/uk/soton/service/mediation) Provides the classes and interfaces necessaries to mediate RDF documents and SPARQL queries using graph rewriting rules.  * [uk.soton.service.mediation.algebra](https://github.com/correndo/mediation/tree/master/src/uk/soton/service/mediation/algebra) Provides the classes and interfaces necessaries to manipulate SPARQL at the algebra level.  * [uk.soton.service.mediation.algebra.operation](https://github.com/correndo/mediation/tree/master/src/uk/soton/service/mediation/algebra/operation) Provides the implementation of SPARQL XPath functions.  * [uk.soton.service.mediation.edoal](https://github.com/correndo/mediation/tree/master/src/uk/soton/service/mediation/edoal) Provides the classes and interfaces necessaries to interface with the [EDOAL][edoal] ontology alignment format.    [edoal]: http://alignapi.gforge.inria.fr/edoal.html     The ontology alignments are represented as RDF files and describe rewriting rules that allows to define class mappings:  ```    	[]    <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                <http://ecs.soton.ac.uk/om.owl#Alignment> ;        <http://ecs.soton.ac.uk/om.owl#hasEntityAlignment>                [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                          <http://ecs.soton.ac.uk/om.owl#EntityAlignment> ;                  <http://ecs.soton.ac.uk/om.owl#hasRelation>                          <http://ecs.soton.ac.uk/om.owl#EQ> ;                  <http://ecs.soton.ac.uk/om.owl#lhs>                          [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                                    <http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#object>                                    <http://correndo.ecs.soton.ac.uk/ontology/target#Boiler> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate>                                    <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject>                                    _:b1                          ] ;                  <http://ecs.soton.ac.uk/om.owl#rhs>                          [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                                    <http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#object>                                    <http://correndo.ecs.soton.ac.uk/ontology/source#Kettle> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate>                                    <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject>                                    _:b1                          ]                ] ;  ```              ...property mappings:    ```        <http://ecs.soton.ac.uk/om.owl#hasEntityAlignment>                [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                          <http://ecs.soton.ac.uk/om.owl#EntityAlignment> ;                  <http://ecs.soton.ac.uk/om.owl#hasRelation>                          <http://ecs.soton.ac.uk/om.owl#EQ> ;                  <http://ecs.soton.ac.uk/om.owl#lhs>                          [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                                    <http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#object>                                    _:b2 ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate>                                    <http://correndo.ecs.soton.ac.uk/ontology/target#boiler> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject>                                    _:b3                          ] ;                  <http://ecs.soton.ac.uk/om.owl#rhs>                          [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                                    <http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#object>                                    _:b2 ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate>                                    <http://correndo.ecs.soton.ac.uk/ontology/source#hasKettle> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject>                                    _:b3                          ]                ] ;  ```    ...and data manipulation:    ```        <http://ecs.soton.ac.uk/om.owl#hasEntityAlignment>                [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                          <http://ecs.soton.ac.uk/om.owl#EntityAlignment> ;                  <http://ecs.soton.ac.uk/om.owl#hasFunctionalDependency>                          [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                                    <http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#object>                                    [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                                              <http://www.w3.org/1999/02/22-rdf-syntax-ns#Seq> ;                                      <http://www.w3.org/1999/02/22-rdf-syntax-ns#_1>                                              _:b4 ;                                      <http://www.w3.org/1999/02/22-rdf-syntax-ns#_2>                                              273.15                                    ] ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate>                                    <http://www.w3.org/2005/xpath-functions/sub> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject>                                    _:b5                          ] ;                          <http://ecs.soton.ac.uk/om.owl#hasRelation>                          <http://ecs.soton.ac.uk/om.owl#EQ> ;                  <http://ecs.soton.ac.uk/om.owl#lhs>                          [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                                    <http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#object>                                    _:b4 ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate>                                    <http://correndo.ecs.soton.ac.uk/ontology/target#temp> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject>                                    _:b6                          ] ;                  <http://ecs.soton.ac.uk/om.owl#rhs>                          [ <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>                                    <http://www.w3.org/1999/02/22-rdf-syntax-ns#Statement> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#object>                                    _:b5 ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#predicate>                                    <http://correndo.ecs.soton.ac.uk/ontology/source#hasTemperature> ;                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#subject>                                    _:b6                          ]                ] ;  ```                                 Once loaded an alignment the tool allows to rewrite a SPARQL SELECT query in order to fit a given schema:      [kettle-boiler] original query:  ```   	PREFIX  rdfs: <http://www.w3.org/2000/01/rdf-schema#>  	PREFIX  source: <http://correndo.ecs.soton.ac.uk/ontology/source#>  	PREFIX  owl:  <http://www.w3.org/2002/07/owl#>  	PREFIX  rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#>  	SELECT DISTINCT  ?v ?y ?z ?lt  	WHERE  	{ ?v rdf:type source:Person .  	?v source:hasKettle ?y .  	?v source:hasKettle ?l .  	?y source:hasTemperature 10 .  	?l source:hasTemperature ?lt .  	} LIMIT   10  ```    [kettle-boiler] translated query:    ```  	SELECT DISTINCT  ?v ?y ?z ?lt  	WHERE  	{ ?v   <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>  <http://correndo.ecs.soton.ac.uk/ontology/target#User> ;  	       <http://correndo.ecs.soton.ac.uk/ontology/target#boiler>  ?y ;  	       <http://correndo.ecs.soton.ac.uk/ontology/target#boiler>  ?l ;  	?y   <http://correndo.ecs.soton.ac.uk/ontology/target#temp>  283.15 .  	?l  <http://correndo.ecs.soton.ac.uk/ontology/target#temp>  ?_12 .  	LET (?lt := ( ?_12 - 273.15 ))  	} LIMIT   10  ```	 """
Semantic web;https://github.com/SmartDataAnalytics/jena-sparql-api;"""## Welcome to the Jena SPARQL API project  An advanced Jena-based SPARQL processing stack for building Semantic Web applications.    Highlights:  * Fluent SPARQL Query API - Transparently enhance query execution with caching, pagination, rewriting, transformations, and so on, without having to worry about that in your application logic.  * Transparent basic (normalized) string caching - Just the usual string based caching as it has been implemented over and over again  * Query Transformations  * SPARQL sub graph isomorphism checker  * Transparent sub graph isomorphy cache - Uses the isomorphism checker for caching - Detects whether prior result sets fit into a current query - regardless of variable naming.  * JPA-based Java<->RDF mapper: Run JPA criteria queries over Java classes which are actually backed by SPARQL.      [![Build Status](http://ci.aksw.org/jenkins/job/jena-sparql-api/badge/icon)](http://ci.aksw.org/jenkins/job/jena-sparql-api/)    This library offers several [Jena](http://jena.apache.org/)-compatible ways to *transparently* add delays, caching, pagination, retry and even query transformations before sending off your original SPARQL query. This frees your application layer from the hassle of dealing with those issues. Also, the server module bundles Jena with the [Atmosphere](https://github.com/Atmosphere/atmosphere) framework, giving you a kickstart for REST and websocket implementations.     ### Maven  Releases are available on [maven central](http://search.maven.org/#search%7Cga%7C1%7Cjena-sparql-api).  Snapshots are presently published in our own archiva:    ```xml  <repositories>  	<repository>  	    <id>maven.aksw.snapshots</id>  	    <name>University Leipzig, AKSW Maven2 Repository</name>  	    <url>http://maven.aksw.org/archiva/repository/snapshots</url>  	</repository>  </repositories>    <dependencies>          <!-- This is the core artifact; several other ones build on that. -->  	<dependency>  		<groupId>org.aksw.jena-sparql-api</groupId>  		<artifactId>jena-sparql-api-core</artifactId>  		<version>{check available versions with the link below}</version>  	</dependency>	  	...  </dependencies>  ```    Latest version(s): [jena-sparql-api on maven central](http://search.maven.org/#search%7Cga%7C1%7Cjena-sparql-api)      ### Project structure    This library is composed of the following modules:  * `jena-sparql-api-core`: Contains the core interfaces and basic implementations.  * `jena-sparql-api-server`: An abstract SPARQL enpdoint class that allows you to easily create your own SPARQL endpoint. For example, the SPARQL-SQL rewriter [Sparqlify](http://github.com/AKSW/Sparqlify) is implemented against these interfaces.  * `jena-sparql-api-utils`: Utilities common to all packages.  * `jena-sparql-api-example-proxy`: An example how to create a simple SPARQL proxy. You can easily adapt it to add pagination, caching and delays.  * `jena-sparql-api-sparql-ext`: SPARQL extensions for processing non-RDF data as part of query evaluation. Most prominently features support for querying JSON documents and unnesting JSON arrays to triples. (We should also add CSV processing for completeness, although covered by the TARQL tool).  * `jena-sparql-api-jgrapht`: Provides a JGraphT wrapper for Jena's Graph interface. Yes, we were aware that RDF is not a plain graph, but a labeled directed pseudo graph and implemented it accordingly. Also contains conversions of SPARQL queries to graphs. Enables e.g. subgraph isomorphism analysis.  * `jena-sparql-api-mapper`: Powerful module to query RDF data transparently with the Java Persistence API (JPA) criteria queries. I.e. queries and updates are expressed over (annotated) Java classes, and no RDF specifics are exposed to the developer.        ### Usage    Here is a brief summary of what you can do. A complete example is avaible [here](https://github.com/AKSW/jena-sparql-api/blob/master/jena-sparql-api-core/src/main/java/org/aksw/jena_sparql_api/example/Example.java).    Http Query Execution Factory  ```Java  QueryExecutionFactory qef = new QueryExecutionFactoryHttp(""http://dbpedia.org/sparql"", ""http://dbpedia.org"");  ```  Adding a 2000 millisecond delay in order to be nice to the backend  ```Java  qef = new QueryExecutionFactoryDelay(qef, 2000);  ```  Set up a cache    ```Java  // Some boilerplace code which may get simpler soon  long timeToLive = 24l * 60l * 60l * 1000l;   CacheCoreEx cacheBackend = CacheCoreH2.create(""sparql"", timeToLive, true);  CacheEx cacheFrontend = new CacheExImpl(cacheBackend);    qef = new QueryExecutionFactoryCacheEx(qef, cacheFrontend);  ```  Add pagination with (for the sake of demonstration) 900 entries per page (we could have used 1000 as well).  Note: Should the pagination abort, such as because you ran out of memory and need to adjust your settings, you can resume from cache!  ```Java  qef = new QueryExecutionFactoryPaginated(qef, 900);  ```  Create and run a query on this fully buffed QueryExecutionFactory  ```Java  String queryString = ""SELECT ?s { ?s a <http://dbpedia.org/ontology/City> } LIMIT 5000"";  QueryExecution qe = qef.createQueryExecution(queryString);  		  ResultSet rs = qe.execSelect();  System.out.println(ResultSetFormatter.asText(rs));  ```    ### Proxy Server Example  This example demonstrates how you can create your own SPARQL web service.  You only have to subclass `SparqlEndpointBase` and override the `createQueryExecution` method.  Look at the [Source Code](https://github.com/AKSW/jena-sparql-api/blob/master/jena-sparql-api-example-proxy/src/main/java/org/aksw/jena_sparql_api/example/proxy/SparqlEndpointProxy.java) to see how easy it is.    Running the example:  ```bash  cd jena-sparql-api-example-proxy  mvn jetty:run  # This will now start the proxy on part 5522  ```  In your browser or a terminal visit:    [http://localhost:5522/sparql?service-uri=http://dbpedia.org/sparql&query=Select * { ?s ?p ?o } Limit 10](http://localhost:5522/sparql?service-uri=http%3A%2F%2Fdbpedia.org%2Fsparql&query=Select%20%2A%20%7B%20%3Fs%20%3Fp%20%3Fo%20%7D%20Limit%2010)      ## License  The source code of this repo is published under the [Apache License Version 2.0](https://github.com/AKSW/jena-sparql-api/blob/master/LICENSE).    This project makes use of several dependencies: When in doubt, please cross-check with the respective projects:  * [Apache Jena](https://jena.apache.org/) (Apache License 2.0)  * [Atmosphere](https://github.com/Atmosphere/atmosphere) (Apache License 2.0/Partially CDDL License)  * [Guava](http://code.google.com/p/guava-libraries/) (Apache License 2.0)  * [commons-lang](http://commons.apache.org/proper/commons-lang/) (Apache License 2.0)  * [rdf-json-writer](https://github.com/kasabi/rdf-json-writer) (currently copied but also under Apache 2.0 license, will be changed to maven dep)       """
Semantic web;https://github.com/lukostaz/shi3ld-http;"""Shi3ld for HTTP operations  ===========    [Shi3ld for HTTP](http://wimmics.inria.fr/projects/shi3ld-ldp) is an access control module for enforcing authorization on triple stores.   Shi3ld for HTTP protects HTTP operations on [Linked Data](linkeddata.org) and relies on attribute-based access policies.    ### Features    * Authorization for r/w HTTP Methods on RDF resources  * RDF Resource-oriented  * Policy Language in RDF/SPARQL or RDF only  * Attribute-based  * ""Context-aware"" Policies    The policy vocabularies namespace documents are available at:  * [S4AC](http://ns.inria.fr/s4ac) - for modelling Access Policies.  * [PRISSMA](http://ns.inria.fr/prissma) - for modelling context client attributes.     ### Scenarios    Shi3ld for HTTP supports three different scenarios and are available in this repository branches:    * Shi3ld for [SPARQL Graph Store Protocol](http://www.w3.org/TR/sparql11-http-rdf-update/)  * Shi3ld for [Linked Data Platform](http://www.w3.org/TR/ldp-ucr/) (SPARQL-based)  * Shi3ld for [Linked Data Platform](http://www.w3.org/TR/ldp-ucr/) (SPARQL-less)    Scenarios are detailed in our paper [Access Control for HTTP Operations on Linked Data](http://hal.inria.fr/docs/00/81/50/67/PDF/eswc2013_shi3ld.pdf)       ### Installation    All Shi3ld scenarios are Java server side modules that run in a java application server (e.g. Tomcat)    The `config.properties` property file needs to be customized with the policy storage path and the triple storage path.     The Shi3ld-GSP scenario is compatible with the GSP-compliant [Fuseki SPARQL engine](http://jena.apache.org/documentation/serving_data/index.html) needs the Fuseki server URL and the Fuseki dataset name.    The Shi3ld-LDP scenarios embed the [Corese/KGRAM](http://wimmics.inria.fr/corese) RDF store and SPARQL processor.    ### Testing    Shi3ld-HTTP can be tested with a [standalone client](http://wimmics.inria.fr/projects/shi3ld-ldp/shi3ld-test-client.zip) shipped with sample client attributes.    Sample Access Policies can be found [here](http://wimmics.inria.fr/projects/shi3ld-ldp/shi3ld-test-policies.zip)."""
Semantic web;https://web.archive.org/web/20180627155808/https://github.com/dice-group/triplestore-benchmarks;"""### How Representative is a SPARQL Benchmark? An Analysis of RDF Triplestore Benchmarks (TheWebConf2019 paper)  We provide a fine-grained comparative analysis of existing triplestore benchmarks. In particular, we have analyzed the data and queries, provided with the existing triplestore benchmarks in addition to several real-world datasets. Further, we have measured the correlation between the query execution time and various SPARQL query features and ranked those features based on their significance levels. Our experiments have revealed  several interesting insights about the design of such benchmarks. We can hope such fine-grained evaluation will be helpful for SPARQL benchmark designers to design diverse benchmarks in the future.  ### Persistent URI, Licence   All of the data and results presented in our evaluation are available online from  https://github.com/dice-group/triplestore-benchmarks under [GNU General Public License v3.0](https://github.com/dice-group/triplestore-benchmarks/blob/master/LICENSE).    ### Benchmark Datasets and Queries  We provide the datasets and queries of the benchmarmks and real-world datasets used in our evaluation. The datasets are also provided as portable virtuoso triplestores which can be started from bin/start_virtuoso.sh.    | *Benchmark/Dataset*   | *RDF Dump* | *Virtuoso Store* | *Queries* |  |-----------------------|------------|---------------------|-----------|  |[Bowlogna](https://exascale.info/assets/pdf/BowlognaBenchSIMPDA2011.pdf)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/datasets-dumps/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/queries/) |  |[TrainBench](http://docs.inf.mit.bme.hu/trainbenchmark/)|[Download](https://www.dropbox.com/s/n7s02dzf0dyf4by/trainbenchmark-models-1-1024.zip?dl=0)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/queries/) |  |[BSBM](https://pdfs.semanticscholar.org/0efc/d1d38ad020da7c01613b7818eb123cb34121.pdf)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/datasets-dumps/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/queries/) |  |[SP2Bench](https://arxiv.org/pdf/0806.4627.pdf)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/datasets-dumps/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/queries/) |  |[WatDiv](https://link.springer.com/chapter/10.1007/978-3-319-11964-9_13)|[Download](http://dsg.uwaterloo.ca/watdiv/watdiv.100M.tar.bz2)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/queries/) |  |[LDBC-SNB](https://ldbc.github.io/ldbc_snb_docs/wiki)|[Download](https://www.dropbox.com/s/uyocuxmx85dce4m/social_network_ttl_sf1.zip?dl=0)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/queries/) |  |[FEASIBLE](https://svn.aksw.org/papers/2015/ISWC_FEASIBLE/public.pdf)|[Download](http://downloads.dbpedia.org/3.5.1/en/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/queries/) |  |[FishMark](http://ceur-ws.org/Vol-943/SSWS_HPCSW2012_paper1.pdf)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/datasets-dumps/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/queries/) |  |[DBPSB](https://link.springer.com/chapter/10.1007/978-3-642-25073-6_29)|[Download](http://downloads.dbpedia.org/3.5.1/en/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/queries/) |  |[BioBench](https://jbiomedsem.biomedcentral.com/track/pdf/10.1186/2041-1480-5-32)|ftp://ftp.dbcls.jp/togordf/bmtoyama/|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/queries/) |  |[DBpedia3.5.1](http://wiki.dbpedia.org/)|[Download](http://downloads.dbpedia.org/3.5.1/en/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-lsq-results.virtuoso.tar.gz)|  |[SWDF](https://old.datahub.io/dataset/semantic-web-dog-food)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/datasets-dumps/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-lsq-results.virtuoso.tar.gz)|  |[NCBIGene](http://download.openbiocloud.org/release/3/ncbigene/ncbigene.html)|[Download](http://download.bio2rdf.org/#/release/3/ncbigene/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-lsq-results.virtuoso.tar.gz)|  |[SIDER](http://download.openbiocloud.org/release/3/sider/sider.html)|[Download](http://download.bio2rdf.org/#/release/3/sider/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-lsq-results.virtuoso.tar.gz)|  |[DrugBank](http://download.openbiocloud.org/release/3/drugbank/drugbank.html)|[Download](http://download.bio2rdf.org/#/release/3/drugbank/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/)|[Download](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-lsq-results.virtuoso.tar.gz)|      ### Analysis  Our analysis is based on the following benchmark design features:  * Dataset structuredness (**dataset related**)  * Dataset relationship specialty (**dataset related**)  * Overall queries diversity score based on important SPARQL query features, i.e., number of triple patterns, number of projection variables, result set sizes, query execution time, number of BGPs, number of join vertices, mean join vertex degree, mean triple pattern selectivities, BGP-restricted and join-restricted triple pattern selectivities, and join vertex types. (**queries related**)  * Percentages-wise distribution of the use of important SPARQL clauses (e.g., LIMIT, OPTIONAL, ORDER BY, DISTINCT,  UNION, FILTER, REGEX) in benchmark queries  (**queries related**)  * SPearsman's correlation of the query runtimes and important SPARQL query features (**queries related**)    The first two features are related to benchmark datasets and later three are related to benchmark queries. Please refer to the manuscript for the details of above design features.    ### Reproducing Results  Please follow the following steps to reproduce the complete results presented in the paper.   1. Download the folder [CLI](https://github.com/AKSW/triplestore-benchmarks/tree/master/cli) which contains a runable jar **benchmark-util.jar**.     2. To calculate the structuredness or relationship specialty of an RDF datasets, we need to first load the dataset into a triple store and provide the endpoint url as input to the jar file. The linux based virtuoso SPARQL endpoints of the datasets of all the triplestore benchmarks and real-world datasets used in our evaluation can be downloaded from [here](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-datasets-virtuoso/). Note the virtuoso can be started from bin/start_virtuoso.sh script. The utility work for any SPARQL endpoint.   3. Download the [Virtuoso](https://hobbitdata.informatik.uni-leipzig.de/benchmarks-data/benchmarks-lsq-results.virtuoso.tar.gz) which contains the LSQ datasets of all the selected 10 triplestores benchmarks and 5 real-world datasets. Please refer to [LSQ homepage](https://github.com/aksw/lsq) for generating an LSQ dataset of the queries of a new RDF benchmark. This step is only required to generate queries related results.     ```html  ###Command line arguments ###  java -jar benchmark-util.jar  -m <measure> -e <endpoint> -g <graph> -q <queriesFile>    where    measure = structuredness or specialty or diversity or percentage or correlation  endpoint = endpoint url  graph = graph name (optional)  queriesFile = queries file (only required to produce queries related statistics, i.e., diversity, percentages, and correlation).   please note the queries files are already provided with the cli folder downloaded in step 1.    An example formats:  java -jar benchmark-util.jar -m structuredness -e http://localhost:8890/sparql  java -jar benchmark-util.jar -m specialty -e http://localhost:8890/sparql -g http://benchmark-eval.aksw.org/feasible    java -jar benchmark-util.jar -m diversity -e http://localhost:8890/sparql -g http://benchmark-eval.aksw.org/feasible -q queries-diversity.txt  java -jar benchmark-util.jar -m percentage -e http://localhost:8890/sparql -q queries-percent.txt -g http://benchmark-eval.aksw.org/biobench  java -jar benchmark-util.jar -m correlation -e http://localhost:8890/sparql -q queries-correlation.txt -g http://benchmark-eval.aksw.org/dbpsb    You can run SPARQL SELECT DISTINCT ?g WHERE { GRAPH ?g {?s ?p ?o }} on the virtuoso downloded in step 2 to get the graph names of all the selected benchmarks and real-world datasets. Note you can add more queries into the input files in -q argument to get results for other features.  ```    ### Complette Evaluation Results  Our complete evaluation results can be found [here](https://github.com/AKSW/triplestore-benchmarks/raw/master/complete-evaluation-results.xlsx)  ### Authors    * [Muhammad Saleem](https://sites.google.com/site/saleemsweb/) (AKSW, University of Leipzig)    * [Gábor	Szárnyas](https://inf.mit.bme.hu/en/members/szarnyasg/) (MTA-BME Lendület Cyber-Physical Systems Research Group, Budapest University of Technology and Economics)    * [Felix Conrads](http://aksw.org/FelixConrads.html) (AKSW, University of Leipzig)    * [Syed Ahmad Chan	Bukhari](http://ahmadchan.com) (Department of Pathology, Yale University School of Medicine)    * [Qaiser Mehmood](https://www.insight-centre.org/users/qaiser-mehmood) (INSIGHT, University of Galway)    * [Axel-Cyrille Ngonga Ngomo](http://aksw.org/AxelNgonga.html) (AKSW, University of Leipzig) """
Semantic web;https://github.com/zazuko/trifid;"""# Zazuko Trifid - Lightweight Linked Data Server and Proxy  <img src=""https://cdn.rawgit.com/zazuko/trifid/master/logo.svg"" width=""140px"" height=""140px"" align=""right"" alt=""Trifid-ld Logo""/>    [![Join the chat at https://gitter.im/zazuko/trifid](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/zazuko/trifid?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    Trifid provides a lightweight and easy way to access Linked Data URIs via HTTP.  In the Linked Data world this is often called [dereferencing](http://en.wikipedia.org/wiki/Dereferenceable_Uniform_Resource_Identifier).  Trifid is inspired by [Pubby](https://www.w3.org/2001/sw/wiki/Pubby) and written in (server side) JavaScript.    ### Features    * Provides a Linked Data interface to SPARQL protocol servers  * Provides a file based interface for testing  * Provides a customizable HTML renderer with embedded RDF  * Takes care of content-negotiation  * Provides a SPARQL proxy and [YASGUI](http://about.yasgui.org/) as web frontend    ### Requirements    * A SPARQL endpoint  * Or for development some triples in a local file.    Trifid supports all content-types provided by the SPARQL endpoint and does not do additional format conversion.    ### Trifid in the wild    Trifid can be completely themed according to your needs. Example resources using Trifid:    * Default view: http://lod.opentransportdata.swiss/didok/8500011  * Customized for one gov entity in Switzerland: https://ld.geo.admin.ch/boundaries/municipality/296    ## Installation    Trifid is a [Node.js](http://nodejs.org/) based application.  To install and run it you will need to install [Node.js](http://nodejs.org/) on your system.    Clone the Github repository and run        npm install    to install all module dependencies.    ## Usage    To start the server execute the following command:        npm start    The server script is also a command line program which can be called like this:        trifid --config=my-trifid-config.json    If you want to run Trifid using a SPARQL endpoint and default settings, you can run it even without a config file:        trifid --sparql-endpoint-url=http://localhost:3030/sparql         ### Parameters    The following parameters are available:    - `-v` or `--verbose`: Verbose output, will show the actual config after expanding  - `-c` or `--config`: Expects a path to a config as value, which will be used by Trifid  - `-p` or `--port`: Expects a port number as value, which will be used by the HTTP listener of Trifid  - `--sparql-endpoint-url`: Expects a SPARQL HTTP query interface URL value, which will be used by the Trifid SPARQL handler  - `--dataset-base-url`: Expects a Base URL value, which will be used to translate the request URLs    ## Configuration    Trifid uses JSON configuration files and supports comments in JavaScript style.  One configuration file can use another file as base.  The `baseConfig` property must point to the other file.  Values of the base file will be overwritten.    ### Examples    #### Default configuration    The default configuration `config.json` uses the file system handler and a [sample dataset](https://github.com/zazukoians/tbbt-ld) with characters from _The Big Bang Theory_.  The following command will run it:        npm start    You will then be able to access its content, e.g. <http://localhost:8080/data/person/amy-farrah-fowler>.    In a production environment the SPARQL handler may be the better choice.    #### SPARQL configuration    For production systems we recommend data access via the [SPARQL 1.1 Protocol](http://www.w3.org/TR/sparql11-protocol/) interface.  `config-sparql.json` can be used as base configuration.  The following lines defines a configuration using a Fuseki SPARQL endpoint:    ```JSON  {    ""baseConfig"": ""trifid:config-sparql.json"",    ""sparqlEndpointUrl"": ""http://localhost:3030/dataset/sparql""  }  ```    The `baseConfig` property defines which file should be used as base configuration.  The `trifid:` prefix prepends the Trifid module path.  The value of the `sparqlEndpointUrl` property is used in the handler and also the SPARQL proxy.    Sometimes SPARQL endpoints are running on TLS/SSL but provide an incomplete configuration or a self-signed certificate. In that case one can disable strict certificate checking by setting the environment variable `NODE_TLS_REJECT_UNAUTHORIZED`. For example:        $ export NODE_TLS_REJECT_UNAUTHORIZED=0    ### Properties    Usually only the following properties must be configured:    - `baseConfig`: Base configuration file for the current configuration file.  - `sparqlEndpointUrl`: URL of the SPARQL HTTP query interface.  - `datasetBaseUrl`: If the dataset is stored with a different base URL this property is used to translate the request URL.    The following properties are already defined in the default configurations:    - `listener`: `port` and `host` of the listener.  - `express`: Express settings as key vale pairs.  - `patchHeaders`: Settings for the `patch-headers` middleware.  - `mediaTypeUrl`: Settings for the `format-to-accept` middleware.  - `rewrite`: Settings for the camouflage-rewrite middleware.  - `handler`: Settings for the graph handler.    ### Prefixes    It is possible to use prefixes in the property values of the configuration.  These prefixes will be translated to specific paths or environment variable values.    - `cwd`: Prepends the current working directory to the value.  - `env`: Uses the value of the environment variable with the name matching the value after the prefix.    (e.g. `""env:SPARQL_ENDPOINT_URL""` will be replaced with the environment variable value of `$SPARQL_ENDPOINT_URL`)  - `trifid`: Prepends the Trifid module path to the value.    ### Multiple Configurations    Most plugins support multiple configurations to support path or hostname specific configurations.  These plugins have an additional level in the config with the config name as key and the actual configuration as value.  Each config can have a `path` property.  If it's not defined, `/` will be used.  Also a `hostname` can be specified to use the config only for matching host names.  The `priority` may be required if multiple configs could match to an URL.    Example:  ```JSON  ""pluginName"": {    ""root"": {      // ""path"": ""/"" will be automatically added if path is not given      ""priority"": 200      ...    },    ""otherPath"": {      ""path"": ""/other/"",      ""priority"": 100      ...    },    ""otherHostname"": {      ""hostname"": ""example.org""      ""priority"": 150    }  }  ```    ### Static Files    With the `staticFiles` property, folders can be mapped into URL paths for static file hosting.  This plugin supports multiple configurations.  The key for a static file hosting can be used to replace values defined in a configuration, which is used as `baseConfig`.  If the first folder does not contain the requested file, the next folder will be used and so on.  The `folder` property points to the folder in the file system.  It's possible to use prefixes in the folder value.    Example:    ```JSON  ""staticFiles"": {    ""rendererFiles"": {      ""hostname"": ""example.org"",      ""path"": ""/"",      ""folder"": ""renderer:public""    }  }  ```    ### Handler    The handler plugin supports multiple configurations.  Properties for the handler configuration:    - `module`: The handler JS file or module.  - `options`: Handler specific options.    More details about the handler specific options can be found in the documentation of the handlers:    - [Fetch files](https://github.com/zazukoians/trifid-handler-fetch)  - [SPARQL](https://github.com/zazukoians/trifid-handler-sparql)    ### SPARQL Proxy    The SPARQL proxy plugin supports multiple configurations.  Properties:    - `options`: Options for the SPARQL proxy.    Options:    - `endpointUrl`: URL to the SPARQL HTTP query interface. (default: sparqlEndpointUrl)  - `authentication`: `user` and `password` for basic authentication.    See `config-virtuoso.json` and `config-stardog.json` for default configuration in case you use either of these stores.    Note that SPARQL is currently not supported by the in-memory store.    ### Patch Headers    The patch headers plugin supports multiple configurations.  See the [patch-headers](https://www.npmjs.com/package/patch-headers) module documentation for more details.    ### Rewrite    The rewrite plugin supports multiple configurations.  See the [camouflage-rewrite](https://www.npmjs.com/package/camouflage-rewrite) module documentation for more details.    Note that this module does _not_ work for most content-types, see the documentation for details. By default it should work for HTML and Turtle. It is merely for testing purposes and should not be active on production.    ## Production Best Practices    Note that it is not recommended to run Node applications on [well-known ports](http://en.wikipedia.org/wiki/List_of_TCP_and_UDP_port_numbers#Well-known_ports) (< 1024). You should use a reverse proxy instead.    ### Installing/Using with Docker    Trifid can be installed using Docker. With this method you only need to have Docker installed, see https://docs.docker.com/installation/ for installation instructions for your platform.    Once Docker is installed clone the Github repository and run        docker build -t trifid .    This creates an image named `trifid` that you can execute with        docker run -ti -p 8080:8080 trifid    Once it is started you can access for example http://localhost:8080/data/person/sheldon-cooper . An example on using Docker can be found at [lod.opentransportdata.swiss](https://github.com/zazuko/lod.opentransportdata.swiss).    #### Trifid environment variables    You can change its behavior by changing the following environment variable:        TRIFID_CONFIG config-sparql.json    This overrides the default configuration `config.json`.    #### Use the pre built image    If you do not want to build your own Docker image, you can pull the official image from [Docker Hub](https://hub.docker.com/r/zazuko/trifid/):        docker pull zazuko/trifid      ### Reverse Proxy    If you run Trifid behind a reverse proxy, the proxy must set the `X-Forwarded-Host` header field.    ## Debugging    This package uses [`debug`](https://www.npmjs.com/package/debug), you can get debug logging via: `DEBUG=trifid:`.  Trifid plugins should also implement `debug` under the `trifid:` prefix, enabling logging from all packages  implementing it can be done this way: `DEBUG=trifid:*`.    ## Support    Issues & feature requests should be reported on Github.    Pull requests are very welcome.    ## License    Copyright 2015-2019 Zazuko GmbH    Trifid is licensed under the Apache License, Version 2.0. Please see LICENSE and NOTICE for details. """
Semantic web;https://github.com/CMU-Q/DREAM;"""#DREAM   **Distributed RDF Engine with Adaptive Query Planner and Minimal Communication**    RDF and SPARQL query language are gaining wide popularity and acceptance. **DREAM** is a hybrid RDF system, which combines the advantages and averts the disadvantages of the centralized and distributed RDF schemes. In particular, DREAM avoids partitioning RDF datasets and reversely partitions SPARQL queries. By not partitioning datasets, DREAM offers a general paradigm for different types of pattern matching queries and entirely precludes intermediate data shuffling (only auxiliary data are shuffled). By partitioning only queries, DREAM suggests an adaptive scheme, which runs queries on different numbers of machines depending on their complexities. DREAM achieves these goals and significantly outperforms related systems via employing a novel graph-based, rule-oriented query planner and a new cost model.    DREAM is implemented in C and C++, and available as open-source under the MIT License.    Download DREAM  ----------------------    You can download DREAM directly from the Github Repository. Github also offers a zip download of the repository if you do not have git.    The git command line for cloning the repository is:  ```  git clone https://github.com/az-hasan/DREAM.git  cd DREAM  ```      Building  ------------------  The current version of DREAM was tested on Ubuntu Linux 64-bit 14.04. It requires a 64-bit operating system.         Dependencies  ------------------    DREAM has the following dependencies.    1. [g++ (>= 4.8)](https://gcc.gnu.org/gcc-4.8/)  2. [MPICH (>= 3.1)](https://www.mpich.org/downloads/)  3. [Boost](http://www.boost.org/)  4. [TBB](https://www.threadingbuildingblocks.org/)     We use the [rdf3x-0.3.8](https://code.google.com/p/rdf3x/downloads/detail?name=rdf3x-0.3.8.zip&can=2&q=) engine as part of DREAM. We use the unpacked binaries [id2name and rdf3xquery](https://github.com/az-hasan/DREAM/wiki/Running-DREAM#rdf3x-binaries).    Usage   ----------------  The [Wiki entry](https://github.com/az-hasan/DREAM/wiki) provides a guide to install and run DREAM on a cluster.      Contributing  -------------------  1. Fork it ( https://github.com/[my-github-username]/DREAM/fork )  2. Create your feature branch (`git checkout -b my-new-feature`)  3. Commit your changes (`git commit -am 'Add some feature'`)  4. Push to the branch (`git push origin my-new-feature`)  5. Create a new Pull Request """
Semantic web;https://github.com/anqit/spanqit;"""# spanqit    ***  ## Exciting update!  Spanqit has been merged into the Eclipse Foundation project [rdf4j](http://rdf4j.org/)! As such, further development will continue there first, and perhaps backported here. Check out links to [docs](https://rdf4j.org/documentation/tutorials/sparqlbuilder/) and the [source code](https://github.com/eclipse/rdf4j/tree/main/core/sparqlbuilder)  ***      A Java-based SPARQL query generator    Spanqit is an open source, fluent Java library to programatically create SPARQL queries. See the [wiki](https://github.com/anqit/spanqit/wiki) and javadocs (coming soon) for more info.      ### New with version 1.1:  - Support for all of the [Update Queries](https://www.w3.org/TR/sparql11-update/)!!  - No more need for adapters, all RDF elements can be created from with Spanqit, including IRI's and blank nodes  - support for more SPARQL syntax constructs like predicate-object lists in triple patterns and blank nodes  - more convenience methods  - some bug fixes and javadocs  Check out the updated [examples project](https://github.com/anqit/spanqit-examples) for detailed usage """
Semantic web;https://github.com/Quetzal-RDF/quetzal;"""Quetzal (*Que*ry Tran*z*l*a*tion *L*ibraries)  =======    SPARQL to SQL translation engine for multiple backends, such as DB2, PostgreSQL and Apache Spark.       # Philosophy  The goal of Quetzal is to provide researchers with a framework to experiment with various techniques to store and query graph data efficiently.  To this end, we provide 3 modular components that:  * Store data:  In the current implementation, data is stored in using a schema similar to the one described in [SIGMOD 2013 paper](http://dl.acm.org/citation.cfm?id=2463718).  The schema lays out all outgoing (or incoming) labeled edges of a given vertex *based on the analysis of data characteristics* to optimize storage for a given dataset.  The goal in the layout is to store the data for a given vertex on a single row in table to optimize for STAR queries which are very common in SPARQL.    * Compile SPARQL to SQL:  In the current implementation, given a set of statistics about the dataset's characteristics, the compiler can compile SPARQL 1.1 queries into SQL.  The compiler will optimize the order in which it executes the SPARQL query based on statistics of the dataset.  * Support for SQL on multiple backends:  In the current implementation, we support DB2, PostgreSQL, and Apache Spark.  The first two are useful for workloads that require characteristics normally supported by relational backends (e.g., transactional support), the third targets analytic workloads that might mix graph analytic workloads with declarative query workloads.     # Overview of Components  * Data Layout:  The current implementation uses a *row based layout of graph data*, such that each vertex's incoming edges or outgoing edges are laid out as much as possible on the same row.  For a detailed set of experiments that examine when this layout is advantageous, see [SIGMOD 2013 paper](http://dl.acm.org/citation.cfm?id=2463718).  Outgoing edges are stored in a table called DPH (direct primary hashtable), and incoming edges are stored in a table called RPH (reverse primary hashtable).  Because RDF can have many thousand properties, dedicating a column per property is not an option (in fact, some datasets will exhaust most database systems limits on the number of columns).  RDF data is sparse though, so each vertex tends to have a small subset of the total number of properties.  The current implementation performs an analysis of which properties co-occur with which others, and uses graph coloring to build a hash function that maps properties to columns.  Properties that co-occur together are typically not assigned to the same row.  If they do get assigned to the same row because a single vertex has several hundred edges to all sorts of properties, then collisions are possible and the schema records this fact, and the SQL is adjusted appropriately.  Note that for multi-valued properties, DPH and RPH record only the existence of the property for a given vertex, actual values require a join with a DS (direct secondary) and RS (reverse secondary) table, respectively.  * SPARQL-SQL compiler:  In the current implementation, this compilation job is done by a class called ``com.ibm.research.rdf.store.sparql11.planner.Planner``, in a method called ``public Plan plan(Query q, Store store, SPARQLOptimizerStatistics stats)``.  The goal of the planner is to compile the SPARQL query into SQL, *re-ordering* the query in order to start with the most selective triples (triples with the least cost), joining it with the second most selective triple based on what becomes available when one evaluates the first triple, and so on.  In doing so, the planner must respect the semantics of SPARQL (e.g., not join two variables that are named the same but are on two separate brances of a UNION).  The Planner employs a greedy algorithm to evaluate what available nodes exist for planning, and which one should be planned first.  AND nodes get collapsed into a single ""region"" of QueryTriple nodes because any triples within an AND node can be targeted first.  Each triple node within an AND can evaluate its cost based on what variables are available, and each node has a notion of what variables it can produce bindings to based on the access method used (e.g., if the access method is DPH, it typically would produce an object variable binding; conversely if the access method is RPH, it would typically produce a subject variable binding).  The cost of producing these bindings is estimated based on the average number of outgoing (DPH) or incoming (RPH) edges in most cases, unless the triple happens to have a popular node which appears in a *top K* set.  Other complex nodes such as EXISTs, UNION or OPTIONAL nodes evaluate their costs recursively by planning for their subtrees. (See https://github.com/Quetzal-RDF/quetzal/tree/master/doc/QuetzalPlanner.pdf)  The planner then chooses the cheapest node to schedule first.  Once it has chosen a node, the set of available variables has changed, so a new of cost computations are performed to find the next step.  The planner proceeds in this manner till there are no more available nodes to plan.  The output of the planner is ``com.ibm.research.rdf.store.sparql11.planner.Plan``, which is basically a binary plan tree that is composed of AND plan nodes, LEFT JOIN nodes, etc.  This serves as the input for the next step.  * SQL generator:  In the current implementation, the plan serves as input to a number of SQL templates, which get created for every type of node in the plan tree.  The ``com.ibm.research.rdf.store.sparql11.sqltemplate`` package contains the templates, which generate SQL modularly per node in the plan tree using common table expressions (CTEs).  The template code is general purpose and keeps track of things such as the specific CTE to node mappings, what external variables need to be projected, which variables should be joined together etc.  The actual job of generating SQL for different backends is accomplished using specialized String Templates from the [String Template](http://www.stringtemplate.org) library.  Example files are ``com.ibm.research.rdf.store.sparql11.sqltemplate.common.stg`` which has the templates that are common to all backends.    For more information on how to get started, click on the Wiki to this repository    # Install and build issues  If you are building from source, get the following:  ``git clone https://github.com/themadcreator/rabinfingerprint`` and build using maven.  * Also install the latest JDBC driver from: https://cloud.google.com/bigquery/partners/simba-drivers/#current_jdbc_driver_releases_1151005 and drop it into lib to compile.  Then clone this repository and build using maven.    # Storage of graph data on cloud SQL backing stores such as Spanner and BigQuery  Since the time we worked on Quetzal, a number of cloud databases have emerged that support the complex SQL queries needed to access graph data. One question that we started to ask recently is whether storage of graph data is better suited for a column oriented, nested type data layout such as BigQuery, or whether a row store such as Spanner is better suited for storage of graph data.  There are tradeoffs to each, and this is by no means an exhaustive comparison of the two different approaches, but we performed some very initial experiments on the following layout on BigQuery versus Spanner for a simple graph query which is not just a 1 hop neighborhood of a node, and we note the rather interesting results here.    * The data and the query:  The graph data are generated from the Lehigh University Benchmark (LUBM) [LUBM](http://swat.cse.lehigh.edu/projects/lubm/) which has a set of students taking courses at a university, and they have advisors.  The data is sparse, and many entities have 1->many edges.  The query is query 9 from that benchmark, which is to find students taking courses taught by their advisors.  Students in that graph take many courses, and have a single advisor.  Each advisor teaches many courses.  And the query asks to find the 'triangle' between them, which is to specify which students take a class that is taught by their advisor.  The graph has 1 billion triples in it, which translates to ~174M rows in an entity oriented store, assuming that 1->many edges such as taking a course, or teaching a course are represented in a single row using arrays or nested data structures.  The dataset is about 79G when written as a JSON file.  * The layout:  Both Spanner and BigQuery provide support for nested data.  Following the entity oriented view of data in Quetzal, the data model is that of a 'subject' or entity, with various edge types mapped to distinct columns. Because BigQuery is ideal for storing columnar, sparse data, we used a 1-1 mapping of each edge type to columns. Furthermore, we did not actually need a reverse mapping since BigQuery has no indexes (every query is a scan).  Instead, it exploits the fact that only specific columns will ever be invoked in a given query. We maintained the same schema for Spanner just to ensure we had an apples to apples comparison.  The layout is therefore like just the DPH table in the [SIGMOD 2013 paper](http://dl.acm.org/citation.cfm?id=2463718), with the one change that we did not separate out the many valued edges into a separate table.  We used Spanner and BigQuery's support for array types to store multi valued predicates in the same column.  Note that Spanner also supports interleaving rows between the two tables which we could have used to support multi valued predicates but we did not do so in this first experiment.  All the code is checked into the spanner-loader and bigquery-loader directories.    * Here are the mappings of edge types to column names in LUBM:    `<http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#undergraduateDegreeFrom>=col_8  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#emailAddress>=col_6  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#mastersDegreeFrom>=col_5  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#memberOf>=col_12  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#worksFor>=col_3  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#teachingAssistantOf>=col_15  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#subOrganizationOf>=col_16  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#researchInterest>=col_9  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#teacherOf>=col_7  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#name>=col_2  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#headOf>=col_11  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#telephone>=col_4  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#publicationAuthor>=col_0  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#takesCourse>=col_14  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#doctoralDegreeFrom>=col_10  <http\://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl\#advisor>=col_13  <http\://www.w3.org/1999/02/22-rdf-syntax-ns\#type>=col_1`  * Here is the schema for all the edges in BigQuery for LUBM:    `{""schema"":{""fields"":[{""name"":""subject"",""type"":""string""},{""name"":""col_8"",""type"":""string""},{""name"":""col_6"",""type"":""string""},{""name"":""col_5"",""type"":""string""},{""name"":""col_12"",""type"":""string""},{""name"":""col_3"",""type"":""string""},{""name"":""col_15"",""type"":""string""},{""name"":""col_9"",""type"":""string""},{""name"":""col_16"",""type"":""string""},{""name"":""col_7"",""type"":""string"",""mode"":""repeated""},{""name"":""col_2"",""type"":""string""},{""name"":""col_11"",""type"":""string""},{""name"":""col_4"",""type"":""string""},{""name"":""col_0"",""type"":""string"",""mode"":""repeated""},{""name"":""col_14"",""type"":""string"",""mode"":""repeated""},{""name"":""col_10"",""type"":""string""},{""name"":""col_13"",""type"":""string""},{""name"":""col_1"",""type"":""string"",""mode"":""repeated""}]}}`    * Here is the corresponding schema for Spanner, written as Java code:    ` static void createDatabase(DatabaseAdminClient dbAdminClient, DatabaseId id) {      Operation<Database, CreateDatabaseMetadata> op = dbAdminClient.createDatabase(          id.getInstanceId().getInstance(), id.getDatabase(),          Arrays.asList(""CREATE TABLE DPH (\n"" + ""  subject STRING(MAX) NOT NULL,\n""              + ""  col_0  ARRAY<STRING(MAX)>, \n"" + ""  col_1  ARRAY<STRING(MAX)>, \n""              + ""  col_2  STRING(MAX), \n"" + ""  col_3  STRING(MAX), \n"" + ""  col_4  STRING(MAX), \n""              + ""  col_5  STRING(MAX), \n"" + ""  col_6  STRING(MAX), \n""              + ""  col_7  ARRAY<STRING(MAX)>, \n"" + ""  col_8  STRING(MAX), \n""              + ""  col_9  STRING(MAX), \n"" + ""  col_10  STRING(MAX), \n"" + ""  col_11  STRING(MAX), \n""              + ""  col_12  STRING(MAX), \n"" + ""  col_13  STRING(MAX), \n""              + ""  col_14  ARRAY<STRING(MAX)>, \n"" + ""  col_15  STRING(MAX), \n""              + ""  col_16  STRING(MAX)) \n"" + "" PRIMARY KEY (subject)""));      Database db = op.waitFor().getResult();      System.out.println(""Created database ["" + db.getId() + ""]"");    }`   * And now for the queries.  BigQuery supports common table expressions which were crucial in providing a nice abstraction to construct complex graph queries.  Here is the query for BigQuery:    `with        t1 as (select subject as student, col_13 as advisor from lubm.DPH where col_13 is not null),       t2 as (select subject as student, col_14 as course from lubm.DPH where col_14 is not null),       t3 as (select subject as teacher, col_7 as course from lubm.DPH where col_7 is not null),       t4 as (select teacher, course from t3 t, t.course course),       t5 as (select student, course from t2 t, t.course course)       select t5.student, t4.teacher, t4.course from t4, t5, t1 where t4.course = t5.course and t4.teacher = t1.advisor and t5.student = t1.student`  * Here is the corresponding query for Spanner because it has no support for Common Table Expressions (CTEs):    `select dph1.subject as student, dph1.col_13 as advisor, course, dph2.subject as teacher, c from DPH as dph1, DPH as dph2 cross join unnest(dph1.col_14) as course cross join unnest(dph2.col_7) as c where dph1.col_13 is not null and dph1.col_14 is not null and dph2.col_7 is not null and course = c and dph1.col_13 = dph2.subject`  * And the results.  BigQuery performed this query in 67.6s, and processed about 26.1 GB.  Spanner timed out after 15 minutes.  It is possible that Spanner does not handle un-nesting of arrays as well as BigQuery, but this is an interesting datapoint, and suggests that Spanner may need a different style schema for storing 1->many edges.  The performance of BigQuery is rather impressive, for comparison with some of the databases Quetzal supports on a 100M edge dataset [see here](https://github.com/Quetzal-RDF/quetzal/wiki/Benchmarks).   * Of course, this is a hand crafted experiment for now - but it seems to suggest that BigQuery has at least one key advantage over Spanner for querying graph data, which is its support for CTEs. """
Semantic web;https://github.com/apseyed/SemantGeo;"""SemantGeo  ========= """
Semantic web;https://github.com/plt-tud/r43ples;"""# R43ples    R43ples (Revision for triples) is an open source Revision Management Tool for the Semantic Web.    It provides different revisions of named graphs via a SPARQL interface. All information about revisions, changes, commits, branches and tags are stored in additional named graphs beside the original graph in an attached external triple store.    [![Build Status](https://travis-ci.org/plt-tud/r43ples.png?branch=develop)](https://travis-ci.org/plt-tud/r43ples)  [![Coverity Scan Build Status](https://scan.coverity.com/projects/2125/badge.svg)](https://scan.coverity.com/projects/2125)  [![codecov](https://codecov.io/gh/plt-tud/r43ples/graph/badge.svg)](https://codecov.io/gh/plt-tud/r43ples)  [![codebeat badge](https://codebeat.co/badges/8b09853d-1312-44a0-979b-579fe8551468)](https://codebeat.co/projects/github-com-plt-tud-r43ples-develop)  [![Ohloh Project Status](https://www.ohloh.net/p/r43ples/widgets/project_thin_badge.gif)](https://www.ohloh.net/p/r43ples)      This project provides an enhanced SPARQL endpoint for revision management of named graphs.  R43ples uses an internal Jena TDB is attached to an existing SPARQL endpoint of a triplestore and acts as another endpoint both for normal SPARQL queries  as well as for revision-enhanced SPARQL queries, named R43ples queries.  The R43ples endpoint allows to specify revisions which should be queried for each named graph used inside a SPARQL query.  The whole revision information is stored in additional graphs in the attached Jena TDB.    The [website](http://plt-tud.github.io/r43ples) of R43ples contains further [project information](http://plt-tud.github.io/r43ples/site/project-reports.html) including [Javadocs](http://plt-tud.github.io/r43ples/site/apidocs/) of the *develop* branch.  A running test server should be available under [http://eatld.et.tu-dresden.de:9998/r43ples/sparql](http://eatld.et.tu-dresden.de:9998/r43ples/sparql)      ## Getting Started  ### Dependencies  * JDK 1.8  * Maven    	sudo apt-get install maven default-jdk      ### Compiling  Maven is used for compiling        mvn compile exec:java    Packages (JAR with dependencies for the webservice) can be be built with:        mvn package    ### Running    R43ples runs with standalone web server    ``` bash  java -jar target/r43ples.jar  ```    ### Releases    Releases are stored on [GitHub](https://github.com/plt-tud/r43ples/releases).    There are also *stable* and *latest* docker images available:  ```  docker pull plttud/r43ples  ```    Run default r43ples via docker  ```  docker run -p 9998:9998 plttud/r43ples  ```    Run with specific configuration  ```  docker run -p 9998:9998 -v $PWD/r43ples.conf:/r43ples.conf plttud/r43ples  ```      ## Configuration    There is a configuration file named *resources/r43ples.conf*. The most important ones are the following:    * *triplestore.type* - type of attached triplestore (can be tdb, virtuoso [not working right now], http)  * *triplestore.uri* - URI or path under which R43ples can access the attached triplestore  * *triplestore.user* - user of attached triplestore if necessary  * *triplestore.password* - password of attached triplestore if necessary  * *revision.graph* - named graph which is used by R43ples to store revision graph information  * *evolution.graph* - named graph which is used by R43ples to store all information regarding evolutions  * *sdg.graph* - named graph for storing the SDG  * *sdg.graph.defaultContent* - default content of SDG which should be stored within named graph (sdg.graph)  * *sdg.graph.defaultSDG* -  Structural Definition Group within the named graph (sdg.graph) which should be associated with new graphs under revision control (mmo:hasDefaultSDG)  * *rules.graph* - named graph for storing the high level change aggregation and co-evolution rules  * *rules.graph.defaultContent* - default content of rules  * *service.host* - host which provides R43ples  * *service.port* - port which should provide R43ples  * *service.path* - path of host which should provide R43ples    The logging configuration is stored in *resources/log4j.properties*      ## Interfaces    ### Extended SPARQL endpoint  SPARQL endpoint is available at:        [uri]:[port]/r43ples/sparql    The endpoint directly accepts SPARQL queries with HTTP GET or HTTP POST parameters for *query* and *format*:        [uri]:[port]/r43ples/sparql?query=[]&format=[]    #### Supported Formats    The formats can be specified as URL Path Parameter *format*, as HTTP post paramter *format* or as HTTP header parameter *Accept*:    * text/turtle  * application/json  * application/rdf+xml  * text/html  * text/plain      #### R43ples keywords    There are some additional keywords which extends SPARQL and can be used to control the revisions of graphs:    * Create graph            CREATE GRAPH <graph>    * Select query            SELECT *          WHERE {          	GRAPH <graph> REVISION ""23"" {?s ?p ?o}      	}      	          SELECT *          WHERE {              GRAPH <graph> REVISION ""master"" {?s ?p ?o}          }    * Update query            USER ""mgraube"" MESSAGE ""test commit""          INSERT {              GRAPH <test> BRANCH ""master"" {                  <a> <b> <c> .              }          }                    USER ""mgraube"" MESSAGE ""test commit""          DELETE {              GRAPH <test> BRANCH ""develop"" {                  <a> <b> <c> .              }          }    * Branching            USER ""mgraube""          MESSAGE ""test commit""          BRANCH GRAPH <test> REVISION ""2"" TO ""unstable""    * Tagging            USER ""mgraube""          MESSAGE ""test commit""          TAG GRAPH <test> REVISION ""2"" TO ""v0.3-alpha""    * Merging    		USER ""Mister X.""  		MESSAGE ""merge example for a common merge""  		MERGE GRAPH <test> BRANCH ""branch-1"" INTO BRANCH ""branch-2""  		  		USER ""Mister X.""          MESSAGE ""merge example for automatica conflict resolution based upon specified SDD""          MERGE AUTO GRAPH <test> BRANCH ""branch-1"" INTO BRANCH ""branch-2""                    USER ""Mister X.""          MESSAGE ""merge example for a common merge with conflict resolution in WITH part""          MERGE GRAPH <test> BRANCH ""branch-1"" INTO BRANCH ""branch-2"" WITH {              <http://test.com/Carlos> <http://test.com/knows> <http://test.com/Danny> .              <http://test.com/Franz> <http://test.com/knows> <http://test.com/Silvia> .          }                    USER ""Mister X.""          MESSAGE ""merge example for manual specification of merged revision content""          MERGE MANUAL GRAPH <test> BRANCH ""branch-1"" INTO BRANCH ""branch-2"" WITH {              <http://test.com/Carlos> <http://test.com/knows> <http://test.com/Danny> .              <http://test.com/Franz> <http://test.com/knows> <http://test.com/Silvia> .          }  		  * Pick a revision into a branch                    USER ""Mister X.""          MESSAGE ""pick single revision example""          PICK GRAPH <test> REVISION ""56"" INTO BRANCH ""develop""                    USER ""Mister X.""          MESSAGE ""pick multiple revisions example""          PICK GRAPH <test> REVISION ""56"" TO REVISION ""62"" INTO BRANCH ""develop""            * Aggregate atomic changes to high level ones (semantic changes)                    AGG GRAPH <test> REVISION ""1"" TO REVISION ""2""            * Coevolve semantic changes to dependent revised graphs                    USER ""Mister X.""          MESSAGE ""Coevolution example""          COEVO GRAPH <test> REVISION ""1"" TO REVISION ""2""      #### Query Rewriting option    There is a new option for R43ples which improves the performance. The necessary revision is not temporarily generated anymore.  The SPARQL query is rewritten in such a way that the branch and the change sets are directly joined inside the query. This includes the order of the change sets.  It is currently under development and further research.    The option can be enabled by passing an additional parameter ""query_rewriting=true""    It currently supports:    * Multiple Graphs  * Multiple TriplePath  * FILTER  * MINUS    For more details, have a look into the *doc/* directory.    ### Debug SPARQL endpoint    R43ples redirects the queries performed on the debug endpoint directly to the attached triplstore.  Thus, this endpoint can be used for debugging purposes.    	[uri]:[port]/r43ples/debug    ### API  R43ples provides some functionalities additionally via an external API, even if all information can also be queried directly from the triplestore    * *api/getRevisedGraphs* lists all graphs managed by R43ples  * *createSampleDataset* generates some sample datasets with revision information  * *revisiongraph?graph=<test>&format=application/json* provides the revision graph for the specified graph (    ## Concept of R43ples    ### Extended SPARQL proxy    R43ples itself does not story any information. All information in the revised graphs and about the revised graphs  are stored in the attached triplestore. R43ples acts only as a proxy which evaluates additional revision information  in the SPARQL queries.    ![System Structure](./doc/r43ples-system.png)      ### Revision information    All information about the revision history of all named graphs is stored in the named graph **http://eatld.et.tu-dresden.de/r43ples-revisions** (as long as not configured otherwise in the configuration file).    Here, the Revision Management Ontology (RMO) is used to model revisions, branches and tags. Furthermore commits are stored which connect each revision, tag and branch with its prior revision.    The RMO is derived from the PROV ontology:  ![RMO example](./doc/ontology/RMO_UML.png)    An exemplary revision graph is shown here:  ![RMO example](./doc/revision management description/r43ples-creategraph.png)        ### HTTP Header information    Each response header contains information about the revision information of the graphs specified in the requests in the *r43ples-revisiongraph* HTTP header field. This information follows the RMO and is transferred as Turtle serialization.    Clients can also pass this information in R43ples update queries to the R43ples server via the *r43ples-revisiongraph* HTTP header attribute.  The server will check if the client is aware of the most recent version of the involved revised graphs. If this is not the case,  the update query will be rejected.        ## Used libraries and frameworks    Following libraries are used in R43ples:    * [Jersey](https://jersey.java.net/) for RestFul web services in Java  * [Grizzly](https://grizzly.java.net/) as web server  * [Jena ARQ](https://jena.apache.org/documentation/query/index.html) for processing SPARQL results  * [Jena TDB](https://jena.apache.org/documentation/tdb/index.html) as triplestore  * [jQuery](http://jquery.com/) as JavaScript framework  * [Bootstrap](http://getbootstrap.com/) as HTML, CSS and JS framework  * [Mustache](https://mustache.github.io/) as template engine  * [SpinRDF](https://github.com/spinrdf/spinrdf) as SPIN engine      """
Semantic web;https://github.com/simonjupp/java-skos-api;"""SKOS API  ========    The SKOS API provides a Java interface and OWL API based implementation of the Simple Knowledge Organization System (SKOS). SKOS is a W3C vocabulary for describing Knowledge Organization Systems (KOS) such as thesauri or concept schemes. For more information about SKOS see [here](http://www.w3.org/2004/02/skos/). An implementation of the SKOS API is provided using the [OWL API](http://owlcs.github.io/owlapi/).    The SKOS API is available in a single jar in [Releases](https://github.com/simonjupp/java-skos-api/releases)    Build from source  ------------    The SKOS API can be built using Apache Maven. This will create a jar in the distribution/target/ folder.    _mvn clean install_    Documentation  -------------    Please refer to [skosapi.sourceforge.net](http://skosapi.sourceforge.net) for documentation. Some examples are available in the skos-example module.    Support  -------    Please post all question to skos-dev@googlegroups.com     """
Semantic web;https://github.com/AKSW/QuitStore;"""  <img alt=""The QuitStore Logo: A glass of quinch jam (German: Quittenmarmelade) with the Git logo on the lid. 'Graph jam in a git glass'"" src=""https://raw.githubusercontent.com/AKSW/QuitStore/master/assets/quitstore.png"" width=""512"" />    # Quit Store    Build status of `master` branch:    [![Build Status](https://travis-ci.org/AKSW/QuitStore.svg?branch=master)](https://travis-ci.org/AKSW/QuitStore)  [![Coverage Status](https://coveralls.io/repos/github/AKSW/QuitStore/badge.svg?branch=master)](https://coveralls.io/github/AKSW/QuitStore)    The *Quit Store* (stands for <em>Qu</em>ads in G<em>it</em>) provides a workspace for distributed collaborative Linked Data knowledge engineering.  You are able to read and write [RDF Datasets](https://www.w3.org/TR/rdf11-concepts/#section-dataset) (aka. multiple [Named Graphs](https://en.wikipedia.org/wiki/Named_graph)) through a standard SPARQL 1.1 [Query](https://www.w3.org/TR/sparql11-query/) and [Update](https://www.w3.org/TR/sparql11-update/) interface.  To collaborate you can create multiple branches of the Dataset and share your repository with your collaborators as you know it from Git.    If you want to read more about the Quit Store we can recommend our paper:    [*Decentralized Collaborative Knowledge Management using Git*](https://natanael.arndt.xyz/bib/arndt-n-2018--jws)  by Natanael Arndt, Patrick Naumann, Norman Radtke, Michael Martin, and Edgard Marx in Journal of Web Semantics, 2018  [[@sciencedirect](https://www.sciencedirect.com/science/article/pii/S1570826818300416)] [[@arXiv](https://arxiv.org/abs/1805.03721)]    ## Getting Started    To get the Quit Store you have three options:    - Download a binary from https://github.com/AKSW/QuitStore/releases (Currently works for amd64 Linux)  - Clone it with Git from our repository: https://github.com/AKSW/QuitStore  - Use Docker and see the section [Docker](#docker) in the README    ### Installation from Source    Install [poetry](https://python-poetry.org/).    Get the Quit Store source code:  ```  $ git clone https://github.com/AKSW/QuitStore.git  $ cd QuitStore  ```  If you are using virtualenvwrapper:  ```  $ poetry install  $ poetry run quitstore --help  ```    ### Git configuration    Configure your name and email for Git.  This information will be stored in each commit you are creating with Git and the Quit Store on your system.  It is relevant so people know which contribution is coming from whom. Execute the following command if you haven't done that before.    ```  $ git config --global user.name ""Your Name""  $ git config --global user.email ""you@e-mail-provider.org""  ```    ### Start with Existing Data (Optional)    If you already have data which you want to use in the quit store follow these steps:    1. Create a repository which will contain your RDF data.    ```  $ git init /path/to/repo  ```    2. Put your RDF data formatted as [N-Triples](https://www.w3.org/TR/n-triples/) and sorted (e.g. using `cat data-in.nt | LC_ALL=C sort -u > data-out.nt`) into files like `<graph>.nt` into this directory.  3. For each `<graph>.nt` file create a corresponding `<graph>.nt.graph` file which must contain the IRI for the respective graph. (These `.graph` files are also used by the [Virtuoso bulk loading process](https://virtuoso.openlinksw.com/dataspace/doc/dav/wiki/Main/VirtBulkRDFLoader#Bulk%20loading%20process)).  4. Add the data to the repository and create a commit.    ```  $ git add …  $ git commit -m ""init repository""  ```    To ingest further versions of your data into the Quit Store you can add further commits by going through steps 2.-4..  Alternatively you are also able to execute SPARQL 1.1. Update operations to create new versions on the Quit Store.    ### Start the Quit Store    If you are using the binary:  ```  $ chmod +x quit #  $ ./quit -t /path/to/repo  ```    If you have it installed from the sources:  ```  $ poetry run quitstore -t /path/to/repo  ```    Open your browser and go to [`http://localhost:5000/`](http://localhost:5000/).    Have a lot of fun!    For more command line options check out the section [Command Line Options](#command-line-options) in the README.        ## Command Line Options    `-b`, `--basepath`    Specify a base path/application root. This will work with WSGI and docker only.    `-t`, `--targetdir`    Specify a target directory where the repository can be found or will be cloned (if remote is given) to.    `-r`, `-repourl`    Specify a link/URL to a remote repository.    `-c`, `--configfile`    Specify a path to a configuration file. (Defaults to ./config.ttl)    `-nv`, `--disableversioning`    Run Quit-Store without versioning activated    `-f`, `--features`    This option enables additional features of the store:    - `provenance` - Enable browsing interfaces for provenance information.  - `persistance` - Store all internal data as RDF graph.  - `garbagecollection` - Enable garbage collection. With this feature enabled, git will check for garbage collection after each commit. This may slow down response time but will keep the repository size small.    `-v`, `--verbose` and `-vv`, `--verboseverbose`    Set the log level for the standard output to verbose (INFO) respective extra verbose (DEBUG).    `-l`, `--logfile`    Write the log output to the given path.  The path is interpreted relative to the current working directory.  The log level for the logfile is always extra verbose (DEBUG).    ## Configuration File    *deprecated* (we plan to remove the configuration file feature)    If you want to work with configuration files you can create a `config.ttl` file.  This configuration file consists of two parts, the store configuration and the graph configuration.  The store configuration manages everything related to initializing the software, the graph configuration maps graph files to their graph IRIs.  The graph configuration in the `config.ttl` is an alternative to using `<graph>.nt.graph` files next to the graphs.  Make sure you put the correct path to your git repository (`""../store""`) and the IRI of your graph (`<http://example.org/>`) and name of the file holding this graph (`""example.nt""`).    ```  conf:store a <YourQuitStore> ;      <pathOfGitRepo> ""../store"" ; # Set the path to the repository that contains the files .      <origin> ""git:github.com/your/repository.git"" . # Optional a git repo that will be cloned into dir given in line above on startup.      conf:example a <Graph> ; # Define a Graph resource for a named graph      <graphUri> <http://example.org/> ; # Set the IRI of named graph      <isVersioned> 1 ; # Defaults to True, future work      <graphFile> ""example.nt"" . # Set the filename  ```    ## API    The Quit-Store comes with three kinds of interfaces, a SPARQL update and query interface, a provenance interface, and a Git management interface.    ### SPARQL Update and Query Interface  The SPARQL interface support update and select queries and is meant to adhere to the [SPARQL 1.1 Protocol](https://www.w3.org/TR/sparql11-protocol/).  You can find the interface to query the current `HEAD` of your repository under `http://your-quit-host/sparql`.  To access any branch or commit on the repository you can query the endpoints under `http://your-quit-host/sparql/<branchname>` resp. `http://your-quit-host/sparql/<commitid>`.  Since the software is still under development there might be some missing features or strange behavior.  If you are sure that the store does not follow the W3C recommendation please [file an issue](https://github.com/AKSW/QuitStore/issues/new).    #### Examples    Execute a select query with curl  ```  curl -d ""select ?s ?p ?o ?g where { graph ?g { ?s ?p ?o} }"" -H ""Content-Type: application/sparql-query"" http://your-quit-host/sparql  curl -d ""select ?s ?p ?o ?g where { graph ?g { ?s ?p ?o} }"" -H ""Content-Type: application/sparql-query"" http://your-quit-host/sparql/develop  ```  If you are interested in a specific result mime type you can use the content negotiation feature of the interface:  ```  curl -d ""select ?s ?p ?o ?g where { graph ?g { ?s ?p ?o} }"" -H ""Content-Type: application/sparql-query"" -H ""Accept: application/sparql-results+json"" http://your-quit-host/sparql  ```    Execute an update query with curl    ```  curl -d ""insert data { graph <http://example.org/> { <urn:a> <urn:b> <urn:c> } }"" -H ""Content-Type: application/sparql-update""  http://your-quit-host/sparql  ```    ### Provenance Interface  To use the provenance browsing feature you have to enable it with the argument `--feature=provenance`.  The provenance browsing feature extracts provenance meta data for the revisions and makes it available through a SPARQL endpoint and the blame interface.  The provenance interface is available under the following two URLs:    - `http://your-quit-host/provenance` which is a SPARQL query interface (see above) to query the provenance graph  - `http://your-quit-host/blame` to get a `git blame` like output per statement in the store    ### Git Management Interface    The git management interface allows access to some operations of quit in conjunction with the underlying git repository.  You can access them with your browser at the following paths.    - `/commits`: See commits, messages, committer, and date of commits.  - `/branch`, `/merge`: allows to manage branches and merge branches with different strategies.  - `/pull`, `/fetch`, `/push` work similar to the respective git commands. (These operations will only works if you have configured remotes on the repository.)    ## Docker    We provide a Docker image for the Quit Store on the [public docker hub](https://hub.docker.com/r/aksw/quitstore/) as well as on the [github docker registry](https://github.com/AKSW/QuitStore/pkgs/container/quitstore).  The image exposes port 8080 by default.  The default user within the image is the user `quit` with the user id `1000`.  For this user a git configuration with `user.name QuitStore` and `user.email quitstore@example.org` is preset.  Without any further configuration, a git repository is initialized within the container in the `/data` directory (owned by the default user `quit`).    To store the data on the host a local directory or volume is required to store the git repository.  An host directory or volume can be linked to the directory `/data`.  Make sure the quit process running with the user id `1000` within the docker container has write access to this directory.    Alternatively the user id within the container can be set using the [`docker run --user $UID …` option](https://docs.docker.com/engine/reference/commandline/run/).  In this case you have to make sure a `user.name` a `user.email` is configure using `git config` within the repository (`.git/config`) or a git config file is mounted to `/.gitconfig` (to `/usr/src/app/.gitconfig` if you are running it with user id `1000`).    Example setup with the default user:    ```  mkdir /store/repo  sudo chown 1000 /store/repo  sudo chmod u+w /store/repo  ```    To run the image execute the following command (maybe you have to replace `docker` with `sudo docker`):    ```  docker run -it --name containername -p 8080:8080 -v /store/repo:/data aksw/quitstore  ```    The following example will start the quit store in the background in the detached mode.    ```  docker run -d --name containername -p 8080:8080 -v /store/repo:/data aksw/quitstore  ```    Now you should be able to access the quit web interface under `http://localhost:8080` and the SPARQL 1.1 interface under `http://localhost:8080/sparql`.    The default configuration is located in `/etc/quit/config.ttl`, which can also be overwritten using a respective volume or by setting the `QUIT_CONFIGFILE` environment variable.    Further options which can be set are:    * `QUIT_TARGETDIR` - the target repository directory on which quit should run  * `QUIT_CONFIGFILE` - the path to the config.ttl (default: `/etc/quit/config.ttl`)  * `QUIT_LOGFILE` - the path where quit should create its logfile  * `QUIT_BASEPATH` - the HTTP base path where quit will be served  * `QUIT_OAUTH_CLIENT_ID` - the GitHub OAuth client id (for OAuth see also the [github docu](https://developer.github.com/apps/building-oauth-apps/authorization-options-for-oauth-apps/))  * `QUIT_OAUTH_SECRET` - the GitHub OAuth secret    ## Run the Tests    You need to have the quitstore installed from source, see section [Installation from Source](#installation-from-source).    ```  poetry run pytest  ```    ## Troubleshooting    ### Use on Windows with restricted permissions    On Windows you might not be able to download the `.exe` file directly.  If so, use the `curl` command in the power shell.    When you start the QuitStore (e.g. with `quit.exe -t .`) it will try to open a port that is available from outside, which will require permission by the administrator user.  To open the port only locally you should start the QuitStore with:    ```  quit.exe -t . -h localhost  ```    The default port is `5000` (`http://localhost:5000/`).    ## Migrate from old Versions    ### Update to 2018-11-20 from 2018-10-29 and older    If you are migrating from an NQuads based repository, as used in older versions of the QuitStore (release 2018-10-29 and older), to an NTriples based repository (release 2018-11-20 and newer) you can use teh following commands to migrate the graphs.  You should know that it is possible to have multiple graphs in one NQuads file, which is not possible for NTriples files.  Thus, you should make sure to have only one graph per file.  You may execute the steps for each NQuads file and replace `graphfile.nq` according to your filenames.    ```  sed ""s/<[^<>]*> .$/./g"" graphfile.nq | LC_ALL=C sort -u > graphfile.nt  mv graphfile.nq.graph graphfile.nt.graph  git rm graphfile.nq  git add graphfile.nq.graph graphfile.nt graphfile.nt.graph  git commit -m ""Migrate from nq to nt""  ```    ## License    Copyright (C) 2017-2022 Norman Radtke <http://aksw.org/NormanRadtke>, Natanael Arndt <http://aksw.org/NatanaelArndt>, and contributors    This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3 of the License, or (at your option) any later version.    This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.    You should have received a copy of the GNU General Public License along with this program; if not, see <http://www.gnu.org/licenses>.  Please see [LICENSE](LICENSE) for further information. """
Semantic web;https://github.com/alangrafu/lodspeakr;"""LODSPeaKr  =========    author: Alvaro Graves (alvaro@graves.cl)    version: 20130612      [LODSPeaKr](http://lodspeakr.org) is a framework for creating Linked Data applications in a simple and easy way. You can see [several applications](http://alangrafu.github.com/lodspeakr/applications.html) created using LODSPeaKr.    Simplest Installation  ---------------------    Simply go to your web server root directory (e.g., /var/www/) and run       bash < <(curl -sL http://lodspeakr.org/install)    You will be required to answer 3 questions:    * What is the location of lodspeakr? If you are running the script in `/var/www/visualizations` it is highly likely it will be `http://localhost/visualizations` or `http://yourdomainname/visualizations`  * What is the domain of the data you want to query? For now, you can leave it as the default (i.e., press Enter)  * What is the URL of your SPARQL endpoint? Where should Visualbox look to execute SPARQL queries.    Finally, give write permissions to the web server in `lodspeakr/meta`, `lodspeakr/cache`, `lodspeakr/settings.inc.php`  and `lodspeakr/components`. This can be done in several ways:    * `sudo chown WEBSERVERUSER lodspeakr/meta lodspeakr/cache lodspeakr/settings.inc.php lodspeakr/components`      * **Note** You can find the name of your web server user by running `ps aux|egrep ""apache|httpd|www"" |egrep -v ""grep|root""|awk '{print $1}'|uniq`  * Alternatively you can run `chdmod -R 777 lodspeakr/meta lodspeakr/cache lodspeakr/settings.inc.php lodspeakr/components` but this is highly discouraged    More documentation on installation of LODSPeaKr is available at the [LODSPeaKr wiki](https://github.com/alangrafu/lodspeakr/wiki) """
Semantic web;https://github.com/ontola/rdfdev-js;"""# js.rdf.dev  [![GitHub stars](https://img.shields.io/github/stars/ontola/rdfdev-js?style=social)](https://github.com/ontola/rdfdev-js)  [![Read the Docs](https://img.shields.io/readthedocs/pip.svg)](https://js.rdf.dev/)  [![Maintainability](https://api.codeclimate.com/v1/badges/292914da43d93b43addd/maintainability)](https://codeclimate.com/github/ontola/rdfdev-js/maintainability)    Collection of libraries to ease in JavaScript RDF development.  Open source (MIT licensed).    ## Packages  ### IRI  [![Read the Docs](https://img.shields.io/readthedocs/pip.svg)](https://js.rdf.dev/iri)  [![npm (tag)](https://img.shields.io/npm/v/@rdfdev/iri)](https://npmjs.com/package/@rdfdev/iri)  [![npm bundle size](https://img.shields.io/bundlephobia/minzip/@rdfdev/iri)](https://bundlephobia.com/result?p=@rdfdev/iri)    A lot of IRI/URI manipulation can happen while working with linked data, this package provides  utility functions to do just that.    ### Actions  [![Read the Docs](https://img.shields.io/readthedocs/pip.svg)](https://js.rdf.dev/actions)  [![npm (tag)](https://img.shields.io/npm/v/@rdfdev/actions)](https://npmjs.com/package/@rdfdev/actions)  [![npm bundle size](https://img.shields.io/bundlephobia/minzip/@rdfdev/actions)](https://bundlephobia.com/result?p=@rdfdev/actions)    Utilities for working with [link actions](https://github.com/rescribet/link-lib/wiki/Hypermedia-API)  and link middleware.    ### Collections  [![Read the Docs](https://img.shields.io/readthedocs/pip.svg)](https://js.rdf.dev/collections)  [![npm (tag)](https://img.shields.io/npm/v/@rdfdev/collections)](https://npmjs.com/package/@rdfdev/collections)  [![npm bundle size](https://img.shields.io/bundlephobia/minzip/@rdfdev/collections)](https://bundlephobia.com/result?p=@rdfdev/collections)    Utilities for reading and manipulating different kinds of RDF collections (rdf:Seq, rdf:List)    ### Delta  [![Read the Docs](https://img.shields.io/readthedocs/pip.svg)](https://js.rdf.dev/delta)  [![npm (tag)](https://img.shields.io/npm/v/@rdfdev/delta)](https://npmjs.com/package/@rdfdev/delta)  [![npm bundle size](https://img.shields.io/bundlephobia/minzip/@rdfdev/delta)](https://bundlephobia.com/result?p=@rdfdev/delta)    Utilities to quickly create [linked deltas](https://github.com/ontola/linked-delta), an rdf-native  way to express and process changes in state.    ### Prop types  [![Read the Docs](https://img.shields.io/readthedocs/pip.svg)](https://js.rdf.dev/prop-types)  [![npm (tag)](https://img.shields.io/npm/v/@rdfdev/prop-types)](https://npmjs.com/package/@rdfdev/prop-types)  [![npm bundle size](https://img.shields.io/bundlephobia/minzip/@rdfdev/prop-types)](https://bundlephobia.com/result?p=@rdfdev/prop-types)    React [prop-type](https://reactjs.org/docs/typechecking-with-proptypes.html) declarations for the  RDF data structures.    ## See also  The following libraries are used by these packages.  ### @ontologies/core  [![GitHub stars](https://img.shields.io/github/stars/ontola/ontologies?style=social)](https://github.com/ontola/ontologies)  [![npm (tag)](https://img.shields.io/npm/v/@ontologies/core/next?label=npm)](https://npmjs.com/package/@ontologies/core)  ![npm bundle size](https://img.shields.io/bundlephobia/minzip/@ontologies/core@next)    Makes working with RDF a breeze:  * Types for RDF including quads, literals, resources.  * A data factory for creating RDF data.  * Access a run-time set datafactory in static context!  * Typeguards to check if something is an RDF object.    ### Link lib    [![GitHub stars](https://img.shields.io/github/stars/rescribet/link-lib?style=social)](https://github.com/rescribet/link-lib)  [![npm (tag)](https://img.shields.io/npm/v/link-lib/light?label=npm)](https://npmjs.com/package/link-lib)  ![npm bundle size](https://img.shields.io/bundlephobia/minzip/link-lib@light)  ![CircleCI](https://img.shields.io/circleci/build/gh/rescribet/link-lib/use-data-factory-and-ontologies)  [![Maintainability](https://api.codeclimate.com/v1/badges/e8824bb0fb4bcf689749/maintainability)](https://codeclimate.com/github/rescribet/link-lib/maintainability)    Fetch, store, write and render linked data.    ### Link redux  [![GitHub stars](https://img.shields.io/github/stars/rescribet/link-redux?style=social)](https://github.com/rescribet/link-redux)  [![npm (tag)](https://img.shields.io/npm/v/link-redux/light?label=npm)](https://npmjs.com/package/link-redux)  ![npm bundle size](https://img.shields.io/bundlephobia/minzip/link-redux@light)  ![CircleCI](https://img.shields.io/circleci/build/gh/rescribet/link-redux/datafactory)  [![Maintainability](https://api.codeclimate.com/v1/badges/6801255f84f20aa73420/maintainability)](https://codeclimate.com/github/rescribet/link-redux/maintainability)    All the tools needed to quickly create interactive web apps consuming RDF.    ## Need help with linked data?    All these package are brought to you by [Ontola](https://ontola.io), we build production-grade  linked data solutions and can help you from advice to building custom web services. """
Semantic web;https://github.com/d2rq/d2rq;"""# D2RQ – A Database to RDF Mapper    D2RQ exposes the contents of relational databases as RDF. It consists of:    * The **D2RQ Mapping Language**. Use it to write mappings between database tables and RDF vocabularies or OWL ontologies.  * The **D2RQ Engine**, a SPARQL-to-SQL rewriter that can evaluate SPARQL queries over your mapped database. It extends ARQ, the query engine that is part of Apache Jena.  * **D2R Server**, a web application that provides access to the database via the SPARQL Protocol, as Linked Data, and via a simple HTML interface.    ## Homepage and Documentation    Learn more about D2RQ at its homepage: http://d2rq.org/    ## License    Apache License, Version 2.0    http://www.apache.org/licenses/LICENSE-2.0.html    ## Contact, feedback, discussion    Please use the issue tracker here on GitHub for feature/bug discussion and support requests.    ## Building from source    ### Prerequisites    You need some tools in order to be able to build D2RQ. Depending on your operating system, they may or may not be already installed.    * [git](http://git-scm.com/), for forking the source code repository from GitHub. Run `git` on the command line to see if it's there.  * [Java JDK](http://www.oracle.com/technetwork/java/javase/downloads/index.html) v5 or later, for compiling Java sources. Run `java -version` and `javac` on the command line to see if it's there.  * [Apache Ant](http://ant.apache.org/), for building D2RQ. Run `ant` on the command line to see if it's there.    ### Getting the source    Get the code by forking the GitHub repository and cloning your fork, or directly clone the main repository:    ```git clone git@github.com:d2rq/d2rq.git```    ### Doing Ant builds    D2RQ uses Apache Ant as its build system. You can run `ant -p` from the project's main directory to get an overview of available targets:    To run the D2RQ tools, you need to do at least `ant jar`.    <table>  <tr><td>ant all</td><td>Generate distribution files in zip and tar.gz formats</td></tr>  <tr><td>ant clean</td><td>Deletes all generated artefacts</td></tr>  <tr><td>ant compile</td><td>Compile project classes</td></tr>  <tr><td>ant compile.tests</td><td>Compile test classes</td></tr>  <tr><td>ant jar</td><td>Generate project jar file</td></tr>  <tr><td>ant javadoc</td><td>Generate Javadoc API documentation</td></tr>  <tr><td>ant tar</td><td>Generate distribution file in tar.gz format</td></tr>  <tr><td>ant test</td><td>Run tests</td></tr>  <tr><td>ant vocab.config</td><td>Regenerate Config vocabulary files from Turtle source</td></tr>  <tr><td>ant vocab.d2rq</td><td>Regenerate D2RQ vocabulary files from Turtle source</td></tr>  <tr><td>ant war</td><td>Generate war archive for deployment in servlet container</td></tr>  <tr><td>ant zip</td><td>Generate distribution file in zip format</td></tr>  </table>    ## Running D2RQ    After building with `ant jar`, you can test-run the various components. Let's assume you have a MySQL database called `mydb` on your machine.    ### Generating a default mapping file    ```./generate-mapping -u root -o mydb.ttl jdbc:mysql:///mydb```    This generates a mapping file `mydb.ttl` for your database.    ### Dumping the database    ```./dump-rdf -m mydb.ttl -o dump.nt```    This creates `dump.nt`, a dump containing the mapped RDF in N-Triples format.    ### Running D2R Server    ```./d2r-server mydb.ttl```    This starts up a server at http://localhost:2020/    ### Deploying D2R Server into a servlet container    Edit `/webapp/WEB-INF/web.xml` to point the `configFile` parameter to the location of your mapping file.    Build a war file with `ant war`.    Deploy the war file, e.g., by copying it into the servlet container's `webapps` directory.    ### Running the unit tests    The unit tests can be executed with `ant test`.    Some unit tests rely on MySQL being present, and require that two databases are created:    1. A database called `iswc` that contains the data from `/doc/example/iswc-mysql.sql`:        echo ""CREATE DATABASE iswc"" | mysql -u root      mysql -u root iswc < doc/example/iswc-mysql.sql    2. An empty database called `D2RQ_TEST`. """
Semantic web;https://github.com/epimorphics/elda;"""<h1>Elda, an implementation of the Linked Data API</h1>    <p>  	Elda is a Java implementation of the   	<a href=""http://code.google.com/p/linked-data-api/"" rel=""nofollow"">Linked Data API</a>,  	which provides a configurable way to access RDF data using simple   	RESTful URLs that are translated into queries to a SPARQL endpoint.   	The API developer (probably you) writes an API spec (in RDF) which   	specifies how to translate URLs into queries.   </p>    <p>  	Elda is the   	<a href=""http://www.epimorphics.com/web/"">Epimorphics</a> implementation of the LDA. The <i>standalone jar</i>  	comes with pre-built examples which allow you to experiment with the style   	of query and get started with building your own configurations using  	<i>elda common</i> or your own webapps.  </p>    <p>  	See <a href=""http://epimorphics.github.io/elda/current/index.html"">  		the current Elda documentation,  	</a> or the forthcoming release's documentation linked from  	<a href=""http://epimorphics.github.io/elda/index.html"">  		the documentation index  	</a>.  </p>   """
Semantic web;https://github.com/kasei/swift-sparql-syntax;"""# SPARQLSyntax    ## SPARQL 1.1 Parser and Abstract Syntax     - [Features](#features)   - [Building](#building)   - [Swift Package Manager](#swift-package-manager)   - [Command Line Usage](#command-line-usage)   - [API](#api)     - [Term](#term)     - [Triple, Quad, TriplePattern and QuadPattern](#triple-quad-triplepattern-and-quadpattern)     - [Algebra](#algebra)     - [Expression](#expression)     - [Query](#query)     - [SPARQLParser](#sparqlparser)     - [SPARQLSerializer](#sparqlserializer)   - [Extensions](#extensions)     - [Window Functions](#window-functions)    ### Features    * [SPARQL 1.1] Parser, Tokenizer, and Serializer available via both API and command line tool  * Abstract syntax representation of SPARQL queries, aligned with the [SPARQL Algebra]  * Supported extensions:    - [Window Functions](#window-functions)    ### Building    ```  % swift build -c release  ```    ### Swift Package Manager    To use SPARQLSyntax with projects using the [Swift Package Manager],  add the following to your project's `Package.swift` file:      ```swift    dependencies: [      .package(url: ""https://github.com/kasei/swift-sparql-syntax.git"", .upToNextMinor(from: ""0.0.91""))    ]    ```    ### Command Line Usage    A command line tool, `sparql-parser`, is provided to parse a SPARQL query and  print its parsed query algebra, its tokenization, or a pretty-printed SPARQL  string:    ```  % ./.build/release/sparql-parser   Usage: ./.build/release/sparql-parser [-v] COMMAND [ARGUMENTS]         ./.build/release/sparql-parser parse query.rq         ./.build/release/sparql-parser lint query.rq         ./.build/release/sparql-parser tokens query.rq  ```    To ""lint"", or ""pretty print"", a SPARQL query:    ```  % cat examples/messy.rq  prefix geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>  select    ?s  where{  ?s geo:lat ?lat ;geo:long ?long   ;  	FILTER(?long < -117.0)  FILTER(?lat >= 31.0)    FILTER(?lat <= 33.0)  } ORDER BY ?s    % ./.build/release/sparql-parser lint examples/messy.rq   PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>  SELECT ?s WHERE {      ?s geo:lat ?lat ;          geo:long ?long ;      FILTER (?long < - 117.0)      FILTER (?lat >= 31.0)      FILTER (?lat <= 33.0)  }  ORDER BY ?s    ```    To parse the query and print the resulting query algebra:    ```  % ./.build/release/sparql-parser parse examples/messy.rq  Query    Select { ?s }          Project { ?s }            OrderBy { ?s }              Filter (((?long < -117.0) && (?lat >= 31.0)) && (?lat <= 33.0))                BGP                  ?s <http://www.w3.org/2003/01/geo/wgs84_pos#lat> ?lat .                  ?s <http://www.w3.org/2003/01/geo/wgs84_pos#long> ?long .    ```    ### API    The `SPARQLSyntax` library provides an API for parsing SPARQL queries  and accessing the resulting abstract data structures.  The primary components of this API are:    * `struct Term` - A representation of an RDF Term (IRI, Literal, or Blank node)  * `enum Algebra` - A representation of the query pattern closely aligned with the formal SPARQL Algebra  * `enum Expression` - A representation of a logical expression  * `struct Query` - A representation of a SPARQL query including: a query form (`SELECT`, `ASK`, `DESCRIBE`, or `CONSTRUCT`), a query `Algebra`, and optional base URI and dataset specification  * `struct SPARQLParser` - Parses a SPARQL query String/Data and returns a `Query`  * `struct SPARQLSerializer` - Provides the ability to serialize a query, optionally applying ""pretty printing"" formatting    #### `Term`    `struct Term` represents an [RDF Term] (an IRI, a blank node, or an RDF Literal).  `Term` also provides some support for XSD numeric types,  bridging between `Term`s and `enum NumericValue` which provides numeric functions and [type-promoting operators](https://www.w3.org/TR/xpath20/#promotion).    #### `Triple`, `Quad`, `TriplePattern`, and `QuadPattern`    `struct Triple` and `struct Quad` combine `Term`s into RDF triples and quads.  `struct TriplePattern` and `struct QuadPattern` represent patterns which can be matched by concrete `Triple`s and `Quad`s.  Instead of `Term`s, patterns are comprised of a `enum Node` which can be either a bound `Term`, or a named `variable`.    #### `Algebra`    `enum Algebra` is an representation of a query pattern aligned with the [SPARQL Algebra].  Cases include simple graph pattern matching such as `triple`, `quad`, and `bgp`,  and more complex operators that can be used to join other `Algebra` values  (e.g. `innerJoin`, `union`, `project`, `distinct`).    `Algebra` provides functions and properties to access features of graph patterns including:  variables used; and in-scope, projectable, and ""necessarily bound"" variables.  The structure of `Algebra` values can be modified using a rewriting API that can:  bind values to specific variables; replace entire `Algebra` sub-trees; and rewrite `Expression`s used within the `Algebra`.    #### `Expression`    `enum Expression` represents a logical expression of variables, values, operators, and functions  that can be evaluated within the context of a query result to produce a  `Term` value.  `Expression`s are used in the following `Algebra` operations: filter, left outer join (""OPTIONAL""), extend (""BIND""), and aggregate.    `Expression`s may be modified using a similar rewriting API to that provided by `Algebra` that can:  bind values to specific variables; and replace entire `Expression` sub-trees.    #### `Query`    `struct Query` represents a SPARQL Query and includes:    * a query form (`SELECT`, `ASK`, `DESCRIBE`, or `CONSTRUCT`, and any associated data such as projected variables, or triple patterns used to `CONSTRUCT` a result graph)  * a graph pattern (`Algebra`)  * an optional base URI  * an optional dataset specification    #### `SPARQLParser`    `struct SPARQLParser` provides an API for parsing a SPARQL 1.1 query string and producing a `Query`.    #### `SPARQLSerializer`    `struct SPARQLSerializer` provides an API for serializing SPARQL 1.1 queries, optionally applying ""pretty printing"" rules to produce consistently formatted output.  It can serialize both structured queries (`Query` and `Algebra`) and unstructured queries (a query `String`).  In the latter case, serialization can be used even if the query contains syntax errors (with data after the error being serialized as-is).    ### Extensions    #### Window Functions    Parsing of window functions is supported as an extension to the SPARQL 1.1 syntax.  A SQL-like syntax is supported for projecting window functions in a `SELECT` clause, as well as in a `HAVING` clause.  In addition to the built-in aggregate functions, the following window functions are supported:  `RANK`, `ROW_NUMBER`.    Shown below are some examples of the supported syntax.    ```swift  # ""Limit By Resource""  # This query limits results to two name/school pairs per person  PREFIX foaf: <http://xmlns.com/foaf/0.1/>  SELECT ?name ?school WHERE {  	?s a foaf:Person ;  		foaf:name ?name ;  		foaf:schoolHomepage ?school  }  HAVING (RANK() OVER (PARTITION BY ?s) < 2)  ```    ```swift  # Use window framing to compute a moving average over the trailing four results  PREFIX : <http://example.org/>  SELECT (AVG(?value) OVER (ORDER BY ?date ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS ?movingAverage) WHERE {  	VALUES (?date ?value) {  		(1 1.0)  		(2 2.0)  		(3 3.0)  		(4 2.0)  		(5 0.0)  		(6 0.0)  		(7 1.0)  	}  }  ```      [SPARQL 1.1]: https://www.w3.org/TR/sparql11-query  [SPARQL Algebra]: https://www.w3.org/TR/sparql11-query/#sparqlAlgebra  [Swift Package Manager]: https://swift.org/package-manager  [RDF Term]: https://www.w3.org/TR/sparql11-query/#sparqlBasicTerms """
Semantic web;https://github.com/dbiir/jdbc-for-rdf3x;"""# jdbc-for-rdf3x  This is a jdbc connector for rdf-3x. """
Semantic web;https://github.com/andrewdbate/Sequoia;"""[![License: GPL v3](https://img.shields.io/badge/license-GNU%20GPL%20v3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)  [![Build Status](https://travis-ci.org/andrewdbate/Sequoia.svg?branch=master)](https://travis-ci.org/andrewdbate/Sequoia)  [![Build Status](https://ci.appveyor.com/api/projects/status/github/andrewdbate/Sequoia?branch=master&svg=true)](https://ci.appveyor.com/project/andrewdbate/sequoia)    # Sequoia: An Open Source OWL 2 DL Reasoner for Java    Welcome to the official repository for Sequoia!    Sequoia is an ontology reasoner that supports OWL 2 DL ontologies. Sequoia can be used from the command line, through  the Protégé plug-in, or via the OWL API.    Feel free to fork this repository and submit pull requests.    If you discover a bug, please report it [here](https://github.com/andrewdbate/Sequoia/issues) on GitHub.    Sequoia is free and open-source software that is licensed under the [GNU GPL v3](https://github.com/andrewdbate/Sequoia/blob/master/LICENSE) only.    _Note:_ To compile Sequoia you will need JDK 8 installed. Compiling with Java 9 is not supported at this time.    #### Current Limitations   * Datatypes are not supported.   * Neither nominals nor ABox assertions are supported.   * SWRL rules are not supported.    ## Publications  The following publications describe the algorithms and implementation of Sequoia.  1. Andrew Bate‚ Boris Motik‚ Bernardo Cuenca Grau‚ František Simančík, Ian Horrocks.     [**Extending Consequence−Based Reasoning to SRIQ**](https://www.cs.ox.ac.uk/files/8182/paper.pdf).     In _Principles of Knowledge Representation and Reasoning: Proceedings of the Fifteenth International Conference_.     Pages 187–196. AAAI Press. 2016.     **(Main reference on Sequoia)**          [Paper (PDF)](https://www.cs.ox.ac.uk/files/8182/paper.pdf) | [Slides (PDF)](https://www.cs.ox.ac.uk/files/8181/slides.pdf)       2. Andrew Bate‚ Boris Motik‚ Bernardo Cuenca Grau‚ František Simančík, Ian Horrocks.     [**Extending Consequence−Based Reasoning to SRIQ (Technical Report)**](https://arxiv.org/abs/1602.04498).     arXiv:1602.04498 \[cs.AI\]. February, 2016.          [Technical Report (PDF)](https://arxiv.org/pdf/1602.04498.pdf)       3. Andrew Bate‚ Boris Motik‚ Bernardo Cuenca Grau‚ František Simančík, Ian Horrocks.     [**Extending Consequence−Based Reasoning to SHIQ**](https://www.cs.ox.ac.uk/files/7444/paper.pdf)     In _Proceedings of the 28th International Workshop on Description Logics_.     Vol. 1350 of CEUR Workshop Proceedings. CEUR−WS.org. 2015.       [Paper (PDF)](https://www.cs.ox.ac.uk/files/7444/paper.pdf) | [Technical Report (PDF)](https://www.cs.ox.ac.uk/files/7864/techreport.pdf)    ## Development and Building    Sequoia is developed by [Andrew Bate](https://www.linkedin.com/in/andrewdbate/) and is actively maintained.    To build Sequoia, you will need [SBT](https://www.scala-sbt.org/) installed on your system.    The Sequoia reasoner is comprised of multiple subprojects:   * `reasoner-macros` contains Scala macros used throughout the other subprojects.   * `reasoner-kernel` contains the implementation of the core algorithm and data structures.   * `reasoner-owl-api` contains the implementation of the OWL API bindings.   * `reasoner-cli` contains the implementation of the command-line interface of the reasoner.   * `reasoner-protege-plugin` contains the implementation of the Protégé plugin.    To compile all subprojects and run all tests in a single command, first clone this repository,  and then from the root directory, type `sbt test`. The tests will take several minutes to complete.    #### Building the Command-Line Client  From the root directory, type `sbt` to launch the SBT REPL. Then type `project cli` and hit Enter, followed by `universal:packageBin` and hit Enter.    #### Building the Protégé Plugin  From the root directory, type `sbt` to launch the SBT REPL. Then type `project protegeplugin` and hit Enter, followed by `osgiBundle` and hit Enter.    ### Acknowledgements    Previous versions of Sequoia were developed at the  [Knowledge Representation and Reasoning group](https://www.cs.ox.ac.uk/activities/knowledge/)  at the  [Department of Computer Science](https://www.cs.ox.ac.uk/)  of the  [University of Oxford](https://www.ox.ac.uk). """
Semantic web;https://github.com/AKSW/Sparqlify-Extensions;"""Sparqlify-Extensions  ====================    Extension projects for Sparqlify"""
Semantic web;https://github.com/factsmission/psps;"""# Personal Structured Publishing Space    Create a linked data site serving RDF data from files in a GitHub repository. For example the [FactsMission Website](https://factsmission.com/) is generated by PSPS from the data in the repository at https://github.com/factsmission/website. All RDF data from your GitHub repository will also be accessible via SPARQL.    ## How to use it?    - Add a BASEURI file to the root of your repo with the base URI of your data (see below)  - Start an instance of  PSPS  - Add a webhook in Github notifying `http(s)://<your-host>/webhook` with the set webhook secret (see below)  - Add RDF data to your repository  - To customize the (client-side) rendering of the resource add a `renderes.ttl`file to the root of your repository. See the [RDF2h-Documentation](https://rdf2h.github.io/rdf2h-documentation/) to learn how the rendering works    ### Specifying base URI    The file BASEURI in the root of the repository can either directly contain the base URI for the branch containg the file or a JSON object with branch names as keys and base URIs as values.  Having such a JSON is handy as it allows to have staging branches with proposed modifications differing from the main branch only in the proposed change and not also in the BASEURI file.      ## Building        docker-compose build    ## Starting    You need to get a GitHub personal access token. You can generate one under [ Account Settings / Developer settings / Personal Access tokens](https://github.com/settings/tokens)    On Unix         GITHUB_TOKEN=""YOUR TOKEN HERE""; WEBHOOK_SECRET=""THE WEBHOOK SECRET""; docker-compose up    On windows         $env:GITHUB_TOKEN = ""YOUR TOKEN HERE""       $env:WEBHOOK_SECRET=""THE WEBHOOK SECRET""       docker-compose up    On [Rancher](https://rancher.com/)     * Add Stack for psps   * Configure using the file [docker-compose-no-build.yml](docker-compose-no-build.yml), set GITHUB_TOKEN to you GitHub Personal Access Token and WEBHOOK_SECRET to the desired    webhook secret.    ## Setting up the webhook    PSPS will download the data from any GitHub repository that send a requests to the webhook. This means that everybody that knows your webhook secret can publish to your PSPS instance!    Add a Webhook under *Project Settings / Webhooks*, the Payload URL is `http(s)://<your-host>/webhook`, as Content type choose application/json, PSPS only needs to be notified on `push` events.    ## What's powering PSPS?    PSPS puts together different pieces of software to provide its functionality.    ### Apache Jena Fuseki    [Apache Jena Fuseki](https://jena.apache.org/documentation/fuseki2/) is a SPARQL Server providing SPARQL 1.1 protocols. By default PSPS doesn't fully expose the Fuseki interface. However SPARQL queries sent to `http(s)://<your-host>/sparql` are forwarded to Fuseki.    ### TLDS / SLDS    The linked data site is provided by [SLDS](https://github.com/linked-solutions/slds) respectively its ""templating"" extension [TLDS](https://github.com/linked-solutions/tlds).    ### Apache Clerezza    [Apache Clerezza](http://clerezza.apache.org/) provides the RDF API and Toolkit used in SLDS, TLDS as well as PSPS itsef.    ### RDF2h / LD2h    The templating mechanism introduced by TLDS bases on [RDF2h](https://github.com/rdf2h/rdf2h) which allows defining renderers in RDF. It uses [LD2h](https://github.com/rdf2h/ld2h) to integrate RDF2h in HTML. LD2h also allows including remote resources alongside the resources originating from the data in the repository."""
Semantic web;https://github.com/KMax/cqels;"""# CQELS (Continuous Query Evaluation over Linked Data)    This repository is a fork of https://code.google.com/p/cqels/ repository on Google Code.    _DISCLAMER: I'm not a developer of the original CQELS._    ## Install    Add the following repository to your pom.xml:  ```  <repository>      <id>cqels.mvn-repo</id>      <url>https://raw.github.com/KMax/cqels/mvn-repo/</url>      <snapshots>          <enabled>true</enabled>          <updatePolicy>always</updatePolicy>      </snapshots>  </repository>  ```    and declare the following dependency:  ```  <dependency>      <groupId>org.deri.cqels</groupId>      <artifactId>cqels</artifactId>      <version>...</version>  </dependency>  ```    ## Releases  ### 1.0.0  * Mavenized build,  * Support of [BIND](http://www.w3.org/TR/sparql11-query/#bind) operator,  * Initial support of remote SPARQL endpoints via [SPARQL Graph Protocol](http://www.w3.org/TR/sparql11-http-rdf-update/),  * Fixed an FileException (at ObjectFileStorage) exception _([commit](https://github.com/KMax/cqels/commit/4382fe7e2f15a8c205a47ab3cd0e25842e558c30))_.    ### 1.1.0  * Updated [Jena TDB](https://jena.apache.org/documentation/tdb/) up to 1.1.2 version    Code license: [LGPLv3.0](https://github.com/KMax/cqels/blob/master/LICENSE) """
Semantic web;https://github.com/oeg-upm/OME;"""  ![OME](static/logo-min.png)    [![Build Status](https://ahmad88me.semaphoreci.com/badges/Morph-OME/branches/master.svg)](https://ahmad88me.semaphoreci.com/projects/Morph-OME)  [![codecov](https://codecov.io/gh/oeg-upm/Morph-OME/branch/master/graph/badge.svg?token=TsSWMQGuoO)](https://codecov.io/gh/oeg-upm/Morph-OME)  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3764202.svg)](https://doi.org/10.5281/zenodo.3764202)      An Online Mapping Editor to generate R2RML, RML, and YARRRML without writing a single line.  It also supports automatic suggestions of the subject and property columns using   the APIs of [tada_web](https://github.com/oeg-upm/tada-web).      <!--  # Run with Docker  1. `sh run_docker.sh`  2. In the browser visit `http://127.0.0.1:5000`      # How to install it locally  1. Create virtual environment [here](https://docs.python-guide.org/dev/virtualenvs/) (recommended by not required) e.g. ```virtualenv -p /usr/bin/python2.7 .venv```  1. Access the virtual environment using `source .venv/bin/activate`  1. Install pip [here](https://pip.pypa.io/en/stable/installing/)  1. Install requirements ``` pip install -r requirements.txt ```  1. Set `TADA_HOST` to the url of the pytada_hdt_entity host. For example (`export TADA_HOST=""http://127.0.0.1:5001/`)  1. Run the application ``` python app.py ```  1. Open the browser to the url [http://127.0.0.1:5000/](http://127.0.0.1:5000/)    -->    # Automatic Suggestions  It uses the APIs of[tada_web](https://github.com/oeg-upm/tada-web). To use it, you need to export an environment variable `TADA_HOST` with the   URL of the `tada-web` host url.  For example, you can set it like that  `export TADA_HOST=""http://127.0.0.1:5001/""`          # Environment Variables  * `SECRET_KEY`:      * A random text  * `TADA_HOST`:      * (Optional)      * The URL of TADA APIs. If it is missing, the class and properties won't be annotated automatically      * Default: """"  * `UPLOAD_ONTOLOGY`:       * (Optional)      * To show/hide an ontology upload page (in the main page) for the autocomplete functionality      * Default: True  * `github_secret`:      * Github app secret    * `github_appid`:      * Github app ID  * `MORPH_PATH`:      * The local path to morph-rdb jar to generate ttl  * `RMLMAPPER_PATH`:      * The local path to rmlmapper jar to generate the ttl      ## To activate_this.py   You can add these environment variables to `activate_this.py` in the virtualenv bin directory.  ```  os.environ['SECRET_KEY']=""""  os.environ['github_appid']=""""  os.environ['github_secret']=""""  os.environ['UPLOAD_ONTOLOGY']=""false""  os.environ['RMLMAPPER_PATH']=""""  os.environ['TADA_HOST']=""""  ```    ## To a shell  ```  export SECRET_KEY=""""  export github_appid=""""  export github_secret=""""  export UPLOAD_ONTOLOGY=""false""  export RMLMAPPER_PATH=""""  export TADA_HOST=""""  ```      <!--  # Screenshot  ![screenshot](https://github.com/oeg-upm/OME/raw/master/screenshot.png)  -->    # Remarks  * To run the application on a specific port (e.g. say port 5001) ``` python app.py 5001```.  * To run the application on a specific port (e.g. say port 5001, and any given host 0.0.0.0) ``` python app.py 0.0.0.0 5001```.    # To cite  ```  @software{alobaid_ahmad_2020_3764202,    author       = {Alobaid, Ahmad and                    Corcho, Oscar},    title        = {OME},    month        = apr,    year         = 2020,    publisher    = {Zenodo},    doi          = {10.5281/zenodo.3764202},    url          = {https://doi.org/10.5281/zenodo.3764202}  }  ```   """
Semantic web;https://github.com/Data-Liberation-Front/csvlint.rb;"""[![Build Status](http://img.shields.io/travis/theodi/csvlint.rb.svg)](https://travis-ci.org/theodi/csvlint.rb)  [![Dependency Status](http://img.shields.io/gemnasium/theodi/csvlint.rb.svg)](https://gemnasium.com/theodi/csvlint.rb)  [![Coverage Status](http://img.shields.io/coveralls/theodi/csvlint.rb.svg)](https://coveralls.io/r/theodi/csvlint.rb)  [![License](http://img.shields.io/:license-mit-blue.svg)](http://theodi.mit-license.org)  [![Badges](http://img.shields.io/:badges-5/5-ff6799.svg)](https://github.com/pikesley/badger)    # CSV Lint    A ruby gem to support validating CSV files to check their syntax and contents. You can either use this gem within your own Ruby code, or as a standalone command line application    ## Summary of features    * Validation that checks the structural formatting of a CSV file    * Validation of a delimiter-separated values (dsv) file accesible via URL, File, or an IO-style object (e.g. StringIO)  * Validation against [CSV dialects](http://dataprotocols.org/csv-dialect/)    * Validation against multiple schema standards; [JSON Table Schema](https://github.com/theodi/csvlint.rb/blob/master/README.md#json-table-schema-support) and [CSV on the Web](https://github.com/theodi/csvlint.rb/blob/master/README.md#csv-on-the-web-validation-support)     ## Development    `ruby version 2.1.4`    ### Tests    The codebase includes both rspec and cucumber tests, which can be run together using:        $ rake    or separately:        $ rake spec      $ rake features    When the cucumber tests are first run, a script will create tests based on the latest version of the [CSV on the Web test suite](http://w3c.github.io/csvw/tests/), including creating a local cache of the test files. This requires an internet connection and some patience. Following that download, the tests will run locally; there's also a batch script:        $ bin/run-csvw-tests    which will run the tests from the command line.    If you need to refresh the CSV on the Web tests:        $ rm bin/run-csvw-tests      $ rm features/csvw_validation_tests.feature      $ rm -r features/fixtures/csvw    and then run the cucumber tests again or:        $ ruby features/support/load_tests.rb      ## Installation    Add this line to your application's Gemfile:        gem 'csvlint'    And then execute:        $ bundle    Or install it yourself as:        $ gem install csvlint    ## Usage    You can either use this gem within your own Ruby code, or as a standalone command line application    ## On the command line    After installing the gem, you can validate a CSV on the command line like so:    	csvlint myfile.csv    You may need to add the gem exectuable directory to your path, by adding '/usr/local/lib/ruby/gems/2.6.0/bin'   or whatever your version is, to your .bash_profile PATH entry. [like so](https://stackoverflow.com/questions/2392293/ruby-gems-returns-command-not-found)    You will then see the validation result, together with any warnings or errors e.g.    ```  myfile.csv is INVALID  1. blank_rows. Row: 3  1. title_row.  2. inconsistent_values. Column: 14  ```    You can also optionally pass a schema file like so:    	csvlint myfile.csv --schema=schema.json    ## In your own Ruby code    Currently the gem supports retrieving a CSV accessible from a URL, File, or an IO-style object (e.g. StringIO)    	require 'csvlint'    	validator = Csvlint::Validator.new( ""http://example.org/data.csv"" )  	validator = Csvlint::Validator.new( File.new(""/path/to/my/data.csv"" ))  	validator = Csvlint::Validator.new( StringIO.new( my_data_in_a_string ) )    When validating from a URL the range of errors and warnings is wider as the library will also check HTTP headers for  best practices    	#invoke the validation  	validator.validate    	#check validation status  	validator.valid?    	#access array of errors, each is an Csvlint::ErrorMessage object  	validator.errors    	#access array of warnings  	validator.warnings    	#access array of information messages  	validator.info_messages    	#get some information about the CSV file that was validated  	validator.encoding  	validator.content_type  	validator.extension  	validator.row_count    	#retrieve HTTP headers from request  	validator.headers    ## Controlling CSV Parsing    The validator supports configuration of the [CSV Dialect](http://dataprotocols.org/csv-dialect/) used in a data file. This is specified by  passing a dialect hash to the constructor:        dialect = {      	""header"" => true,      	""delimiter"" => "",""      }  	validator = Csvlint::Validator.new( ""http://example.org/data.csv"", dialect )    The options should be a Hash that conforms to the [CSV Dialect](http://dataprotocols.org/csv-dialect/) JSON structure.    While these options configure the parser to correctly process the file, the validator will still raise errors or warnings for CSV  structure that it considers to be invalid, e.g. a missing header or different delimiters.    Note that the parser will also check for a `header` parameter on the `Content-Type` header returned when fetching a remote CSV file. As  specified in [RFC 4180](http://www.ietf.org/rfc/rfc4180.txt) the values for this can be `present` and `absent`, e.g:    	Content-Type: text/csv; header=present    ## Error Reporting    The validator provides feedback on a validation result using instances of `Csvlint::ErrorMessage`. Errors are divided into errors, warnings and information  messages. A validation attempt is successful if there are no errors.    Messages provide context including:    * `category` has a symbol that indicates the category or error/warning: `:structure` (well-formedness issues), `:schema` (schema validation), `:context` (publishing metadata, e.g. content type)  * `type` has a symbol that indicates the type of error or warning being reported  * `row` holds the line number of the problem  * `column` holds the column number of the issue  * `content` holds the contents of the row that generated the error or warning    ## Errors    The following types of error can be reported:    * `:wrong_content_type` -- content type is not `text/csv`  * `:ragged_rows` -- row has a different number of columns (than the first row in the file)  * `:blank_rows` -- completely empty row, e.g. blank line or a line where all column values are empty  * `:invalid_encoding` -- encoding error when parsing row, e.g. because of invalid characters  * `:not_found` -- HTTP 404 error when retrieving the data  * `:stray_quote` -- missing or stray quote  * `:unclosed_quote` -- unclosed quoted field  * `:whitespace` -- a quoted column has leading or trailing whitespace  * `:line_breaks` -- line breaks were inconsistent or incorrectly specified    ## Warnings    The following types of warning can be reported:    * `:no_encoding` -- the `Content-Type` header returned in the HTTP request does not have a `charset` parameter  * `:encoding` -- the character set is not UTF-8  * `:no_content_type` -- file is being served without a `Content-Type` header  * `:excel` -- no `Content-Type` header and the file extension is `.xls`  * `:check_options` -- CSV file appears to contain only a single column  * `:inconsistent_values` -- inconsistent values in the same column. Reported if <90% of values seem to have same data type (either numeric or alphanumeric including punctuation)  * `:empty_column_name` -- a column in the CSV header has an empty name  * `:duplicate_column_name` -- a column in the CSV header has a duplicate name  * `:title_row` -- if there appears to be a title field in the first row of the CSV    ## Information Messages    There are also information messages available:    * `:nonrfc_line_breaks` -- uses non-CRLF line breaks, so doesn't conform to RFC4180.  * `:assumed_header` -- the validator has assumed that a header is present    ## Schema Validation    The library supports validating data against a schema. A schema configuration can be provided as a Hash or parsed from JSON. The structure currently  follows JSON Table Schema with some extensions and rudinmentary [CSV on the Web Metadata](http://www.w3.org/TR/tabular-metadata/).    An example JSON Table Schema schema file is:    	{  		""fields"": [  			{  				""name"": ""id"",  				""constraints"": {  					""required"": true,  					""type"": ""http://www.w3.org/TR/xmlschema-2/#integer""  				}  			},  			{  				""name"": ""price"",  				""constraints"": {  					""required"": true,  					""minLength"": 1   				}  			},  			{  				""name"": ""postcode"",  				""constraints"": {  					""required"": true,  					""pattern"": ""[A-Z]{1,2}[0-9][0-9A-Z]? ?[0-9][A-Z]{2}""  				}  			}  		]  	}    An equivalent CSV on the Web Metadata file is:    	{  		""@context"": ""http://www.w3.org/ns/csvw"",  		""url"": ""http://example.com/example1.csv"",  		""tableSchema"": {  			""columns"": [  				{  					""name"": ""id"",  					""required"": true,  					""datatype"": { ""base"": ""integer"" }  				},  				{  					""name"": ""price"",  					""required"": true,  					""datatype"": { ""base"": ""string"", ""minLength"": 1 }  				},  				{  					""name"": ""postcode"",  					""required"": true  				}  			]  		}  	}    Parsing and validating with a schema (of either kind):    	schema = Csvlint::Schema.load_from_json(uri)  	validator = Csvlint::Validator.new( ""http://example.org/data.csv"", nil, schema )    ### CSV on the Web Validation Support    This gem passes all the validation tests in the [official CSV on the Web test suite](http://w3c.github.io/csvw/tests/) (though there might still be errors or parts of the [CSV on the Web standard](http://www.w3.org/TR/tabular-metadata/) that aren't tested by that test suite).    ### JSON Table Schema Support    Supported constraints:    * `required` -- there must be a value for this field in every row  * `unique` -- the values in every row should be unique  * `minLength` -- minimum number of characters in the value  * `maxLength` -- maximum number of characters in the value  * `pattern` -- values must match the provided regular expression  * `type` -- specifies an XML Schema data type. Values of the column must be a valid value for that type  * `minimum` -- specify a minimum range for values, the value will be parsed as specified by `type`  * `maximum` -- specify a maximum range for values, the value will be parsed as specified by `type`  * `datePattern` -- specify a `strftime` compatible date pattern to be used when parsing date values and min/max constraints    Supported data types (this is still a work in progress):    * String -- `http://www.w3.org/2001/XMLSchema#string` (effectively a no-op)  * Integer -- `http://www.w3.org/2001/XMLSchema#integer` or `http://www.w3.org/2001/XMLSchema#int`  * Float -- `http://www.w3.org/2001/XMLSchema#float`  * Double -- `http://www.w3.org/2001/XMLSchema#double`  * URI -- `http://www.w3.org/2001/XMLSchema#anyURI`  * Boolean -- `http://www.w3.org/2001/XMLSchema#boolean`  * Non Positive Integer -- `http://www.w3.org/2001/XMLSchema#nonPositiveInteger`  * Positive Integer -- `http://www.w3.org/2001/XMLSchema#positiveInteger`  * Non Negative Integer -- `http://www.w3.org/2001/XMLSchema#nonNegativeInteger`  * Negative Integer -- `http://www.w3.org/2001/XMLSchema#negativeInteger`  * Date -- `http://www.w3.org/2001/XMLSchema#date`  * Date Time -- `http://www.w3.org/2001/XMLSchema#dateTime`  * Year -- `http://www.w3.org/2001/XMLSchema#gYear`  * Year Month -- `http://www.w3.org/2001/XMLSchema#gYearMonth`  * Time -- `http://www.w3.org/2001/XMLSchema#time`    Use of an unknown data type will result in the column failing to validate.    Schema validation provides some additional types of error and warning messages:    * `:missing_value` (error) -- a column marked as `required` in the schema has no value  * `:min_length` (error) -- a column with a `minLength` constraint has a value that is too short  * `:max_length` (error) -- a column with a `maxLength` constraint has a value that is too long  * `:pattern` (error) --  a column with a `pattern` constraint has a value that doesn't match the regular expression  * `:malformed_header` (warning) -- the header in the CSV doesn't match the schema  * `:missing_column` (warning) -- a row in the CSV file has a missing column, that is specified in the schema. This is a warning only, as it may be legitimate  * `:extra_column` (warning) -- a row in the CSV file has extra column.  * `:unique` (error) -- a column with a `unique` constraint contains non-unique values  * `:below_minimum` (error) -- a column with a `minimum` constraint contains a value that is below the minimum  * `:above_maximum` (error) -- a column with a `maximum` constraint contains a value that is above the maximum    ### Other validation options    You can also provide an optional options hash as the fourth argument to Validator#new. Supported options are:    * :limit_lines -- only check this number of lines of the CSV file. Good for a quick check on huge files.    ```  options = {    limit_lines: 100  }  validator = Csvlint::Validator.new( ""http://example.org/data.csv"", nil, nil, options )  ```    * :lambda -- Pass a block of code to be called when each line is validated, this will give you access to the `Validator` object. For example, this will return the current line number for every line validated:    ```      options = {        lambda: ->(validator) { puts validator.current_line }      }      validator = Csvlint::Validator.new( ""http://example.org/data.csv"", nil, nil, options )      => 1      2      3      4      .....  ``` """
Semantic web;https://github.com/R2RML-api/R2RML-api;"""[![Build Status](https://travis-ci.org/R2RML-api/R2RML-api.svg?branch=master)](https://travis-ci.org/R2RML-api)    R2RML-api  =========    ## How to use R2RML-api as a Maven dependency    * put the following fragments into your `pom.xml`    ```xml              <dependencies>  		<!-- Optique R2RML API -->  		<dependency>  			<groupId>eu.optique-project</groupId>  			<artifactId>r2rml-api-core</artifactId>  			<version>0.6.0</version>  		</dependency>            <!-- Optique R2RML API RDF4J Binding -->  		<dependency>  			<groupId>eu.optique-project</groupId>  			<artifactId>r2rml-api-rdf4j-binding</artifactId>  			<version>0.6.0</version>  		</dependency>            <!-- Optique R2RML API Jena Binding -->          <dependency>  			<groupId>eu.optique-project</groupId>  			<artifactId>r2rml-api-jena-binding</artifactId>  			<version>0.6.0</version>  		</dependency>  	</dependencies>  ```      ## Release history    <a name=""v0.6.0""></a>  ### 1 March, 2017 ::  Version 0.6.0   * code refactor      <a name=""v0.5.0""></a>  ### 13 January, 2017 ::  Version 0.5.0   * A major rewriting using commons-rdf   * retired owlapi-binding     <a name=""v0.4.0""></a>  ### 3 January, 2017 ::  Version 0.4.0   * Upgrade Sesame to RDF4J 2.1.4      <a name=""v0.3.0""></a>  ### 26 Feb, 2016 ::  Version 0.3.0   * Upgrade Jena to v3    <a name=""v0.2.1""></a>  ### 23 Feb, 2016 ::  Version 0.2.1   * Fix an issue of deploying `r2rml-api-jena-bridge` to central repository    <a name=""v0.2.0""></a>  ### 23 Feb, 2016 ::  Version 0.2.0  * Upgrade OWL-API to v4  * Deployed to central maven repository    <a name=""v0.1.0""></a>  ### 2014 ::  Version 0.1.0  * First release    ## Reference    [1] ""An R2RML Mapping Management API in Java"", Marius Strandhaug, Master’s Thesis Spring 2014, University of Oslo.    [2] The javadoc documentation of the API v0.1 can be found by following this URL: http://folk.uio.no/marstran/doc/.   """
Semantic web;https://github.com/antoniogarrote/json-ld-macros;"""# JSON-LD Macros [![Build Status](https://travis-ci.org/antoniogarrote/json-ld-macros.svg?branch=master)](https://travis-ci.org/antoniogarrote/json-ld-macros)    JSON-LD Macros is a library to define declarative transformations of JSON objects obtained from a remote web service into JSON-LD objects. The ultimate goal of the library is to make it easier the process of consuming JSON APIs from RDF/JSON-LD applications. Similar ideas for transforming JSON documents into RDF have been explored in projects like [jsonGRDDL](http://buzzword.org.uk/2008/jsonGRDDL/spec.20100903).  JSON-LD Macros supports the serialisation of the macro itself as JSON-LD and the deserialisation back into the macro description.    A demo is available [here](http://antoniogarrote.github.com/json-ld-macros/) .    ## A Minimal example    ``` javascript        // requires the library      var macros = require('jsonld_macros');        macros.registerAPI({          // URI template for a remote service (Github Users' API)        ""https://api.github.com/users/{username}"":          {""$"": // selects the root node / list of root nodes of the JSON document            { // a JSON-LD context that will be added to all the slected nodes            ""@context"": {""data"":""http://socialrdf.org/github/datafeed""},            // removes the meta property and associated value from the selected nodes            ""@remove"":""meta""},            ""$.data"": // selects the root node/data objects             {// by default, all properties in the selected nodes will have the 'gh' prefix            ""@ns"": {""ns:default"": ""gh""},            // a JSON-LD context declaration that will be added to all the selecte nodes            ""@context"": {""gh"":""http://socialrdf.org/github/""},            // a JSON-LD type declaration that will be added to all the selecte nodes            ""@type"": ""http://socialrdf.org/github/User""}}      });        // We retrieve the data using whatever transport layer is      // available: AJAX, TCP sockets...      var resourceURI = ""https://api.github.com/users/1"";      retrieveRemoteData(resourceURI, function(data){           // we can apply the transformation to the retrieved data         // passing the URI used to retrieve the data         // as a selector for the transformation         var jsonld = macros.resolve(resourceURI, data);      });    ```    ## Definition of transformations    JSON-LD Macros fundamental concept is the description of JSON documents transformations encoded as JSON objects.  Transformation objects can be used to describe a service API associating a transformation to a list of URIs templates.  Transformations in turn are composed of pairs key-values where the key declares a 'selector' of nodes in the JSON object to transform. The value consist in a collection of transformation rules from a fixes set of possible rules: ""@context"", ""@id"", ""@type"", ""@remove"", ""@only"", ""@ns"" and ""@transform"".  When the transformation is applied to a JSON object retrieved from a URI matching one of the declared templates, each of the node selectors defined for that transformation is evaluated in the retrieved object. The output of this evaluation is a collection of nodes per node selector. For every collection of nodes, the transformation rules are applied inplace. After applying all the transformations, the resulting DOM document is returned as the final output.  Transformation bodies can consist, in some cases, in an array of objects containing functions that can be applied to the selected node to obtain the value that will be used by the transformation.    The following grammar describes the structure of an API transformation definition:    - API ::= {@declare:functionDeclarations}? , {URIPatterns: Transformation}*  - URIPatterns ::= URIPattern[\\n URIPattern]*  - Transformation ::= {NodeSelector: TransformationRules}  - TransformationRules ::= {TransformationRuleName: TransformationRuleBody}*  - TransformationRuleName ::= @context | @id | @type | @remove | @only | @ns | @transform  - TransformationRuleBody ::= FunctionsArray | JSON String | JSON Object    The following sections describe how to declare URI patterns, node selectors and transformations.    ### URI Patterns    URI patterns are regular URLs patterns as defined in [RFC6570](https://tools.ietf.org/html/rfc6570).    ### Node Selectors    Node selectors syntax is taken from [JSON Path](http://goessner.net/articles/JsonPath/) but the semantics are slightly modified to match the behaviour os selector libraries like jQuery.  Paths are chains of names identifying JSON objects propertes separated by '.' characters. Some characters can be used for special purposes:    - '$': Selects the root of the document. It can be a single JSON object if the document includes a single object or a collection of objects if the root object in the document is an array.  - '*': Selects all the objects linked to any property of the selected nodes.  - '..': Recursive evaluation of the rest of the path expression.  - 'propertyName[ * ]': if 'propertyName' returns an array of objects, 'propertyName[*]' aggregates all the objects in the selected arrays.      Evaluation of the selector is accomplished from left to right. For every component in the path, it is evaluated in the current set of selected nodes. After evaluation, the selected nodes set is replaced by the output of the evaluation. The set of selected nodes start with the empty set.      ### Transformation Rules    Transformation rules are JSON objects where the keys of the object identified certain standard transformations that can be performed in the input object and the values describe particular details of the transformation rule. A fixed set of transformation rules is available: ""@explode"", ""@compact"", ""@context"", ""@id"", ""@type"", ""@add"", ""@remove"", ""@only"", ""@ns"" and ""@transform"".    Rules are applied in the following order:    - @explode  - @add  - @context, @id, @type, @transform  - @remove  - @only  - @ns  - @compact    Rules are applied inplace in the target object without cloning or reserving any additional memory.  Some transformation rules like ""@id"", ""@type"" and ""@transform"" accept as the rule body an array of functions that will applied to the target object to obtain the final value generated by the rule.  Additional functions can be declared in the API definition.    This is a description of the different transformations    ### @explode    Transforms a pair property - value into  a pair property - nested object where the nested object has only the property value with a property specified in the transformation.    ``` javascript    // input node  - {""contact_url"": ""http://test.com/people/dmm4""}    // rule  - {""@explode"": ""@id""}    // output node  - {""contact_url"": { ""@id"": ""http://test.com/people/dmm4""} }    ```    ### @compact    Transforms a pair property - node into a pair property - value where the value is the value of the selected property in the node.    ``` javascript    // input node  {""contact_url"": {""$ref"": ""http://test.com/people/dmm4""}}    // rule  {""@compact"": ""$ref""}    // output node  {""contact_url"": ""http://test.com/people/dmm4""}    ```    ### @context    Defines a context JSON-LD object that is inserted in the target object. The body of the rule is the JSON object defining the JSON-LD context that will be inserted    ``` javascript    // input node  {""contact_url"": ""http://test.com/people/dmm4""}    // rule  {""@context"": {""contact_url"": { ""@id"": ""foaf:knowks"", ""@type"": ""@id""}, ""foaf"":""http://xmlns.com/foaf/0.1/"" }    // output node  {""@context"": {""contact_url"": { ""@id"": ""foaf:knowks"", ""@type"": ""@id""}, ""foaf"":""http://xmlns.com/foaf/0.1/""  },   ""contact_url"": ""http://test.com/people/dmm4"" }    ```    ### @id    Defines how the @id JSON-LD attribute will be generated in the transformed object. Possible rule values can be:    - JSON string: a fixed string that will be inserted as the value of the @id property in all the nodes  - An array of functions that will be applied to each selected node to obtain the value of the @id JSON-LD object.    ``` javascript    // input node  {""prop"":""value""}    // rule  {""@id"":""http://test.com/user#me""}    // output node  {""@id"":""http://test.com/user#me"", ""prop"":""value""}    ```    ### @type    Defines how the @type JSON-LD attribute will be generated in the transformed object. Possible rule values can be:    - JSON string: a fixed string that will be inserted as the value of the @type property in all the nodes  - JSON array: an array of fixed strings that will be inserted as the value of the @type property in all the nodes  - An array of functions that will be applied to each selected node to obtain the value of the @id JSON-LD object.    ``` javascript    // input node  {""prop"":""value""}    // rule  {""type"":[""http://test.com/vocab/Type1"", ""http://test.com/vocab/Type2""]}    // output node  {""@type"":[""http://test.com/vocab/Type1"", ""http://test.com/vocab/Type2""],   ""prop"":""value""}    ```    ### @transform    Defines a generic transformation for a property of the selected nodes that will be applied to the initial value of the property to obtain the final value for that property in the transformed object.  The body of the rule must be a JSON object with a single key with the name of the property to transform and a value containing the array of function to apply to the initial value.    ### @remove    This rule can be used to delete properties of the selected nodes. Possible values are a single string with the name of the property to remove or an array of properties that will be removed.    ### @add    This rule can be used to add properties of the selected nodes. The value must be a object with the properties and values to be added to the node.    ### @only    Collects a set of properties from the selected nodes and delete the remaining properties. Possible values for the this rule body are a single property to select or an array witht the properties that must be collected.    ### @ns    This rule transforms the names of the properties in the selected nodes. The rule body consist of an object containgin functions that will be applied to the object property names to obtain the final properties. This rule is applied after all other rules have been applied. When referring to property names in other rules, the name of the property before applying this rule must be used.  Possible functions that can be used in the rule body are:    - 'ns:default': the value of this function is a default prefix that will be prepended to all the properties in the current node to transform them into CURIEs  - 'ns:append': accepts an object with prefixes as keys and a property name or array of property names as value. When applied, this function prepends the prefix to all the selected property names.  - 'ns:replace': Similar to 'ns:append', but instead of a prefix, it accepts as key fo the rule body object a string that will replace enterily the selected property names  - 'ns:omit': accepts a single string property name or an array of properties name that will be not affected by any other function in the rule body.    ## Functions    Functions are expressed as a single object or an array of JSON objects where each object contains the declaration of a function application that will be issued to the selected node.  Function applications contain the name of the function as the key of the object and a parameter as the value.  When an array of functions is declared, each function application will be applied consequtively, receiving as parameters the argument defined in the function application, the output of the previous function application in the array and the selected node where the tansformation is being applied. The first function in the chain will receive null as the input value.  New functions can be defined in the API declaration using a prefixed name for the functions and invoked in the body of rules.    A collection of functions are already available for transformations:    - 'f:valueof': selects the value of function argument in the context object and returns it.  - 'f:defaultvalue': sets a default value if the current value in the function application chain is null  - 'f:select': selects the value of the function argument in the input object and returns it.  - 'f:prefix': adds a prefix passed as the function argument and add it to the input object before returning it.  - 'f:urlencode': Performs URL encoding into the input object. The function argument is ignored.  - 'f:apply': Accepts a string of JavaScript code as the function argument, evaluates it and applies the resulting function to the input object. Evaluation is scoped with the input object using code like: (new Function('with(this) { return '+functionArgumentTexT+';}')).call(inputObject)  - 'f:basetemplateurl': Transforms a URL template with terminal variables into the base URL without the variables. e.g.: 'https://api.github.com/users/octocat/starred{/owner}{/repo}' becomes 'https://api.github.com/users/octocat/starred'    ## Null properties and function application exception    One main problem when applying transformations with null properties. Some object in the input data may have optional values, or the application of a function may return an unexpected null value. The library can react to this events in two different ways depending of the value of the 'behaviour' property. If the 'behaviour' property is set to the value 'loose', exceptions in the application of function chains will be catched and a null value will be returned as the result of the function chain application. Additionally, after transforming a node, properties with null values will be removed, including the '@id' property.    If the value of the 'behaviour' property is set to 'strict', exceptions will not be catched and final values of the transformations will be returned including null values.      ## Function declarations    Additional functions can be declared in the definition of a API using the '@declare' property. Function declarations accepts as the value of the '@declare' property a JSON object containing  pairs of CURIEs and function literals. for every prefix used in the curies, an additional property must map the prefix to the URI prefix.    The following code shows an example of how a function can be declared in an API definition:    ``` javascript        {        '@declare':        {          // 'test' is a prefix          'test': 'http://socialrdf.org/functions/',          // declaration of the 'test:f' function          'test:f': 'function(argument, input, obj){ return ""the ""+argument+"" ""+input }'        },          ""https://api.github.com/users/{username}\\n\         https://api.github.com/users/{username}/following/{other_user}"":        {           '$': {'@ns': {'ns:default': 'gh'},                 '@context': {'gh':'http://socialrdf.org/github/'},                 '@type': 'http://socialrdf.org/github/User',                 '@transform': {           	      'name': [{'f:valueof':'name'},                             // we can apply the declared function            	               {'test:f': 'user name:'}]                        }                }         }      }    ```    Function will receive three arguments, the function argument declared in the rule body, the input object from the previous function application and the context object.    ## JSON-LD Serialisation    To export the registered macros as a JSON-LD document, the ```toJSONLD``` function can be used. The output of the serialisation is a JSON-LD document that uses a small vocabulary to expose the macro.  The main properties in the vocabulary are:    - jldm:JsonLDMacro : class for all the JSON-LD macro descriptions  - jldm:uriTemplate : a URI template used to match the transformations of the macro.  - jldm:specification : a property pointing to each node transformation in the macro.  - jldm:Specification : class for all JSON-LD node transformations.  - jldm:transformation : a transformation for a sigle JSON node.  - jldm:Transformation : class for particular node transformations.  - jldm:ruleName : type of node transformation '@add', '@remove', '@id', etc.  - jldm:ruleBody : JSON encoded body for the described rule.    ``` javascript    macro.clearAPIs();    macro.registerAPI({  ""https://api.github.com/users/{username}"":    {https://api.github.com/users/{username}/commits/{sha1}"":    {'$': {'@ns': {'ns:default': 'gh'},  	   '@context': {'gh':'http://socialrdf.org/github/'},  	   '@type': 'http://socialrdf.org/github/Commit'}}  });    var jsonld = macro.toJSONLD();    ```  The output of the previous code is the following JSON-LD document:    ``` json  [    {      ""@type"": ""jldm:JsonLDMacro"",      ""@context"": {        ""jldm"": ""http://jsonld-macros.org/vocab#""      },      ""jldm:uriTemplate"": [        ""https://api.github.com/users/{username}/commits/{sha1}""      ],      ""jldm:specification"": [        {          ""@type"": ""jldm:Specification"",          ""jldm:transformation"": [            {              ""@type"": ""jldm:Transformation"",              ""jldm:ruleName"": ""@ns"",              ""jldm:ruleBody"": ""{\""ns:default\"":\""gh\""}""            },            {              ""@type"": ""jldm:Transformation"",              ""jldm:ruleName"": ""@context"",              ""jldm:ruleBody"": ""{\""gh\"":\""http://socialrdf.org/github/\""}""            },            {              ""@type"": ""jldm:Transformation"",              ""jldm:ruleName"": ""@type"",              ""jldm:ruleBody"": ""\""http://socialrdf.org/github/Commit\""""            }          ],          ""jldm:pathSelector"": ""$""        }      ]    }  ]    ```      ## JSON-LD De-serialisation    Macros exported as JSON-LD documents can be de-serialised using the ```fromJSONLD``` function. The function requires an instance of the [RDFStore-JS](https://github.com/antoniogarrote/rdfstore-js) module to work.  This module is not included with the library to not increase the size of the library. If you want to use this functionality, you need to include rdfstore-js as an additional dependency into your project.    ``` javascript  var rdfstore = require('rdfstore');  macro.fromJSONLD(rdfstore, jsonld, function(err, macro){    // macro can be used here.  });  ```      ## RDFStore-JS integration    One goal in the development of the library was to make it easier to consume non RDF APIs from web applications using [RDFStore-JS](https://github.com/antoniogarrote/rdfstore-js) in the data layer. Once a API has been registered in the library, an instance of RDFStore-JS can be wrapped using the *wrapRDFStoreJSNetworkTransport* function.The wrapped store instance will use then the library to transform JSON objects loaded by the store using the *load* or a SPARQL ""LOAD"" query, matching one the registered API service URIs templates.      ## Author an license    This library is released under the LGPL V3 license. Copyright, Antonio Garrote 2012.  If you have any problem you can find me at antoniogarrote@gmail.com """
Semantic web;https://github.com/MakoLab/RomanticWeb;"""# [![romantic web logo](http://romanticweb.net/images/logo.png)](http://romanticweb.net/) [![teamcity build status](http://ci.t-code.pl/app/rest/builds/buildType:bt12/statusIcon)](http://ci.t-code.pl/viewType.html?buildTypeId=bt12)     ## Relational Object Model for SemanticWeb in .net!    While the hype about the Semantic Web technologies is rising, making the developers working with Resource Description Framework (RDF) data representing a graph might be not a good solution.    RomanticWeb is the world’s first ORM class solution for graph-based data written fully in C# that allows developers to work with the RDF data in a way the would work with any other data in an object oriented manner. This can be achieved by creating data models that can then be mapped to RDF statements in a fully transparent way.    RomanticWeb is also the first solution that in conjuction with it’s mapping abilities allows to query for the data in a native .net way with LINQ. Developers can use their natural approach while working with data and objects and query for them with strongly typed queries, which are then translated into a SPARQL Protocol And RDF Query Language.    Now it’s possible to work with the full power of elastic RDF data sets with a simplicity of a classic object oriented programming!    ============    __Read more on the [project page](http://romanticweb.net/)__ """
Semantic web;https://github.com/TheOntologist/OntoVerbal;"""OntoVerbal  ==========    OntoVerbal is a Protege 4.2 plugin that generates natural language descriptions for classes for an ontology written in OWL (roughly the OWL EL profile). OntoVerbal was written by Fennie Liang, in collaboraitn with Donia Scott, Alan Rector and Robert Stevens, as part of the EPSRC funded Semantic Web Authoring Tool Project (EP/G032459/1).  """
Semantic web;https://github.com/sputniq-space/ontodia;"""# Ontodia [![npm](https://img.shields.io/npm/v/ontodia.svg)](https://www.npmjs.com/package/ontodia) [![CircleCI](https://circleci.com/gh/sputniq-space/ontodia.svg?style=svg)](https://circleci.com/gh/sputniq-space/ontodia) #    Ontodia is a JavaScript library that allows to visualize, navigate and explore data in the form of an interactive graph based on underlying data sources.    ## What is Ontodia for?    Ontodia allows you to create and persist diagrams made from existing data - relational, object, semantic.    It was designed to visualize RDF data sets in particular, but could be tailored to almost any data source by implementing a data provider interface.      ## Core features    - Visual navigation and diagramming over large graph data sets  - Rich graph visualization and context-aware navigation features   - Ability to store and retrieve diagrams  - User friendly - no graph query language or prior knowledge of the schema required  - Customizable user interface (by modifying templates for nodes and links) and data storage back-end    ## How to try it?    You can follow developer tutorials at the [developer documentation page](https://github.com/metaphacts/ontodia/wiki)    ## License    The Ontodia library is distributed under LGPL-2.1. A commercial license with additional features, support and custom development is available, please contact us at [info@metaphacts.com](info@metaphacts.com).         ## Developer documentation and contributing    Developer documentation is available at [wiki page](https://github.com/metaphacts/ontodia/wiki).    ## Giving Ontodia people credit    If you use the Ontodia library in your projects, please provide a link to this repository in your publication and a citation reference to the following paper:     Mouromtsev, D., Pavlov, D., Emelyanov, Y., Morozov, A., Razdyakonov, D. and Galkin, M., 2015. The Simple Web-based Tool for Visualization and Sharing of Semantic Data and Ontologies. In International Semantic Web Conference (Posters & Demos).    ```  @inproceedings{Mouromtsev2015,      author = {Mouromtsev, Dmitry and Pavlov, Dmitry and Emelyanov, Yury and          Morozov, Alexey and Razdyakonov, Daniil and Galkin, Mikhail},      year = {2015},      month = {10},      title = {The Simple Web-based Tool for Visualization and Sharing of Semantic Data and Ontologies},      booktitle = {International Semantic Web Conference (Posters & Demos)}  }  ```    It really helps our team to gain publicity and acknowledgment for our efforts.  Thank you for being considerate! """
Semantic web;https://github.com/arc-lasalle/Map-On;"""# Map-On Ontology Mapping environment  Map-On is a web-based editor for visual ontology mapping developed at the Architecture, Representation and Computation research group of La Salle, Ramon Llull University. The Map-On editor provides a graphical environment for the ontology mapping creation using an interactive graph layout. A point-and-click interface simplifies the mapping creation process. The editor automatically generates a R2RML document based on user inputs, particularly producing IRI patterns and SQL queries. It has been used in real scenarios alleviating the effort of coding R2RML statements which is one of the main barriers for adopting R2RML in research and in the industry communities.    The Map-On features:  -	Multiuser web environment for manual creation of relational-to-ontology mappings.  -	Mapping spaces for distribution of the mapping creation process.  -	Top-down visual representation of relational source schema, ontology structure, and mappings based on a graph layout which can be customised by users.  -	Visual representation of an ontology using [WebVOWL](https://github.com/VisualDataWeb/WebVOWL) and a relational source based on Entity-Relationship diagrams.  -	Input relational sources can be a SQL database or a tabular source such as comma separated values (CSV) file.  -	Support of R2RML recommendation.   -	R2RML documents generated by [AutoMap4OBDA](https://github.com/arc-lasalle/AutoMap4OBDA) can be imported in Map-On.   -	Automated generation of IRI patterns and SQL queries based on mappings defined by users.  -	Dialog window in input boxes with suggestions of elements to be used in the mappings based on the text introduced by users.  -	Point-and-click interface for reducing the effort required for mapping activities .  -	Ontology-driven mapping approach, where the mapping process starts from the ontology instead of working with the database.  -	Contextual menus to help users in mapping creation.  -	Log of the activities carried out by users.  -	Pop-ups with tips as an integrated help.    http://semanco-tools.eu/map-on    http://arc.salleurl.edu/    Map-On tool has been developed in PHP using the framework Code Igniter. The graphical ontology representation has been implement using the VivaGraphJS library and ARC to parse RDF files. The mapping file generated is written in the R2RML Mapping Language.    - CodeIgniter, Open source PHP web application framework – http://codeigniter.com  - Graph drawing library for JavaScript – https://github.com/anvaka/VivaGraphJS  - Text editor implemented in JavaScript with turtle syntax style – http://codemirror.net/  - ARC, Appmosphere RDF classes – https://github.com/semsol/arc2/wiki  - R2RML: RDB to RDF Mapping Language – http://www.w3.org/TR/r2rml/    For the Map-On installation please go to the [Installation guide](./docs/installation.md)    Please cite Map-On with the following reference:    >Sicilia, Álvaro; Nemirovski, German; Nolle, Andreas. [Map-on: A web-based editor for visual ontology mapping](http://www.semantic-web-journal.net/system/files/swj1266.pdf). Semantic Web, vol. 8, no. 6, pp. 969-980, 2017.    Copyright (C) 2019 ARC Engineering and Architecture La Salle, Ramon Llull University.     for comments please contact Álvaro Sicilia (alvaro.sicilia@salle.url.edu) """
Semantic web;https://github.com/ruby-rdf/rdf-tabular;"""# Tabular Data RDF Reader and JSON serializer    [CSV][] reader for [RDF.rb][] and fully JSON serializer.    [![Gem Version](https://badge.fury.io/rb/rdf-tabular.png)](https://badge.fury.io/rb/rdf-tabular)  [![Build Status](https://github.com/ruby-rdf/rdf-tabular/workflows/CI/badge.svg?branch=develop)](https://github.com/ruby-rdf/rdf-tabular/actions?query=workflow%3ACI)  [![Coverage Status](https://coveralls.io/repos/ruby-rdf/rdf-tabular/badge.svg?branch=develop)](https://coveralls.io/github/ruby-rdf/rdf-tabular?branch=develop)  [![Gitter chat](https://badges.gitter.im/ruby-rdf/rdf.png)](https://gitter.im/ruby-rdf/rdf)    ## Features    RDF::Tabular parses CSV or other Tabular Data into [RDF][] and JSON using the [W3C CSVW][] specifications, currently undergoing development.    * Parses [number patterns](https://www.unicode.org/reports/tr35/tr35-39/tr35-numbers.html#Number_Patterns) from [UAX35][]  * Parses [date formats](https://www.unicode.org/reports/tr35/tr35-39/tr35-dates.html#Contents) from [UAX35][]  * Returns detailed errors and warnings using optional `Logger`.    ## Installation  Install with `gem install rdf-tabular`    ## Description  RDF::Tabular parses CSVs, TSVs, and potentially other tabular data formats. Using rules defined for [W3C CSVW][], it can also parse metadata files (in JSON-LD format) to find a set of tabular data files, or locate a metadata file given a CSV:    * Given a CSV `http://example.org/mycsv.csv` look for `http://example.org/mycsv.csv-metadata.json` or `http://example.org/metadata.json`. Metadata can also be specified using the `describedby` link header to reference a metadata file.  * Given a metadata file, locate one or more CSV files described within the metadata file.  * Also, extract _embedded metadata_ from the CSV (limited to column titles right now).    Metadata can then provide datatypes for the columns, express foreign key relationships, and associate subjects and predicates with columns. An example [metadata file for the project DOAP description](https://raw.githubusercontent.com/ruby-rdf/rdf-tabular/develop/etc/doap.csv-metadata.json) is:        {        ""@context"": ""http://www.w3.org/ns/csvw"",        ""url"": ""doap.csv"",        ""tableSchema"": {          ""aboutUrl"": ""https://rubygems.org/gems/rdf-tabular"",          ""propertyUrl"": ""http://usefulinc.com/ns/doap#{_name}"",          ""null"": """",          ""columns"": [            {""titles"": ""name""},            {""titles"": ""type"", ""propertyUrl"": ""rdf:type"", ""valueUrl"": ""{+type}""},            {""titles"": ""homepage"", ""valueUrl"": ""{+homepage}""},            {""titles"": ""license"", ""valueUrl"": ""{+license}""},            {""titles"": ""shortdesc"", ""lang"": ""en""},            {""titles"": ""description"", ""lang"": ""en""},            {""titles"": ""created"", ""datatype"": {""base"": ""date"", ""format"": ""M/d/yyyy""}},            {""titles"": ""programming_language"", ""propertyUrl"": ""http://usefulinc.com/ns/doap#programming-language""},            {""titles"": ""implements"", ""valueUrl"": ""{+implements}""},            {""titles"": ""category"", ""valueUrl"": ""{+category}""},            {""titles"": ""download_page"", ""propertyUrl"": ""http://usefulinc.com/ns/doap#download-page"", ""valueUrl"": ""{+download_page}""},            {""titles"": ""mailing_list"", ""propertyUrl"": ""http://usefulinc.com/ns/doap#mailing-list"", ""valueUrl"": ""{+mailing_list}""},            {""titles"": ""bug_database"", ""propertyUrl"": ""http://usefulinc.com/ns/doap#bug-database"", ""valueUrl"": ""{+bug_database}""},            {""titles"": ""blog"", ""valueUrl"": ""{+blog}""},            {""titles"": ""developer"", ""valueUrl"": ""{+developer}""},            {""titles"": ""maintainer"", ""valueUrl"": ""{+maintainer}""},            {""titles"": ""documenter"", ""valueUrl"": ""{+documenter}""},            {""titles"": ""maker"", ""propertyUrl"": ""foaf:maker"", ""valueUrl"": ""{+maker}""},            {""titles"": ""dc_title"", ""propertyUrl"": ""dc:title""},            {""titles"": ""dc_description"", ""propertyUrl"": ""dc:description"", ""lang"": ""en""},            {""titles"": ""dc_date"", ""propertyUrl"": ""dc:date"", ""datatype"": {""base"": ""date"", ""format"": ""M/d/yyyy""}},            {""titles"": ""dc_creator"", ""propertyUrl"": ""dc:creator"", ""valueUrl"": ""{+dc_creator}""},            {""titles"": ""isPartOf"", ""propertyUrl"": ""dc:isPartOf"", ""valueUrl"": ""{+isPartOf}""}          ]        }      }    This associates the metadata with the CSV [doap.csv](https://raw.githubusercontent.com/ruby-rdf/rdf-tabular/develop/etc/doap.csv), creates a common subject for all rows in the file, and a common predicate using the URI Template [URI Template](https://tools.ietf.org/html/rfc6570) `http://usefulinc.com/ns/doap#\{_name\}` which uses the `name` of each column (defaulted from `titles`) to construct a URI in the DOAP vocabulary, and constructs object URIs for object-valued properties from the contents of the column cells. In some cases, the predicates are changed on a per-column basis by using a different `propertyUrl` property on a given column.    This results in the following Turtle:        @prefix csvw: <http://www.w3.org/ns/csvw#> .      @prefix dc: <http://purl.org/dc/terms/> .      @prefix doap: <http://usefulinc.com/ns/doap#> .      @prefix foaf: <http://xmlns.com/foaf/0.1/> .      @prefix prov: <http://www.w3.org/ns/prov#> .      @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .      @prefix xsd: <http://www.w3.org/2001/XMLSchema#> .        <https://rubygems.org/gems/rdf-tabular> a doap:Project,           <http://www.w3.org/ns/earl#TestSubject>,           <http://www.w3.org/ns/earl#Software>;         dc:title ""RDF::Tabular"";         dc:creator <http://greggkellogg.net/foaf#me>;         dc:date ""2015-01-05""^^xsd:date;         dc:description ""RDF::Tabular processes tabular data with metadata creating RDF or JSON output.""@en;         dc:isPartOf <https://rubygems.org/gems/rdf>;         doap:blog <http://greggkellogg.net/>;         doap:bug-database <https://github.com/ruby-rdf/rdf-tabular/issues>;         doap:category <http://dbpedia.org/resource/Resource_Description_Framework>,           <http://dbpedia.org/resource/Ruby_(programming_language)>;         doap:created ""2015-01-05""^^xsd:date;         doap:description ""RDF::Tabular processes tabular data with metadata creating RDF or JSON output.""@en;         doap:developer <http://greggkellogg.net/foaf#me>;         doap:documenter <http://greggkellogg.net/foaf#me>;         doap:download-page <https://rubygems.org/gems/rdf-tabular>;         doap:homepage <http://ruby-rdf.github.com/rdf-tabular>;         doap:implements <http://www.w3.org/TR/tabular-data-model/>,           <http://www.w3.org/TR/tabular-metadata/>,           <http://www.w3.org/TR/csv2rdf/>,           <http://www.w3.org/TR/csv2json/>;         doap:license <https://unlicense.org/1.0/>;         doap:mailing-list <http://lists.w3.org/Archives/Public/public-rdf-ruby/>;         doap:maintainer <http://greggkellogg.net/foaf#me>;         doap:name ""RDF::Tabular"";         doap:programming-language ""Ruby"";         doap:shortdesc ""Tabular Data RDF Reader and JSON serializer.""@en;         foaf:maker <http://greggkellogg.net/foaf#me> .         [          a csvw:TableGroup;          csvw:table [            a csvw:Table;            csvw:row [              a csvw:Row;              csvw:describes <https://rubygems.org/gems/rdf-tabular>;              csvw:rownum 1;              csvw:url <file://users/gregg/Projects/rdf-tabular/etc/doap.csv#row=2>            ],  [              a csvw:Row;              csvw:describes <https://rubygems.org/gems/rdf-tabular>;              csvw:rownum 2;              csvw:url <file://users/gregg/Projects/rdf-tabular/etc/doap.csv#row=3>            ],  [              a csvw:Row;              csvw:describes <https://rubygems.org/gems/rdf-tabular>;              csvw:rownum 3;              csvw:url <file://users/gregg/Projects/rdf-tabular/etc/doap.csv#row=4>            ],  [              a csvw:Row;              csvw:describes <https://rubygems.org/gems/rdf-tabular>;              csvw:rownum 4;              csvw:url <file://users/gregg/Projects/rdf-tabular/etc/doap.csv#row=5>            ];            csvw:url <file://users/gregg/Projects/rdf-tabular/etc/doap.csv>          ];          prov:wasGeneratedBy [            a prov:Activity;            prov:endedAtTime ""2015-04-11T12:33:26Z""^^xsd:dateTime;            prov:qualifiedUsage [              a prov:Usage;              prov:entity <file://users/gregg/Projects/rdf-tabular/etc/doap.csv>;              prov:hadRole csvw:csvEncodedTabularData            ],  [              a prov:Usage;              prov:entity <file://users/gregg/Projects/rdf-tabular/etc/doap.csv-metadata.json>;              prov:hadRole csvw:tabularMetadata            ];            prov:startedAtTime ""2015-04-11T12:33:25Z""^^xsd:dateTime;            prov:wasAssociatedWith <https://rubygems.org/gems/rdf-tabular>          ]       ] .    The provenance on table-source information can be excluded by using the `:minimal` option to the reader.    It can also generate JSON output (not complete JSON-LD, but compatible with it), using the {RDF::Tabular::Reader#to_json} method:        {        ""table"": [          {            ""url"": ""file://users/gregg/Projects/rdf-tabular/etc/doap.csv"",            ""row"": [              {                ""url"": ""file://users/gregg/Projects/rdf-tabular/etc/doap.csv#row=2"",                ""rownum"": 1,                ""describes"": [                  {                    ""@id"": ""https://rubygems.org/gems/rdf-tabular"",                    ""http://usefulinc.com/ns/doap#name"": ""RDF::Tabular"",                    ""@type"": ""http://usefulinc.com/ns/doap#Project"",                    ""http://usefulinc.com/ns/doap#homepage"": ""http://ruby-rdf.github.com/rdf-tabular"",                    ""http://usefulinc.com/ns/doap#license"": ""https://unlicense.org/1.0/"",                    ""http://usefulinc.com/ns/doap#shortdesc"": ""Tabular Data RDF Reader and JSON serializer."",                    ""http://usefulinc.com/ns/doap#description"": ""RDF::Tabular processes tabular data with metadata creating RDF or JSON output."",                    ""http://usefulinc.com/ns/doap#created"": ""2015-01-05"",                    ""http://usefulinc.com/ns/doap#programming-language"": ""Ruby"",                    ""http://usefulinc.com/ns/doap#implements"": ""http://www.w3.org/TR/tabular-data-model/"",                    ""http://usefulinc.com/ns/doap#category"": ""http://dbpedia.org/resource/Resource_Description_Framework"",                    ""http://usefulinc.com/ns/doap#download-page"": ""https://rubygems.org/gems/rdf-tabular"",                    ""http://usefulinc.com/ns/doap#mailing-list"": ""http://lists.w3.org/Archives/Public/public-rdf-ruby/"",                    ""http://usefulinc.com/ns/doap#bug-database"": ""https://github.com/ruby-rdf/rdf-tabular/issues"",                    ""http://usefulinc.com/ns/doap#blog"": ""http://greggkellogg.net/"",                    ""http://usefulinc.com/ns/doap#developer"": ""http://greggkellogg.net/foaf#me"",                    ""http://usefulinc.com/ns/doap#maintainer"": ""http://greggkellogg.net/foaf#me"",                    ""http://usefulinc.com/ns/doap#documenter"": ""http://greggkellogg.net/foaf#me"",                    ""foaf:maker"": ""http://greggkellogg.net/foaf#me"",                    ""dc:title"": ""RDF::Tabular"",                    ""dc:description"": ""RDF::Tabular processes tabular data with metadata creating RDF or JSON output."",                    ""dc:date"": ""2015-01-05"",                    ""dc:creator"": ""http://greggkellogg.net/foaf#me"",                    ""dc:isPartOf"": ""https://rubygems.org/gems/rdf""                  }                ]              },              {                ""url"": ""file://users/gregg/Projects/rdf-tabular/etc/doap.csv#row=3"",                ""rownum"": 2,                ""describes"": [                  {                    ""@id"": ""https://rubygems.org/gems/rdf-tabular"",                    ""@type"": ""http://www.w3.org/ns/earl#TestSubject"",                    ""http://usefulinc.com/ns/doap#implements"": ""http://www.w3.org/TR/tabular-metadata/"",                    ""http://usefulinc.com/ns/doap#category"": ""http://dbpedia.org/resource/Ruby_(programming_language)""                  }                ]              },              {                ""url"": ""file://users/gregg/Projects/rdf-tabular/etc/doap.csv#row=4"",                ""rownum"": 3,                ""describes"": [                  {                    ""@id"": ""https://rubygems.org/gems/rdf-tabular"",                    ""@type"": ""http://www.w3.org/ns/earl#Software"",                    ""http://usefulinc.com/ns/doap#implements"": ""http://www.w3.org/TR/csv2rdf/""                  }                ]              },              {                ""url"": ""file://users/gregg/Projects/rdf-tabular/etc/doap.csv#row=5"",                ""rownum"": 4,                ""describes"": [                  {                    ""@id"": ""https://rubygems.org/gems/rdf-tabular"",                    ""http://usefulinc.com/ns/doap#implements"": ""http://www.w3.org/TR/csv2json/""                  }                ]              }            ]          }        ]      }    ## Tutorials    * [CSV on the Web](https://www.greggkellogg.net/2015/08/csv-on-the-web-presentation/)  * [Implementing CSV on the Web](https://greggkellogg.net/2015/04/implementing-csv-on-the-web/)    ## Command Line  When the `linkeddata` gem is installed, RDF.rb includes a `rdf` executable which acts as a wrapper to perform a number of different  operations on RDF files using available readers and writers, including RDF::Tabular. The commands specific to RDF::Tabular is     * `tabular-json`: Parse the CSV file and emit data as Tabular JSON    To use RDF::Tabular specific features, you must use the `--input-format tabular` option to the `rdf` executable.    Other `rdf` commands and options treat CSV as a standard RDF format.    Example usage:        rdf serialize https://raw.githubusercontent.com/ruby-rdf/rdf-tabular/develop/etc/doap.csv \        --output-format ttl      rdf tabular-json --input-format tabular https://raw.githubusercontent.com/ruby-rdf/rdf-tabular/develop/etc/doap.csv      rdf validate https://raw.githubusercontent.com/ruby-rdf/rdf-tabular/develop/etc/doap.csv --validate    Note that the `--validate` option must be used with the `validate` (or other) command to detect parse-time errors in addition to validating any resulting RDF triples.    ## RDF Reader  RDF::Tabular also acts as a normal RDF reader, using the standard RDF.rb Reader interface:        graph = RDF::Graph.load(""etc/doap.csv"", minimal: true)    ## Documentation  Full documentation available on [RubyDoc](https://rubydoc.info/gems/rdf-tabular/file/README.md)    ### Principal Classes  * {RDF::Tabular}    * {RDF::Tabular::JSON}    * {RDF::Tabular::Format}    * {RDF::Tabular::Metadata}    * {RDF::Tabular::Reader}    ## Dependencies  * [Ruby](https://ruby-lang.org/) (>= 2.6)  * [RDF.rb](https://rubygems.org/gems/rdf) (~> 3.2)  * [JSON](https://rubygems.org/gems/json) (>= 2.6)    ## Installation  The recommended installation method is via [RubyGems](https://rubygems.org/).  To install the latest official release of the `RDF::Tabular` gem, do:        % [sudo] gem install rdf-tabular    ## Mailing List  * <https://lists.w3.org/Archives/Public/public-rdf-ruby/>    ## Author  * [Gregg Kellogg](https://github.com/gkellogg) - <https://greggkellogg.net/>    ## Contributing  * Do your best to adhere to the existing coding conventions and idioms.  * Don't use hard tabs, and don't leave trailing whitespace on any line.  * Do document every method you add using [YARD][] annotations. Read the    [tutorial][YARD-GS] or just look at the existing code for examples.  * Don't touch the `rdf-tabular.gemspec`, `VERSION` or `AUTHORS` files. If you need to change them, do so on your private branch only.  * Do feel free to add yourself to the `CREDITS` file and the corresponding list in the the `README`. Alphabetical order applies.  * Do note that in order for us to merge any non-trivial changes (as a rule    of thumb, additions larger than about 15 lines of code), we need an    explicit [public domain dedication][PDD] on record from you,    which you will be asked to agree to on the first commit to a repo within the organization.    Note that the agreement applies to all repos in the [Ruby RDF](https://github.com/ruby-rdf/) organization.    License  -------    This is free and unencumbered public domain software. For more information,  see <https://unlicense.org/> or the accompanying {file:UNLICENSE} file.    [Ruby]:             https://ruby-lang.org/  [RDF]:              https://www.w3.org/RDF/  [YARD]:             https://yardoc.org/  [YARD-GS]:          https://rubydoc.info/docs/yard/file/docs/GettingStarted.md  [PDD]:              https://unlicense.org/#unlicensing-contributions  [RDF.rb]:           https://rubygems.org/gems/rdf  [CSV]:              https://en.wikipedia.org/wiki/Comma-separated_values  [W3C CSVW]:         https://www.w3.org/2013/csvw/wiki/Main_Page  [URI template]:     https://tools.ietf.org/html/rfc6570  [UAX35]:            https://www.unicode.org/reports/tr15/ """
Semantic web;https://github.com/clarkparsia/csv2rdf;"""csv2rdf  =======    csv2rdf is a simple tool for generating RDF output from CSV/TSV files. The conversion is done by a template file  that shows how the RDF output will look for one row. See [examples/cars](examples/cars) for details.     Building  --------    `ant clean dist` will create a local build in the `dist` sub-directory.    Running  -------    You can run the tool with the command `java -jar dist/lib/csv2rdf.jar` followed by arguments.    You can see the help screen with the command `java -jar dist/lib/csv2rdf.jar help convert`.    You can run the conversion for the example using `java -jar dist/lib/csv2rdf.jar examples/cars/template.ttl examples/cars/cars.csv cars.ttl`. """
Semantic web;https://github.com/creativesoftwarefdn/weaviate;"""<h1>Weaviate <img alt='Weaviate logo' src='https://raw.githubusercontent.com/semi-technologies/weaviate/19de0956c69b66c5552447e84d016f4fe29d12c9/docs/assets/weaviate-logo.png' width='124' align='right' /></h1>    ## The ML-first vector search engine    [![Build Status](https://api.travis-ci.org/semi-technologies/weaviate.svg?branch=master)](https://travis-ci.org/semi-technologies/weaviate/branches)  [![Go Report Card](https://goreportcard.com/badge/github.com/semi-technologies/weaviate)](https://goreportcard.com/report/github.com/semi-technologies/weaviate)  [![Coverage Status](https://codecov.io/gh/semi-technologies/weaviate/branch/master/graph/badge.svg)](https://codecov.io/gh/semi-technologies/weaviate)  [![Slack](https://img.shields.io/badge/slack--channel-blue?logo=slack)](https://join.slack.com/t/weaviate/shared_invite/zt-goaoifjr-o8FuVz9b1HLzhlUfyfddhw)  [![Newsletter](https://img.shields.io/badge/newsletter-blue?logo=revue)](http://weaviate-newsletter.semi.technology/)    ## Description    **Weaviate in a nutshell**: Weaviate is a vector search engine and vector database. Weaviate uses machine learning to vectorize and store data, and to find answers to natural language queries. With Weaviate you can also bring your custom ML models to production scale.    **Weaviate in detail**: Weaviate is a low-latency vector search engine with out-of-the-box support for different media types (text, images, etc.). It offers Semantic Search, Question-Answer-Extraction, Classification, Customizable Models (PyTorch/TensorFlow/Keras), and more. Built from scratch in Go, Weaviate stores both objects and vectors, allowing for combining vector search with structured filtering with the fault-tolerance of a cloud-native database, all accessible through GraphQL, REST, and various language clients.    ## Weaviate helps ...    1. **Software Engineers** ([docs](https://weaviate.io/developers/weaviate/current/)) - Who use Weaviate as an ML-first database for your applications.       * Out-of-the-box modules for: NLP/semantic search, automatic classification and image similarity search.      * Easy to integrate in your current architecture, with full CRUD support like you're used to from other OSS databases.      * Cloud-native, distributed, runs well on Kubernetes and scales with your workloads.    2. **Data Engineers** ([docs](https://weaviate.io/developers/weaviate/current/)) - Who use Weaviate as a vector database that is built up from the ground with ANN at its core, and with the same UX they love from Lucene-based search engines.      * Weaviate has a modular setup that allows to use your own ML models inside Weaviate, but you can also use out-of-the-box ML models (e.g., SBERT, ResNet, fasttext, etc).      * Weaviate takes care of the scalability, so that you don't have to.      * Deploy and maintain ML models in production reliably and efficiently.    3. **Data Scientists** ([docs](https://weaviate.io/developers/weaviate/current/)) - Who use Weaviate for a seamless handover of their Machine Learning models to MLOps.      * Deploy and maintain your ML models in production reliably and efficiently.      * Weaviate's modular design allows you to easily package any custom trained model you want.      * Smooth and accelerated handover of your Machine Learning models to engineers.    ## GraphQL interface demo    <a href=""https://weaviate.io/developers/weaviate/current/"" target=""_blank""><img src=""https://weaviate.io/img/weaviate-demo.gif?i=8"" alt=""Demo of Weaviate"" width=""100%""></a>    <sup>Weaviate GraphQL demo on news article dataset containing: Transformers module, GraphQL usage, semantic search, _additional{} features, Q&A, and Aggregate{} function. You can the demo on this dataset in the GUI here: <a href=""https://console.semi.technology/console/query#weaviate_uri=https://demo.dataset.playground.semi.technology&graphql_query=%7B%0A%20%20Get%20%7B%0A%20%20%20%20Article(%0A%20%20%20%20%20%20nearText%3A%20%7B%0A%20%20%20%20%20%20%20%20concepts%3A%20%5B%22Housing%20prices%22%5D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20where%3A%20%7B%0A%20%20%20%20%20%20%20%20operator%3A%20Equal%0A%20%20%20%20%20%20%20%20path%3A%20%5B%22inPublication%22%2C%20%22Publication%22%2C%20%22name%22%5D%0A%20%20%20%20%20%20%20%20valueString%3A%20%22The%20Economist%22%0A%20%20%20%20%20%20%7D%0A%20%20%20%20)%20%7B%0A%20%20%20%20%20%20title%0A%20%20%20%20%20%20inPublication%20%7B%0A%20%20%20%20%20%20%20%20...%20on%20Publication%20%7B%0A%20%20%20%20%20%20%20%20%20%20name%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20_additional%20%7B%0A%20%20%20%20%20%20%20%20certainty%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D"" target=""_blank"">semantic search</a>, <a href=""https://console.semi.technology/console/query#weaviate_uri=https://demo.dataset.playground.semi.technology&graphql_query=%7B%0A%20%20Get%7B%0A%20%20%20%20Article(%0A%20%20%20%20%20%20ask%3A%20%7B%0A%20%20%20%20%20%20%20%20question%3A%20%22What%20did%20Jemina%20Packington%20predict%3F%22%0A%20%20%20%20%20%20%20%20properties%3A%20%5B%22summary%22%5D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20limit%3A%201%0A%20%20%20%20)%7B%0A%20%20%20%20%20%20title%0A%20%20%20%20%20%20inPublication%20%7B%0A%20%20%20%20%20%20%20%20...%20on%20Publication%20%7B%0A%20%20%20%20%20%20%20%20%20%20name%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20_additional%20%7B%0A%20%20%20%20%20%20%20%20answer%20%7B%0A%20%20%20%20%20%20%20%20%20%20endPosition%0A%20%20%20%20%20%20%20%20%20%20property%0A%20%20%20%20%20%20%20%20%20%20result%0A%20%20%20%20%20%20%20%20%20%20startPosition%0A%20%20%20%20%20%20%20%20%7D%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D"" target=""_blank"">Q&A</a>, <a href=""https://console.semi.technology/console/query#weaviate_uri=https://demo.dataset.playground.semi.technology&graphql_query=%7B%0A%20%20Aggregate%20%7B%0A%20%20%20%20Article%20%7B%0A%20%20%20%20%20%20meta%20%7B%0A%20%20%20%20%20%20%20%20count%0A%20%20%20%20%20%20%7D%0A%20%20%20%20%7D%0A%20%20%7D%0A%7D"" target=""_blank"">Aggregate</a>.</sup>    ## Features    Weaviate makes it easy to use state-of-the-art ML models while giving you the scalability, ease of use, safety and cost-effectiveness of a purpose-built vector database. Most notably:    * **Fast queries**<br>     Weaviate typically performs a 10-NN neighbor search out of millions of objects in considerably less than 100ms.     <br><sub></sub>    * **Any media type with Weaviate Modules**<br>    Use State-of-the-Art ML model inference (e.g. Transformers) for Text, Images, etc. at search and query time to let Weaviate manage the process of vectorizing your data for your - or import your own vectors.    * **Combine vector and scalar search**<br>    Weaviate allows for efficient combined vector and scalar searches, e.g “articles related to the COVID 19 pandemic published within the past 7 days”. Weaviate stores both your objects and the vectors and make sure the retrieval of both is always efficient. There is no need for a third party object storage.     * **Real-time and persistent**<br>  Weaviate let’s you search through your data even if it’s currently being imported or updated. In addition, every write is written to a Write-Ahead-Log (WAL) for immediately persisted writes - even when a crash occurs.    * **Horizontal Scalability**<br>    Scale Weaviate for your exact needs, e.g. High-Availability, maximum ingestion, largest possible dataset size, maximum queries per second, etc. (Multi-Node sharding since `v1.8.0`, Replication under development)     * **Cost-Effectiveness**<br>    Very large datasets do not need to be kept entirely in memory in Weaviate. At the same time available memory can be used to increase the speed of queries. This allows for a conscious speed/cost trade-off to suit every use case.    * **Graph-like connections between objects**<br>    Make arbitrary connections between your objects in a graph-like fashion to resemble real-life connections between your data points. Traverse those connections using GraphQL.    ## Documentation    You can find detailed documentation in the [developers section of our website](https://weaviate.io/developers/weaviate/current/) or directly go to one of the docs using the links in the list below.    ## Additional material    ### Video    - [Weaviate introduction video](https://www.youtube.com/watch?v=IExopg1r4fw)    ### Reading    - [Weaviate is an open-source search engine powered by ML, vectors, graphs, and GraphQL (ZDNet)](https://www.zdnet.com/article/weaviate-an-open-source-search-engine-powered-by-machine-learning-vectors-graphs-and-graphql/)  - [Weaviate, an ANN Database with CRUD support (DB-Engines.com)](https://db-engines.com/en/blog_post/87)  - [A sub-50ms neural search with DistilBERT and Weaviate (Towards Datascience)](https://towardsdatascience.com/a-sub-50ms-neural-search-with-distilbert-and-weaviate-4857ae390154)  - [Getting Started with Weaviate Python Library (Towards Datascience)](https://towardsdatascience.com/getting-started-with-weaviate-python-client-e85d14f19e4f)    ## Examples    You can find [code examples here](https://github.com/semi-technologies/weaviate-examples)    ## Support    - [Stackoverflow for questions](https://stackoverflow.com/questions/tagged/weaviate)  - [Github for issues](https://github.com/semi-technologies/weaviate/issues)  - [Slack channel to connect](https://join.slack.com/t/weaviate/shared_invite/zt-goaoifjr-o8FuVz9b1HLzhlUfyfddhw)  - [Newsletter to stay in the know](http://weaviate-newsletter.semi.technology/)    ## Contributing    - [How to Contribute](https://weaviate.io/developers/contributor-guide/current/) """
Semantic web;https://github.com/egonw/rrdf;"""  # About    Package to bring RDF and SPARQL functionality to R, using the Jena libraries (needs Java7 or higher).        java/ -> Helper classes for the R rdf package.      rdf/ -> R package for dealing with RDF, using Jena.      rrdflibs/ -> Jena libraries    Information about this package can be found in this preprint:        Willighagen E. (2014) Accessing biological data in R with semantic web      technologies. PeerJ PrePrints 2:e185v3    See https://dx.doi.org/10.7287/peerj.preprints.185v3    (Please cite this paper if you use this package.)    To cite a specific release, you can use these http://zenodo.org/ DOIs:    * rrdf 2.1.2 [![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.34307.svg)](http://dx.doi.org/10.5281/zenodo.34307)  * rrdf 2.1.1 [![DOI](https://zenodo.org/badge/doi/10.5281/zenodo.20717.svg)](http://dx.doi.org/10.5281/zenodo.20717)    # User mailing list        https://groups.google.com/forum/#!forum/rrdf-user    # Copyright / License    ## rrdflibs package    Copyright (C) 2011-2015  Egon Willighagen and contributors    Apache License 2.0 for for the rrdflibs package files.    Copyright for Jena is described in the LICENSE and java/NOTICE  files. Please also visit https://jena.apache.org/.    ## rrdf package    Copyright (C) 2011-2015  Egon Willighagen and contributors    License AGPL v3 for the rrdf package.    ## Authors / Contributors    Authors:    Egon Willighagen    Contributions from:    Carl Boettiger,  Ryan Kohl    (See: https://github.com/egonw/rrdf/graphs/contributors)    # Install from R    Previously, the packages were available from CRAN, but this is no longer the case.    Mind you, the below install_github() method will attempt to rebuild the vignette  and therefore at this moment require a LaTeX distribution with pdflatex and a few  packages installed. See also issue https://github.com/egonw/rrdf/issues/28 and  https://github.com/egonw/rrdf/issues/29.        > install.packages(""rJava"") # if not present already      > install.packages(""devtools"") # if not present already      > library(devtools)      > install_github(""egonw/rrdf"", subdir=""rrdflibs"")      > install_github(""egonw/rrdf"", subdir=""rrdf"", build_vignettes = FALSE)    # Compile from source        $ cd rrdf/java      $ groovy build.groovy      $ cd ../..      $ R CMD build rrdflibs      $ R CMD check --as-cran rrdflibs_1.4.0.tar.gz      $ R CMD build rrdf      $ tar xvf rrdf_2.0.4.tar.gz rrdf/inst/doc/tutorial.pdf      $ R CMD check --as-cran rrdf_2.0.4.tar.gz    # Error Handling    In case of issues on Mac, follow the instructions below:    - check that the $JAVA_PATH variable is correctly set. If not:   1. Run `touch ~/.bash_profile; open ~/.bash_profile` in a Terminal window.   2. add the following lines to your .bash_profile and then save:      export JAVA_HOME=$(/usr/libexec/java_home)      export LD_LIBRARY_PATH=/Library/Java/JavaVirtualMachines/jdk1.8.0_31.jdk/Contents/Home/jre/lib/server       export PATH=$PATH:$JAVA_HOME/bin   3. Run `source ~/.bash_profile` in a terminal window.    - check that that Java >=1.7 is installed in your system. If not, go to [https://java.com/it/download/](https://java.com/it/download/)  - check that R sees the latest Java version (`.jinit();.jcall(""java/lang/System"", ""S"", ""getProperty"", ""java.runtime.version"")`). If not [1,2]:   1. Download and install Apple’s Java version 1.6 like you were asked to.   2. Reconfigure your R installation by typing `sudo R CMD javareconf` in a Terminal window.   3. Trigger a recompile by reinstalling rJava by typing `install.packages('rJava', type='source')`.   4. Run `sudo ln -f -s $(/usr/libexec/java_home)/jre/lib/server/libjvm.dylib /usr/lib` in a Terminal window. """
Semantic web;https://github.com/RDFLib/graph-pattern-learner;"""Graph Pattern Learner  =====================    (Work in progress...)    In this repository you find the code for a graph pattern learner. Given a list  of source-target-pairs and a SPARQL endpoint, it will try to learn SPARQL  patterns. Given a source, the learned patterns will try to lead you to the right  target.    The algorithm was first developed on a list of human associations that had been  mapped to DBpedia entities, as can be seen in  [data/gt_associations.csv](./data/gt_associations.csv):    | source                            | target                            |  | --------------------------------- | --------------------------------- |  | http://dbpedia.org/resource/Bacon | http://dbpedia.org/resource/Egg   |  | http://dbpedia.org/resource/Baker | http://dbpedia.org/resource/Bread |  | http://dbpedia.org/resource/Crow  | http://dbpedia.org/resource/Bird  |  | http://dbpedia.org/resource/Elm   | http://dbpedia.org/resource/Tree  |  | http://dbpedia.org/resource/Gull  | http://dbpedia.org/resource/Bird  |  | ...                               | ...                               |    As you can immediately see, associations don't only follow a single pattern. Our  algorithm is designed to be able to deal with this. It will try to learn several  patterns, which in combination model your input list of source-target-pairs. If  your list of source-target-pairs is less complicated, the algorithm will happily  terminate earlier.    You can find more information about the algorithm and learning patterns for  human associations on https://w3id.org/associations . The page also includes  publications, as well as the resulting patterns learned for human associations  from a local DBpedia endpoint including wikilinks.      Requirements  ------------    To run the graph pattern learner, we recommend:  - 8 cores (for parallel execution)  - more than 8 GB free RAM  - Linux 64 bit with Python 2.7      Installation  ------------    For now, the suggested installation method is via git clone (also allows easier  contributions):        git clone https://github.com/RDFLib/graph-pattern-learner.git      cd graph-pattern-learner    Afterwards, to setup the virtual environment and install all dependencies in it:        virtualenv venv &&      . venv/bin/activate &&      pip install --upgrade pip setuptools &&      pip install -r requirements.txt &&      deactivate      Running the learner  -------------------    Before actually running the evolutionary algorithm, please consider that it will  issue a lot of queries to the endpoint you're specifying. Please don't run this  against public endpoints without asking the providers first. It is likely that  you will disrupt their service or get blacklisted. I suggest running against an  own local endpoint filled with the datasets you're interested in. If you really  want to run this against public endpoints, at least don't run the multi-process  version, but restrict yourself to one process.    Always feel free to reach out for help or feedback via the issue tracker or via  associations at joernhees de. We might even run the learner for you ;)    To get a list of all available options run:        . venv/bin/activate && python run.py --help ; deactivate    Don't be scared by the length, most options use sane defaults, but it's nice to  be able to change things once you become more familiar with your data and the  learner.    The options you will definitely be interested are:        --associations_filename (defaults to ./data/gt_associations.csv)      --sparql_endpoint (defaults to http://localhost:8890/sparql)    To run a full training cycle, you probably might want to execute this:        ./run_create_bundle.sh --processes=8 --sparql_endpoint=... --visualise \          ./results/your_bundle_name \          --associations_filename=... # ... other-options ...    The algorithm will then by default randomly split your input list of  source-target-pairs into a training and a test set, train on the training set,  visualise the resulting learned patterns in `./results/bundle_name/visualise`,  before evaluating predictions on first the training- and then the test-set.    To use a learned model for prediction, you can run:        . venv/bin/activate && \      PYTHONIOENCODING=utf-8 python \          -m scoop -n8 run.py --associations_filename=... --sparql_endpoint=... \          --RES_DIR=./results/your_bundle_name/results \          --predict=manual ; \      deactivate      Contributors  ------------   * Jörn Hees   * Rouven Bauer (visualise code) """
Semantic web;https://github.com/spy16/fabric;"""> WIP    # Fabric      [![GoDoc](https://godoc.org/github.com/spy16/fabric?status.svg)](https://godoc.org/github.com/spy16/fabric) [![Go Report Card](https://goreportcard.com/badge/github.com/spy16/fabric)](https://goreportcard.com/report/github.com/spy16/fabric)    Fabric is a triple-store written in `Go`. Fabric provides simple functions  and store options to deal with ""Subject->Predicate->Object"" relations or so called  triples.    ## Usage    Get fabric by using `go get -u github.com/spy16/fabric` (Fabric as a library has no external dependencies)    ```go  mem := &fabric.InMemoryStore{}    fab := fabric.New(mem)    fab.Insert(context.Background(), fabric.Triple{      Source: ""Bob"",      Predicate: ""Knows"",      Target: ""John"",  })    fab.Query(context.Background(), fabric.Query{      Source: fabric.Clause{          Type: ""equal"",          Value: ""Bob"",      },  })  ```    To use a SQL database for storing the triples, use the following snippet:    ```go  db, err := sql.Open(""sqlite3"", ""fabric.db"")  if err != nil {      panic(err)  }    store := &fabric.SQLStore{      DB: db,  }  store.Setup(context.Background()) // to create required tables    fab := fabric.New(store)  ```    > Fabric `SQLStore` uses Go's standard `database/sql` package. So any SQL database supported  > through this interface (includes most major SQL databases) should work.    Additional store support can be added by implementing the `Store` interface.    ```go  type Store interface {  	Insert(ctx context.Context, tri Triple) error  	Query(ctx context.Context, q Query) ([]Triple, error)  	Delete(ctx context.Context, q Query) (int, error)  }  ```    Optional `Counter` and `ReWeighter` can be implemented by the store implementations  to support extended query options. """
Semantic web;https://github.com/gniezen/n3pygments;"""==========  n3pygments  ==========    This is a [Pygments](http://http://pygments.org/) lexer that performs syntax highlighting for:    * n3, turtle : Turtle/N3/NT (*.ttl, *.n3 and *.NT)   * sparql : SPARQL (*.sparql)    Make sure you're running Pygments 1.7 or higher and run        sudo python setup.py install        to install and e.g.        pygmentize -l turtle filename.ttl    to run Pygments.    This is mostly code from [Openvest](http://www.openvest.com/trac/wiki/n3SyntaxHighlighting#Pygments) which seems to be abandoned. The original instructions on that site only works when using `pygmentize` from the command-line. This implementation registers the package as a proper Pygments plugin which you can use from within Python, e.g.:        from pygments.lexers import (get_lexer_by_name,get_lexer_for_filename)      get_lexer_by_name(""turtle"")    should return `<pygments.lexers.Notation3Lexer>`.     n3pygments was created based on [this answer](http://tex.stackexchange.com/a/14929/8419) on the TeX StackExchange site. So yes, you can use it to perform using syntax highlighting on your code in LaTeX using [Minted](http://code.google.com/p/minted/). I have also used it with success to perform syntax highlighting on an [Octopress 2.0](http://octopress.org) blog.    Thanks go out to [Raphaël Pinson](http://www.raphink.info) and [Philip Cooper](http://Openvest.com).      """
Semantic web;https://github.com/abcoates/sublime-text-turtle-sparql;"""README for sublime-text-turtle-sparql  =====================================    Sublime Text syntax and completions files for Turtle (and, later, SPARQL). Developed for Sublime Text 3 beta.    Copy the 'Turtle' and 'SPARQL' directories into your Sublime text 'Packages' directory (use 'Preferences | Browse Packages...' in Sublime Text to find where that is).    Sublime Text uses the same syntax mechanism as TextMate and other editors.  [See unofficial Sublime Text documentation for details.](http://docs.sublimetext.info/en/latest/extensibility/syntaxdefs.html)    For information on Sublime Text completions, [see the unofficial Sublime Text documentation for details.](http://docs.sublimetext.info/en/latest/extensibility/completions.html) """
Semantic web;https://github.com/jpcik/morph-streams;"""morph-streams  =============    To build morph-streams you need:    * jvm7  * sbt (www.scala-sbt.org)    To compile it, run sbt after downloading the code:    ```  >sbt  >compile  ```    To see some sparql-stream queries running with esper, you can check some simple tests (QueryExecutionTest):    ```  >sbt  >project adapter-esper  >test  ```   """
Semantic web;https://github.com/blokhin/genealogical-trees;"""Semantic Web Genealogical Trees  ======  [![DOI](https://zenodo.org/badge/18811/blokhin/genealogical-trees.svg)](https://zenodo.org/badge/latestdoi/18811/blokhin/genealogical-trees)    Rationale  ------    This is an example of logical reasoning applied to the graphs of genealogical trees. Defining the kinship types on top of genealogical trees seems to be the perfect use case for semantic technologies.    The source of data is the GEDCOM file format (**.ged**) common for exchange of genealogical information. With the aid of **RDFLib** and **gedcom** Python libraries GEDCOM files are converted into the OWL 2 ontologies (ABox) in Turtle syntax (**.ttl** file extension), adopting the TBox of the Family History Knowledge Base ([FHKB](http://ceur-ws.org/Vol-1207/paper_11.pdf), see ```data/header.ttl``` file). Note that the FHKB ontology is although very small but uses unusually complex role hierarchy and is rather hard for modern reasoners. After reasoning with the naive Python implementation of the OWL 2 RL Profile and inferring all possible triples the ontologies are finally converted to the JSON-formatted graphs for in-browser visualization. This is done inside the bundled ```index.html``` HTML5 web-app by means of **D3.js** JavaScript library.    Using this repository  ------    The above is summarized in the ```gedcom2json.sh``` script, which is used like this:    ```shell  ./gedcom2json.sh path/to/your/gedcom.ged path/to/entailed_graph.json  ```    Resulting file ```entailed_graph.json``` is to be uploaded and visualized in the bundled HTML5 web-app ```index.html``` (no server scripting is used). Its copy is currently hosted at GitHub: [http://blokhin.github.io/genealogical-trees](http://blokhin.github.io/genealogical-trees). Locally it should be served from a web-server (e.g. ```python -m SimpleHTTPServer``` or ```php -S localhost:8000```).    Before processing, the required Python libraries listed in ```requirements.txt``` should be installed (virtualenv is highly recommended).    Blog tutorial  ------    https://blog.tilde.pro/semantic-web-technologies-on-an-example-of-family-trees-7518f3f835a9    Remark on FHKB  ------    Note however the [following comment](http://www.researchgate.net/publication/271131820_Manchester_Family_History_Advanced_OWL_Tutorial) from FHKB authors:    > We probably do not wish to drive a genealogical application using an FHKB in this form. Its purpose is educational. It touches most of OWL 2 and shows a lot of what it can do, but also a considerable amount of what it cannot do.  > As inference is maximised, the FHKB breaks most of the OWL 2 reasoners at the time of writing. However, it serves its role to teach about OWL 2.  > OWL 2 on its own and using it in this style, really does not work for family history.    Remark on reasoning  ------    Reasoning with the naive Python implementation of the OWL 2 RL Profile is very slow and takes hours for relatively big family trees. Therefore use of the fast native reasoner (like Fact++) is very desirable. Wrapped in the [owl-cpp](http://owl-cpp.sourceforge.net) Python bindings, Fact++ performs up to two orders of magnitude faster. """
Semantic web;https://github.com/SDM-TIB/SDM-RDFizer;"""# SDM-RDFizer  [![License](https://img.shields.io/pypi/l/rdfizer.svg)](https://github.com/SDM-TIB/SDM-RDFizer/blob/master/LICENSE)  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6225573.svg)](https://doi.org/10.5281/zenodo.6225573)  [![Latest PyPI version](https://img.shields.io/pypi/v/rdfizer?style=flat)](https://pypi.org/project/rdfizer/)  [![Python Version](https://img.shields.io/pypi/pyversions/rdfizer.svg)](https://pypi.org/project/rdfizer/)  [![PyPI status](https://img.shields.io:/pypi/status/rdfizer?)](https://pypi.org/project/rdfizer/)    This project presents the SDM-RDFizer, an interpreter of mapping rules that allows the transformation of (un)structured data into RDF knowledge graphs. The current version of the SDM-RDFizer assumes mapping rules are defined in the [RDF Mapping Language (RML) by Dimou et al](https://rml.io/specs/rml/). The SDM-RDFizer implements optimized data structures and relational algebra operators that enable an efficient execution of RML triple maps even in the presence of Big data. SDM-RDFizer is able to process data from heterogeneous data sources (CSV, JSON, RDB, XML) processing each set of RML rules (TriplesMap) in a multi-thread safe procedure.    ![SDM-RDFizer workflow](https://raw.githubusercontent.com/SDM-TIB/SDM-RDFizer/beta/images/architecture.png ""SDM-RDFizer workflow"")    # The new features presented by SDM-RDFizer version4.0    In version 4.0 of SDM-RDFizer, we have addressed the problem of efficiency in KG creation in terms of memory storage. SDM-RDFizer version4.0 includes a new module called ""TriplesMap Planning"" a.k.a. TMP which defines an optimized evaluation plan for the execution of triples maps. Additionally, version4.0 extends the previously included module (i.e. TriplesMap Execution a.k.a. TME) by introducing a new operator for compressing data stored in the data structures. These new features can be configured using two new parameters added to the configuration file, named ""large_file"" and ""ordered"".     We have performed extensive empirical evaluation on SDM-RDFizer version4.0 in terms of execution time and memory usage. The experiments are set up to empirically compare the impact of data duplicate rates, data size, and the complexity and the execution order of the triples maps on two versions of SDM-RDFizer (i.e. version4.0 and version3.6) and other exisiting engines icluding [RMLMapper v4.7](https://github.com/RMLio/rmlmapper-java) and [RocketRML](https://github.com/semantifyit/RocketRML) ), in terms of execution time and memory usage. The experiments are performed on two different benchmarks:   - From [SDM-Genomic-datasets](https://figshare.com/articles/dataset/SDM-Genomic-Datasets/14838342/1), datasets including 10k, 100k, and 1M records with 25% and 75% duplicates rates, over six mapping rules with different complexities (1/4 simple object map, 2/5 object reference maps, 2/5 object join maps)  - From [GTFS-Madrid](https://github.com/oeg-upm/gtfs-bench), datasets with scale values of 1-csv, 5-csv, 10-csv, and 50-csv, over two different mapping rules (72 simple object maps and 11 object join maps).     The results of explained experiments can be summarized as the following:  ![Overview of Results (Execution Time Comparison)](https://raw.githubusercontent.com/SDM-TIB/SDM-RDFizer/beta/images/time.png ""Execution Time Comparison"")  As observed in the figures above, both versions of SDM-RDFizer completed all the testbeds successfully while the other two engines have cases of timeout. SDM-RDFizer version3.6 and RocketRML version 1.7.0 are competitve in simple testbeds, however, SDM-RDFizer version4.0 shows the best performance in all the testbeds.   ![Overview of Results (Memory Consumption Comparison)](https://raw.githubusercontent.com/SDM-TIB/SDM-RDFizer/beta/images/memory.png ""Memory Consumption Comparison"")  As illustrated in the figures above, SDM-RDFizer version4.0 has the smallest peak in memory usage compared to the previous version of SDM-RDFizer.        The results of the execution of SDM-RDFizer has been described in the following research reports:    - Enrique Iglesias, Samaneh Jozashoori, David Chaves-Fraga, Diego Collarana, and Maria-Esther Vidal. 2020. SDM-RDFizer: An RML Interpreter for the Efficient Creation of RDF Knowledge Graphs. The 29th ACM International Conference on Information and Knowledge Management (CIKM ’20).    - Samaneh Jozashoori, David Chaves-Fraga, Enrique Iglesias, Oscar Corcho, and Maria-Esther Vidal. 2020. FunMap: Efficient Execution of Functional Mappings for Knowledge Graph Creation. The 19th International Semantic Web Conference - Research Track (ISWC 2020).    - Samaneh Jozashoori and Maria-Esther Vidal. MapSDI: A Scaled-up Semantic Data Integrationframework for Knowledge Graph Creation. The 27th International Conference on Cooperative Information Systems (CoopIS 2019).     - David Chaves-Fraga, Kemele M. Endris, Enrique Iglesias, Oscar Corcho, and Maria-Esther Vidal. What are the Parameters that Affect the Construction of a Knowledge Graph?. The 18th International Conference on Ontologies, DataBases, and Applications of Semantics (ODBASE 2019).    - David Chaves-Fraga, Antón Adolfo, Jhon Toledo, and Oscar Corcho. ONETT: Systematic Knowledge Graph Generation for National Access Points. The 1st International Workshop on Semantics for Transport co-located with SEMANTiCS 2019.    - David Chaves-Fraga, Freddy Priyatna, Andrea Cimmino, Jhon Toledo, Edna Ruckhaus, and Oscar Corcho. GTFS-Madrid-Bench: A benchmark for virtual knowledge graph access in the transport domain. Journal of Web Semantics, 2020.    Additional References:    - Dimou et al. 2014. Dimou, A., Sande, M.V., Colpaert, P., Verborgh, R., Mannens, E., de Walle, R.V.:RML: A generic language for integrated RDF mappings of heterogeneous data. In:Proceedings of the Workshop on Linked Data on the Web co-located with the 23rdInternational World Wide Web Conference (WWW 2014)     # Projects where the SDM-RDFizer has been used    The SDM-RDFizer is used in the creation of the knowledge graphs of EU H2020 projects and national projects where the Scientific Data Management group participates. These projects include:    - iASiS (http://project-iasis.eu/): big data for precision medicine, based on patient data insights. The iASiS RDF knowledge graph comprises more than 1.2B RDF triples collected from more than 40 heterogeneous sources using over 1300 RML triple maps.   - BigMedilytics (https://www.bigmedilytics.eu/): lung cancer pilot. 800 RML triple maps are used to create the lung cancer knowledge graph from around 25 data sources with 500M RDF triples.  - CLARIFY (https://www.clarify2020.eu/): predict poor health status after specific oncological treatments  - P4-LUCAT (https://www.tib.eu/de/forschung-entwicklung/projektuebersicht/projektsteckbrief/p4-lucat)  - ImProVIT (https://www.tib.eu/de/forschung-entwicklung/projektuebersicht/projektsteckbrief/improvit)  - PLATOON (https://platoon-project.eu/)   - EUvsVirus Hackathon (April 2020) (https://blogs.tib.eu/wp/tib/2020/05/06/how-do-knowledge-graphs-contribute-to-understanding-covid-19-related-treatments/). SDM-RDFizer created the Knowledge4COVID-19 knowledge graph during the participation of the team of the Scientific Data Management group. By June 7th, 2020, this KG is comprised of 28M RDF triples describing at a fine-grained level 63527 COVID-19 scientific publications and COVID-19 related concepts (e.g., 5802 substances, 1.2M drug-drug interactions, and 103 molecular disfunctions).     The SDM-RDFizer is also used in EU H2020, EIT-Digital and Spanish national projects where the Ontology Engineering Group (Technical University of Madrid) participates. These projects, mainly focused on the transportation and smart cities domain, include:    - H2020 - SPRINT (http://sprint-transport.eu/): performance and scalability to test a semantic architecture for the Interoperability Framework on Transport across Europe.  - EIT-SNAP (https://www.snap-project.eu/): innovation project on the application of semantic technologies for national access points.  - Open Cities (https://ciudades-abiertas.es/): national project on creating common and shared vocabularies for Spanish Cities  - Drugs4Covid (https://drugs4covid.oeg.fi.upm.es/): NLP annotations and metadata from more than 60,000 scientific papers about COVID viruses are integrated in a KG with almost 44M of facts (triples). SDM-RDFizer was used for creating this KG.    Other projects were the SDM-RDFizer is also used:  -  Virtual Platform for the H2020 European Joint Programme on Rare Disease (https://www.ejprarediseases.org)      # Installing and Running the SDM-RDFizer   From PyPI (https://pypi.org/project/rdfizer/):  ```  python3 -m pip install rdfizer  python3 -m rdfizer -c /path/to/config/file  ```    From Github/Docker:  Visit the [wiki](https://github.com/SDM-TIB/SDM-RDFizer/wiki) of the repository to learn how to install and run the SDM-RDFizer. You can also take a look to our demo at: https://www.youtube.com/watch?v=DpH_57M1uOE  - Install and run the SDM-RDFizer: https://github.com/SDM-TIB/SDM-RDFizer/wiki/Install&Run  - Parameters to configure SDM-RDFizer: https://github.com/SDM-TIB/SDM-RDFizer/wiki/The-Parameters-of-the-Configuration-file  - FAQ: https://github.com/SDM-TIB/SDM-RDFizer/wiki/FAQ    ## Configurations  You can easily customize your own configurations from the set of features that SDM-RDFzier offers by changing the values of the parameters in the config file. The descriptions of each parameter and the possible values are provided [here](https://github.com/SDM-TIB/SDM-RDFizer/wiki/The-Parameters-of-the-Configuration-file); ""ordered"" and ""large_file"" are the new features provided by SDM-RDFizer version4.0.        ## Version   ```  4.2  ```    ## RML-Test Cases  See the results of the SDM-RDFizer over the RML test-cases at the [RML Implementation Report](http://rml.io/implementation-report/). SDM-RDFizer version4.0 is tested over the latest published [test cases](https://rml.io/test-cases/) before the release.    ## Experimental Evaluations  See the results of the experimental evaluations of SDM-RDFizer version 3.* at [SDM-RDFizer-Experiments repository](https://github.com/SDM-TIB/SDM-RDFizer-Experiments)      ## License  This work is licensed under Apache 2.0    # Authors  The SDM-RDFizer has been developed by members of the Scientific Data Management Group at TIB, as an ongoing research effort. The development is coordinated and supervised by Maria-Esther Vidal (maria.vidal@tib.eu). We strongly encourage you to please report any issues you have with the SDM-RDFizer. You can do that over our contact email or creating a new issue here on Github. The SDM-RDFizer has been implemented by Enrique Iglesias (current version, iglesias@l3s.de) and Guillermo Betancourt (version 0.1, guillermojbetancourt@gmail.com) under the supervision of Samaneh Jozashoori (samaneh.jozashoori@tib.eu), David Chaves-Fraga (dchaves@fi.upm.es), and Kemele Endris (kemele.endris@tib.eu)   """
Semantic web;https://github.com/WileyLabs/askos;"""# A SKOS browser and editor    Early stage [SKOS](https://www.w3.org/TR/skos-primer/) browser and editor.    Built with [Vue.js](https://vuejs.org/), [LevelGraph](https://levelgraph.io/),  and [Semantic-UI](https://semantic-ui.com/).    ## Usage    ```sh  $ npm i  $ npm run dev  ```    The `dev` script will build, watch, and serve Askos making it available at  `http://localhost:8888/`.    ## Design Drafts    Early drafts of the overall intent of the design...highly subject to change.    ![Concept Listing](concept-listing.png)  ![Browsing to a Narrower Concept](concept-narrower.png)    See also the [design.svg](design.svg) file--which you're welcome to contribute  to!    ## License    MIT """
Semantic web;https://github.com/hyrise/hyrise;"""[![Build Status](https://hyrise-ci.epic-hpi.de/buildStatus/icon?job=Hyrise/hyrise/master)](https://hyrise-ci.epic-hpi.de/blue/organizations/jenkins/hyrise%2Fhyrise/activity/)  [![Coverage Status](https://hyrise-coverage-badge.herokuapp.com/coverage_badge.svg)](https://hyrise-ci.epic-hpi.de/job/Hyrise/job/hyrise/job/master/lastStableBuild/Llvm-cov_5fReport/)  [![CodeFactor](https://www.codefactor.io/repository/github/hyrise/hyrise/badge)](https://www.codefactor.io/repository/github/hyrise/hyrise)    # Welcome to Hyrise    Hyrise is a research in-memory database system that has been developed [by HPI since 2009](https://www.vldb.org/pvldb/vol4/p105-grund.pdf) and has been entirely [rewritten in 2017](https://openproceedings.org/2019/conf/edbt/EDBT19_paper_152.pdf). Our goal is to provide a clean and flexible platform for research in the area of in-memory data management. Its architecture allows us, our students, and other researchers to conduct experiments around new data management concepts. To enable realistic experiments, Hyrise features comprehensive SQL support and performs powerful query plan optimizations. Well-known benchmarks, such as TPC-H or TPC-DS, can be executed with a single command and without any preparation.    This readme file focuses on the technical aspects of the repository. For more background on our research and for a list of publications, please visit the [Hyrise project page](https://hpi.de/plattner/projects/hyrise.html).    You can still find the (archived) previous version of Hyrise on [Github](https://github.com/hyrise/hyrise-v1).    ## Citation    When referencing this version of Hyrise, please use the following bibtex entry:  <details><summary>(click to expand)</summary>      ```bibtex  @inproceedings{DBLP:conf/edbt/DreselerK0KUP19,    author    = {Markus Dreseler and                 Jan Kossmann and                 Martin Boissier and                 Stefan Klauck and                 Matthias Uflacker and                 Hasso Plattner},    editor    = {Melanie Herschel and                 Helena Galhardas and                 Berthold Reinwald and                 Irini Fundulaki and                 Carsten Binnig and                 Zoi Kaoudi},    title     = {Hyrise Re-engineered: An Extensible Database System for Research in                 Relational In-Memory Data Management},    booktitle = {Advances in Database Technology - 22nd International Conference on                 Extending Database Technology, {EDBT} 2019, Lisbon, Portugal, March                 26-29, 2019},    pages     = {313--324},    publisher = {OpenProceedings.org},    year      = {2019},    url       = {https://doi.org/10.5441/002/edbt.2019.28},    doi       = {10.5441/002/edbt.2019.28},    timestamp = {Mon, 18 Mar 2019 16:09:00 +0100},    biburl    = {https://dblp.org/rec/conf/edbt/DreselerK0KUP19.bib},    bibsource = {dblp computer science bibliography, https://dblp.org}  }  ```  </details>    ## Supported Systems  Hyrise is developed for Linux (preferrably the most current Ubuntu version) and optimized to run on server hardware. We support Mac to facilitate the local development of Hyrise, but do not recommend it for benchmarking.    ## Supported Benchmarks  We support a number of benchmarks out of the box. This makes it easy to generate performance numbers without having to set up the data generation, loading CSVs, and finding a query runner. You can run them using the `./hyriseBenchmark*` binaries.    | Benchmark  | Notes                                                                                                                    |  | ---------- | ------------------------------------------------------------------------------------------------------------------------ |  | TPC-DS     | [Query Plans](https://hyrise-ci.epic-hpi.de/job/hyrise/job/hyrise/job/master/lastStableBuild/artifact/query_plans/tpcds) |  | TPC-H      | [Query Plans](https://hyrise-ci.epic-hpi.de/job/hyrise/job/hyrise/job/master/lastStableBuild/artifact/query_plans/tpch)  |  | Join Order | [Query Plans](https://hyrise-ci.epic-hpi.de/job/hyrise/job/hyrise/job/master/lastStableBuild/artifact/query_plans/job)   |  | JCC-H      | Call the hyriseBenchmarkTPCH binary with the -j flag.                                                                    |   | TPC-C      | In development, no proper optimization done yet                                                                          |    # Getting started    *Have a look at our [contributor guidelines](CONTRIBUTING.md)*.    You can find definitions of most of the terms and abbreviations used in the code in the [glossary](GLOSSARY.md). If you cannot find something that you are looking for, feel free to open an issue.    The [Step by Step Guide](https://github.com/hyrise/hyrise/wiki/Step-by-Step-Guide) is a good starting point to get to know Hyrise.    ## Native Setup  You can install the dependencies on your own or use the install_dependencies.sh script (**recommended**) which installs all of the therein listed dependencies and submodules.  The install script was tested under macOS Big Sur (10.16) and Ubuntu 20.10 (apt-get).    See [dependencies](DEPENDENCIES.md) for a detailed list of dependencies to use with `brew install` or `apt-get install`, depending on your platform. As compilers, we generally use the most recent version of clang and gcc (Linux only). Please make sure that the system compiler points to the most recent version or use cmake (see below) accordingly.  Older versions may work, but are neither tested nor supported.    **Note about LLVM 13 and TBB 2021:** Hyrise can currently not be built with LLVM 13. We hope to get LLVM 13 running soon. For TBB, please use a `2020*` version until https://github.com/oneapi-src/oneTBB/issues/378 is resolved. On MacOS with brew, LLVM 12 and TBB 2020 can be installed as follows: `brew install tbb@2020 && brew install llvm@12`. Keep in mind that these package versions are alternate versions and, thus, not symlinked into `/usr/local`. `brew link tbb@2020` and `brew link llvm@12` symlinks these packages.    ## Setup using Docker  If you want to create a Docker-based development environment using CLion, head over to our [dedicated tutorial](https://github.com/hyrise/hyrise/wiki/Use-Docker-with-CLion).     Otherwise, to get all dependencies of Hyrise into a Docker image, run  ```  docker build -t hyrise .  ```    You can start the container via  ```  docker run -it hyrise  ```    Inside the container, you can then checkout Hyrise and run `./install_dependencies.sh` to download the required submodules.    ## Building and Tooling  It is highly recommended to perform out-of-source builds, i.e., creating a separate directory for the build.  Advisable names for this directory would be `cmake-build-{debug,release}`, depending on the build type.  Within this directory call `cmake ..` to configure the build.  By default, we use very strict compiler flags (beyond `-Wextra`, including `-Werror`). If you use one of the officially supported environments, this should not be an issue. If you simply want to test Hyrise on a different system and run into issues, you can call `cmake -DHYRISE_RELAXED_BUILD=On ..`, which will disable these strict checks.  Subsequent calls to CMake, e.g., when adding files to the build will not be necessary, the generated Makefiles will take care of that.    ### Compiler choice  CMake will default to your system's default compiler.  To use a different one, call `cmake -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ ..` in a clean build directory. See [dependencies](DEPENDENCIES.md) for supported compiler versions.    ### Unity Builds  Starting with cmake 3.16, you can use `-DCMAKE_UNITY_BUILD=On` to perform unity builds. For a complete (re-)build or when multiple files have to be rebuilt, these are usually faster, as the relative cost of starting a compiler process and loading the most common headers is reduced. However, this only makes sense for debug builds. See our [blog post](https://medium.com/hyrise/reducing-hyrises-build-time-8523135aed72) on reducing the compilation time for details.    ### ccache  For development, you may want to use [ccache](https://ccache.samba.org/), which reduces the time needed for recompiles significantly. Especially when switching branches, this can reduce the time to recompile from several minutes to one or less. On the downside, we have seen random build failures on our CI server, which is why we do not recommend ccache anymore but merely list it as an option. To use ccache, add `-DCMAKE_CXX_COMPILER_LAUNCHER=ccache` to your cmake call. You will need to [adjust some ccache settings](https://ccache.dev/manual/latest.html#_precompiled_headers) either in your environment variables or in your [ccache config](https://ccache.dev/manual/latest.html#_configuration) so that ccache can handle the precompiled headers. On our CI server, this worked for us: `CCACHE_SLOPPINESS=file_macro,pch_defines,time_macros CCACHE_DEPEND=1`.    ### Build  Simply call `make -j*`, where `*` denotes the number of threads to use.    Usually debug binaries are created.  To configure a build directory for a release build make sure it is empty and call CMake like `cmake -DCMAKE_BUILD_TYPE=Release`    ### Lint  `./scripts/lint.sh` (Google's cpplint is used for the database code. In addition, we use _flake8_ for linting the Python scripts under /scripts.)    ### Format  `./scripts/format.sh` (clang-format is used for the database code. We use _black_ for formatting the Python scripts under /scripts.)    ### Test  Calling `make hyriseTest` from the build directory builds all available tests.  The binary can be executed with `./<YourBuildDirectory>/hyriseTest`.  Subsets of all available tests can be selected via `--gtest_filter=`.    ### Coverage  `./scripts/coverage.sh` will print a summary to the command line and create detailed html reports at ./coverage/index.html    *Supports only clang on MacOS and only gcc on linux*    ### Address/UndefinedBehavior Sanitizers  `cmake -DENABLE_ADDR_UB_SANITIZATION=ON` will generate Makefiles with AddressSanitizer and Undefined Behavior options.  Compile and run them as normal - if any issues are detected, they will be printed to the console.  It will fail on the first detected error and will print a summary.  To convert addresses to actual source code locations, make sure llvm-symbolizer is installed (included in the llvm package) and is available in `$PATH`.  To specify a custom location for the symbolizer, set `$ASAN_SYMBOLIZER_PATH` to the path of the executable.  This seems to work out of the box on macOS - If not, make sure to have llvm installed.  The binary can be executed with `LSAN_OPTIONS=suppressions=asan-ignore.txt ./<YourBuildDirectory>/hyriseTest`.    `cmake -DENABLE_THREAD_SANITIZATION=ON` will work as above but with the ThreadSanitizer. Some sanitizers are mutually exclusive, which is why we use two configurations for this.    ### Compile Times  When trying to optimize the time spent building the project, it is often helpful to have an idea how much time is spent where.  `scripts/compile_time.sh` helps with that. Get usage instructions by running it without any arguments.    ## Maintainers  - Jan Kossmann  - Marcel Weisgut  - Martin Boissier  - Stefan Halfpap    Contact: firstname.lastname@hpi.de    ## Maintainer emeritus  - Markus Dreseler    ## Contributors  -   Yannick   Bäumer  -   Lawrence  Benson  -   Timo      Djürken  -   Alexander Dubrawski  -   Fabian    Dumke  -   Leonard   Geier  -   Richard   Ebeling  -   Fabian    Engel  -   Moritz    Eyssen  -   Martin    Fischer  -   Christian Flach  -   Pedro     Flemming  -   Mathias   Flüggen  -   Johannes  Frohnhofen  -   Pascal    Führlich  -   Carl      Gödecken  -   Adrian    Holfter  -   Ben       Hurdelhey  -   Sven      Ihde  -   Ivan      Illic  -   Jonathan  Janetzki  -   Michael   Janke  -   Max       Jendruk  -   David     Justen  -   Youri     Kaminsky  -   Marvin    Keller  -   Mirko     Krause  -   Eva       Krebs  -   Sven      Lehmann  -   Till      Lehmann  -   Tom       Lichtenstein  -   Daniel    Lindner  -   Alexander Löser  -   Jan       Mattfeld  -   Arne      Mayer  -   Dominik   Meier  -   Julian    Menzler  -   Torben    Meyer  -   Leander   Neiß  -   Hendrik   Rätz  -   Alexander Riese  -   Johannes  Schneider  -   David     Schumann  -   Simon     Siegert  -   Arthur    Silber  -   Toni      Stachewicz  -   Daniel    Stolpe  -   Jonathan  Striebel  -   Nils      Thamm  -   Hendrik   Tjabben  -   Justin    Trautmann  -   Carsten   Walther  -   Lukas     Wenzel  -   Fabian    Wiebe  -   Tim       Zimmermann """
Semantic web;https://github.com/SDM-TIB/FunMap;"""# FunMap: Functional Mappings for Scaled-Up Knowledge Graph Creation    We present FunMap, an interpreter of [RML](https://rml.io/docs/rml/introduction/)+[FnO](https://fno.io/),that converts a data integration system defined using RML+FnO into an equivalent data integration system where RML mappings are function-free. FunMap resembles existing mapping translation proposals and empowers the  knowledge  graph  creation  process  with  optimization  techniques  to  reduce execution  time.  Transformations  of  data  sources  include  the  projection  of  the attributes used in the RML+FnO mappings. They are supported on well-known properties of the relational algebra, e.g., the pushing down of projections and selections into the data sources, and enable not only the reduction of the size of  data  sources  but  also  the  elimination  of  duplicates.  Additionally,  FunMap materializes  functions  –expressed  in  FnO–  and  represents  the  results  as  data sources of the generated data integration system; the translation of RML+FnO into RML mappings that integrate the materialization of functions is performed by means of joins between the generated RML mappings.     ![FunMap-workflow](images/architecture.png?raw=true ""FunMap-workflow"")      ## Research papers:    Samaneh Jozashoori, David Chaves-Fraga, Enrique Iglesias, Oscar Corcho, and Maria-Esther Vidal. 2020. FunMap: Efficient Execution of Functional Mappings for Knowledge Graph Creation. The 19th International Semantic Web Conference - Research Track (ISWC 2020). *[Fully Reproduced Paper](https://github.com/SDM-TIB/FunMap/tree/eval-iswc2020)* [Online](https://www.researchgate.net/publication/346220361_FunMap_Efficient_Execution_of_Functional_Mappings_for_Knowledge_Graph_Creation)      ## How to run FunMap?    ### Configuration file  ```  [default]  main_directory: /     [datasets]  number_of_datasets: 1  # name of dataset  name: funmap   # path of results  output_folder: ${default:main_directory}results/   # yes executes FunMap, no executes FunMap- (without projections)  enrichment: yes  # only for RDB instance  dbType: mysql     [dataset1]  name: funmap  # mapping path  mapping: ${default:main_directory}mappings/mapping.ttl   # only for RDB  user: user   password: pass  host: 127.0.0.1  port: 3306   db: dbName  ```    ### Run with Docker   ```  # Preparation  docker build -t funmap .    # For CSV files  docker-compose up -d  # The path of the files in the mappings has to be: /data/nameOfTheFile.csv.  cp csvFiles.csv data/  cp mapping.ttl mappings/  cp config.ini funmap/    # For RDB instance  mkdir sql  cp sqlScript.sql sql/  docker-compose up -d   cp mapping.ttl mappings/  cp config.ini funmap/    # Execution  docker exec -it funmap python3 /funmap/run_translator.py /funmap/config[_rdb].ini  ```    ### Run with Python3  ```  pip install -r requirements.txt  python3 run_translator.py config[_rdb].ini  ```    ## Authors    - Samaneh Jozashoori (samaneh.jozashoori@tib.eu)  - David Chaves-Fraga (dchaves@fi.upm.es)  - Enrique Iglesias ( s6enigle@uni-bonn.de)  - Oscar Corcho (ocorcho@fi.upm.es)  - Maria-Esther Vidal (maria.vidal@tib.eu) """
Semantic web;https://github.com/albertmeronyo/docker2rdf;"""# docker2rdf  Mapper to represent Dockerfiles as RDF triples """
Semantic web;https://github.com/lorenae/qb4olap;"""  QB4OLAP is a Vocabulary for Business Intelligence over Linked Data.    It is an extension of the [DataCube](http://www.w3.org/TR/vocab-data-cube/) vocabulary that allows to represent OLAP cubes in RDF,   and to implement OLAP operators (such as Roll-up, Slice, and Dice) as SPARQL queries directly on this RDF representation.    #The vocabulary     You can find the current version of the vocabulary in Turtle format in this repository.   The latest version (v1.3) is available [here](https://github.com/lorenae/qb4olap/blob/master/rdf/qb4olap.ttl). Previous stable version of the vocabulary (v1.2) is available [here](https://github.com/lorenae/qb4olap/tree/master/rdf).    The main improvement in v1.3 is the ability to represent custom rollup relationships.    We have defined the following purls to refere to each version:    * Version 1.2: http://purl.org/qb4olap/cubes_v1.2  * Version 1.3: http://purl.org/qb4olap/cubes    We suggest to use dcterms:conformsTo property to indicate which version of QB4OLAP is used in a dataset.    #Documentation and related resources  Our [wiki](https://github.com/lorenae/qb4olap/wiki) contains a detailed description of the elements of the vocabulary, examples of QB4OLAP in-use, and related publications.    Check our [QB4OLAP-tools](https://github.com/lorenae/qb4olap-tools) project to see some examples of what can be done with this vocabulary.    #Examples    The [examples](https://github.com/lorenae/qb4olap/tree/master/examples) folder contains ttl files that represent the schema and instances of different cubes in QB4OLAP """
Semantic web;https://github.com/mmlab/TurtleValidator;"""TurtleValidator  ===========  RDF NTriples/Turtle validator using Ruben Verborgh's [N3 NodeJS library](https://github.com/RubenVerborgh/N3.js). Validate Turtle and Ntriples documents on syntax and XSD datatype errors through command line.    © 2014, 2015 - IDLab - Ghent University - imec  Source code: https://github.com/MMLab/TurtleValidator    Install:        npm install -g turtle-validator    Examples:        $ ttl <path-to-file ...>      $ curl http://data.linkeddatafragments.org/dbpedia -H ""accept: text/turtle"" | ttl      $ ttl http://triples.demo.thedatatank.com/demo.ttl    ## Or install the browser client    ```bash  # Equivalent to: npm build  npm install  browserify lib/validator.js -o public/js/ttl.js  ```    Then use it in your browser using the index.html in the public folder.  You can run this locally as follows.    ```bash  # Equivalent to: npm start  npm install  browserify lib/validator.js -o public/js/ttl.js  ws  ``` """
Semantic web;https://github.com/jsonresume/resume-schema;"""# JSON Resume Schema    [![GitHub Releases](https://badgen.net/github/tag/jsonresume/resume-schema)](https://github.com/jsonresume/resume-schema/releases)  [![NPM Release](https://badgen.net/npm/v/resume-schema)](https://www.npmjs.com/package/resume-schema)  [![Latest Status](https://github.com/jsonresume/resume-schema/workflows/Latest/badge.svg)](https://github.com/vanillawc/wc-template/actions)  [![Release Status](https://github.com/jsonresume/resume-schema/workflows/Release/badge.svg)](https://github.com/vanillawc/wc-template/actions)    Standard, Specification, Schema    ### Getting started    ```  npm install --save resume-schema  ```    To use    ```js  const resumeSchema = require(""resume-schema"");  resumeSchema.validate(    { name: ""Thomas"" },    function (err, report) {      if (err) {        console.error(""The resume was invalid:"", err);        return;      }      console.log(""Resume validated successfully:"", report);    },    function (err) {      console.error(""The resume was invalid:"", err);    }  );  ```    More likely    ```js  var fs = require(""fs"");  var resumeSchema = require(""resume-schema"");  var resumeObject = JSON.parse(fs.readFileSync(""resume.json"", ""utf8""));  resumeSchema.validate(resumeObject);  ```    The JSON Resume schema is available from:    ```js  require(""resume-schema"").schema;  ```    ### Contribute    We encourage anyone who's interested in participating in the formation of this standard to join the discussions [here on GitHub](https://github.com/jsonresume/resume-schema/issues). Also feel free to fork this project and submit new ideas to add to the JSON Resume Schema standard. To make sure all formatting is kept in check, please install the [EditorConfig plugin](http://editorconfig.org/) for your editor of choice.    ### Versioning    JSON Resume Schema adheres to Semantic Versioning 2.0.0. If there is a violation of  this scheme, report it as a bug. Specifically, if a patch or minor version is  released and breaks backward compatibility, that version should be immediately  yanked and/or a new version should be immediately released that restores  compatibility. Any change that breaks the public API will only be introduced at  a major-version release. As a result of this policy, you can (and should)  specify any dependency on JSON Resume Schema by using the Pessimistic Version  Constraint with two digits of precision.    We use automatic semver system.    Pull requests titles should be formatted as such    ```  ""fix: added something"" - will bump the patch version  ""feat: added something"" - will bump the minor version  ```    `major` version bumps will be few and far between for this schema.    ### Other resume standards    - [HR-XML](https://schemas.liquid-technologies.com/HR-XML/2007-04-15/)  - [Europass](http://europass.cedefop.europa.eu/about-europass) """
Semantic web;https://github.com/modelfabric/reactive-sparql;"""reactive-sparql  ===============    *""A Reactive SPARQL Client for Scala and Akka""*    This client uses [akka-streams](http://doc.akka.io/docs/akka/2.4/scala.html) to do as much as possible asynchronously, with back pressure  support around the HTTP connection towards the triple store. There are no blocking calls crossing process boundaries.    The older Spray HTTP client no longer supported, however it is still available as  release [v0.1.3](https://github.com/agnos-ai/reactive-sparql/tree/v0.1.3).    The akka-streams APIs currently supports 3 flavours of flows:    * [Flavour #1](#flavour-1-run-sparql): Execute SPARQL  * [Flavour #2](#flavour-2-construct-models): Construct Models  * [Flavour #3](#flavour-3-manipulate-graphs): Manipulate Graphs    ### Flavour #1: Run SPARQL    Use the `SparqlQuery(stmt: String)` or `SparqlUpdate(stmt: String)` case class and embed it in a `SparqlRequest()` to be passed to the flow. On the other end a  `SparqlResponse()` pops out. Support for custom mappings is available, where the resulting values get marshaled to a custom domain object.  This is however not mandatory, there is a default result mapper available that will return a [standard  result set model](src/main/scala/ai/agnos/sparql/api/SparqlResult.scala#L25) based on the `application/sparql-results+json` content type.    It is possible to use a single wrapper flow of [`Flow[SparqlRequest, SparqlResponse, _]`](src/main/scala/ai/agnos/sparql/stream/client/SparqlRequestFlowBuilder.scala)  to run both `SparqlUpdate()` and `SparqlQuery()` statements. There is an option to use specialised [query](src/main/scala/ai/agnos/sparql/stream/client/SparqlQueryFlowBuilder.scala)  and [update](src/main/scala/ai/agnos/sparql/stream/client/SparqlUpdateFlowBuilder.scala) flows as well.    The underlying implementation communicates with the triple store via the HTTP endpoints, as documented here  for [queries](https://www.w3.org/TR/2013/REC-sparql11-query-20130321/)  and [updates](https://www.w3.org/TR/2013/REC-sparql11-update-20130321/).    #### Example #1: Run a simple Sparql query    ```scala  /* Define domain case class and mappings */  object Person extends ResultMapper[Person] {    override def map(qs: QuerySolution): Person = {      Person(qs.uri(""g"").get, qs.string(""c"").get)    }  }  case class Person(id: URI, name: String) extends SparqlResult    /* Create a bespoke SparqlQuery with a mapping to a Person */  val mappingQuery2Get = SparqlQuery( """"""    |SELECT ?g ?b ?c    |FROM NAMED <urn:test:agnos:data>    |WHERE {    |  GRAPH ?g {    |   <urn:test:whatever> ?b ?c    |  }    |}"""""", mapping = Person, reasoningEnabled = true)    /* Create the Flow and Probes */  val sparqlRequestFlowUnderTest = SparqlRequestFlowBuilder.sparqlRequestFlow(testServerEndpoint)  val (source, sink) = TestSource.probe[SparqlRequest]    .via(sparqlRequestFlowUnderTest)    .toMat(TestSink.probe[SparqlResponse])(Keep.both)    .run()    /* Send the request to the stream and expect the result */  sink.request(1)  source.sendNext(SparqlRequest(mappingQuery2Get))  sink.expectNext(receiveTimeout) match {    case SparqlResponse(_, true, results, None) =>      val persons: Seq[Person] = results //the  mapped collection is returned      assert(persons.contains(...)    case r@_ =>      fail(r)  }  ```    ### Flavour #2: Construct Models    Working with Sparql query solutions (rows of result bindings as returned by a SELECT statement) is not always suitable. This is because the result  is not plain RDF.    Use of [SPARQL CONSTRUCT](https://www.w3.org/TR/sparql11-query/#construct)s is suitable in cases where we are only interested in triples (i.e. not  quads, where the graph IRI is missing)    At the moment there is no way to write the following statement, so that the resulting RDF is returned in ""quads"" format (N-QUADS or JSON-LD)  ```sparql  CONSTRUCT {    GRAPH ?g {      ?s ?p ?o .    }  } WHERE {  ...  }  ```    This flow has been created to circumvent the problem. It is an extension of the API used in [Flavour #1](#flavour-1-run-sparql).    Instead of a `SparqlQuery()` this flow works with a `SparqlConstruct()` inside the `SparqlRequest()`  ```scala  object SparqlConstruct {    def apply(resourceIRIs: Seq[URI] = Nil,              propertyIRIs: Seq[URI] = Nil,              graphIRIs: Seq[URI] = Nil,              reasoningEnabled: Boolean = false)(      implicit _paging: PagingParams = NoPaging    ): SparqlConstruct = {      ...    }  }  ```  By specifying a set of matching resource, property and/or graph IRIs, we limit the number of results that are returned.  Internally this flow will generate a reified SELECT statement that allows us to capture all 4 properties of the RDF Model, including the graph IRI.    The flow responds with a `SparqlModelResult(model: Model)` within a `SparqlResult()` which contains the RDF4J Model (Graph) instance.  ```scala  case class SparqlModelResult(model: Model) extends SparqlResult  ```    Refer to [`Flow[SparqlRequest, SparqlResponse, _]`](src/main/scala/ai/agnos/sparql/stream/client/SparqlConstructToModelFlowBuilder.scala)  for more detail.    ### Flavour #3: Manipulate Graphs    This flow allows for basic graph manipulation, as defined by the [graph-store protocol](https://www.w3.org/TR/2013/REC-sparql11-http-rdf-update-20130321/).  Not all aspects of the protocol are supported, however it is possible to:    #### Retrieve Graphs    Retrieve an entire graph using `GetGraph(graphUri: Option[URI])` wrapped in a `GraphStoreRequest()`    If no graphIri is specified the query returns the contents of the DEFAULT graph.    #### Drop Graphs    Drop an entire graph using `DropGraph(graphUri: Option[URI])` wrapped in a `GraphStoreRequest()`    If no graphIri is specified the request drops the DEFAULT graph, so be careful with that if you don't use named graphs in your triple store.    #### Insert Models into Graphs    Insert the contents of an RDF Model into the specified graph. There are 3 variants:    * `InsertGraphFromModel(graphModel: Model, graphUri: Option[URI])`: inserts an in-memory RDF Model;  * `InsertGraphFromPath(filePath: Path, graphUri: Option[URI], format: RDFFormat)`: inserts the contents of the specified file in the given RDF format;  * `InsertGraphFromURL(url: URL, format: RDFFormat, graphUri: Option[URI])`: inserts the contents of the file behind the specified HTTP URL in the given RDF format.    All the operations above return a `GraphStoreResponse` which contains the success status of the operation and a optional model (for `GetGraph()` queries only)  ```scala  case class GraphStoreResponse  (    request: GraphStoreRequest,    success: Boolean,    statusCode: Int,    statusText: String,    model: Option[Model] = None  )  ```    There is a `mergeGraphs: Boolean` parameter for all insert messages, that allows us to control how the resulting graph will deal with  the newly inserted triples.    * `mergeGraphs = true` will perform a HTTP PUT operation, which merges the content of the graph being sent with the graph that    is already in the triple store;  * `mergeGraphs = false` is the DEFAULT option and will perform a HTTP POST operation, which replaces the content of the graph with    the one being sent over.    If no graph is specified, the insert will use the DEFAULT graph in the triple store.    Refer to [`Flow[GraphStoreRequest, GraphStoreResponse, _]`](src/main/scala/ai/agnos/sparql/stream/client/GraphStoreRequestFlowBuilder.scala)  for more detail. """
Semantic web;https://github.com/white-gecko/TriplePlace;"""TriplePlace  ===========    This is TriplePlace a light weight and flexible Triple Store for Android. It uses a indexing structure similar to the  one in [Hexastore](http://www.zora.uzh.ch/8938/2/hexastore.pdf). TriplePlace uses  [TokyoCabinet](http://fallabs.com/tokyocabinet/) as persistent storage system. I've also published a patched version of [TokyoCabinet](https://github.com/white-gecko/TokyoCabinet) and TokyoCabinet-Java-API resp. [TokyoCabinet-Android-API](https://github.com/white-gecko/TokyoCabinet-Android-API).    TriplePlace is free software: you can redistribute it and/or modify it under the terms of the GNU General Public  License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later  version.    TriplePlace is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied  warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.    You should have received a copy of the GNU General Public License along with TriplePlace.  If not, see <[http://www.gnu.org/licenses/](http://www.gnu.org/licenses/)>.    To be done/To be implemented  ----------    - Implement some kind of logging to implement atomicity and consistency (ACID) between the indices  - Implement edit and maybe delete operations (norman says delete would be to expencive, maybe we have to mark those  triples as deleted)  - Implement query on graph patherns  - Implement RDF/XML, N-Triples/Turtle/N3 and ... import/export """
Semantic web;https://github.com/srdc/ontmalizer;"""<!--  Copyright (C) 2013 SRDC Yazilim Arastirma ve Gelistirme ve Danismanlik Tic. Ltd. Sti.    Licensed under the Apache License, Version 2.0 (the ""License"");  you may not use this file except in compliance with the License.  You may obtain a copy of the License at       http://www.apache.org/licenses/LICENSE-2.0    Unless required by applicable law or agreed to in writing, software  distributed under the License is distributed on an ""AS IS"" BASIS,  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and  limitations under the License.  -->    Ontmalizer [![License Info](http://img.shields.io/badge/license-Apache%202.0-brightgreen.svg)](https://github.com/srdc/ontmalizer/blob/master/LICENSE.txt)  ===    Ontmalizer performs comprehensive transformations of XML Schemas (XSD) and XML data to RDF/OWL automatically. Through this tool, it is possible to create RDF/OWL representation of XML Schemas, and XML instances that comply with such XML Schemas.    The state of the art open source and/or free tools for RDFizing XSD and XML are not able to handle complex schemas and XML instances such as HL7 Clinical Document Architecture (CDA) R2. Only a few commercial tools such as TopBraid Composer are successfully able to do so. However, we do not want to use commercial tools in our SALUS Project: http://www.srdc.com.tr/projects/salus/. As a result, we implemented our own solution. We make use of Sun's XSOM library for processing XML Schemas, Apache Xerces for processing XML data and Apache Jena for managing RDF data.    Further information and technical details can be found in our blog post accessible at http://www.srdc.com.tr/projects/salus/blog/?p=189.    ## Installation    Apache Maven is required to build the Ontmalizer. Please visit http://maven.apache.org/ in order to install Maven on your system.    Under the root directory of the Ontmalizer project run the following:    	$ ontmalizer> mvn install    In order to make a clean install run the following:    	$ ontmalizer> mvn clean install    These will build the Ontmalizer and also run a number of test cases, which will transform some XML Schemas (e.g. HL7 CDA R2, SALUS Common Information Model) and corresponding XML instances to RDF/OWL.     ## Transforming XSD to RDF/OWL    XSD2OWLMapper is the main class to transform XML Schemas to RDF/OWL. The constructor of this class gets the root XSD file to be transformed. Configuration of the transformation operation is quite simple: the caller can set the prefixes for the object property and datatype property names to be created. Then, the call to the convertXSD2OWL() method performs the transformation.     XSD2OWLMapper is able to print the output ontology in one of these formats: RDF/XML, RDF/XML-ABBREV, N-TRIPLE and N3. An example transformation routine is provided below for the HL7 CDA R2 XML Schema:    ```java      // This part converts XML schema to OWL ontology.      XSD2OWLMapper mapping = new XSD2OWLMapper(new File(""src/test/resources/CDA/CDA.xsd""));      mapping.setObjectPropPrefix("""");      mapping.setDataTypePropPrefix("""");      mapping.convertXSD2OWL();        // This part prints the ontology to the specified file.      FileOutputStream ont;      try {          File f = new File(""src/test/resources/output/cda-ontology.n3"");          f.getParentFile().mkdirs();          ont = new FileOutputStream(f);          mapping.writeOntology(ont, ""N3"");          ont.close();      } catch (Exception e) {          e.printStackTrace();      }  ```    ## Transforming XML to RDF/OWL    XML2OWLMapper is the main class to transform XML data to RDF/OWL by creating instances of the necessary OWL classes, RDFS datatypes, OWL datatype and object properties. The constructor of this class gets the XML file to be transformed together with an instance of XSD2OWLMapper that is already initialized with the corresponding XML Schema of the XML data. No other configuration is necessary for the transformation operation; the prefixes for the object property and datatype property names to be created are gathered from the XSD2OWLMapper configuration. Then, the call to the convertXML2OWL() method performs the transformation.    Similar to XSD2OWLMapper, XML2OWLMapper is able to print the output ontology instance in one of these formats: RDF/XML, RDF/XML-ABBREV, N-TRIPLE and N3. An example transformation routine is provided below for a complete HL7 CDA R2 instance, which is compliant with the HL7/ASTM Continuity of Care Document (CCD) and IHE Patient Care Coordination (PCC) templates:    ```java      // This part converts XML schema to OWL ontology.      XSD2OWLMapper mapping = new XSD2OWLMapper(new File(""src/test/resources/CDA/CDA.xsd""));      mapping.setObjectPropPrefix("""");      mapping.setDataTypePropPrefix("""");      mapping.convertXSD2OWL();        // This part converts XML instance to RDF data model.      XML2OWLMapper generator = new XML2OWLMapper(          new File(""src/test/resources/CDA/SALUS-sample-full-CDA-instance.xml""), mapping);      generator.convertXML2OWL();            // This part prints the RDF data model to the specified file.      try{          File f = new File(""src/test/resources/output/salus-cda-instance.n3"");          f.getParentFile().mkdirs();          FileOutputStream fout = new FileOutputStream(f);          generator.writeModel(fout, ""N3"");          fout.close();        } catch (Exception e){          e.printStackTrace();      }  ```    Please refer to our blog post (http://www.srdc.com.tr/projects/salus/blog/?p=189) for further details. """
Semantic web;https://github.com/IKCAP/wings;"""[![Test](https://github.com/KnowledgeCaptureAndDiscovery/wings/actions/workflows/maven.yml/badge.svg)](https://github.com/KnowledgeCaptureAndDiscovery/wings/actions/workflows/maven.yml)    # Wings    ## Installation    ### Docker     You must install [Docker](https://www.docker.com/) and [docker-compose](https://docs.docker.com/compose/install/).    Deploy the container with the following command:    ```bash  $ docker-compose up -d  ```    Open the browser [http://localhost:8080/wings-portal](http://localhost:8080/wings-portal) to access the Wings portal.      Go to [README Docker](wings-docker/) for additional instructions on running the Docker image.      ### Maven    Please follow the instructions in [README Maven](docs/maven.md) to install the Wings project."""
Semantic web;https://github.com/oeg-upm/morph-kgc;"""<p align=""center"">  <img src=""https://github.com/oeg-upm/morph-kgc/blob/main/logo.png"" height=""100"" alt=""morph"">  </p>    [![License](https://img.shields.io/pypi/l/morph-kgc.svg)](https://github.com/oeg-upm/morph-kgc/blob/main/LICENSE)  [![DOI](https://zenodo.org/badge/311956260.svg?style=flat)](https://zenodo.org/badge/latestdoi/311956260)  [![Latest PyPI version](https://img.shields.io/pypi/v/morph-kgc?style=flat)](https://pypi.python.org/pypi/morph-kgc)  [![Python Version](https://img.shields.io/pypi/pyversions/morph-kgc.svg)](https://pypi.python.org/pypi/morph-kgc)  [![PyPI status](https://img.shields.io:/pypi/status/morph-kgc?)](https://pypi.python.org/pypi/morph-kgc)  [![build](https://github.com/oeg-upm/morph-kgc/actions/workflows/continuous-integration.yml/badge.svg)](https://github.com/oeg-upm/morph-kgc/actions/workflows/continuous-integration.yml)    Morph-KGC is an engine that constructs [RDF](https://www.w3.org/TR/rdf11-concepts/) knowledge graphs from heterogeneous data sources with [R2RML](https://www.w3.org/TR/r2rml/) and [RML](https://rml.io/specs/rml/) mapping languages. Morph-KGC is built on top of [pandas](https://pandas.pydata.org/) and it leverages *mapping partitions* to significantly reduce execution times and memory consumption for large data sources.    ## Main Features    - Supports [R2RML](https://www.w3.org/TR/r2rml/) and [RML](https://rml.io/specs/rml/) mapping languages.  - Input data formats:    - Relational databases: [MySQL](https://www.mysql.com/), [PostgreSQL](https://www.postgresql.org/), [Oracle](https://www.oracle.com/database/), [Microsoft SQL Server](https://www.microsoft.com/sql-server), [MariaDB](https://mariadb.org/), [SQLite](https://www.sqlite.org/index.html).    - Tabular files: [CSV](https://en.wikipedia.org/wiki/Comma-separated_values), [TSV](https://en.wikipedia.org/wiki/Tab-separated_values), [Excel](https://www.microsoft.com/en-us/microsoft-365/excel), [Parquet](https://parquet.apache.org/documentation/latest/), [Feather](https://arrow.apache.org/docs/python/feather.html), [ORC](https://orc.apache.org/), [Stata](https://www.stata.com/), [SAS](https://www.sas.com), [SPSS](https://www.ibm.com/analytics/spss-statistics-software), [ODS](https://en.wikipedia.org/wiki/OpenDocument).    - Hierarchical files: [JSON](https://www.json.org/json-en.html), [XML](https://www.w3.org/TR/xml/).  - Output RDF serializations: [N-Triples](https://www.w3.org/TR/n-triples/), [N-Quads](https://www.w3.org/TR/n-quads/).  - Runs on Linux, Windows and macOS systems.  - Compatible with Python 3.7 or higher.  - Optimized to materialize large knowledge graphs.  - Highly configurable.  - Available under the [Apache License 2.0](https://github.com/oeg-upm/Morph-KGC/blob/main/LICENSE).    ## Installation and Usage    [PyPi](https://pypi.org/project/morph-kgc/) is the fastest way to install Morph-KGC:  ```  pip install morph-kgc  ```    To run the engine you just need to execute the following:  ```  python3 -m morph_kgc config.ini  ```    [Here](https://github.com/oeg-upm/Morph-KGC/wiki/Configuration) you can see how to generate the configuration file. It is also possible to run Morph-KGC as a library with [RDFlib](https://rdflib.readthedocs.io/en/stable/):  ```python  import morph_kgc    # generate the triples and load them to an RDFlib graph  graph = morph_kgc.materialize('/path/to/config.ini')    # work with the graph  graph.query(' SELECT DISTINCT ?classes WHERE { ?s a ?classes } ')  ```    ## Documentation    Check the **[wiki](https://github.com/oeg-upm/Morph-KGC/wiki)** with all the information:    **[Getting Started](https://github.com/oeg-upm/Morph-KGC/wiki/Getting-Started)**    **[Usage](https://github.com/oeg-upm/Morph-KGC/wiki/Usage)**    **[Configuration](https://github.com/oeg-upm/Morph-KGC/wiki/Configuration)**  - **[Engine](https://github.com/oeg-upm/Morph-KGC/wiki/Engine-Configuration)**  - **[Data Sources](https://github.com/oeg-upm/Morph-KGC/wiki/Data-Source-Configuration)**    - [Relational Databases](https://github.com/oeg-upm/Morph-KGC/wiki/Relational-Databases)    - [Data Files](https://github.com/oeg-upm/Morph-KGC/wiki/Data-Files)    **[Features](https://github.com/oeg-upm/Morph-KGC/wiki/Features)**    **[Academic Publications](https://github.com/oeg-upm/Morph-KGC/wiki/Academic-Publications)**    **[License](https://github.com/oeg-upm/Morph-KGC/wiki/License)**    **[FAQ](https://github.com/oeg-upm/Morph-KGC/wiki/FAQ)**    ## Contact    - **Julián Arenas-Guerrero (julian.arenas.guerrero@upm.es)**    *Ontology Engineering Group, Universidad Politécnica de Madrid | 2020 - Present* """
Semantic web;https://github.com/VeritasOS/fedx;"""# Welcome to the FedX Repository    FedX is a practical framework for transparent access to Linked Data sources through a federation.   It incorporates new sophisticated optimization techniques combined with effective variants of existing  techniques and is thus a highly scalable solution for practical federated query processing.      # FedX has moved    Veritas [contributed](https://rdf4j.org/news/2019/10/15/fedx-joins-rdf4j/) FedX to the Eclipse [RDF4J project](https://rdf4j.org/). Development continues there, please see https://github.com/eclipse/rdf4j.   """
Semantic web;https://github.com/MM2-0/rdf4a;"""# RDF4A - Porting RDF4J to Android    This is a port of RDF4J to the Android platform.    ## What is RDF4J?    > Eclipse RDF4J (formerly known as Sesame) is a powerful Java framework for processing and handling RDF data. This includes creating, parsing, scalable storage, reasoning and querying with RDF and Linked Data. It offers an easy-to-use API that can be connected to all leading RDF database solutions.    [RDF4J homepage](http://rdf4j.org/)    ## Setup    Install to local Maven repository  ```  mvn package -DskipTests && mvn install -DskipTests  ```    Include in an Android project  ```  //build.gradle  repositories {      mavenLocal()  }  dependencies {      compile 'de.mm20.rdf4a:rdf4j-model:1.1-SNAPSHOT'      [...]  }  ``` """
Semantic web;https://github.com/UMKC-BigDataLab/RIQ;"""# `RIQ`: RDF Indexing on Quadruples    ## Summary    `RIQ` is a new approach for fast processing of SPARQL queries on large  datasets containing RDF quadruples (or quads).   (These queries are also called named graph queries.)  `RIQ` employs a *decrease-and-conquer*  strategy: Rather than indexing the entire RDF dataset, `RIQ` identifies  groups of similar RDF graphs and indexes each group separately. During  query processing, `RIQ` uses novel filtering index to first identify  candidate groups that may contain matches for the query. On these  candidates, it executes optimized queries using a conventional SPARQL  processor (e.g., Jena TDB) to produce the final results.    ## Publications    * Anas Katib, Praveen Rao, Vasil Slavov. ``[A Tool for Efficiently Processing SPARQL Queries on RDF Quads](http://ceur-ws.org/Vol-1963/paper472.pdf)."" In the 16th International Semantic Web Conference (ISWC 2017), 4 pages, Austria, Vienna, October 2017. (demo)    * Anas Katib, Vasil Slavov, Praveen Rao. ``[RIQ: Fast Processing of SPARQL Queries on RDF Quadruples](http://dx.doi.org/10.1016/j.websem.2016.03.005)."" In the Journal of Web Semantics (JWS), Vol. 37, pages 90-111, March 2016. (Elsevier)     * Vasil Slavov, Anas Katib, Praveen Rao, Vinutha Nuchimaniyanda. ""Fast Processing of SPARQL Queries on RDF Quadruples."" The 8th IEEE Symposium on Computational Intelligence for Security and Defense Applications (CISDA 2015), Verona, NY, May 2015. (poster)    * Vasil Slavov, Anas Katib, Praveen Rao, Srivenu Paturi, Dinesh Barenkala. ``[Fast Processing of SPARQL Queries on RDF Quadruples](http://arxiv.org/pdf/1506.01333v1.pdf)."" [*Proceedings of the 17th International Workshop on the Web and Databases*](http://webdb2014.eecs.umich.edu/) (**WebDB 2014**), Snowbird, UT, June 2014.    ## Contributors    ***Faculty:*** Praveen Rao (PI)    ***PhD Students:*** Vasil Slavov, Anas Katib    ***MS Students:*** Srivenu Paturi, Dinesh Barenkala, Vinutha Nuchimaniyanda    ## Acknowledgments    This work was supported by the National Science Foundation under Grant Nos. 1115871 and 1620023. """
Semantic web;https://github.com/phillord/tawny-owl;"""Tawny-OWL  ===========    [![Build Status](https://travis-ci.org/phillord/tawny-owl.png?branch=master)](https://travis-ci.org/phillord/tawny-owl)    ## Introduction    <img src=""docs/tawny-cartoon-only-owl.png""   alt=""Tawny Logo"" title=""Tawny OWL Logo"" height=""300"" align=""right"" />    Tawny-OWL allows construction of OWL ontologies, in a evaluative, functional  and fully programmatic environment. Think of it as the ontology engineering  equivalent of [R](http://www.r-project.org/). It has many advantages over  traditional ontology engineering tools, also described in a  [video introduction](https://vimeo.com/89782389).    - An interactive shell or REPL to explore and create ontologies.  - Source code, with comments, editable using any of a range of IDEs.  - Fully extensible -- new syntaxes, new data sources can be added by users  - Patterns can be created for individual ontologies; related classes can be    built easily, accurately and maintainably.  - A unit test framework with fully reasoning.  - A clean syntax for versioning with any VCS, integrated with the IDE  - Support for packaging, dependency resolution and publication  - Enabled continuous integration with both ontology and software dependencies    Tawny-OWL is implemented as a domain-specific language but built over a full  programming language called [Clojure](http://www.clojure.org). Many of the  features described (REPL, patterns, unit tests, extensibility) derive directly  from the Clojure language, or from general-purpose programming tools (IDEs,  versioning, continuous integration). The core ontology features are  implemented directly using the [OWL API](http://owlapi.sourceforge.net/).  These features are, therefore, industry strength, standards-compliant and  well-supported independently of the Tawny-OWL developers.    OWL is a W3C standard ontology representation language; an ontology is a fully  computable set of statements, describing the things and their relationships.  They are used, mostly notable in biomedicine, to describe complex areas of  knowledge such as [genetics](http://www.geneontology.org/) or  [clinical terminology](http://en.wikipedia.org/wiki/SNOMED_CT), but can  describe anything, including [e-commerce](http://purl.org/goodrelations/). For  more tutorial information, please see http://ontogenesis.knowledgeblog.org.    A [full-length manual](https://phillord.github.io/take-wing/) is also  [available](http://github.com/phillord/take-wing). The original,  slightly older [getting started](docs/getting-started.md) document is  available.    ## For the Clojure developer    Tawny-OWL is predominately designed as a programmatic application for ontology  development, but it can be used as an API. OWL ontologies are a set of  statements about things and their relationships; underneath these statements  map to a subset of first-order logic which makes it possible to answer   questions about these statements using highly-optimised reasoners.    Currently, the use of ontologies as a tool within general-purpose programming  is relatively under-developed. Part of the intention behind Tawny-OWL is to  embed ontologies deeply within a programmatic framework, to see whether  ontologies are useful in this way.    Further information on the use of Tawny-OWL is available in the  [documentation](docs/tawny-as-an-api.md).      ## Motivation    I discuss the development of this code base in my  [journal](http://www.russet.org.uk/blog). Two posts include one on the  [motivation](http://www.russet.org.uk/blog/2214) and another on making  the library more [""lispy""](http://www.russet.org.uk/blog/2254). All  revelevant posts are  [categorised](http://www.russet.org.uk/blog/category/all/professional/tech/tawny-owl).    ## Installation    Tawny-OWL requires no installation *per se* and is used as any Clojure  library. It is available from  [clojars](https://clojars.org/uk.org.russet/tawny-owl), so just add:    `[uk.org.russet/tawny-owl]` to your `project.clj` file.    I use Leiningen 2 on the current version 16.04 Ubuntu and, occasionally, on  Windows. Editing of both tawny-owl and the ontologies using it, is with Emacs  25 using Clojure mode and nrepl, currently both installed from their  respective versioning systems. The library should not depend on this  environment, however.    ## Author    Phillip Lord, Newcastle University.    http://www.russet.org.uk/blog    ## Mailing List    There is a [mailing list](mailto:tawny-owl@googlegroups.com).    ## Version    [![Clojars Project](http://clojars.org/uk.org.russet/tawny-owl/latest-version.svg)](http://clojars.org/uk.org.russet/tawny-owl)        ## License    The contents of this file are subject to the LGPL License, Version 3.0.    Copyright (C) 2012, 2013, Newcastle University    This program is free software: you can redistribute it and/or modify it under  the terms of the GNU Lesser General Public License as published by the Free  Software Foundation, either version 3 of the License, or (at your option) any  later version.    This program is distributed in the hope that it will be useful, but WITHOUT  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS  FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.    You should have received a copy of the GNU Lesser General Public License along  with this program. If not, see http://www.gnu.org/licenses/. """
Semantic web;https://github.com/sebferre/sparklis;"""<meta charset=""UTF-8""/>    # What is Sparklis?    Sparklis is a query builder in natural language that allows people to explore and query SPARQL endpoints with all the power of SPARQL and without any knowledge of SPARQL, nor of the endpoint vocabulary.    Sparklis is a Web client running entirely in the browser. It directly connects to SPARQL endpoints to retrieve query results and suggested query elements. It covers a large subset of SPARQL 1.1 `SELECT` queries: basic graph patterns including cycles, `UNION`, `OPTIONAL`, `NOT EXISTS`, `FILTER`, `BIND`, complex expressions, aggregations, `GROUP BY`, `ORDER BY`. All those features can be combined in a flexible way, like in SPARQL. Results are presented as tables, and also on maps. A  configuration panel offers a few configuration options to adapt to different endpoints (e.g., GET/POST, labelling properties and language tags). Sparklis also includes the YASGUI editor to let advanced users access and modify the SPARQL translation of the query.    Sparklis reconciles expressivity and usability in semantic search by tightly combining a Query Builder, a Natural Language Interface, and a Faceted Search system. As a *Query Builder* it lets users build complex queries by composing elementary queries in an incremental fashion. An elementary query can be a class (e.g., ""a film""), a property (e.g., ""that has a director""), a RDF node (e.g., ""Tim Burton""), a reference to another node (e.g., ""the film""), or an operator (e.g., ""not"", ""or"", ""highest-to-lowest"", ""+"", ""average"").    As a *Faceted Search system*, at every step, the query under construction is well-formed, query results are computed and displayed, and the suggested query elements are derived from actual data - not only from schema - so as to prevent the construction of non-sensical or empty results. The display of results and data-relevant suggestions at every step provides constant and acurate feedback to users during the construction process. This supports exploratory search, serendipity, and confidence about final results.    As a *Natural Language Interface*, everything presented to users - queries, suggested query elements, and results - are verbalized in natural language, completely hidding SPARQL behind the user interface. Compared to Query Answering (QA) systems, the hard problem of spontaneous NL understanding is avoided by controlling query formulation through guided query construction, and replaced by the simpler problem of NL generation. The user interface lends itself to multilinguality, and is so far available in English, French, Spanish, and Dutch.    When refering to Sparklis in scientific documents, please use the following citation.    > Sébastien Ferré: *Sparklis: An expressive query builder for SPARQL endpoints with guidance in natural language.* Semantic Web 8(3): 405-418 (2017)    # Where can I try Sparklis?    Simply follow those steps:  1. Go to the [online application](http://www.irisa.fr/LIS/ferre/sparklis/)  2. Select a SPARQL endpoint in the dropdown list at the top (the default one is *Core English DBpedia*, a core subset of DBpedia)  3. Build your query incrementally by clicking suggested query elements (in the three lists of suggestions, and through the hamburger menu in the query), and clicking different parts of the query to change focus. The suggestions are relative to the current focus.    We recommend to visit the [*Examples page*](http://www.irisa.fr/LIS/ferre/sparklis/examples.html) where there are 100+ example queries over several datasets. Every example query can be opened in Sparklis in one click, and the query can then be further modified. For a number of them, there is a YouTube screencast to show how they are built step by step.    # How can I use Sparklis on my own RDF dataset?    It is enough to have a SPARQL endpoint for your dataset that is visible from your machine. It can be a publicly open endpoint (like for DBpedia or Wikidata), or a localhost endpoint (I personally use Apache Jena Fuseki but other stores should work too). The one important condition is that the endpoint server be [CORS-enabled](https://www.w3.org/wiki/CORS_Enabled) so that HTTP requests can be made to it from your browser, where Sparklis runs.    Here a few recommendations about the contents of your store for best results:  * include RDFS/OWL triples declaring classes (`rdfs:Class`, `owl:Class`) and properties (`rdf:Property`, `owl:ObjectProperty`, `owl:DataProperty`), as well as their labels (`rdfs:label`) and their hierarchy (`rdfs:subClassOf`, `rdfs:subPropertyOf`)  * ensure that all URI-resources have their label defined, preferably with `rdfs:label` and possibly with other standard properties (e.g., `skos:prefLabel`)  * if named graphs are used, make sure to configure your store so that the default graphs contains the union of those named graphs    The *Configure* menu offers a number of options to adapt Sparklis to your endpoint, and control the display. Here is a non-exhaustive list:  * Endpoint and queries: max numbers of results/suggestions, GET vs POST, credentials  * Ontology: display of class/property hierarchies, filtering of classes/properties, use of Sparklis-specific schema properties (see below)  * Language and labels: interface language, labelling properties, fulltext search support    Sparklis makes use of standard and non-standard properties to get more control on the building of queries, and on the display of suggestions and results. For ech property, there is generally a configuration option to activate its use.  * `sdo:position` (`sdo: = https://schema.org/`): it is possible to control the ordering of suggestions (classes, properties, and individuals) by setting this property with the desired rank of the suggestion. The related option is in *Configure advanced features*.  * `sdo:logo`: it is possible to have small icons in front of entity labels by setting this property with URLs to those icons. Several icons can be attached to a same entity. The related option is in *Configure advanced features*, where the size of icons can be defined.  * `rdfs:inheritsThrough`: suppose you have a `ex:location` property whose range `ex:Place` is organized into a hierarchy through the property `ex:isPartOf`. By adding to your dataset the triple `ex:location rdfs:inheritsThrough ex:isPartOf`, you get that whenever property `ex:location` is inserted into the query, inheritance through the place hierarchy is performed, and place suggestions are displayed as a tree. This is a generalization of the well-known `rdf:type` inheritance through `rdfs:subClassOf`. By adding triple `ex:Place rdfs:inheritsThrough ex:isPartOf`, the same effect is obtained when inserting class `ex:Place`. The related option in *Configure the ontology* must be activated.  * `lis-owl:transitiveOf` (`lis-owl: = http://www.irisa.fr/LIS/ferre/vocab/owl#`): the use of `rdfs:inheritsThrough` entails the insertion of transitive paths (e.g., `ex:isPartOf*`) in the SPARQL query, which are very costly to evaluate. One solution is to materialize the transitive closure as a new property `ex:isPartOf_star` in the dataset, and to add the triple `ex:isPartOf_star lis-owl:transitiveOf ex:isPartOf`. By activating the related option in *Configure the ontology*, property path `ex:isPartOf*` will be replaced by `ex:isPartOf_star`.  * `nary:subjectObject` (`nary: = http://www.irisa.fr/LIS/ferre/vocab/nary#`): this property handles the case of a reified relationship where there is a property `PS` from reification node to subject, and a property `PO` from reification node to object. By adding triple `PS nary:subjectObject PO`, the reification becomes transparent in Sparkls. See cyan properties on the Mondial endpoint for examples. The related option in *Configure the ontology* must be activated.  * `nary:eventObject`: this is similar as above, except there is a property `PE` from subject to reification node (instead of the inverse `PS`). See cyan properties in the Wikidata endpoint for examples.    Once you have found a good configuration of your endpoint, you can generate a *permalink* with the button at the top, which you can share to the endpoint users. Those permalinks also include the current query and current focus, so you can also share example queries and template queries. You can also save queries by simply adding bookmarks in your browser.    If you find your endpoint of general interest, you are welcome to suggest me to add it to the list of SPARQL endpoints.    # How do I reuse Sparklis in my web site?    As Sparklis is only client-side code, it is possible to integrate Sparklis into your website by simply copying the contents of the `webapp` folder among your website files, and adding links to it from your web pages, with URL arguments containing the endpoint, the configuration, and possibly an initial query. To get those URLs, simply navigate in Sparklis and copy (and adapt as needed) the browser URL.    You can adapt the appearance of the main HTML file (`osparklis.html`, `osparklis.css`) as long as you retain the *Sparklis* name, and the credits in the page footer. You can for instance hide some configuration options and elements, you can change the look-and-feel, and the layout of elements. Be careful not to delete element ids and classes that are used by the JS code of Sparklis.    Let me know of successful integrations, and also of problems you encounter in the process.    # Credits    Author: [Sébastien Ferré](http://people.irisa.fr/Sebastien.Ferre/)    Affiliation: Univ. Rennes 1, team [SemLIS](http://www-semlis.irisa.fr/) at IRISA    Copyright © 2013 Sébastien Ferré, IRISA, Université de Rennes 1, France    Licence: Apache Licence 2.0    Citation: *Ferré, Sébastien. ‘Sparklis: An Expressive Query Builder for SPARQL Endpoints with Guidance in Natural Language’. Semantic Web 8(3) : 405-418. IOS Press, 2017.* [PDF](https://hal.inria.fr/hal-01485093/file/sparklis-preprint.pdf) """
Semantic web;https://github.com/schlegel/balloon;"""![](http://schlegel.github.io/balloon/images/balloon_logo_big.png)  =======  A tool-suite for Linked Data consumption. **balloon** aims in offering public services and tools to take advantage of the semantic web with less effort.  The basic motivation is to establish a foundation for Linked Data as a Service (**LDaaS**).     **!!! The project is currently under development and will be available soon !!!**    More detailed information available online:  [http://schlegel.github.io/balloon](http://schlegel.github.io/balloon)    #Features  - **balloon Overflight**  	-	SPARQL based LOD crawling  	-	Index of basic structural relationships  	- Bird's-eye view on Linked Data  - **balloon Fusion**   	- SPARQL query federation service based on equivalence resources  	- Automatic endpoint discovery  - **balloon Synopsis** ([moved to a seperate github repository](https://github.com/schlegel/balloon-synopsis))  	- HTML/JavaScript RDF Viewer & Browser  	- jQuery Plugin   	-	Automatic information enhancement  	-	Configurable Templating  -	**balloon Commonalities**  	-	Finding common (relevant) super types of entities  	-	Finding instances for given type  	-	Finding related entities based on types and predicates    #Overview  Today’s vision of a common Web of Data is mostly achieved and coined by the Linked Open Data movement. The first wave of this movement transformed silo-based portions of data into a plethora of open accessible and interlinked data sets. The community itself provided guidelines (e.g., 5 ★ Open Data ) as well as open source tools to foster interactions with the Web of data. Harmonization between those data sets has been established at the modeling level with unified description schemes characterizing a formal syntax and common data semantic. Without doubt, Linked Open Data is the de-facto standard to publish and interlink distributed data sets in the Web commonly exposed in SPARQL endpoints. However, a convenient request considering the globally described data set is only possible with strong limitations. **balloon** wants to overcome this issue by providing a large choice of services to simplify utilizing the web of data for your projects. We think that Linked Data tools should be available as a service (**LDaaS**), to avoid high setup and integration costs and data duplication. Therefore all balloon services will be public available and don't need to be installed locally.    ---------------------------------------  This software has been brought to you by the [Chair of Distributed Information Systems (DIMIS)](dimis.fim.uni-passau.de) and [Media Computer Science (MiCS)](http://mics.fim.uni-passau.de/) at the University Passau. The project was developed within the CODE project funded by the EU Seventh Framework Programme, grant agreement number 296150. """
Semantic web;https://github.com/AtomGraph/CSV2RDF;"""# CSV2RDF  Streaming, transforming CSV to RDF converter    Reads CSV/TSV data as generic CSV/RDF, transforms each row using SPARQL `CONSTRUCT` or `DESCRIBE`, and streams the output triples.  The generic CSV/RDF format is based on the minimal mode of [Generating RDF from Tabular Data on the Web](https://www.w3.org/TR/2015/REC-csv2rdf-20151217/#dfn-minimal-mode).    Such transformation-based approach enables:  * building resource URIs on the fly  * fixing/remapping datatypes  * mapping different groups of values to different RDF structures    CSV2RDF differs from [tarql](https://tarql.github.io) in the way how mapping queries use graph patterns in the `WHERE` clause. tarql queries operate on a table of bindings  (provided as an implicit `VALUES` block) in which CSV column names become variable names. CSV2RDF generates an intermediary RDF graph for each CSV row (using column names as relative-URI properties)  that the `WHERE` patterns explicitly match against.    Build  -----        mvn clean install    That should produce an executable JAR file `target/csv2rdf-2.0.0-jar-with-dependencies.jar` in which dependency libraries will be included.    Usage  -----    The CSV data is read from `stdin`, the resulting RDF data is written to `stdout`.    CSV2RDF is available as a `.jar` as well as a Docker image [atomgraph/csv2rdf](https://hub.docker.com/r/atomgraph/csv2rdf) (recommended).    Parameters:  * `query-file` - a text file with SPARQL 1.1 [`CONSTRUCT`](https://www.w3.org/TR/sparql11-query/#construct) query string  * `base` - the base URI for the data (also becomes the `BASE` URI of the SPARQL query). Property namespace is constructed by adding `#` to the base URI.    Options:  * `-d`, `--delimiter` - value delimiter character, by default `,`.  * `--max-chars-per-column` - max characters per column value, by default 4096  * `--input-charset` - CSV input encoding, by default UTF-8  * `--output-charset` - RDF output encoding, by default UTF-8    _Note that delimiters might have a [special meaning](https://www.tldp.org/LDP/abs/html/special-chars.html) in shell._ Therefore, always enclose them in single quotes, e.g. `';'` when executing CSV2RDF from shell.    If you want to retrieve the raw CSV/RDF output, use the [identity transform](https://en.wikipedia.org/wiki/Identity_transform) query `CONSTRUCT WHERE { ?s ?p ?o }`.    Example  -------    CSV data in `parking-facilities.csv`:            postDistrict,roadCode,houseNumber,name,FID,long,lat,address,postcode,parkingSpace,owner,parkingType,information      1304 København K,24,5,Adelgade 5 p_hus.0,p_hus.0,12.58228733,55.68268042,Adelgade 5,1304,92,Privat,P-Kælder,""Adelgade 5-7, Q-park.""    `CONSTRUCT` query in `parking-facilities.rq`:    ```sparql  PREFIX schema:     <https://schema.org/>   PREFIX geo:        <http://www.w3.org/2003/01/geo/wgs84_pos#>   PREFIX xsd:        <http://www.w3.org/2001/XMLSchema#>   PREFIX rdf:        <http://www.w3.org/1999/02/22-rdf-syntax-ns#>    CONSTRUCT  {      ?parking a schema:ParkingFacility ;          geo:lat ?lat ;          geo:long ?long ;          schema:name ?name ;          schema:streetAddress ?address ;          schema:postalCode ?postcode ;          schema:maximumAttendeeCapacity ?spaces ;          schema:additionalProperty ?parkingType ;          schema:comment ?information ;          schema:identifier ?id .  }  WHERE  {      ?parkingRow <#FID> ?id ;          <#name> ?name ;          <#address> ?address ;          <#lat> ?lat_string ;          <#postcode> ?postcode ;          <#parkingSpace> ?spaces_string ;          <#parkingType> ?parkingType ;          <#information> ?information ;          <#long> ?long_string .         BIND(URI(CONCAT(STR(<>), ?id)) AS ?parking) # building URI from base URI and ID      BIND(xsd:integer(?spaces_string) AS ?spaces)      BIND(xsd:float(?lat_string) AS ?lat)      BIND(xsd:float(?long_string) AS ?long)  }  ```  Java execution from shell:        cat parking-facilities.csv | java -jar csv2rdf-2.0.0-jar-with-dependencies.jar parking-facilities.rq https://localhost/ > parking-facilities.ttl    Alternatively, Docker execution from shell:        cat parking-facilities.csv | docker run -i -a stdin -a stdout -a stderr -v ""$(pwd)/parking-facilities.rq"":/tmp/parking-facilities.rq atomgraph/csv2rdf /tmp/parking-facilities.rq https://localhost/ > parking-facilities.ttl    Note that using Docker you need to:  * [bind](https://docs.docker.com/engine/reference/commandline/run/#attach-to-stdinstdoutstderr--a) `stdin`/`stdout`/`stderr` streams  * [mount](https://docs.docker.com/storage/volumes/) the query file to the container, and use the filepath from _within the container_ as `query-file`    Output in `parking-facilities.ttl`:        <https://localhost/p_hus.0> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <https://schema.org/ParkingFacility> .      <https://localhost/p_hus.0> <http://www.w3.org/2003/01/geo/wgs84_pos#long> ""12.58228733""^^<http://www.w3.org/2001/XMLSchema#float> .      <https://localhost/p_hus.0> <https://schema.org/identifier> ""p_hus.0"" .      <https://localhost/p_hus.0> <https://schema.org/additionalProperty> ""P-Kælder"" .      <https://localhost/p_hus.0> <https://schema.org/comment> ""Adelgade 5-7, Q-park."" .      <https://localhost/p_hus.0> <https://schema.org/postalCode> ""1304"" .      <https://localhost/p_hus.0> <http://www.w3.org/2003/01/geo/wgs84_pos#lat> ""55.68268042""^^<http://www.w3.org/2001/XMLSchema#float> .      <https://localhost/p_hus.0> <https://schema.org/streetAddress> ""Adelgade 5"" .      <https://localhost/p_hus.0> <https://schema.org/name> ""Adelgade 5 p_hus.0"" .      <https://localhost/p_hus.0> <https://schema.org/maximumAttendeeCapacity> ""92""^^<http://www.w3.org/2001/XMLSchema#integer> .    Query examples  --------------    More mapping query examples can be found under [LinkedDataHub](https://github.com/AtomGraph/LinkedDataHub)'s [`city-graph`](https://github.com/AtomGraph/LinkedDataHub-Apps/tree/master/demo/city-graph/queries) demo app.    Performance  -----------    Largest dataset tested so far: 2.8 GB / 3709725 rows of CSV to 21.7 GB / 151348939 triples in under 27 minutes. Hardware: x64 Windows 10 PC with Intel Core i5-7200U 2.5 GHz CPU and 16 GB RAM.    Dependencies  ------------    * [Apache Jena](https://jena.apache.org/)  * [uniVocity-parsers](https://www.univocity.com/pages/univocity_parsers_tutorial)  * [picocli](https://picocli.info)"""
Semantic web;https://github.com/labra/shaclex;"""# SHaclEX    Scala implementation of SHEX and SHACL.    This project contains an implementation of  [SHACL](http://w3c.github.io/data-shapes/shacl/) and  [ShEx](http://www.shex.io)    [![Build Status](https://travis-ci.org/weso/shaclex.svg?branch=master)](https://travis-ci.org/weso/shaclex)  [![Codacy Badge](https://api.codacy.com/project/badge/Grade/1c264d2087734a80b4cecf071bb5eaad)](https://www.codacy.com/gh/weso/shaclex?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=weso/shaclex&amp;utm_campaign=Badge_Grade)  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1214239.svg)](https://doi.org/10.5281/zenodo.1214239)  [![Maven Central](https://maven-badges.herokuapp.com/maven-central/es.weso/shaclex_2.13/badge.svg)](https://maven-badges.herokuapp.com/maven-central/es.weso/shaclex_2.13)    ## Introduction    This project contains an implementation of [SHACL](https://www.w3.org/TR/shacl/) and [ShEx](http://shex.io/).    Both are implemented in Scala using the same underlying mechanism using a purely functional approach.    The library handles RDF using a  [simple RDF library](https://github.com/weso/srdf)  which has 2 implementations,  one using [Apache Jena](https://jena.apache.org/)  and another one using [RDF4j](http://rdf4j.org/),  this means that it is possible to use this library to validate RDF models from any of those RDF libraries,   as well as from external SPARQL endpoints.    ## Installation and compilation    The project uses [sbt](http://www.scala-sbt.org/) for compilation as well as Java 1.8.    * `sbt test` compiles and runs the tests    ## Command line usage    Once compiled, the program can be run as a command line tool.  It is possible to run the program inside `sbt` as:    ### Validating RDF data with SHACL    Example:    ```sh  sbt ""run --data examples/shacl/good1.ttl \           --engine shaclex \           --showValidationReport""  ```    It is also possible to use [Jena SHACL](https://jena.apache.org/documentation/shacl/) using:      ```sh  sbt ""run --data examples/shacl/good1.ttl \           --engine JenaSHACL \           --showValidationReport""  ```    or [Top Braid SHACL API] using:    ```sh  sbt ""run --data examples/shacl/good1.ttl \           --engine shacl-tq \           --showValidationReport""  ```      ### Validating RDF with ShEx     Example:    ```sh  sbt ""run --engine=ShEx            --schema examples/shex/good1.shex            --schemaFormat ShExC            --data examples/shex/good1.ttl""  ```    ### Validating RDF data through an SPARQL endpoint    The following example validates RDF nodes from wikidata using [Gene-wiki ShEx](https://github.com/SuLab/Genewiki-ShEx):    ```sh  sbt ""run --endpoint=https://query.wikidata.org/sparql            --schemaUrl=https://raw.githubusercontent.com/SuLab/Genewiki-ShEx/master/diseases/wikidata-disease-ontology.shex            --shapeMap=examples/shex/wikidata/disease1.shapeMap            --schemaFormat=ShExC            --engine=ShEx            --trigger=ShapeMap            --showResult            --resultFormat=JSON""  ```          ### Interactive mode with `sbt`     It is usually faster to run the `sbt` command, which opens the interactive `sbt` shell and inside that shell, execute   the different commands.     ```sh  $ sbt  ... several information about loading libraries  sbt> run -d examples/shacl/good1.ttl --engine ShaClex    ```    ### Binary mode    The fastest way to run Shaclex is to compile the code and generate a comand line binary file.     The following command will generate a binary file:    ```sh  $ sbt universal:packageBin  ...generates the file...  target/universal/shaclex-N.N.N.zip  ```    which contains the compressed binary code.    ## Programmatic usage    The Shaclex library can be invoked programmatically.    * [Simple project validating ShEx](https://github.com/weso/simpleShExScala)  * Simple example validating SHACL    ## Implementation details    * The engine is based on Monads using the [cats library](http://typelevel.org/cats/)  * The ShEx compact syntax parser    is implemented using the following [Antlr grammar](https://github.com/shexSpec/grammar/blob/master/ShExDoc.g4) (previous versions used Scala Parser Combinators)    which is based on this [grammar](https://github.com/shexSpec/shex.js/blob/master/doc/bnf)  * JSON encoding and decoding uses the Json structure [defined here](https://shexspec.github.io/spec/) and is implemented using [Circe](https://github.com/travisbrown/circe)    ## Compatibility tests    The current implementation passes all [shacl-core tests](https://w3c.github.io/data-shapes/data-shapes-test-suite/).    In order to generate the EARL report, run:      ```  $ sbt   [...]  sbt:shaclex> project shacl   sbt:shacl> testOnly es.weso.shacl.report.ReportGeneratorCompatTest  ```     We also aim to pass the [ShEx test-suite](https://github.com/shexSpec/shexTest).    In order to run the shex test-suite and generate the EARL report, you can do the following:    ```  sbt  ...  sbt:shaclex> project shex  sbt:shex> compat:test  ```    ## Convert between Schema formats    Shaclex can be used to convert between different schema formats.   The following example shows how to convert between ShExC to ShExJ:    ```  $ sbt ""run --schema examples/shex/good1.shex              --schemaFormat ShExC             --outSchemaFormat ShExJ             --showSchema""  ```    ## Convert between ShEx and SHACL    Shaclex can be used to convert schemas from ShEx to SHACL and viceversa.    The following example shows how to convert a SHACL schema to ShEx.     ```  $ sbt ""run --schema examples/shacl/good1.ttl              --schemaFormat Turtle              --outSchemaFormat ShExC              --engine SHACLEX              --outEngine SHEX              --showSchema              --no-validate""  ```    The conversion code is work in progress. This [issue tracks ShEx->SHACL conversion](https://github.com/weso/shaclex/issues/114) and this one tracks [SHACL->ShEx    conversion](https://github.com/weso/shaclex/issues/113).       ## Clingo validation    The project supports experimental Answer Set Programming based validation by converting the validation process to a [Clingo program])(https://potassco.org/clingo/). To run this, use the option `--showClingo` which will generate a Cling program. Example:    ```sh  sbt ""run --engine=ShEx            --schema examples/shex/good1.shex            --schemaFormat ShExC            --data examples/shex/good1.ttl           --showClingo           --clingoFile clingoProgram.pl""   ```    Once you generate the Clingo program and have installed Clingo itself, you can run the program with:    ```sh  clingo clingoProgram.pl  ```    This feature is experimental. This [issue tracks the Clingo conversion](https://github.com/weso/shaclex/issues/316).    ## More information    * The aim of Shaclex is to support both ShEx and SHACL and to provide conversions between both languages.     More information about both languages can be read in the [Validating RDF data](http://book.validatingrdf.com) written by the authors.  * An online demo based on this library is available at [http://rdfshape.weso.es](http://rdfshape.weso.es).  * Another online demo based on this library customized for Wikidata is available at [http://wikidata.weso.es](http://wikidata.weso.es).  * This project was based on [ShExcala](http://labra.github.io/ShExcala/) which was focused on Shape Expressions only.    ## Publishing to OSS-Sonatype    This project uses [the sbt ci release](https://github.com/olafurpg/sbt-ci-release) plugin for publishing to [OSS Sonatype](https://oss.sonatype.org/).    ##### SNAPSHOT Releases  Open a PR and merge it to watch the CI release a -SNAPSHOT version    ##### Full Library Releases  1. Push a tag and watch the CI do a regular release  2. `git tag -a v0.1.0 -m ""v0.1.0""`  3. `git push origin v0.1.0`  _Note that the tag version MUST start with v._    ## Author & contributors    * Author: [Jose Emilio Labra Gayo](http://labra.weso.es)    Contributors:    * [Eric Prud'hommeaux](https://www.w3.org/People/Eric/)  * [Bogdan Roman](https://github.com/bogdanromanx)  * [Toni Cebrían](http://www.tonicebrian.com/)  * [Andrew Berezovskyi](https://github.com/berezovskyi)    ## Adopters    * [RDFShape](http://rdfshape.weso.es): An online demo powered by this library.  * [Wikishape](http://wikishape.weso.es): An online demo powered by this library for Wikidata.  * [Eclipse lyo](http://www.eclipse.org/lyo/): An SDK and a modelling environment to design and develop linked data applications based on the [OSLC standards](http://open-services.net/). The validation library is [lyo-validation](https://github.com/eclipse/lyo-validation).    ## Contribution    Contributions are greatly appreciated.  Please fork this repository and open a  pull request to add more features or [submit issues](https://github.com/labra/shaclex/issues)      <a href=""https://github.com/weso/shaclex/graphs/contributors"">    <img src=""https://contributors-img.web.app/image?repo=weso/shaclex"" />  </a>    ```Made with [contributors-img](https://contributors-img.web.app). """
Semantic web;https://github.com/architolk/Linked-Data-Theatre;"""# Linked Data Theatre  The Linked Data Theatre (LDT) is a platform for an optimal presentation of Linked Data.    ### Installation and usage  The easiest way to use the LDT is to install the latest release in a Tomcat container, by following the instructions in [DEPLOY.md](docs/DEPLOY.md):    - [ldt-1.25.2.war](https://github.com/architolk/Linked-Data-Theatre/releases/download/v1.25.2/ldt-1.25.2.war ""ldt-1.25.2.war"")    **NB: Users that upgrade from version 1.17.0 or lower: from version 1.18.0 the LDT can work with any triplestore via the RDF4J interface. You might need to change your configuration a bit. Please look at [DEPLOY.md](docs/DEPLOY.md), sections 4.3 and 4.4.2 for further instructions.**    **NB: Users that upgrade from version 1.9.0 or lower: the config.xml has changed with release 1.10.0. Please make sure that a `<date/>` entry exists after the upgrade!**    The Linked Data Theatre uses a configuration graph containing all the triples that make up the LDT configuration. Instructions and examples how to create such a configuration can be found at the [wiki](https://github.com/architolk/Linked-Data-Theatre/wiki). A [basic-configuration](basic-configuration.ttl) is provided to get you started.    The wiki contains a [tutorial](https://github.com/architolk/Linked-Data-Theatre/wiki/Tutorial) to guide you through the most common features of the Theatre.    ### Build it yourself, linux and docker installations  See [BUILD.md](docs/BUILD.md) for instructions to build the Linked Data Theatre yourself. To deploy the Linked Data Theatre in a Tomcat container, follow the instructions in [DEPLOY.md](docs/DEPLOY.md). A step-by-step installation guide for Linux is also available: [LINUX_SETUP.md](docs/LINUX_SETUP.md). You can also opt for a docker installation, see [DOCKER.md](docs/DOCKER.md).    ### Advanced installation - production settings  * The default setting of the LDT is for development purposes. Read [PRODUCTION.md](docs/PRODUCTION.md) for information about securing the LDT for a production environment.    * To create linked data, the LDT can be extended with the [Linked Data Studio](https://github.com/architolk/Linked-Data-Studio) (LDS). If you install a version of the LDS, it includes a version of the LDT.    * If you want to create a new release of the LDT, please look into [BUILD-LICENSE.md](docs/BUILD-LICENSE.md) for instructions to create the approriate license headers. See [RELEASE.md](docs/RELEASE.md) for all steps to make a release, including upload to github.    * To add security to the Linked Data Theatre, follow the instructions in [SECURE.md](docs/SECURE.md).    * If you run the Linked Data Theatre behind a corporate firewall and access to the internet is restricted by a proxy, follow the instructions in [PROXY.md](docs/PROXY.md).    * If you want to access a secure endpoint (https), but the certificate is untrusted, you will have to set up a keystore. Follow the instructions in [KEYSTORE.md](docs/KEYSTORE.md). """
Semantic web;https://github.com/avicomp/ont-api;"""# ONT-API (ver. 1.4.2)    ## Notice  **The activity in this repository is frozen.  The v1.4.2 is the last release under the domain 'ru.avicomp'.   The new project's home is https://github.com/owlcs/ont-api**    ## Summary  ONT-API is an implementation of OWL-API over Apache Jena.    For more info see [wiki](https://github.com/avicomp/ont-api/wiki) page.     ## Dependencies  - **[Apache Jena](https://github.com/apache/jena)**, version **3.12.0**  - **[OWL-API](https://github.com/owlcs/owlapi)**, version **5.1.11**    ## License  * Apache License Version 2.0  * GNU LGPL Version 3.0   """
Semantic web;https://github.com/dkmfbk/rdfpro;"""RDFpro: an extensible tool for building stream-oriented RDF processing pipelines  ================================================================================    RDFpro (RDF Processor) is a public domain (Creative Commons CC0) Java command line tool and embeddable library that offers a suite of stream-oriented, highly optimized processors for common tasks such as data filtering, RDFS inference, smushing and statistics extraction.  RDFpro processors are extensible by users and can be freely composed to form complex pipelines to efficiently process RDF data in one or more passes.  RDFpro model and multi-threaded design allow processing billions of triples in few hours in typical Linked Open Data integration scenarios.    [RDFpro Web site](http://rdfpro.fbk.eu/)    Building RDFpro  ---------------  To build RDFpro, you need to have [`Maven`](https://maven.apache.org/) installed on your machine.   In order to build RDFpro, you can run the following commands:        $ git clone https://github.com/dkmfbk/rdfpro.git  (1)      $ cd rdfpro                                       (2)      $ git checkout BRANCH_NAME                        (3)      $ mvn package -DskipTests -Prelease               (4)    Step (3) is optional, if you want to build a specific branch, otherwise the version on top of the `master` branch will be built.      The `-DskipTests` flag in step (4) disable unit testing to speed up the building process: if you want to run the tests, just omit the flag. The `-Prelease` flag activates a Maven profile called ""release"" that enables the generation of the same `tar.gz` archive including everything that we distribute as RDFpro binaries on the website. This `tar.gz` is located under:        rdfpro-dist/target/rdfpro-dist-VERSION-bin.tar.gz      You may copy it wherever you want, extract it and run rdfpro via the included `rdfpro` script."""
Semantic web;https://github.com/chrdebru/r2rml;"""# R2RML-F: an R2RML Implementation    ## Building and using the code    Note: precompiled packages can be found [here](https://github.com/chrdebru/r2rml-distributions).    To build the project and copy its dependencies, execute    ```bash  $ mvn clean  $ mvn package  $ mvn dependency:copy-dependencies  ```    Note: in order to support connection to Oracle databases, we rely on a library that is not available from the Maven repository. If you have that library not installed manually, run `mvn clean` before `mvn package` and it will install the library locally prior to packaging.     You can also avail of a compiled from that resides in the `dist` directory.    The run the R2RML processor, execute the following command:    ```bash  $ java -jar r2rml.jar config.properties  ```  A fat jar is also provided with the [Apache Maven Shade Plugin](https://maven.apache.org/plugins/maven-shade-plugin/). It does not depend on the `dependency` folder and can be executed as follows:    ```bash  $ java -jar r2rml-fat.jar config.properties  ```    Where `config.properties` is a properties file containing:    - `connectionURL`, a JDBC connection URL to a database (required)  - `user`, username for the user connecting to the database  - `password`, password for the user connecting to the database  - `mappingFile`, the R2RML mapping file (required)  - `outputFile`, the output file (required)  - `format`, format of the output files (default ""TURTLE"")  - `filePerGraph`, flag to write the different graphs in separate files (default ""false"")  - `baseIRI`, used in resolving relative IRIs produced by the R2RML mapping  - `CSVFiles`, a list of paths to CSV files that are separated by semicolons  - `prefixFile`, an RDF file from which name space prefixes will be reused.    When named graphs are used in the R2RML mapping, one should use serialization that support graphs such as N-QUADS and TRIG. The use of other serializations formats (such as TURTLE) results in all triples of all graphs being written away to that file. When setting the flag `filePerGraph` to `true` for serialization formats that do not support graphs, however, the value for `outputFile` will be used to create a directory in which a file will be created for each graph in the RDF dataset.    Note that you cannot use both `CSVFiles` and `connectionURL` at the same time. For each CSV file, the name of the table will be the base name of that file.    ## Example    The directory `example` contains an example of a mapping and configuration file. The example assumes the MySQL database to be called `r2rml`, be running on `localhost` and accessible to the user `foo` with password `bar`. The configuration file looks as follows:    ```properties  connectionURL = jdbc:mysql://localhost/r2rml  user = foo  password = bar  mappingFile = mapping.ttl  outputFile = output.ttl  format = TURTLE  ```    The output, after passing the properties file as an argument to the R2RML processor, should look as follows:    ```turtle  <http://data.example.com/employee/7369>          a                             <http://example.com/ns#Employee> ;          <http://example.com/ns#name>  ""SMITH"" .  ```    ## Run with command line arguments    R2RML can be run with command line arguments similar to the configuration properties.     ```bash  $ java -jar r2rml.jar --connectionURL jdbc:mysql://localhost/r2rml \    --user foo --password bar \    --mappingFile mapping.ttl \    --outputFile output.ttl \    --format TURTLE  ```    ## Function with R2RML-F    This implementation of R2RML re-implemented the ideas presented in [1], allowing one to declare and use functions in ECMAScript as (Function Valued) TermMaps in the mapping. R2RML-F extends R2RML's vocabulary with predicates for declaring functions, function calls and parameter bindings. These are declared in the namespace [rrf](http://kdeg.scss.tcd.ie/ns/rrf/index.html).    ```turtle  @prefix rr: <http://www.w3.org/ns/r2rml#> .  @prefix ex: <http://example.com/ns#> .  @prefix rrf: <http://kdeg.scss.tcd.ie/ns/rrf#>    <#TriplesMap1>      rr:logicalTable [ rr:tableName ""EMP"" ];      rr:subjectMap [          rr:template ""http://data.example.com/employee/{EMPNO}"";          rr:class ex:Employee;      ];      rr:predicateObjectMap [          rr:predicate ex:name;          rr:objectMap [ rr:column ""ENAME"" ];      ];      rr:predicateObjectMap [          rr:predicate ex:test;          rr:objectMap [  	        rrf:functionCall [  	 			rrf:function <#Concat> ;  	 			rrf:parameterBindings (  	 				[ rr:column ""ENAME"" ]  	 				[ rr:column ""EMPNO"" ]  	 			) ;  	 		] ;   	 	]      ]          .        <#Concat>  	rrf:functionName ""concat"" ;  	rrf:functionBody """"""  		function concat(var1, var2) {  		return var1 + "" "" + var2 ;  	}  	"""""" ;  .  ```    ## License  This implementation of R2RML is written by [Christophe Debruyne](http://www.christophedebruyne.be/) and released under the [MIT license](http://opensource.org/licenses/MIT).    ## References    [1]  C. Debruyne and D. O'Sullivan. R2RML-F: Towards Sharing and Executing Domain Logic in R2RML Mappings. In Proceedings of the Workshop on Linked Data on the Web, LDOW 2016, co-located with the 25th International World Wide Web Conference (WWW 2016), Montreal, Canada, April 12th, 2016, 2016 """
Semantic web;https://github.com/deiu/rdf2go;"""# rdf2go    [![Build Status](https://api.travis-ci.org/deiu/rdf2go.svg?branch=master)](https://travis-ci.org/deiu/rdf2go)  [![Coverage Status](https://coveralls.io/repos/github/deiu/rdf2go/badge.svg?branch=master)](https://coveralls.io/github/deiu/rdf2go?branch=master)    Native golang parser/serializer from/to Turtle and JSON-LD.    # Installation    Just go get it!    `go get -u github.com/deiu/rdf2go`    # Example usage    ## Working with graphs    ```golang  // Set a base URI  baseUri := ""https://example.org/foo""    // Create a new graph  g, err := NewGraph(baseUri)  if err != nil {  	// deal with err  }    // Add a few triples to the graph  triple1 := NewTriple(NewResource(""a""), NewResource(""b""), NewResource(""c""))  g.Add(triple1)  triple2 := NewTriple(NewResource(""a""), NewResource(""d""), NewResource(""e""))  g.Add(triple2)    // Get length of Graph (nr of triples)  g.Len() // -> 2    // Dump graph contents to NTriples  out := g.String()  // <a> <b> <c> .  // <a> <d> <e> .    // Delete a triple  g.Remove(triple2)  ```    ## Looking up triples from the graph    ### Returning a single match    The `g.One()` method returns the first triple that matches against any (or all) of Subject, Predicate, Object patterns.    ```golang  // Create a new graph  g, _ := NewGraph(""https://example.org"")    // Add a few triples  g.Add(NewTriple(NewResource(""a""), NewResource(""b""), NewResource(""c"")))    // Look up one triple matching the given subject  triple := g.One(NewResource(""a""), nil, nil) // -> <a> <b> <c> .    // Look up one triple matching the given predicate  triple = g.One(nil, NewResource(""b""), nil) // -> <a> <b> <c> .    // Look up one triple matching the given object  triple = g.One(nil, nil, NewResource(""c"")) // -> <a> <b> <c> .    // Look up one triple matching the given subject and predicate  triple = g.One(NewResource(""a""), NewResource(""b""), nil) // -> <a> <b> <c> .    // Look up one triple matching the a bad predicate  triple = g.One(nil, NewResource(""z""), nil) // -> nil  ```    ### Returning a list of matches    Similar to `g.One()`, `g.All()` returns all triples that match the given pattern.    ```golang  // Create a new graph  g, _ := NewGraph(""https://example.org"")    // Add a few triples  g.Add(NewTriple(NewResource(""a""), NewResource(""b""), NewResource(""c"")))  g.Add(NewTriple(NewResource(""a""), NewResource(""b""), NewResource(""d"")))    // Look up one triple matching the given subject  triples := g.All(nil, nil, NewResource(""c"")) //  for triple := range triples {  	triple.String()  }  // Returns a single triple that matches object <c>:  // <a> <b> <c> .    triples = g.All(nil, NewResource(""b""), nil)  for triple := range triples {  	triple.String()  }  // Returns all triples that match subject <b>:   // <a> <b> <c> .  // <a> <b> <d> .  ```    ## Different types of terms (resources)    ### IRIs    ```golang  // Create a new IRI  iri := NewResource(""https://example.org"")  iri.String() // -> <https://example.org>  ```    ### Literals    ```golang  // Create a new simple Literal  lit := NewLiteral(""hello world"")  lit.String() // -> ""hello word""    // Create a new Literal with language tag  lit := NewLiteralWithLanguage(""hello world"", ""en"")  lit.String() // -> ""hello word""@en    // Create a new Literal with a data type  lit := NewLiteralWithDatatype(""newTypeVal"", NewResource(""https://datatype.com""))  lit.String() // -> ""newTypeVal""^^<https://datatype.com>  ```    ### Blank Nodes    ```golang  // Create a new Blank Node with a given ID  bn := NewBlankNode(9)  bn.String() // -> ""_:n9""    // Create an anonymous Blank Node with a random ID  abn := NewAnonNode()  abn.String() // -> ""_:n192853""  ```      ## Parsing data    The parser takes an `io.Reader` as first parameter, and the string containing the mime type as the second parameter.    Currently, the supported parsing formats are Turtle (with mime type `text/turtle`) and JSON-LD (with mime type `application/ld+json`).    ### Parsing Turtle from an io.Reader    ```golang  // Set a base URI  baseUri := ""https://example.org/foo""    // Create a new graph  g, _ := NewGraph(baseUri)    // r is of type io.Reader  g.Parse(r, ""text/turtle"")  ```    ### Parsing JSON-LD from an io.Reader    ```golang  // Set a base URI  baseUri := ""https://example.org/foo""    // Create a new graph  g, _ := NewGraph(baseUri)    // r is an io.Reader  g.Parse(r, ""application/ld+json"")  ```    ### Parsing either Turtle or JSON-LD from a URI on the Web    In this case you don't have to specify the mime type, as the internal http client will try to content negotiate to either Turtle or JSON-LD. An error will be returned if it fails.    **Note:** The `NewGraph()` function accepts an optional parameter called `skipVerify` that is used to tell the internal http client whether or not to ignore bad/self-signed server side certificates. By default, it will not check if you omit this parameter, or if you set it to `true`.    ```golang  // Set a base URI  uri := ""https://example.org/foo""    // Check remote server certificate to see if it's valid   // (don't skip verification)  skipVerify := false    // Create a new graph. You can also omit the skipVerify parameter  // and accept invalid certificates (e.g. self-signed)  g, _ := NewGraph(uri, skipVerify)    err := g.LoadURI(uri)  if err != nil {  	// deal with the error  }  ```      ## Serializing data      The serializer takes an `io.Writer` as first parameter, and the string containing the mime type as the second parameter.    Currently, the supported serialization formats are Turtle (with mime type `text/turtle`) and JSON-LD (with mime type `application/ld+json`).      ### Serializing to Turtle    ```golang  // Set a base URI  baseUri := ""https://example.org/foo""    // Create a new graph  g, _ := NewGraph(baseUri)    triple := NewTriple(NewResource(""a""), NewResource(""b""), NewResource(""c""))  g.Add(triple)    // w is of type io.Writer  g.Serialize(w, ""text/turtle"")  ```    ### Serializing to JSON-LD    ```golang  // Set a base URI  baseUri := ""https://example.org/foo""    // Create a new graph  g, _ := NewGraph(baseUri)    triple := NewTriple(NewResource(""a""), NewResource(""b""), NewResource(""c""))  g.Add(triple)    // w is of type io.Writer  g.Serialize(w, ""application/ld+json"")  ``` """
Semantic web;https://github.com/OnToology/OnToology;"""# ![alt text](https://raw.githubusercontent.com/OnToology/OnToology/master/media/icons/logoprop1_readme.png ""OnToology"")  <!--[![Build Status](https://semaphoreci.com/api/v1/ahmad88me/ontoology/branches/master/badge.svg)](https://semaphoreci.com/ahmad88me/ontoology)  -->  <!--  [![Build Status](https://ahmad88me.semaphoreci.com/badges/OnToology.svg)](https://ahmad88me.semaphoreci.com/projects/OnToology)   [![codecov](https://codecov.io/gh/OnToology/OnToology/branch/master/graph/badge.svg)](https://codecov.io/gh/OnToology/OnToology)  -->  [![Build Status](https://ahmad88me.semaphoreci.com/badges/OnToology/branches/master.svg)](https://ahmad88me.semaphoreci.com/projects/OnToology)   [![codecov](https://codecov.io/gh/OnToology/OnToology/branch/master/graph/badge.svg?token=PJgHWaaa9l)](https://codecov.io/gh/OnToology/OnToology)  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1317786.svg)](https://doi.org/10.5281/zenodo.1317786)  [![Twitter](https://img.shields.io/twitter/follow/OnToology.svg?style=social&label=@OnToology)](https://twitter.com/OnToology)    A system for collaborative ontology development process. Given a repository with an owl file, **OnToology** will survey it and produce diagrams, a complete documentation and validation based on common pitfalls.    You can find a live version of OnToology online: http://ontoology.linkeddata.es.    Team: Ahmad Alobaid, Daniel Garijo, Maria Poveda, Idafen Santa, Alba Fernandez Izquierdo, Oscar Corcho    License: Apache License v2 (http://www.apache.org/licenses/LICENSE-2.0)    If you want to cite Ontoology in a scientific paper or technical report, you can use the following [Bibtex citation](/media/references/ontoology.bib) or directly this text: Alobaid A, Garijo D, Poveda-Villalón M, Santana-Pérez I, Fernández-Izquierdo A, Corcho O (2019) Automating ontology engineering support activities with OnToology. Journal of Web Semantics 57:100472, https://doi.org/10.1016/j.websem.2018.09.003    # Funding  The development of OnToology has been supported by the Spanish national project Datos 4.0 (TIN2016-78011-C4-4-R)    # Tools  Here is a list of tools being used by OnToology.  * [owl2jsonld](https://github.com/stain/owl2jsonld) ( [zenodo](http://dx.doi.org/10.5281/zenodo.10565) )  * [Widoco](https://github.com/dgarijo/Widoco) ( [zenodo](https://zenodo.org/badge/latestdoi/11427075) )  * [OOPS!](http://oops.linkeddata.es)  * [AR2DTool](https://github.com/idafensp/ar2dtool)  * [oops-report](https://github.com/OnToology/oops-report)  * [Themis](https://github.com/oeg-upm/Themis)      # Documentation for users  If you are an ontology engineering willing to use Ontoology, you can check our [step by step documentation](http://ontoology.linkeddata.es/stepbystep). Please check also our list of [Frequently Asked Questions](http://ontoology.linkeddata.es/faqs)      # Documentation for developers  Next we provide some documentation for developers who want to contribute to the further development Ontoology or for those who are interested in deploying Ontoology locally or in their servers. Feel free to contact us if you are interested in contributing of fixing some functionality      ## To run automated tests  1. You should have [docker](https://docs.docker.com/) and [docker-compose](https://docs.docker.com/compose/) installed  2. You need to have a GitHub user to act as ""OnToologyUser"" (you can choose any username you like).  3. Add the details as in the *secret setup* section below.  4. Run the automated tests script `sh scripts/run_tests.sh`       ## Run Locally  ### via script  1. `sh scripts/run_web.sh`  ### manual  1. `cp -Rf ~/.ssh/ ssh` (assuming you have a *nix and that you already have an ssh key)  1. `mkdir -p .git`  1. `docker-compose build --no-cache`  1. `docker-compose run -p 8000:8000 web .venv/bin/python manage.py runserver 0.0.0.0:8000`  1. ~~Run the RabbitMQ server (consumers).~~      - ~~Locally: `python OnToology/rabbit.py`~~      - ~~For a linux server: `nohup .venv/bin/python OnToology/rabbit.py &`~~  1. ~~(Optional) you can run it with multiple threads `nohup .venv/bin/python OnToology/rabbit.py 3 &`~~  1. Now, this is run automatically. But, make sure that the environment variable `rabbit_processes` is set to a value > 0      ## Development  For development, you can run the db `sh scripts/run_db.sh`. And then locally,   you can access that db. Or you can install mongo db locally on your machine.      ## To access the command line  `sh scripts/run_docker.sh`      ### Secret setup  This file should be added in `scripts/secret_setup.sh`  ```  #!/bin/sh  export github_password=""""  export github_email=""""  export client_id_login=""""  export client_id_public=""""  export client_id_private=""""  export client_secret_login=""""  export client_secret_public=""""  export client_secret_private=""""  export test_user_token=""""  export test_user_email=""""  export rabbit_host=""""  ```    ### Environment variables  Here we describe some of the main ones  * `rabbit_processes` : The number of rabbit processes to automatically run (0 means do not run it automatically).      ### How to contribute  There are two workflows:      ##### Case 1: If you are a contributor:  1. Create a new branch from the current live one (now it is `master`). Make sure to give it a presentive name. In case it is for a specific issue, include the issue number in the branch name, e.g. change-spinner-123.  2. Once you push your changes on the new branch, **create a pull request** and one of the admins will check your code base and will merge if it is ok.      ##### Case 2: If you are not added as a contributor yet (or you are a contributor who prefers this workflow):  1. Fork from the current live branch (now it is `master`).  2. Create a pull request, we will review it and merge if it is ok.      ### Dependency notice  * To run the tests, we use the `mock` option for github api. It was rejected by the `PyGithub` maintainers, so make sure to use  the version in `ahmad88me/PyGithub`.  (see below)      ## Local Setup  ### On Linux  (tested on ubuntu, debian, mint and fedora)  #### To install the tools  1. Open the terminal and `cd` to the location of choice.  2. `export PLAYGROUND=$PWD`.  3. Copy and paste the commands of choice to the terminal from `scripts/setup_docker_base.sh`      ### Install Pygithub (not the upstream version)  #### either directly from github  `pip install git+https://github.com/ahmad88me/PyGithub.git`  #### or locally  1. `git clone https://github.com/ahmad88me/PyGithub.git`  1. `cd OnToology` (assuming both are on the same level/directory)  1. `pip install -e ../Pygithub` (change this to any directory you want)   """
Semantic web;https://github.com/drobilla/serd;"""Serd  ====    Serd is a lightweight C library for RDF syntax which supports reading and  writing [Turtle][], [TriG][], [NTriples][], and [NQuads][].  Serd is suitable  for performance-critical or resource-limited applications, such as serialising  very large data sets or embedded systems.    Features  --------     * **Free:** Serd is [Free Software][] released under the extremely liberal     [ISC license][].     * **Portable and Dependency-Free:** Serd has no external dependencies other     than the C standard library.  It is known to compile with GCC, Clang, and     MSVC (as C++), and is tested on GNU/Linux, MacOS, and Windows.     * **Small:** Serd is implemented in a few thousand lines of C.  It typically     compiles to about 100 KiB, or about 50 KiB stripped with size optimizations.     * **Fast and Lightweight:** Serd can stream abbreviated Turtle, unlike many     tools which must first build an internal model.  This makes it particularly     useful for writing very large data sets, since it can do so using only a     small amount of memory.  Serd is, to the author's knowledge, the fastest     Turtle reader/writer by a wide margin (see [Performance](#performance)     below).     * **Conformant and Well-Tested:** Serd passes all tests in the Turtle and TriG     test suites, correctly handles all ""normal"" examples in the URI     specification, and includes many additional tests which were written     manually or discovered with fuzz testing.  The test suite is run     continuously on many platforms, has 100% code coverage by line, and runs     with zero memory errors or leaks.    Performance  -----------    The benchmarks below compare `serdi`, [rapper][], and [riot][] re-serialising  Turtle data generated by [sp2b][] on an i7-4980HQ running Debian 9.  Of the  three, `serdi` is the fastest by a wide margin, and the only one that uses a  constant amount of memory (a single page) for all input sizes.    ![Time](doc/serdi-time.svg)  ![Throughput](doc/serdi-throughput.svg)  ![Memory](doc/serdi-memory.svg)    Documentation  -------------     * [API reference (single page)](https://drobilla.gitlab.io/serd/c/singlehtml)   * [API reference (paginated)](https://drobilla.gitlab.io/serd/c/html)   * [`serdi` man page](https://drobilla.gitlab.io/serd/man/serdi.html)     -- David Robillard <d@drobilla.net>    [Turtle]: https://www.w3.org/TR/turtle/  [TriG]: https://www.w3.org/TR/trig/  [NTriples]: https://www.w3.org/TR/n-triples/  [NQuads]: https://www.w3.org/TR/n-quads/  [Free Software]: http://www.gnu.org/philosophy/free-sw.html  [ISC license]: http://opensource.org/licenses/isc  [rapper]: http://librdf.org/raptor/  [riot]: https://jena.apache.org/  [sp2b]: http://www2.informatik.uni-freiburg.de/~mschmidt/docs/sp2b.pdf """
Semantic web;https://github.com/cmungall/sparqlprog;"""# sparqlprog - programming with SPARQL    [![Build Status](https://travis-ci.org/cmungall/sparqlprog.svg?branch=master)](https://travis-ci.org/cmungall/sparqlprog)  [![Join the chat at https://gitter.im/sparqlprog/Lobby](https://badges.gitter.im/sparqlprog/Lobby.svg)](https://gitter.im/sparqlprog/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)  [**pack**](http://www.swi-prolog.org/pack/list?p=sparqlprog)    sparqlprog is a programming language and environment that can be used  to write composable modular building blocks that can be executed as  federated SPARQL queries.    Example of use (command line):    ```  pl2sparql  -u sparqlprog/ontologies/ebi -u sparqlprog/ontologies/faldo  -s ebi ""\    protein_coding_gene(G), \    location(G,L,B,E,grcm38:'11'), \    B >= 101100523,E =< 101190725, \    orthologous_to(G,H),in_taxon(H,taxon:'9606')"" \    ""h(G,H)""  ```    The command passes a *logic program query* to sparqlprog. In this  case, the query is a conjunction of conditions involving different  variables (each indicated with a leading upper-case letter):     1. `G` is a *protein coding gene*   2. `G` is located on mouse chromosome 11, with an interval bounded by `B` (begin) and `E` (end)   3. The interval is within a certain range   4. `G` is *homologus to* `H`   5. `H` is a human gene (indicated by taxon ID 9606)   6. The results are bound to a tuples `h(G,H)` (i.e. two column table)    This logic query compiles down to a SPARQL query for fetching G and  H. The query is then executed on the [EBI RDF  Platform](https://www.ebi.ac.uk/rdf/services/sparql), giving:    |Mouse Gene|Human Gene|  |---|---|  |ensembl:ENSMUSG00000035198|ensembl:ENSG00000131462|  |ensembl:ENSMUSG00000017167|ensembl:ENSG00000108797|  |ensembl:ENSMUSG00000044052|ensembl:ENSG00000184451|  |ensembl:ENSMUSG00000017802|ensembl:ENSG00000141699|  |ensembl:ENSMUSG00000045007|ensembl:ENSG00000037042|  |ensembl:ENSMUSG00000035172|ensembl:ENSG00000068137|    How does this work? The query compilation makes use of pre-defined  n-ary predicates, such as this one defined in the [faldo  module](https://www.swi-prolog.org/pack/file_details/sparqlprog/prolog/sparqlprog/ontologies/faldo.pl):    ```  location(F,L,B,E,R) :-    rdf(F,faldo:location,L),    begin(L,PB),position(PB,B),reference(PB,R),    end(L,PE),position(PE,E),reference(PE,R).  ```    The `:-` connects a rule head to a rule body. In this case the body is  a conjuncation of goals. Each of these may be defined in their own  rules. Typically everything bottoms out at a call over a 3-ary  predicate `rdf(S,P,O)` which maps to a single triple. In this case the vocabulary used for genomic locations is [faldo](https://github.com/OBF/FALDO).    This approach allows for *composability* of queries. Rather that  repeating the same verbose SPARQL each time in different queries,  reusable modules can be defined.    In addition to providing a composable language that compiles to  SPARQL, this package provides a complete turing-complete environment  for mixing code and queries in a relational/logic programming  paradigm. See below for examples.    ## Quick Start (for prolog hackers)    See the [sparqlprog module docs](https://www.swi-prolog.org/pack/file_details/sparqlprog/prolog/sparqlprog.pl)    See also the [specification](SPECIFICATION.md)    ## Quick Start (for Python hackers)    See the [sparqlprog-python](https://github.com/cmungall/sparqlprog-python) package    This provides a Python interface to a sparqlprog service    You can also see demonstration notebooks:     * [Basic SPARQLProg](https://nbviewer.jupyter.org/github/cmungall/sparqlprog-python/blob/master/Notebook_01_Basics.ipynb)   * [sending programs over the wire](https://nbviewer.jupyter.org/github/cmungall/sparqlprog-python/blob/master/Notebook_02_Programs.ipynb)    ## Quick Start (for everyone else)    There are a variety of ways to use this framework:     * Executing queries on remote services via command line   * Compiling logic queries to SPARQL queries, for use in another framework   * Programmatically within a logic program (interleaving remote and local operations)   * Programmatically from a language like python/javascript, using a __sparqlprog service__    Consult the appropriate section below:    ### Running queries from the command line    See the [examples](./examples/) directory for all command line examples    First [install](INSTALL.md), making sure the [bin](bin) directory is  in your path. This will give you access to the the pl2sparql script.    For full options, run:    ```  pl2sparql --help  ```    Note you should also have a number of convenience scripts in your  path. For example the `pq-wd` script is simply a shortcut for    ```  pl2sparql -s wikidata -u sparqlprog/ontologies/wikidata  ARGS  ```    This will give you access to a number of convenience predicates such  as positive_therapeutic_predictor/2 (for drug queries). The `-u`  option uses the wikidata module, and the `-s` option sets the service  to the one with handle `dbpedia` (the mapping from a handle to the  full service URL is defined in the wikidata module).    The best way to learn is to look at the [examples/](examples),  together with the corresponding set of rules in  [prolog/sparqlprog/ontologies](prolog/sparqlprog/ontologies).    For example [examples/monarch-examples.sh](examples/monarch-examples.sh) has:    ```  pq-mi  'label(D,DN),literal_exact_match(DN,""peroxisome biogenesis disorder""),\     rdfs_subclass_of(D,C),owl_equivalent_class(C,E),has_phenotype(E,Z)'\     'x(C,CN,E,Z)'  ```    This finds a disease with a given name, finds equivalent classes of  transitive reflexive subclasses, and then finds phenotypes for each    ### Compiling logic programs to SPARQL    You can use pl2sparql (see above for installation) to compile a  program with bindings to a SPARQL query by using the `-C` option. The  SPARQL query can then be used without any dependence on  sparqlprog. E.g.    ```  pq-ebi -C ""\    protein_coding_gene(G), \    location(G,L,B,E,grcm38:'11'), \    B >= 101100523,E =< 101190725, \    homologous_to(G,H),in_taxon(H,taxon:'9606')"" \    ""h(G,H)""  ```    will generate the following SPARQL:    ```  SELECT ?g ?h WHERE {?g <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://purl.obolibrary.org/obo/SO_0001217> . ?g <http://biohackathon.org/resource/faldo#location> ?l . ?l <http://biohackathon.org/resource/faldo#begin> ?v0 . ?v0 <http://biohackathon.org/resource/faldo#position> ?b . ?v0 <http://biohackathon.org/resource/faldo#reference> <http://rdf.ebi.ac.uk/resource/ensembl/90/mus_musculus/GRCm38/11> . ?l <http://biohackathon.org/resource/faldo#end> ?v1 . ?v1 <http://biohackathon.org/resource/faldo#position> ?e . ?v1 <http://biohackathon.org/resource/faldo#reference> <http://rdf.ebi.ac.uk/resource/ensembl/90/mus_musculus/GRCm38/11> . FILTER (?b >= 101100523) . FILTER (?e <= 101190725) . ?g <http://semanticscience.org/resource/SIO_000558> ?h . ?h <http://purl.obolibrary.org/obo/RO_0002162> <http://identifiers.org/taxonomy/9606>}  ```    withOUT executing it remotely    note: indentation and URI shortening are on the cards for future releases.    ### Using a public sparqlprog service    Public pengines service: https://evening-falls-87315.herokuapp.com/pengine    [Pengines](http://pengines.swi-prolog.org/) is a framework for running logic program environments as a  web service. They can be used by clients in any language (client  libraries in python, javascript seem to be mature; as well as separate  prolog clients as well).    See the docs on the [pengines framework](http://pengines.swi-prolog.org/).    There is an example of how to contact this service in javascript in  [bin/sprog-client.js](bin/sprog-client.js). You will need to do a `npm  install pengines`, and change the server URL.    Pengines allows the client to send logic programs to the server, and  then to invoke them. For example:    ```  pengines = require('pengines');    peng = pengines({      server: ""https://evening-falls-87315.herokuapp.com/pengine"",      ask: ""q(X)"",      chunk: 100,      sourceText: ""q(X):- (wd ?? continent(X)).\n""  }  ).on('success', handleSuccess).on('error', handleError);  function handleSuccess(result) {      console.log('# Results: '+ result.data.length);      for (var i = 0; i < result.data.length; i++) {          console.log(result.data[i])      }      if (result.data.length == 0) {          console.log(""No results!"")      }  }  function handleError(result) {      console.error(result)  }  ```    Note that *any* safe subset of prolog can be passed as a program. In  this case we are passing a small program:    `q(X):- (wd ?? continent(X))`    This trivially defines a unary predicate `q/1`. The argument is bound  to any continent. The `??` is a special infix binary predicate, the  left side is the service name and the right side is the query to be  compiled.    The `ask` portion of the javascript will simply pass the query to the  server.    ### Using a local sparqlprog service    You can start a sparqlprog service running locally:        docker run -p 9083:9083 cmungall/sparqlprog    (requires docker)    This creates a pengines service at http://localhost:9083/pengine    There is an example of how to contact this service in javascript in  [sprog-client.js](bin/sprog-client.js). You will need to do:        npm install pengines    ### SWISH    TODO    ### Use within logic programs    For this example, consider writing a music band recommender, based on  similarity of genres. dbpedia has triples linking bands to genres, so  we will use that.    We will write a program  [dbpedia_rules.pl](examples/dbpedia/dbpedia_rules.pl) that contains  definitions of predicates we will use.    First we define a binary predicate that counts the number of bands per genre:    ```  genre_num_bands(G,Count) :-          aggregate_group(count(distinct(B)),[G],(rdf(B,dbont:genre,G),band(B)),Count).  ```    you can try this with:    `pq-dbpedia -c examples/dbpedia/dbpedia_rules.pl ""genre_num_bands(G,Count)""`    this will give results like:    ```  http://dbpedia.org/resource/Independent_music,184  http://dbpedia.org/resource/Funky_Club_Music,1  http://dbpedia.org/resource/Ghettotech,2  http://dbpedia.org/resource/Indian_folk_music,1  http://dbpedia.org/resource/Bakersfield_Sound,1  http://dbpedia.org/resource/Punk_Rawk,1  http://dbpedia.org/resource/Go-go,6  http://dbpedia.org/resource/Jazz_pop,3  http://dbpedia.org/resource/Dubstep,74  http://dbpedia.org/resource/Alt.folk,1  http://dbpedia.org/resource/AfroHouse,1  http://dbpedia.org/resource/Electro-disco,1  http://dbpedia.org/resource/Math_Rock,15  ```    we are doing this because we want to weight band similarity according  to how rare a genre is. If two bands share the genre of 'independent  music' it is not remarkable, but if two bands share a rarer genre like  'Ghettotech' then we will weight that higher.    we can explicitly bind this to dbpedia using `??/2`:    ```  get_genre_num_bands(G,Count) :-          ??(dbpedia,genre_num_bands(G,Count)).  ```    we can define the Information Content (IC) of a genre `G` as `-log2(Pr(G))`:    ```  genre_ic(G,IC) :-          get_genre_num_bands(G,Count),          get_num_bands(Total),          seval(-log(Count/Total)/log(2), IC).  ```    This makes use of:    ```  :- table get_num_bands/1.  get_num_bands(Count) :-          ??(dbpedia,num_bands(Count)).  num_bands(Count) :-          aggregate(count(distinct(B)),band(B),Count).  ```    Note we are tabling (memoizing) the call to fetch the total number of  bands. This means it will only be called once per sparqlprog session.    Finally we can define a 3-ary predicate that compares any two bands  and bindings the 3rd arg to a similarity score that is the sum of the  ICs of all genres held in common. (for simplicity, we do not penalize  unmatched genres, or try to use sub/super genre categories yet):    ```  pair_genre_ic(A,B,SumIC) :-          get_all_genres(A,SA),          get_all_genres(B,SB),          ord_intersection(SA,SB,I),          aggregate(sum(IC),G^(member(G,I),genre_ic(G,IC)),SumIC).  ```    This is a normal prolog goal and can be executed in a normal prolog context, or from the command line:    `pq-dbpedia -c examples/dbpedia/dbpedia_rules.pl -e  ""pair_genre_ic(dbr:'Metallica',dbr:'Megadeth',IC)""`    The `-e` option tells the script to execute the query directly rather  than try and compile everything to a single SPARQL query (this may be  possible, but could be highly inefficient). It is only when the prolog  engine executes the `??` goals that a remote SPARQL will be executed.    If we want to adapt this program to search rather than compare two  given bands, we can modify it slightly so that it does not waste  cycles querying on bands that have no genres in common:    ```  pair_genre_ic(A,B,SumIC) :-          get_all_genres(A,SA),          ??(dbpedia,has_shared_genre(A,B,_)),          get_all_genres(B,SB),          ord_intersection(SA,SB,I),          aggregate(sum(IC),G^(member(G,I),genre_ic(G,IC)),SumIC).  ```    Example of running this:    `pq-dbpedia -c examples/dbpedia/dbpedia_rules.pl -e  ""pair_genre_ic(dbr:'Voivod_(band)',B,IC),IC>=10""`    Note this is slow, as it will iterate across each band performing  queries to gather stats. There are various approaches to optimizing  this, but the core idea here is that the logic can be shuffled back  and forth between the portion that is compiled to SPARQL and executed  remotely, and the portion that is executed locally by a logic engine.    ### Using a local triplestore    You can use sparqlprog with any local or remote triplestore that  supports the SPARQL protocol. If you have RDF files and want to get  started, here is one quick route (assuming you have docker):     1. Place your files in [data](examples/data)   2. Run `make bg-run`    This will run blazegraph within a docker container    ## Discussion      SPARQL provides a declarative way of querying a triplestore. One of  its limitations is the lack of ability to *compose* queries and reuse  repeated patterns across multiple queries. Sparqlprog is an extension  of SPARQL and a subset of Prolog for relational rule-oriented  programming using SPARQL endpoints.    ## Prolog programmers guide    This package provides a more natural (from a Prolog point of view) interface  to SPARQL endpoints. There are two layers. The first, lower layer, defines a  DCG for generating SPARQL queries from a structured term. The second provides  a translation from representation that looks more or less like a Prolog goal  built from rdf/3 goals (with conjunction, disjunction etc) to a term in the  term language understood by the SPARQL DCG.    In addition, the library provides a mechanism to register known SPARQL endpoints  so that they can be referred to by a short name, or to enable a query to be  run against all registered endpoints.    The library is based on the idea implemented in Yves Raimond's swic package,  but the code has been completely re-implemented.    You just need SWI Prolog with its Semantic Web libraries.    ## Simple usage    The `(??)/2`  and `(??)/1` operators have a high precedence so that conjuction and disjunctive  queries can be written to the right of it without parentheses:    ```  ?- rdf_register_prefix(foaf,'http://xmlns.com/foaf/0.1/')  ?- rdf_register_prefix(dbont,'http://dbpedia.org/ontology/')  ?- sparql_endpoint( dbp, 'http://dbpedia.org/sparql/').  ?- debug(sparkle).  % to show queries    ?-	dbp ?? rdf(Class,rdf:type,owl:'Class'), rdf(Instance,rdf:type,Class).  ?- dbp ?? rdf(Person,rdf:type,foaf:'Person'),             rdf(Person,foaf:Name,Name),            filter(regex('Colt.*',Name)).  ?- dbp ?? rdf(A,rdf:type,dbont:'Photographer'); rdf(A, rdf:type, dbont:'MusicalArtist').  ```      ## Clause expansion    If the following clause is defined:    ```  cls(Class) :-          rdf(Class,rdf:type,owl:'Class').  ```    Then cls/1 can be used in queries, e.g.    ```  ?-  dbp ?? cls(X).  ```    The cls/1 goal will be expanded.    More complex goals can be defined; for example, this queries for existential restrictions:    ```  subclass_of(C,D) :- rdf(C,rdfs:subClassOf,Restr).  svf_edge(C,P,D) :-          subclass_of(C,Restr),          rdf(Restr,owl:onProperty,P),          rdf(Restr,owl:someValuesFrom,D).  ```    Only a subset of prolog can be expanded in this way. Conjunction,  disjunction (or multiple clauses), negation are supported. Terminals  rdf/3, rdf/4, and some predicates from the rdfs library are  supported. In future a wider set of constructs may be supported,  e.g. setof/3.    It is also possible to use create_sparql_construct/3 and  create_sparl_construct/4 to generate SPARQL queries for a  limited subset of pure prolog that can be executed outside  the prolog environment - effectively a limited prolog to SPARQL  compiler.    ## Comparison with SPIN    TODO https://spinrdf.org/    ## Credits    The majority of code in this repo was developed by Samer Abdallah, as  part of the [sparkle  package](http://www.swi-prolog.org/pack/list?p=sparkle). Some of this  code came from Yves Raimond's swic package.    Extensions were implemented by Chris Mungall. In particular     - goal rewriting   - DCG extensions: aggregates, filter operators   - predicate definitions for vocabularies used by various triplestores (faldo, ebi, wikidata, dbpedia, go, monarch) """
Semantic web;https://github.com/drlivingston/kr;"""# Clojure API for RDF and SPARQL    The Knowledge Representation and Reasoning Tools library enables easy Clojure use of RDF and SPARQL, provinging a unified interface for both Jena and Sesame.  (KR can be extended for other APIs and underlying triplestores.)      ## Overview    Currently it facilitates use of RDF-based representations backed by triple-/quad- stores.  It provides a consistent clojure based way of interacting with its backing implementations, which currently include the Jena and Sesame APIs. The library enables easy working with knowledge representations and knowledge bases, and provides support for some common tasks including forward-chaining and reification.    [Release Notes]    update: see the note on [Sesame Versions]      ## Basic Setup    The primary api functions you're likely to use come from the kr-core apis:  ```clj  (use 'edu.ucdenver.ccp.kr.kb)  (use 'edu.ucdenver.ccp.kr.rdf)  (use 'edu.ucdenver.ccp.kr.sparql)  ```    To actually get a KB instance to work with you'll need to make sure the implementation-specific code is loaded:  ```clj  (require 'edu.ucdenver.ccp.kr.sesame.kb)  ;; OR  (require 'edu.ucdenver.ccp.kr.jena.kb)  ```    a kb instance can then be acquired with the kb function, for example:  ```clj  (kb :sesame-mem)  ; an in-memory sesame kb  ```  The `kb` function can take keyword arguments such as `:sesame-mem` or `:jena-mem` or it can take names of several native jena or sesame objects or pre-constructed jena or sesame instances to create a `kb` wrapper around (e.g., a jena `Model` or a sesame `Sail`).    kb's need some help knowing what the namespace mappings are, the server mappings can be brought down from a third party kb by calling `(synch-ns-mappings my-kb)` or you can add a few:  ```clj  (register-namespaces my-kb                       '((""ex"" ""http://www.example.org/"")                          (""rdf"" ""http://www.w3.org/1999/02/22-rdf-syntax-ns#"")                         (""foaf"" ""http://xmlns.com/foaf/0.1/"")))  ;;the return value is the new modified kb - hang onto it  ```    ## Basic Use    Once you have a KB you can load rdf triple or files:  ```clj    ;;in parts    (add my-kb 'ex/KevinL 'rdf/type 'ex/Person)    ;;as a triple    (add my-kb '(ex/KevinL foaf/name ""Kevin Livingston""))  ```    Query for RDF triples:  ```clj  (ask-rdf my-kb nil nil 'ex/Person)  ;;true    (query-rdf my-kb nil nil 'ex/Person)  ;;((ex/KevinL rdf/type ex/Person))  ```    Query with triple patterns (SPARQL):  ```clj  (query my-kb '((?/person rdf/type ex/Person)                 (?/person foaf/name ?/name)                 (:optional ((?/person foaf/mbox ?/email)))))  ;;({?/name ""Kevin Livingston"", ?/person ex/KevinL})  ```    ## More Details    The examples also provide details on how to interact with a KB, with run-able poms:  https://github.com/drlivingston/kr/tree/master/kr-examples    These include examples of connecting to a remote repository and a local in-memory repository.      More detailed uses can be found in the test cases for both the KB, RDF, and SPARQL APIs.  They are here:  https://github.com/drlivingston/kr/tree/master/kr-core/src/test/clojure/edu/ucdenver/ccp/test/kr      ## Maven    releases are deployed to clojars:  ```xml  <repository>    <id>clojars.org</id>    <url>http://clojars.org/repo</url>  </repository>  ```    the core dependency is kr-core:  ```xml  <dependency>    <groupId>edu.ucdenver.ccp</groupId>    <artifactId>kr-core</artifactId>    <version>1.4.17</version>  </dependency>  ```    but the core dependency is unnecessary if you are brining in either the sesame or jena implementations:  ```xml  <dependency>    <groupId>edu.ucdenver.ccp</groupId>    <artifactId>kr-sesame-core</artifactId>    <version>1.4.17</version>  </dependency>    <dependency>    <groupId>edu.ucdenver.ccp</groupId>    <artifactId>kr-jena-core</artifactId>    <version>1.4.17</version>  </dependency>  ```      ## Acknowledgements  open sourced by: <br />  [CCP Lab][] <br />  [University of Colorado Denver][] <br />  primary developer: [Kevin Livingston][]    ----      [CCP Lab]: http://compbio.ucdenver.edu/Hunter_lab/CCP_website/index.html  [University of Colorado Denver]: http://www.ucdenver.edu/  [Kevin Livingston]: https://github.com/drlivingston  [Sesame Versions]:https://github.com/drlivingston/kr/wiki/versions-and-sesame  [Release Notes]:https://github.com/drlivingston/kr/wiki/Release-notes """
Semantic web;https://github.com/RMLio/rmlmapper-java;"""# RMLMapper <!-- omit in toc -->    [![Maven Central](https://img.shields.io/maven-central/v/be.ugent.rml/rmlmapper.svg?label=Maven%20Central)](https://search.maven.org/search?q=g:%22be.ugent.rml%22%20AND%20a:%22rmlmapper%22)    The RMLMapper execute RML rules to generate Linked Data.  It is a Java library, which is available via the command line ([API docs online](https://javadoc.io/doc/be.ugent.rml/rmlmapper)).  The RMLMapper loads all data in memory, so be aware when working with big datasets.    Want to get started quickly? Check out [Releases](#releases) on where to find the latest CLI build as a jar,  and see [Usage](#cli) on how to use the commandline interface!    ## Table of contents <!-- omit in toc -->    - [Features](#features)    - [Supported](#supported)    - [Future](#future)  - [Releases](#releases)  - [Build](#build)  - [Usage](#usage)    - [CLI](#cli)    - [Library](#library)    - [Docker](#docker)    - [Including functions](#including-functions)    - [Generating metadata](#generating-metadata)  - [Testing](#testing)    - [RDBs](#rdbs)  - [Dependencies](#dependencies)  - [Commercial Support](#commercial-support)  - [Remarks](#remarks)    - [Typed spreadsheet files](#typed-spreadsheet-files)    - [XML file parsing performance](#xml-file-parsing-performance)    - [Language tag support](#language-tag-support)    - [Duplicate removal and serialization format](#duplicate-removal-and-serialization-format)    - [I have a question! Where can I get help?](#i-have-a-question-where-can-i-get-help)  - [Documentation](#documentation)    - [UML Diagrams](#uml-diagrams)    ## Features    ### Supported  - local data sources:    - Excel (.xlsx)    - LibreOffice (.ods)    - CSV files (including CSVW)    - JSON files (JSONPath)    - XML files (XPath)  - remote data sources:    - relational databases (MySQL, PostgreSQL, Oracle, and SQLServer)    - Web APIs with W3C Web of Things    - SPARQL endpoints    - files via HTTP urls (via GET)      - CSV files      - JSON files (JSONPath (`@` can be used to select the current object.))      - XML files (XPath)  - functions (most cases)    - For examples on how to use functions within RML mapping documents, you can have a look at the [RML+FnO test cases](https://github.com/RMLio/rml-fno-test-cases)  - configuration file  - metadata generation  - output formats: nquads (default), turtle, trig, trix, jsonld, hdt  - join conditions  - targets:    - local file    - VoID dataset    - SPARQL endpoint with SPARQL UPDATE    ### Future  - functions (all cases)  - conditions (all cases)  - data sources:    - NoSQL databases    - TPF servers    ## Releases    The standalone jar file (that has a [commandline interface](#cli)) for every release can be found on the release's page on GitHub.  You can find the latest release [here](https://github.com/RMLio/rmlmapper-java/releases/latest).  This is the recommended way to get started with RMLMapper.  Do you want to build from source yourself? Check [Build](#build).    ## Build  The RMLMapper is build using Maven.  As it is also tested against Oracle (check [here](#accessing-oracle-database) for details),  it needs a specific set-up to run all tests.  That's why we recommend to build without testing: `mvn install -DskipTests=true`.  If you want, you can install with tests, and just skip the Oracle tests: `mvn test -Dtest=!Mapper_OracleDB_Test`.    A standalone jar can be found in `/target`.    Two jars are found in `/target`: a slim jar without bundled dependencies, and a standalone jar (suffixed with `-all.jar`) with all dependencies bundled.    ## Usage    ### CLI  The following options are most common.    - `-m, --mapping <arg>`: one or more mapping file paths and/or strings (multiple values are concatenated).  - `-o, --output <arg>`: path to output file  - `-s,--serialization <arg>`: serialization format (nquads (default), trig, trix, jsonld, hdt)    All options can be found when executing `java -jar rmlmapper.jar --help`,  that output is found below.    ```  usage: java -jar mapper.jar <options>  options:   -c,--configfile <arg>               path to configuration file   -d,--duplicates                     remove duplicates in the output   -dsn,--r2rml-jdbcDSN <arg>          DSN of the database when using R2RML                                       rules   -e,--metadatafile <arg>             path to output metadata file   -f,--functionfile <arg>             one or more function file paths (dynamic                                       functions with relative paths are found                                       relative to the cwd)   -h,--help                           show help info   -l,--metadataDetailLevel <arg>      generate metadata on given detail level                                       (dataset - triple - term)   -m,--mappingfile <arg>              one or more mapping file paths and/or                                       strings (multiple values are                                       concatenated). r2rml is converted to rml                                       if needed using the r2rml arguments.   -psd,--privatesecuritydata <arg>    one or more private security files                                        containing all private security                                        information such as usernames, passwords,                                        certificates, etc.   -o,--outputfile <arg>               path to output file (default: stdout)   -p,--r2rml-password <arg>           password of the database when using                                       R2RML rules   -s,--serialization <arg>            serialization format (nquads (default),                                       turtle, trig, trix, jsonld, hdt)   -t,--triplesmaps <arg>              IRIs of the triplesmaps that should be                                       executed in order, split by ',' (default                                       is all triplesmaps)   -u,--r2rml-username <arg>           username of the database when using                                       R2RML rules   -v,--verbose                        show more details in debugging output   --strict                            Enable strict mode. In strict mode, the                                        mapper will fail on invalid IRIs instead                                        of skipping them.   -b --base-IRI <arg>                 base IRI used to expand relative IRIs in                                        mapped terms. If not set and not in --strict                                        mode, will default to the @base directive                                        inside the provided mapping file.                                                                     ```    #### Accessing Web APIs with authentication    The [W3C Web of Things Security Ontology](https://www.w3.org/2019/wot/security)  is used to describe how Web APIs authentication should be performed   but does not include the necessary credentials to access the Web API.  These credentials can be supplied using the `-psd <PATH>` CLI argument.  The `PATH` argument must point to one or more private security files  which contain the necessary credentials to access the Web API.    An example can be found in the test cases   [src/test/resources/web-of-things](src/test/resources/web-of-things).    #### Accessing Oracle Database    You need to add the Oracle JDBC driver manually to the class path  if you want to access an Oracle Database.  The required driver is `ojdbc8`.    - Download `ojdbc8.jar` from [Oracle](https://www.oracle.com/database/technologies/jdbc-ucp-122-downloads.html).  - Execute the RMLMapper via     ```  java -cp 'rmlmapper.jar:ojdbc8-12.2.0.1.jar' be.ugent.rml.cli.Main -m rules.rml.ttl  ```    The options do the following:    - `-cp 'rmlmapper.jar:ojdbc8-12.2.0.1.jar'`: Put the jar of the RMLMapper and JDBC driver in the classpath.  - `be.ugent.rml.cli.Main`: `be.ugent.rml.cli.Main` is the entry point of the RMLMapper.  - `-m rules.rml.ttl`: Use the RML rules in the file `rules.rml`.ttl.  The exact same options as the ones mentioned earlier are supported.    ### Library    An example of how you can use the RMLMapper as an external library can be found  at [./src/test/java/be/ugent/rml/readme/ReadmeTest.java](https://github.com/RMLio/rmlmapper-java/blob/master/src/test/java/be/ugent/rml/readme/ReadmeTest.java)    ### Docker    #### Dockerhub    We publish our Docker images automatically on Dockerhub for every release.  You can find our images here: [rmlio/rmlmapper-java](https://hub.docker.com/r/rmlio/rmlmapper-java).    #### Build image    You can use Docker to run the RMLMapper by following these steps:    - Build the Docker image: `docker build -t rmlmapper .`.  - Run a Docker container: `docker run --rm -v $(pwd):/data rmlmapper -m mapping.ttl`.    The same parameters are available as via the CLI.  The RMLMapper is executed in the `/data` folder in the Docker container.    ### Including functions    There are two ways to include (new) functions within the RML Mapper    * dynamic loading: you add links to java files or jar files, and those files are loaded dynamically at runtime    * preloading: you register functionality via code, and you need to rebuild the mapper to use that functionality    Registration of functions is done using a Turtle file, which you can find in `src/main/resources/functions.ttl`    The snippet below for example links an fno:function to a library, provided by a jar-file (`GrelFunctions.jar`).    ```turtle  @prefix dcterms: <http://purl.org/dc/terms/> .  @prefix doap:    <http://usefulinc.com/ns/doap#> .  @prefix fno:     <https://w3id.org/function/ontology#> .  @prefix fnoi:    <https://w3id.org/function/vocabulary/implementation#> .  @prefix fnom:    <https://w3id.org/function/vocabulary/mapping#> .  @prefix grel:    <http://users.ugent.be/~bjdmeest/function/grel.ttl#> .  @prefix grelm:   <http://fno.io/grel/rmlmapping#> .  @prefix rdfs:    <http://www.w3.org/2000/01/rdf-schema#> .    grel:toUpperCase a fno:Function ;    fno:name ""to Uppercase"" ;    rdfs:label ""to Uppercase"" ;    dcterms:description ""Returns the input with all letters in upper case."" ;    fno:expects ( grel:valueParam ) ;    fno:returns ( grel:stringOut ) .    grelm:javaString      a                  fnoi:JavaClass ;      doap:download-page ""GrelFunctions.jar"" ;      fnoi:class-name    ""io.fno.grel.StringFunctions"" .    grelm:uppercaseMapping      a                    fnoi:Mapping ;      fno:function         grel:toUpperCase ;      fno:implementation   grelm:javaString ;      fno:methodMapping    [ a                fnom:StringMethodMapping ;                             fnom:method-name ""toUppercase"" ] .  ```    #### Dynamic loading    Just put the java or jar-file in the resources folder,  at the root folder of the jar-location,  or the parent folder of the jar-location,  it will be found dynamically.    > Note: the java or jar-files are found relative to the cwd.  You can change the functions.ttl path (or use multiple functions.ttl paths) using a commandline-option (`-f`).    #### Preloading    This overrides the dynamic loading.  An example of how you can use Preload a custom function can be found  at [./src/test/java/be/ugent/rml/readme/ReadmeFunctionTest.java](https://github.com/RMLio/rmlmapper-java/blob/master/src/test/java/be/ugent/rml/readme/ReadmeFunctionTest.java)    ### Generating metadata    Conform to how it is described in the scientific paper [1],  the RMLMapper allows to automatically generate [PROV-O](https://www.w3.org/TR/prov-o/) metadata.  Specifically, you need the CLI arguments below.  You can specify in which output file the metadata should be stored,  and up to which level metadata should be stored (dataset, triple, or term level metadata).    ```   -e,--metadatafile <arg>          path to output metadata file   -l,--metadataDetailLevel <arg>   generate metadata on given detail level                                    (dataset - triple - term)  ```    ## Testing    Run the tests via `test.sh`.    #### Derived tests  Some tests (Excel, ODS) are derived from other tests (CSV) using a script (`./generate_spreadsheet_test_cases.sh`)    ### RDBs  Make sure you have [Docker](https://www.docker.com) running.    #### Problems  * A problem with Docker (can't start the container) causes the SQLServer tests to fail locally. These tests will always succeed locally.  * A problem with Docker (can't start the container) causes the PostgreSQL tests to fail locally on Windows 7 machines.    ## Dependencies    | Dependency                              | License                                                            |  |:---------------------------------------:|--------------------------------------------------------------------|  | ch.qos.logback logback-classic          | Eclipse Public License 1.0 & GNU Lesser General Public License 2.1 |  | commons-cli commons-lang                | Apache License 2.0                                                 |  | com.opencsv opencsv                     | Apache License 2.0                                                 |  | commons-cli commons-cli                 | Apache License 2.0                                                 |  | org.eclipse.rdf4j rdf4j-runtime         | Eclipse Public License 1.0                                         |  | junit junit                             | Eclipse Public License 1.0                                         |  | com.jayway.jsonpath json-path           | Apache License 2.0                                                 |  | javax.xml.parsers jaxp-api              | Apache License 2.0                                                 |  | org.jsoup                               | MIT                                                                |  | mysql mysql-connector-java              | GNU General Public License v2.0                                    |  | ch.vorbuger.mariaDB4j mariaDB4j         | Apache License 2.0                                                 |  | postgresql postgresql                   | BSD                                                                |  | com.microsoft.sqlserver mssql-jdbc      | MIT                                                                |  | com.spotify docker-client               | Apache License 2.0                                                 |  | com.fasterxml.jackson.core jackson-core | Apache License 2.0                                                 |  | org.eclipse.jetty jetty-server          | Eclipse Public License 1.0 & Apache License 2.0                    |  | org.eclipse.jetty jetty-security        | Eclipse Public License 1.0 & Apache License 2.0                    |  | org.apache.jena apache-jena-libs        | Apache License 2.0                                                 |  | org.apache.jena jena-fuseki-embedded    | Apache License 2.0                                                 |  | com.github.bjdmeest hdt-java            | GNU Lesser General Public License v3.0                             |  | commons-validator commons-validator     | Apache License 2.0                                                 |  | com.github.fnoio grel-functions-java    | MIT                                                                |    ## Commercial Support    Do you need...    -   training?  -   specific features?  -   different integrations?  -   bugfixes, on _your_ timeline?  -   custom code, built by experts?  -   commercial support and licensing?    You're welcome to [contact us](mailto:info@rml.io) regarding  on-premise, enterprise, and internal installations, integrations, and deployments.    We have commercial support available.    We also offer consulting for all-things-RML.    ## Remarks    ### Typed spreadsheet files  All spreadsheet files are as of yet regarded as plain CSV files. No type information like Currency, Date... is used.    ### XML file parsing performance    The RMLMapper's XML parsing implementation (`javax.xml.parsers`) has been chosen to support full XPath.  This implementation causes a large memory consumption (up to ten times larger than the original XML file size).  However, the RMLMapper can be easily adapted to use a different XML parsing implementation that might be better suited for a specific use case.    ### Language tag support    The processor checks whether correct language tags are not, using a regular expression.  The regex has no support for languages of length 5-8, but this currently only applies to 'qaa..qtz'.    ### Duplicate removal and serialization format    Performance depends on the serialization format (`--serialization <format>`)  and if duplicate removal is enabled (`--duplicates`).  Experimenting with various configurations may lead to better performance for   your use case.    ### I have a question! Where can I get help?    Do you have any question related to writing RML mapping rules,   the RML specification, etc., feel free to ask them   here: https://github.com/kg-construct/rml-questions !  If you have found a bug or need a feature for the RMLMapper itself,   you can make an issue in this repository.    ## Documentation  Generate static files at /docs/apidocs with:  ```  mvn javadoc:javadoc  ```    ### UML Diagrams    #### Architecture UML Diagram  ##### How to generate with IntelliJ IDEA  (Requires Ultimate edition)    * Right click on package: ""be.ugent.rml""  * Diagrams > Show Diagram > Java Class Diagrams  * Choose what properties of the classes you want to show in the upper left corner  * Export to file > .png  | Save diagram > .uml    #### Sequence Diagram  ##### Edit on [draw.io](https://www.draw.io)  * Go to [draw.io](https://www.draw.io)  * Click on 'Open Existing Diagram' and choose the .html file    [1]: A. Dimou, T. De Nies, R. Verborgh, E. Mannens, P. Mechant, and R. Van de Walle, “Automated metadata generation for linked data generation and publishing workflows,” in Proceedings of the 9th Workshop on Linked Data on the Web, Montreal, Canada, 2016, pp. 1–10.  [PDF](http://events.linkeddata.org/ldow2016/papers/LDOW2016_paper_04.pdf) """
Semantic web;https://github.com/Swirrl/grafter;"""# Grafter - Linked Data & RDF Processing    [![Clojars Project](https://img.shields.io/clojars/v/grafter.svg)](https://clojars.org/grafter)        ""For the hard graft of linked data processing.""    Grafter is a [Clojure](http://clojure.org/) library for linked data  processing.  It is mature and under active development.    It provides support for all common RDF serialisations and  includes a library of functions for querying and writing to SPARQL  repositories.    ## FAQ    *Where can I find the api-docs?*    [Latest docs](http://swirrl.github.io/grafter)    Legacy docs [api.grafter.org](http://api.grafter.org/)    *Didn't grafter also contain tools for tabular processing?*    As of 0.9.0 the `grafter.tabular` library has been moved into a  [separate repository](https://github.com/Swirrl/grafter.tabular) so  the core grafter library can focus on processing linked data.    This part of the library is now considered deprecated.  If you depend  on it you can still use it, and it may receive occaisional  maintainance updates.    If you're looking to start a greenfield project then you can easily  wire up any capable CSV/excel parser to the RDF processing side of  grafter.    ## License    Copyright © 2014 Swirrl IT Ltd.    Distributed under the Eclipse Public License version 1.0, the same as  Clojure. """
Semantic web;https://github.com/lanthaler/HydraConsole;"""HydraConsole  ============    [Hydra][1] is a lightweight vocabulary to create hypermedia-driven Web APIs.  By specifying a number of commonly used concepts it renders the creation of  generic API clients possible. The HydraConsole is such a generic API client  in the form of a single-page web application.    For a high level description of how the HydraConsole works, please refer to  my dissertation  [Third Generation Web APIs—Bridging the Gap between REST and Linked Data][2].      Installation  ------------    At the moment, the HydraConsole uses a [JSON-LD Processor][3] and a proxy  written in PHP to access and process responses of Web APIs. Thus, the  simplest way to install the HydraConsole is to use [Composer][4].    If you don't have Composer yet, download it following the instructions on  http://getcomposer.org/ or just run the following command:        curl -s http://getcomposer.org/installer | php    Then, use Composer's `create-project` command to download the HydraConsole  and install all it's dependencies:        php composer.phar create-project -s dev ml/hydra-console path/to/install    You can now serve the HydraConsole with PHP's built-in web server:        php -S localhost:8000 -t path/to/install    That's it. Just fire up your browser and point it to        http://localhost:8000      Collaboration  ------------    To participate in the development please file bugs and issues in the  issue tracker or submit pull requests. If there's enough interest I'll  create a dedicated mailing list in the future.    You can find more information about Hydra and a demo installation of the  HydraConsole on my homepage: http://www.markus-lanthaler.com/hydra/      [1]: http://www.markus-lanthaler.com/hydra/  [2]: http://m.lanthi.com/3gen-web-apis-p171  [3]: http://m.lanthi.com/json-ld  [4]: http://getcomposer.org/ """
Semantic web;https://github.com/tkurz/sesame-vocab-builder;"""# Sesame Vocabulary Builder    Sesame Vocabulary Builder provides a command line tool and maven plugin that allows to create constants for RDF primitives for a given namespace from RDF ontology files.    ## How To    1. Download the latest version [here](https://github.com/tkurz/sesame-vocab-builder/releases).  1. Run jar from command line (Java 7 required): `java -jar vocab-builder-cli-{VERSION}-exe.jar <input-file> [<output-file>]`  1. Additional information can be configured using command-line parameters    ## Command Line Options    ```    <input-file>                            the input file to read from    [<output-file>]                         the output file to write, StdOut if                                            omitted    -b,--languageBundles                    generate L10N LanguageBundles    -c,--constantCase <constantCase>        case to use for URI constants,                                            possible values: LOWER_UNDERSCORE,                                            LOWER_CAMEL, UPPER_CAMEL,                                            UPPER_UNDERSCORE    -C,--stringConstantCase <constantCase>  case to use for String constants, see                                            constantCase    -f,--format <input-format>              mime-type of the input file (will try                                            to guess if absent)    -h,--help                               print this help    -l,--language <prefLang>                preferred language for vocabulary                                            labels    -n,--name <ns>                          the name of the namespace (will try to                                            guess from the input file if absent)    -P,--stringConstantPrefix <prefix>      prefix to create string constants                                            (e.g. _)    -p,--package <package>                  package declaration (will use default                                            (empty) package if absent)    -s,--spaces <indent>                    use spaces for indentation (tabs if                                            missing, 4 spaces if no number given)    -S,--stringConstantSuffix <suffix>      suffix to create string constants                                            (e.g. _STRING)    -u,--uri <prefix>                       the prefix for the vocabulary (if not                                            available in the input file)  ```    ## Run from Git    1. Clone from https://github.com/tkurz/sesame-vocab-builder.git  1. Run `./sesame-vocab-builder  <input-file> <output-file>`  1. Additional information can be configured using command-line parameters    ## Maven Plugin    ```xml  <build>      <plugins>          <plugin>              <groupId>com.github.tkurz.sesame</groupId>              <artifactId>vocab-builder-maven-plugin</artifactId>              <version>1.3</version>              <executions>                  <execution>                      <id>generate-vocabularies</id>                      <phase>generate-sources</phase>                      <goals>                          <goal>generate</goal>                      </goals>                  </execution>              </executions>              <configuration>                  <output>${project.build.directory}/generated-sources/sesame-vocabs</output>                  <packageName>com.example.sesame.vocabularies</packageName>                  <mimeType>text/turtle</mimeType>                  <preferredLanguage>en</preferredLanguage>                  <createResourceBundles>true</createResourceBundles>                  <constantCase>UPPER_UNDERSCORE</constantCase>                  <createStringConstants>true</createStringConstants>                  <stringConstantCase>UPPER_UNDERSCORE</stringConstantCase>                  <stringConstantPrefix>_</stringConstantPrefix>                  <stringConstantSuffix>_STRING</stringConstantSuffix>                  <vocabularies>                      <vocabulary>                          <className>LDP</className>                          <prefix>http://www.w3.org/ns/ldp#</prefix>                          <file>sesame-vocab-builder-core/src/test/resources/ldp.ttl</file>                      </vocabulary>                      <vocabulary>                          <className>RDF</className>                          <url>http://www.w3.org/1999/02/22-rdf-syntax-ns</url>                      </vocabulary>                  </vocabularies>              </configuration>          </plugin>      </plugins>  </build>  ``` """
Semantic web;https://github.com/jpcik/morph-starter;"""morph-starter  =============    Getting started with morph. this project is a simple Java (and Scala) demo of how to use morph.    Currently it shows how to generate RDF data from relational databases, using an [R2RML](http://www.w3.org/TR/r2rml/) mapping.    **Requirements**  * Java7  * Sbt 0.13 (or maven)    **Running**    To run the example, download the code and run Sbt:    ```  >sbt run  ```    It will run the main method that has a configured small HSQLDB memory database, and uses predefined mappings.  The RDF is output to the console.   You can check the `DemoQueryJava` file to tweak and change whaterver you want.    The script to create the test DB (.sql) and the mappings (.r2rml) are is in `src/main/resources/data`.   The database jdbc config is in `src/main/resources/application.conf`    **Eclipse**    If you want to use Eclipse, you can type the following to generate the .project files:  ```  sbt eclipse  ```    This will generate the necessary Eclipse project files, classpath dependencies, etc. Then you can import the project in your Eclipse installation.  If you plan to use Scala we recommend installing the Scala IDE plugin.    **Maven**    If you prefer to use maven instead of sbt, there is a pom.xml file available. Otherwise you can just ignore its existence.  You can compile the code as usual: `mvn compile`, import it to Eclipse using the m2e plugin, etc. """
Semantic web;https://github.com/nichtich/wdq;"""# NAME    wdq - command line access to Wikidata Query Service    # STATUS    [![Build Status](https://travis-ci.org/nichtich/wdq.png)](https://travis-ci.org/nichtich/wdq)  [![Coverage Status](https://coveralls.io/repos/nichtich/App-wdq/badge.png)](https://coveralls.io/r/nichtich/App-wdq)  [![Kwalitee Score](http://cpants.cpanauthors.org/dist/App-wdq.png)](http://cpants.cpanauthors.org/dist/App-wdq)    # SYNOPSIS    Access [Wikidata Query Service](https://query.wikidata.org/) via command line  to perform SPARQL queries (`query` mode), lookup entities (`lookup`), or  search items and properties (`search` or `psearch`):        wdq -g en solar system        # search 'solar system' in English      wdq psearch -g es parte       # search property 'parte' in Spanish      wdq P361 Q544                 # lookup properties and items      wdq '?c wdt:P361 wd:Q544'     # query parts of the solar system    See the manual for details or get help via `wdq help`:        wdq help options              # list and explain command line options      wdq help modes                # list and explain request modes      wdq help output               # explain output control      wdq help formats              # list and explain output formats      wdq help ontology             # show Wikidata ontology in a nutshell      wdq help prefixes             # list RDF prefixes allowed in queries      wdq help version              # show version of wdq    # DESCRIPTION    The command line script `wdq`, included in CPAN module [App::wdq](https://metacpan.org/pod/App::wdq), provides a  tool to access [Wikidata Query Service](https://query.wikidata.org/). It  supports formulation and execution of [SPARQL SELECT  queries](http://www.w3.org/TR/sparql11-query/#select) to extract selected  information from Wikidata or other Wikibase instances.    # INSTALLATION    Perl should already installed at most operating systems. Otherwise  [get Perl!](https://www.perl.org/get.html)    ## FROM CPAN    Install sources from CPAN including all dependencies:        cpanm App::wdq    First [install cpanm](https://github.com/miyagawa/cpanminus/#installation) if  missing. If installation of `App::wdq` fails try cpanm option `--notest` or  install dependencies as packages as described below.    ## PREBUILD PACKAGES    Install dependencies as prebuild packages for your operating system:        # Debian based systems e.g. Ubuntu (>= 14.04)      sudo apt-get install libhttp-tiny-perl librdf-query-perl        # Windows/ActiveState      ppm install HTTP-Tiny      ppm install RDF-Query    Then install `wdq` from CPAN as described above or copy the script to some  place in your `$PATH`:        wget https://raw.githubusercontent.com/nichtich/wdq/main/script/wdq      chmod +x wdq    The latter method will not install this documentation.    # MODES    Request mode `query` (default), `lookup`, `serch`, or `psearch` can  explicitly be set via first argument or it's guessed from arguments.    ## query    Read SPARQL query from STDIN, option `--query|-q`, or argument. Namespace  definitions and `SELECT` clause are added if missing.        wdq '?c wdt:P361 wd:Q544'      wdq '{ ?c wdt:P361 wd:Q544 }'                 # equivalent      wdq 'SELECT * WHERE { ?c wdt:P361 wd:Q544 }'  # equivalent      wdq < queryfile    ## lookup    Read Wikidata entity ids, URLs, or Wikimedia project URLs from STDIN or  arguments. Result fields are `label`, `description`, and `id`:        wdq Q1      wdq lookup Q1                                 # equivalent      echo Q1 | wdq lookup                          # equivalent      wdq http://de.wikipedia.org/wiki/Universum    # same result    ## search / psearch    Search for items or properties. Result fields are `label`, `id`,  `description`, and possibly matched `alias`. Search and result language is  read from environment or option `--language`/`-g`:        wdq search -g sv Pippi Långstrump    Default output format in search mode is `text`.    # OPTIONS    - --query|-q QUERY        Query or query file (`-` for STDIN as default)    - --format|-f FORMAT|TEMPLATE        Output format or string template. Call `wdq help formats` for details.    - --export EXPORTER        Use a [Catmandu](https://metacpan.org/pod/Catmandu) exporter as output format.    - --no-header|-H        Exclude header in CSV output or other exporter.    - --enumerate|-e        Enumerate results by adding a counter variable `n`    - --limit INTEGER        Add or override a LIMIT clause to limitate the number of results. Single-digit      options such as `-1` can also be used to also set a limit.    - --ids|-i        Abbreviate Wikidata identifier URIs as strings.    - --language|-g        Language to query labels and descriptions in. Set to the locale by default.      This option is currentl only used on lookup mode.    - --count|-c VARS        Prepend SPARQL QUERY to count distinct values    - --label|-l VARS  - --description|-d VARS  - --text|-t VARS        Add label, description, or both. Adds `label`/`description` for variable `id`      or `xLabel`/`xDescription` for any `x`.    - --ignore        Ignore empty results instead of issuing warning and exit code.    - --color|-C        By default output is colored if writing to a terminal. Disable this with      `--no-color`, `--monochrome`, or `-M`. Force color with `--color` or `-C`.    - --api URL        SPARQL endpoint. Default value:      `https://query.wikidata.org/bigdata/namespace/wdq/sparql`    - --no-mediawiki|-m        Don't query MediaWiki API to map URLs to Wikidata items.    - --no-execute|-n        Don't execute SPARQL queries but show them in expanded form. Useful to      validate and pretty-print queries. MediaWiki API requests may be    - -N        Don't execute any queries. Same as `--no-mediawiki --no-execute`.    - --help|-h|-?        Show usage help    - --ontology        Show information about the Wikidata Ontology    - --no-default-prefixes        Don't add default namespace prefixes to the SPARQL query    - --man        Show detailled manual    - --version|-V        Show version if this script    # OUTPUT    Output can be controlled with options `--format`/`-f`, `--export`,  `--header`/`--no-header`/`-H`, and `--color`/`--no-color`/`-C`.    ## Formats    Option `--format`/`-f` sets an output format or string template:    - `simple` (default in query and lookup mode)        Flat JSON without language tags    - `text` (default in search mode)        Print `label`, `alias`, `id` and `description` or `count` when counting.      Also sets option `--ids`.    - `ldjson`        Line delimited Flat JSON    - `csv`        SPARQL Query Results CSV Format. Suppress header with option      `--no-header`/`-H`.  Use Catmandu CSV exporter for more options    - `tsv`        SPARQL Query Results TSV Format    - `xml`        SPARQL Query Results XML Format    - `json`        SPARQL Query Results JSON Format    - `...`        String template.  Call `wdq help pretty` for details    ## Pretty    Option `--format` can be set to a string template with bracket expressions  with optional template parameters (for instance `{id|pre= (|post=)}`).    - style        Highlight `n` name, `v` value, `i` identifier, `t` title, or `e` error    - length        Abbreviate long values    - align        Use `left` or `right` to align short values to a given `length`    - pre/post        Add string before/after value    ## Export    Option `--export` sets a [Catmandu](https://metacpan.org/pod/Catmandu) exporter to create output with.  Given  the corresponding exporter modules installed, one can write results as `YAML`,  Excel (`XLS`), and Markdown table (`Table`) among other formats:        wdq --export YAML                               # short form      wdq --format ldjson | catmandu convert to YAML  # equivalent    Use Catmandu config file (`catmandu.yml`) to further configure export.  See  also tools such as [jq](http://stedolan.github.io/jq/) and  [miller](http://johnkerl.org/miller/) for processing results.    # EXAMPLES        # search ""solar system"" in English (=> Q544)      wdq -g en solar system        # search part-of property (=> P361)      wdq psearch -g en part        # get all parts of the solar system      wdq '?c wdt:P361 wd:Q544'        # look up label and description      wdq Q42 P9        # look up German Wikipedia article and get label description in French      wdq -g fr http://de.wikipedia.org/wiki/Argon        # get all references used at an item      wdq 'wd:Q1 ?prop [ prov:wasDerivedFrom ?ref ]'        # get doctoral advisor graph (academic genealogy) as CSV      wdq '?student wdt:P184 ?advisor' --ids --format csv        # print expanded SPARQL query      wdq -n '?c wdt:P361 wd:Q544'        # execute query and return first 10 tab-separated values      wdq -f tsv --limit 10 < query        # print result as Markdown Table (requires Catmandu::Exporter::Table)      wdq --export Table < query        # count instances (P31) of books (Q571)      wdq --count x '?x wdt:P31 wd:Q571'        # list types (P279) of Exoplanets (Q44559) with label and description      wdq '?id wdt:P279 wd:Q44559:' --text id --format text    # WIKIDATA ONTOLOGY        Entity (item/property)       wd:Q* <-- owl:sameAs --> wd:Q*             --> rdfs:label, skos:altLabel, schema:description ""*""@*             --> schema:dateModified, schema:version             --> wdt:P* ""*"", URI, _:blank             --> p:P* Statement        Item       wd:Q* <-- schema:about <http://*.wikipedia.org/wiki/*>                                --> schema:inLanguage, wikibase:badge        Property       wd:P* --> wikibase:propertyType PropertyType             --> wkibase:directClaim        wdt:P*             --> wikibase:claim             p:P*             --> wikibase:statementProperty ps:P*             --> wikibase:statementValue    psv:P*             --> wikibase:qualifier         pq:P*             --> wikibase:qualifierValue    pqv:P*             --> wikibase:reference         pr:P*             --> wikibase:referenceValue    prv:P*             --> wikibase:novalue           wdno:P*        PropertyType       wikibase: String, Url, WikibaseItem, WikibaseProperty, CommonsMedia, Math,                 Monolingualtext, GlobeCoordinate, Quantity, Time, ExternalId          Statement       wds:* --> wikibase:rank Rank             --> a wdno:P*             --> ps:P* ""*"", URI, _:blank             --> psv:P* Value             --> pq:P* ""*"", URI, _:blank             --> pqv:P* Value             --> prov:wasDerivedFrom Reference        Reference       wdref:* --> pr:P* ""*"", URI               --> prv:P* Value        Rank       wikibase: NormalRank, PreferredRank, DeprecatedRank, BestRank        Value (GlobecoordinateValue/QuantityValue/TimeValue)       wdv:* --> wikibase: geoLatitude, geoLongitude, geoPrecision, geoGlobe URI             --> wikibase: timeValue, timePrecision, timeTimezone, timeCalendarModel             --> wikibase: quantityAmount, quantityUpperBound, quantityLowerBound,                           quantityUnit URI    # COPYRIGHT AND LICENSE    Copyright by Jakob Voss `voss@gbv.de`    Based on a PHP script by Marius Hoch `hoo@online.de`  at [https://github.com/mariushoch/asparagus](https://github.com/mariushoch/asparagus).    Licensed under GPL 2.0+ """
Semantic web;https://github.com/dgarijo/Widoco;"""# WIzard for DOCumenting Ontologies (WIDOCO)  [![DOI](https://zenodo.org/badge/11427075.svg)](https://zenodo.org/badge/latestdoi/11427075) [![](https://jitpack.io/v/dgarijo/Widoco.svg)](https://jitpack.io/#dgarijo/Widoco) [![Project Status: Active – The project has reached a stable, usable state and is being actively developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)    ![Logo](src/main/resources/logo/logo2.png)    WIDOCO helps you to publish and create an enriched and customized documentation of your ontology automatically, by following a series of steps in a GUI.    **Author**: Daniel Garijo Verdejo (@dgarijo)    **Contributors**: María Poveda, Idafen Santana, Almudena Ruiz, Miguel Angel García, Oscar Corcho, Daniel Vila, Sergio Barrio, Martin Scharm, Maxime Lefrancois, Alfredo Serafini, @kartgk, Pat Mc Bennett, Christophe Camel, Jacobus Geluk, Martin Scharm, @rpietzsch, Jonathan Leitschuh, Jodi Schneider, Giacomo Lanza, Alejandra Gonzalez-Beltran, Mario Scrocca, Miguel Angel García, Flores Bakker and @JohnnyMoonlight.    **Citing WIDOCO**: If you used WIDOCO in your work, please cite the ISWC 2017 paper: https://iswc2017.semanticweb.org/paper-138    ```bib  @inproceedings{garijo2017widoco,    title={WIDOCO: a wizard for documenting ontologies},    author={Garijo, Daniel},    booktitle={International Semantic Web Conference},    pages={94--102},    year={2017},    organization={Springer, Cham},    doi = {10.1007/978-3-319-68204-4_9},    funding = {USNSF ICER-1541029, NIH 1R01GM117097-01},    url={http://dgarijo.com/papers/widoco-iswc2017.pdf}  }  ```  If you want to cite the latest version of the software, you can do so by using: https://zenodo.org/badge/latestdoi/11427075.    ## Downloading the executable    To download WIDOCO, you need to download a JAR executable file. Check the latest release for more details: (https://github.com/dgarijo/WIDOCO/releases/latest).    ## Importing WIDOCO as a dependency  Just add the dependency and repository to your `pom.xml` file as follows. See the [WIDOCO JitPack](https://jitpack.io/#dgarijo/Widoco) page to find alternative means to incorporate WIDOCO to your project.    ```xml  <dependencies>    <dependency>        <groupId>com.github.dgarijo</groupId>        <artifactId>Widoco</artifactId>        <version>v1.4.16</version>    </dependency>  </dependencies>    [ ... ]    <repositories>  	<repository>  	    <id>jitpack.io</id>  	    <url>https://jitpack.io</url>  	</repository>  </repositories>  ```    ## Description  WIDOCO helps you to publish and create an enriched and customized documentation of your ontology, by following a series of steps in a wizard. We extend the LODE framework by Silvio Peroni to describe the classes, properties and data properties of the ontology, the OOPS! webservice by María Poveda to print an evaluation and the Licensius service by Victor Rodriguez Doncel to determine the license URI and title being used. In addition, we use WebVowl to visualize the ontology and have extended Bubastis to show a complete changelog between different versions of your ontology.    Features of WIDOCO:  * Automatic documentation of the terms in your ontology (based on [LODE](http://www.essepuntato.it/lode/)). Now you can use Markdown on your class descriptions (see [example](doc/gallery/index.html))  * Automatic annotation in JSON-LD snippets of the html produced.  * Association of a provenance page which includes the history of your vocabulary (W3C PROV-O compliant).  * Metadata extraction from the ontology plus the means to complete it on the fly when generating your ontology. Check the [best practice document](http://dgarijo.github.io/Widoco/doc/bestPractices/index-en.html) to know more about the terms recognized by WIDOCO.  * Guidelines on the main sections that your document should have and how to complete them.  * Integration with diagram creators ([WebVOWL](http://vowl.visualdataweb.org/webvowl/)).  * Automatic changelog of differences between the actual and the previous version of the ontology (based on [Bubastis](http://www.ebi.ac.uk/efo/bubastis/)).  * Separation of the sections of your html page so you can write them independently and replace only those needed.  * Content negotiation and serialization of your ontology according to [W3C best practices](https://www.w3.org/TR/swbp-vocab-pub/)  * Evaluation reports of your ontology (using the [OOPS! web service](http://oops.linkeddata.es/))  * Integration with license metadata services ([Licensius](http://licensius.com/)) to automatically describe the license used in your ontology.    ## Examples  Examples of the features of WIDOCO can be seen on [the gallery](http://dgarijo.github.io/Widoco/doc/gallery/)    ## GUI Tutorial  A tutorial explaining the main features of the GUI can be found [here](http://dgarijo.github.io/Widoco/doc/tutorial/)        ## How to use WIDOCO    ### JAR execution    Download the latest `.jar` [WIDOCO available release](https://github.com/dgarijo/WIDOCO/releases/latest) (it will be something like `widoco-VERSION-jar-with-dependencies.jar`). Then just double click the `.jar` file.    You may also execute WIDOCO through the command line. Usage:  ```bash  java -jar widoco-VERSION-jar-with-dependencies.jar [OPTIONS]  ```    ### Docker execution    First build the image using the `Dockerfile` in project folder:    ```bash  docker build -t dgarijo/widoco .  ```    You can now execute WIDOCO through the command line. Usage:    ```bash  docker run -ti --rm dgarijo/widoco [OPTIONS]  ```    If you want to share data between the Docker Container and your Host, for instance to load a local ontology file (from PATH), you will need to mount the container  with host directories. For instance:    ```bash  docker run -ti --rm \    -v `pwd`/test:/usr/local/widoco/in \    -v `pwd`/target/generated-doc:/usr/local/widoco/out \    dgarijo/widoco -ontFile in/bne.ttl -outFolder out -rewriteAll  ```    ### Options    `-ontFile PATH`  [required (unless -ontURI is used)]: Load a local ontology file (from PATH) to document. This option is incompatible with -ontURI    `-ontURI  URI`   [required (unless -ontFile is used)]: Load an ontology to document from its URI. This option is incompatible with -ontFile    `-outFolder folderName`: Specifies the name of the folder where to save the documentation. By default is 'myDocumentation'    `-confFile PATH`: Load your own configuration file for the ontology metadata. Incompatible with -getOntologyMetadata    `-getOntologyMetadata`: Extract ontology metadata from the given ontology    `-oops`: Create an html page with the evaluation from the OOPS service (http://oops.linkeddata.es/)    `-rewriteAll`: Replace any existing files when documenting an ontology (e.g., from a previous execution)    `-crossRef`: ONLY generate the overview and cross reference sections. The index document will NOT be generated. The htaccess, provenance page, etc., will not be generated unless requested by other flags. This flag is intended to be used only after a first version of the documentation exists.    `-saveConfig PATH`: Save a configuration file on PATH with the properties of a given ontology    `-useCustomStyle`: Export the documentation using alternate css files (by Daniel Vila).    `-lang LANG1-LANG2`: Generate documentation in multiple languages (separated by ""-""). Note that if the language is not supported, the system will load the labels in english. For example: en-pt-es    `-includeImportedOntologies`: Indicates whether the terms of the imported ontologies of the current ontology should be documented as well or not.    `-htaccess`: Create a bundle for publication ready to be deployed on your Apache server.    `-webVowl`: Create a visualization based on WebVowl (http://vowl.visualdataweb.org/webvowl/index.html#) in the documentation.    `-licensius`: Use the Licensius web services (http://licensius.com/apidoc/index.html) to retrieve license metadata. Only works if the -getOntologyMetadata  flag is enabled.    `-ignoreIndividuals`: Individuals will not be included in the documentation.    `-includeAnnotationProperties`: Include annotation properties defined in your ontology in the documentation (by default they are not included)    `-analytics CODE`: Add a code snippet for Google analytics to track your HTML documentation. You need to add your CODE next to the flag. For example: UA-1234    `-doNotDisplaySerializations`: The serializations of the ontology will not be displayed.    `-displayDirectImportsOnly`: Only those imported ontologies that are directly imported in the ontology being documented.    `-rewriteBase PATH`: Change the default rewrite base path. The default value is ""/"". This flag can only be used with the htaccess option.    `-excludeIntroduction`: Skip the introduction section in the documentation.    `-uniteSections`: Write all HTML sections into a single HTML document.    `-noPlaceHolderText`: Do not add any placeholder text (this will remove intro, abstract (if empty) and description sections).    `--help`: Shows a help message and exits.      ## How can I make WIDOCO automatically recognize my vocabulary annotations?  There are two alternative ways for making WIDOCO get your vocabulary metadata annotations and use them automatically to document the ontology.    * The recommended way: add them in your OWL file. For guidelines on which ones to include, follow our [best practices document](https://w3id.org/widoco/bestPractices), which indicates which ones we recommend.  * Alternatively, edit the project properties of /config/config.properties. This is a key-value pair file with metadata properties. Some people consider it easier than adding the property annotations to the OWL file, although I recommend doing the former option. Note that the character "";"" is used for lists (for instance first author; second author; third author).    ## Browser issues (Why can't I see the generated documentation / visualization?)  WIDOCO separates the contents of different sections in HTML files, which are then loaded in the `index.html` file. WIDOCO was designed this way because it's easier to edit your introduction or description sections independently without being all aggregated together in a huge HTML document.  **When all the contents generated by WIDOCO are stored in a server, you will be able to see the documentation of your ontology using any browser**. However, if you open the `index.html` file **on your local browser**, you may see a document missing most of the sections in your documentation. This happens because browsers don't allow loading separate content when opening a file locally for security reasons. If you want to explore how your ontology would look locally, you have two options:    * a) Execute WIDOCO with the `-uniteSections` flag; or select the option `add al sections in a single document` in the ""load sections"" step in the WIDOCO GUI. This will make all the sections of WIDOCO to be in the `index.html`; and you will be able to see it in your browser. Note that the **LODE visualization will not be available** when exploring your ontology locally.  * b) Create a local server: Set up a local server (e.g., using XAMPP or Tomcat) and serve the files WIDOCO generates (in the `htdocs` folder for Apache servers).    If you place the files generated by WIDOCO in a server and access them via its URL (for example, a Github page), you should be able to see your documentation appropriately.    ## Current improvements  For a complete list of the current improvements and next features, check the [project open issues](https://github.com/dgarijo/Widoco/issues) and [milestones](https://github.com/dgarijo/Widoco/milestones) in the repository.    ## Requirements  You will need Java 1.8 or higher (SDK 1.8 or JRE 8) for WIDOCO to work  Otherwise, you will probably experience an ""Unsupported major.minor version 52.0"" exception when executing the JAR file.    ## Contribution guidelines  Contributions to address any of the current issues are welcome. In order to push your contribution, just **push your pull request to the develop branch**. The master branch has only the code associated to the latest release. """
Semantic web;https://github.com/SANSA-Stack/SANSA-RDF;"""# SANSA-Stack  <!-- [![Maven Central](https://maven-badges.herokuapp.com/maven-central/net.sansa-stack/sansa-parent/badge.svg)](https://maven-badges.herokuapp.com/maven-central/net.sansa-stack/sansa-parent) -->  [![Build Status](https://github.com/SANSA-Stack/SANSA-Stack/workflows/CI/badge.svg)](https://github.com/SANSA-Stack/SANSA-Stack/actions?query=workflow%3ACI)  [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)  [![Twitter](https://img.shields.io/twitter/follow/SANSA_Stack.svg?style=social)](https://twitter.com/SANSA_Stack)    This project comprises the whole Semantic Analytics Stack (SANSA). At a glance, it features the following functionality:    * Ingesting RDF and OWL data in various formats into RDDs  * Operators for working with RDDs and data frames of RDF data at various levels (triples, bindings, graphs, etc)  * *Transformation* of RDDs to data frames and *partitioning* of RDDs into R2RML-mapped data frames  * Distributed SPARQL querying over R2RML-mapped data frame partitions using RDB2RDF engines (Sparqlify & Ontop)  * Enrichment of RDDs with inferences  * Application of machine learning algorithms    For a detailed description of SANSA, please visit http://sansa-stack.net.     ## Layers  The SANSA project is structured in the following five layers developed in their respective sub-folders:    * [RDF](sansa-rdf)  * [OWL](sansa-owl)  * [Query](sansa-query)  * [Inference](sansa-inference)  * [ML](sansa-ml)    ## Release Cycle  A SANSA stack release is done every six months and consists of the latest stable versions of each layer at this point. This repository is used for organising those joint releases.    ## Usage    ### Spark    #### Requirements    We currently require a Spark 3.x.x with Scala 2.12 setup. A Spark 2.x version can be built from source based on the [spark2](https://github.com/SANSA-Stack/SANSA-Stack/tree/spark2) branch.    #### Release Version  Some of our dependencies are not in Maven central (yet), so you need to add following Maven repository to your project POM file `repositories` section:  ```xml  <repository>     <id>maven.aksw.internal</id>     <name>AKSW Release Repository</name>     <url>http://maven.aksw.org/archiva/repository/internal</url>     <releases>        <enabled>true</enabled>     </releases>     <snapshots>        <enabled>false</enabled>     </snapshots>  </repository>  ```    If you want to import the full SANSA Stack, please add the following Maven dependency to your project POM file:  ```xml  <!-- SANSA Stack -->  <dependency>     <groupId>net.sansa-stack</groupId>     <artifactId>sansa-stack-spark_2.12</artifactId>     <version>$LATEST_RELEASE_VERSION$</version>  </dependency>  ```  If you only want to use particular layers, just replace `$LAYER_NAME$` with the corresponding name of the layer  ```xml  <!-- SANSA $LAYER_NAME$ layer -->  <dependency>     <groupId>net.sansa-stack</groupId>     <artifactId>sansa-$LAYER_NAME$-spark_2.12</artifactId>     <version>$LATEST_RELEASE_VERSION$</version>  </dependency>  ```    #### SNAPSHOT Version  While the release versions are available on Maven Central, latest SNAPSHOT versions have to be installed from source code:  ```bash  git clone https://github.com/SANSA-Stack/SANSA-Stack.git  cd SANSA-Stack  ```  Then to build and install the full SANSA Spark stack you can do  ```bash  ./dev/mvn_install_stack_spark.sh   ```  or for a single layer `$LAYER_NAME$` you can do  ```bash  mvn -am -DskipTests -pl :sansa-$LAYER_NAME$-spark_2.12 clean install   ```    Alternatively, you can use the following Maven repository and add it to your project POM file `repositories` section:  ```xml  <repository>     <id>maven.aksw.snapshots</id>     <name>AKSW Snapshot Repository</name>     <url>http://maven.aksw.org/archiva/repository/snapshots</url>     <releases>        <enabled>false</enabled>     </releases>     <snapshots>        <enabled>true</enabled>     </snapshots>  </repository>  ```  Then do the same as for the release version and add the dependency:  ```xml  <!-- SANSA Stack -->  <dependency>     <groupId>net.sansa-stack</groupId>     <artifactId>sansa-stack-spark_2.12</artifactId>     <version>$LATEST_SNAPSHOT_VERSION$</version>  </dependency>  ```    ## How to Contribute  We always welcome new contributors to the project! Please see [our contribution guide](http://sansa-stack.net/contributing-to-sansa/) for more details on how to get started contributing to SANSA. """
Semantic web;https://github.com/anapsid/anapsid;"""ANAPSID  =======    An adaptive query processing engine for SPARQL endpoints.    [1] Maribel Acosta, Maria-Esther Vidal, Tomas Lampo, Julio Castillo,  Edna Ruckhaus: ANAPSID: An Adaptive Query Processing Engine for SPARQL  Endpoints. International Semantic Web Conference (1) 2011: 18-34    [2] Gabriela Montoya, Maria-Esther Vidal, Maribel Acosta: A  Heuristic-Based Approach for Planning Federated SPARQL Queries. COLD  2012    Installing ANAPSID  ==================    ANAPSID is known to run on Debian GNU/Linux and OS X. These instructions were tested   on the latest Debian Stable and OS X. The recommended way to  execute ANAPSID is to use Python 2.7.       1. Download ANAPSID.       You can do this by cloning this repository using Git.       `$ git clone https://github.com/anapsid/anapsid.git ~/anapsid`          OR       You can download the latest release from Github [here](https://github.com/anapsid/anapsid/releases)     2. Go to your local copy of ANAPSID and run:       `$ pip install -r requirements.txt`       This will install ANAPSID's Python dependencies. Right now, the only library required to execute ANAPSID is ply 3.3 (https://pypi.python.org/pypi/ply/3.3)    3. When step 2 is done you can now install ANAPSID. This will install     it only to your current user caged VirtualEnv as to prevent     polluting Python's global site-packages.       `$ python setup.py install`    4. Go ahead and move to the next section on configuring ANAPSID.    Setting up ANAPSID  ==================    Running ANAPSID depends on a endpoint description file. This file  describes each endpoint URL and the predicates this endpoint  handles. ANAPSID comes bundled with a helper script to generate your  endpoints descriptions as to prevent errors.    1. Create a file, e.g. endpointsURLs.txt, with the URLs of your     endpoints, one per line.    2. Run the script. It will contact each endpoint and retrieve their     predicates, so it might take a while. This will save your endpoint     descriptions on endpointsDescriptions.txt       `$ get_predicates endpointsURLs.txt endpointsDescriptions.txt`    3. You are ready to run ANAPSID.    About supported endpoints  ------------------------    ANAPSID currently supports endpoints that answer queries either on XML  or JSON. Expect hard failures if you intend to use ANAPSID on  endpoints that answer in any other format.    Running ANAPSID  ===============    Once you have installed ANAPSID and retrieved endpoint descriptions,  you can run ANAPSID using our run_anapsid script.    `$ run_anapsid`    It will output a usage text and the options switches you can  select. We run our experiments, however, using the scripts bundled on  utils/ so you might want to check that out to get an idea.    ANAPSID Parameters  ------------------  Alternatively,  you can execute the following command to run a given query with ANAPSID:    ```  $python $ANAPSIDROOT/run_anapsid -e $ENDPOINTS -q $query -p <planType> -s False   -o False -d <TypeofDecompostion> -a True -w False [-k <special>]  [-V <typeOfEndpoint>] -r False  ```    Where:    `$ANAPSIDROOT`: directory where ANAPSID is stored.    `$ENDPOINTS`: path and name of the file where the description of the endpoints is stored.    `$query`: path and name of the filw where the query is stored.    `<planType>`: can be **b** if the plan is bushy, **ll** is the plan is left linear, and **naive** for naive binary tree plan.     `-o`: can be True or False. **True** indicates that the input query is in SPARQL1-1 and no decomposition is needed; **False**, otherwise.    `-d`: indicates the type of Decomposition. <TypeofDecompostion> can be **SSGM** (Star Shaped  Group Multiple Endpoints), **SSGS** (Star Shaped Group Single Endpoint), **EG** (Exclusive Groups), Recommended SSGM.    `-a`: indicates if the adaptive operators will be used. Recommended value **True**.    `-w`:  can be True or False. Indicates if the cardinality of the queries will be estimated by contacting the sources (**True**) or by using a cost model (**False**). Turning True this feature may affect execution time.    `-w`: can be y or c. The value **y** indicates that the plan will be produced, while **c** asks that decomposition. This parameter is optional, and should be set up only if the plan of the query wants to be produced.     `-r`: can be True or False. Use **True** if the answer of the query will be output and **False** if only a summary of the execution will be produced.     `-V`: can be True or False. **True** indicates if the endpoints to contact are Virtuoso, **False** is of any other type, e.g., OWLIM.      Included query decomposing heuristics  =====================================    We include three heuristics used for decomposing queries to be  evaluated by a federation of endpoints. These are:    1. Exclusive Groups (EG).  2. Star-Shaped Group Single endpoint selection (SSGS). See [2].  3. Star-Shaped Group Multiple endpoint selection (SSGM). See [2].     Running FedBench with ANAPSID  =============================  FedBench (see http://fedbench.fluidops.net) is a benchmark for testing federated query processing on RDF data sets.    In order to execute ANAPSID, it is necessary first to provide the endpoint descriptions. Endpoint descriptions are of the form `<URLEndpoint> <LISTOfPredicates>`. The file endpoints/endpointsFedBench provides   the description of the endpoints of the dataset collections in FedBench. The current URLs of the endpoints  have to be included as follows:  ```   <http://URLnytimes_dataset/sparql> URL of the NYTime endpoint   <http://URLchebi_dataset/sparql> URL of the Chebi endpoint   <http://URLSWDF_dataset/sparql> URL of the SW Dog Food endpoint   <http://URLdrugbank_dataset/sparql> URL of the Drugbank endpoint   <http://URLjamendo_dataset/sparql> URL of the Jamendo endpoint   <http://URLkegg_dataset/sparql> URL of the Kegg endpoint   <http://URLlinkedmdb_dataset/sparql> URL of the LinkedMDB endpoint   <http://URLSP2B/sparql> URL of the SP^2Bench 10M endpoint   <http://URLgeonames/sparql> URL of the Geonames endpoint  ```    The FedBench queries (see http://fedbench.fluidops.net/resource/Queries) are also available in the folder queries/fedbBench.      About and Contact  =================    ANAPSID was developed at  [Universidad Simón Bolívar](http://www.usb.ve) as an ongoing academic effort. You  can contact the current maintainers by email at mvidal[at]ldc[dot]usb[dot]ve.    We strongly encourage you to please report any issues you have with  ANAPSID. You can do that over our contact email or creating a new  issue here on Github.    - Simón Castillo: scastillo [at] ldc [dot] usb [dot] ve  - Guillermo Palma: gpalma [at] ldc [dot] usb [dot] ve  - Maria-Esther Vidal: mvidal [at] ldc [dot] usb [dot] ve  - Gabriela Montoya: Gabriela [dot] Montoya [at] univ-nantes [dot] fr  - Maribel Acosta: maribel [dot] acosta [at] kit [dot] edu      License  =======    This work is licensed under [GNU/GPL v2](https://www.gnu.org/licenses/gpl-2.0.html). """
Semantic web;https://github.com/RDFLib/pySHACL;"""![](pySHACL-250.png)    # pySHACL  A Python validator for SHACL.    [![Build Status](https://drone.rdflib.ashs.dev/api/badges/RDFLib/pySHACL/status.svg)](https://drone.rdflib.ashs.dev/RDFLib/pySHACL)    [![DOI](https://zenodo.org/badge/147505799.svg)](https://zenodo.org/badge/latestdoi/147505799) [![Downloads](https://pepy.tech/badge/pyshacl)](https://pepy.tech/project/pyshacl) [![Downloads](https://pepy.tech/badge/pyshacl/month)](https://pepy.tech/project/pyshacl/month) [![Downloads](https://pepy.tech/badge/pyshacl/week)](https://pepy.tech/project/pyshacl/week)    This is a pure Python module which allows for the validation of [RDF](https://www.w3.org/2001/sw/wiki/RDF) graphs against Shapes Constraint Language ([SHACL](https://www.w3.org/TR/shacl/)) graphs. This module uses the [rdflib](https://github.com/RDFLib/rdflib) Python library for working with RDF and is dependent on the [OWL-RL](https://github.com/RDFLib/OWL-RL) Python module for [OWL2 RL Profile](https://www.w3.org/TR/owl2-overview/#ref-owl-2-profiles) based expansion of data graphs.    This module is developed to adhere to the SHACL Recommendation:  > Holger Knublauch; Dimitris Kontokostas. *Shapes Constraint Language (SHACL)*. 20 July 2017. W3C Recommendation. URL: <https://www.w3.org/TR/shacl/> ED: <https://w3c.github.io/data-shapes/shacl/>    # Community for Help and Support  The SHACL community has a discord server for discussion of topics around SHACL and the SHACL specification.    [Use this invitation link: https://discord.gg/RTbGfJqdKB to join the server](https://discord.gg/RTbGfJqdKB)    There is a \#pyshacl channel in which discussion around this python library can held, and you can ask for general pyshacl help too.    ## Installation  Install with PIP (Using the Python3 pip installer `pip3`)  ```bash  $ pip3 install pyshacl  ```    Or in a python virtualenv _(these example commandline instructions are for a Linux/Unix based OS)_  ```bash  $ python3 -m virtualenv --python=python3 --no-site-packages .venv  $ source ./.venv/bin/activate  $ pip3 install pyshacl  ```    To exit the virtual enviornment:  ```bash  $ deactivate  ```    ## Command Line Use  For command line use:  _(these example commandline instructions are for a Linux/Unix based OS)_  ```bash  $ pyshacl -s /path/to/shapesGraph.ttl -m -i rdfs -a -j -f human /path/to/dataGraph.ttl  ```  Where   - `-s` is an (optional) path to the shapes graph to use   - `-e` is an (optional) path to an extra ontology graph to import   - `-i` is the pre-inferencing option   - `-f` is the ValidationReport output format (`human` = human-readable validation report)   - `-m` enable the meta-shacl feature   - `-a` enable SHACL Advanced Features   - `-j` enable SHACL-JS Features (if `pyhsacl[js]` is installed)    System exit codes are:  `0` = DataGraph is Conformant  `1` = DataGraph is Non-Conformant  `2` = The validator encountered a RuntimeError (check stderr output for details)  `3` = Not-Implemented; The validator encountered a SHACL feature that is not yet implemented.    Full CLI Usage options:  ```bash  $ pyshacl -h  $ python3 -m pyshacl -h  usage: pyshacl [-h] [-s [SHACL]] [-e [ONT]] [-i {none,rdfs,owlrl,both}] [-m]                 [-im] [-a] [-j] [-it] [--abort] [--allow-infos] [-w] [-d]                 [-f {human,table,turtle,xml,json-ld,nt,n3}]                 [-df {auto,turtle,xml,json-ld,nt,n3}]                 [-sf {auto,turtle,xml,json-ld,nt,n3}]                 [-ef {auto,turtle,xml,json-ld,nt,n3}] [-V] [-o [OUTPUT]]                 DataGraph    PySHACL 0.18.1 command line tool.    positional arguments:    DataGraph             The file containing the Target Data Graph.    optional arguments:    -h, --help            show this help message and exit    -s [SHACL], --shacl [SHACL]                          A file containing the SHACL Shapes Graph.    -e [ONT], --ont-graph [ONT]                          A file path or URL to a document containing extra                          ontological information to mix into the data graph.    -i {none,rdfs,owlrl,both}, --inference {none,rdfs,owlrl,both}                          Choose a type of inferencing to run against the Data                          Graph before validating.    -m, --metashacl       Validate the SHACL Shapes graph against the shacl-                          shacl Shapes Graph before validating the Data Graph.    -im, --imports        Allow import of sub-graphs defined in statements with                          owl:imports.    -a, --advanced        Enable features from the SHACL Advanced Features                          specification.    -j, --js              Enable features from the SHACL-JS Specification.    -it, --iterate-rules  Run Shape's SHACL Rules iteratively until the                          data_graph reaches a steady state.    --abort               Abort on first invalid data.    --allow-infos         Shapes marked with severity of Info will not cause                          result to be invalid.    -w, --allow-warnings  Shapes marked with severity of Warning or Info will                          not cause result to be invalid.    -d, --debug           Output additional runtime messages.    -f {human,table,turtle,xml,json-ld,nt,n3}, --format {human,table,turtle,xml,json-ld,nt,n3}                          Choose an output format. Default is ""human"".    -df {auto,turtle,xml,json-ld,nt,n3}, --data-file-format {auto,turtle,xml,json-ld,nt,n3}                          Explicitly state the RDF File format of the input                          DataGraph file. Default=""auto"".    -sf {auto,turtle,xml,json-ld,nt,n3}, --shacl-file-format {auto,turtle,xml,json-ld,nt,n3}                          Explicitly state the RDF File format of the input                          SHACL file. Default=""auto"".    -ef {auto,turtle,xml,json-ld,nt,n3}, --ont-file-format {auto,turtle,xml,json-ld,nt,n3}                          Explicitly state the RDF File format of the extra                          ontology file. Default=""auto"".    -V, --version         Show PySHACL version and exit.    -o [OUTPUT], --output [OUTPUT]                          Send output to a file (defaults to stdout).  ```    ## Python Module Use  For basic use of this module, you can just call the `validate` function of the `pyshacl` module like this:    ```python  from pyshacl import validate  r = validate(data_graph,        shacl_graph=sg,        ont_graph=og,        inference='rdfs',        abort_on_first=False,        allow_infos=False,        allow_warnings=False,        meta_shacl=False,        advanced=False,        js=False,        debug=False)  conforms, results_graph, results_text = r  ```    Where:  * `data_graph` is an rdflib `Graph` object or file path of the graph to be validated  * `shacl_graph` is an rdflib `Graph` object or file path or Web URL of the graph containing the SHACL shapes to validate with, or None if the SHACL shapes are included in the data_graph.  * `ont_graph` is an rdflib `Graph` object or file path or Web URL a graph containing extra ontological information, or None if not required.  * `inference` is a Python string value to indicate whether or not to perform OWL inferencing expansion of the `data_graph` before validation.  Options are 'rdfs', 'owlrl', 'both', or 'none'. The default is 'none'.  * `abort_on_first` (optional) `bool` value to indicate whether or not the program should abort after encountering the first validation failure or to continue. Default is to continue.  * `allow_infos` (optional) `bool` value, Shapes marked with severity of Info will not cause result to be invalid.  * `allow_warnings` (optional) `bool` value, Shapes marked with severity of Warning or Info will not cause result to be invalid.  * `meta_shacl` (optional) `bool` value to indicate whether or not the program should enable the Meta-SHACL feature. Default is False.  * `advanced`: (optional) `bool` value to enable SHACL Advanced Features  * `js`: (optional) `bool` value to enable SHACL-JS Features (if `pyshacl[js]` is installed)  * `debug` (optional) `bool` value to indicate whether or not the program should emit debugging output text, including violations that didn't lead to non-conformance overall. So when debug is True don't judge conformance by absense of violation messages. Default is False.    Some other optional keyword variables available on the `validate` function:  * `data_graph_format`: Override the format detection for the given data graph source file.  * `shacl_graph_format`: Override the format detection for the given shacl graph source file.  * `ont_graph_format`: Override the format detection for the given extra ontology graph source file.  * `iterate_rules`: Interate SHACL Rules until steady state is found (only works with advanced mode).  * `do_owl_imports`: Enable the feature to allow the import of subgraphs using `owl:imports` for the shapes graph and the ontology graph. Note, you explicitly cannot use this on the target data graph.  * `serialize_report_graph`: Convert the report results_graph into a serialised representation (for example, 'turtle')  * `check_dash_result`: Check the validation result against the given expected DASH test suite result.  * `check_sht_result`: Check the validation result against the given expected SHT test suite result.    Return value:  * a three-component `tuple` containing:    * `conforms`: a `bool`, indicating whether or not the `data_graph` conforms to the `shacl_graph`    * `results_graph`: a `Graph` object built according to the SHACL specification's [Validation Report](https://www.w3.org/TR/shacl/#validation-report) structure    * `results_text`: python string representing a verbose textual representation of the [Validation Report](https://www.w3.org/TR/shacl/#validation-report)      ## Python Module Call    You can get an equivalent of the Command Line Tool using the Python3 executable by doing:    ```bash  $ python3 -m pyshacl  ```      ## Errors  Under certain circumstances pySHACL can produce a `Validation Failure`. This is a formal error defined by the SHACL specification and is required to be produced as a result of specific conditions within the SHACL graph.  If the validator produces a `Validation Failure`, the `results_graph` variable returned by the `validate()` function will be an instance of `ValidationFailure`.  See the `message` attribute on that instance to get more information about the validation failure.    Other errors the validator can generate:  - `ShapeLoadError`: This error is thrown when a SHACL Shape in the SHACL graph is in an invalid state and cannot be loaded into the validation engine.  - `ConstraintLoadError`: This error is thrown when a SHACL Constraint Component is in an invalid state and cannot be loaded into the validation engine.  - `ReportableRuntimeError`: An error occurred for a different reason, and the reason should be communicated back to the user of the validator.  - `RuntimeError`: The validator encountered a situation that caused it to throw an error, but the reason does concern the user.    Unlike `ValidationFailure`, these errors are not passed back as a result by the `validate()` function, but thrown as exceptions by the validation engine and must be  caught in a `try ... except` block.  In the case of `ShapeLoadError` and `ConstraintLoadError`, see the `str()` string representation of the exception instance for the error message along with a link to the relevant section in the SHACL spec document.      ## Windows CLI    [Pyinstaller](https://www.pyinstaller.org/) can be  [used](https://pyinstaller.readthedocs.io/en/stable/usage.html) to create an  executable for Windows that has the same characteristics as the Linux/Mac  CLI program.  The necessary ``.spec`` file is already included in ``pyshacl/pyshacl-cli.spec``.  The ``pyshacl-cli.spec`` PyInstaller spec file creates a ``.exe`` for the  pySHACL Command Line utility. See above for the pySHACL command line util usage instructions.    See [the PyInstaller installation guide](https://pyinstaller.readthedocs.io/en/stable/installation.html#installing-in-windows) for info on how to install PyInstaller for Windows.    Once you have pyinstaller, use pyinstaller to generate the ``pyshacl.exe`` CLI file like so:  ```bash powershell  $ cd src/pyshacl  $ pyinstaller pyshacl-cli.spec  ```  This will output ``pyshacl.exe`` in the ``dist`` directory in ``src/pyshacl``.    You can now run the pySHACL Command Line utility via ``pyshacl.exe``.  See above for the pySHACL command line util usage instructions.      ## Compatibility  PySHACL is a Python3 library. For best compatibility use Python v3.7 or greater. Python3 v3.6 or below is _**not supported**_ and this library _**does not work**_ on Python v2.7.x or below.    PySHACL is now a PEP518 & PEP517 project, it uses `pyproject.toml` and `poetry` to manage dependencies, build and install.    For best compatibility when installing from PyPI with `pip`, upgrade to pip v18.1.0 or above.    - If you're on Ubuntu 16.04 or 18.04, you will need to run `sudo pip3 install --upgrade pip` to get the newer version.      ## Features  A features matrix is kept in the [FEATURES file](https://github.com/RDFLib/pySHACL/blob/master/FEATURES.md).      ## Changelog  A comprehensive changelog is kept in the [CHANGELOG file](https://github.com/RDFLib/pySHACL/blob/master/CHANGELOG.md).      ## Benchmarks  This project includes a script to measure the difference in performance of validating the same source graph that has been inferenced using each of the four different inferencing options. Run it on your computer to see how fast the validator operates for you.      ## License  This repository is licensed under Apache License, Version 2.0. See the [LICENSE deed](https://github.com/RDFLib/pySHACL/blob/master/LICENSE.txt) for details.      ## Contributors  See the [CONTRIBUTORS file](https://github.com/RDFLib/pySHACL/blob/master/CONTRIBUTORS.md).      ## Citation  DOI: [10.5281/zenodo.4750840](https://doi.org/10.5281/zenodo.4750840) (For all versions/latest version)    ## Contacts  Project Lead:  **Nicholas Car**  *Senior Experimental Scientist*  CSIRO Land & Water, Environmental Informatics Group  Brisbane, Qld, Australia  <nicholas.car@csiro.au>  <http://orcid.org/0000-0002-8742-7730>    Lead Developer:  **Ashley Sommer**  *Informatics Software Engineer*  CSIRO Land & Water, Environmental Informatics Group  Brisbane, Qld, Australia  <Ashley.Sommer@csiro.au>  <https://orcid.org/0000-0003-0590-0131> """
Semantic web;https://github.com/ropensci/jsonld;"""# jsonld    > JSON for Linking Data    [![Project Status: Active – The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)  [![Build Status](https://travis-ci.org/ropensci/jsonld.svg?branch=master)](https://travis-ci.org/ropensci/jsonld)  [![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/ropensci/jsonld?branch=master&svg=true)](https://ci.appveyor.com/project/jeroen/jsonld)  [![Coverage Status](https://codecov.io/github/ropensci/jsonld/coverage.svg?branch=master)](https://codecov.io/github/ropensci/jsonld?branch=master)  [![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/jsonld)](https://cran.r-project.org/package=jsonld)  [![CRAN RStudio mirror downloads](http://cranlogs.r-pkg.org/badges/jsonld)](https://cran.r-project.org/package=jsonld)  [![Github Stars](https://img.shields.io/github/stars/ropensci/jsonld.svg?style=social&label=Github)](https://github.com/ropensci/jsonld)      JSON-LD is a light-weight syntax for expressing linked data. It is primarily  intended for web-based programming environments, interoperable web services and for   storing linked data in JSON-based databases. This package provides bindings to the   JavaScript library for converting, expanding and compacting JSON-LD documents.    ## Hello World        Example from https://github.com/digitalbazaar/jsonld.js#quick-examples. Example data:      ```r  doc <- '{    ""http://schema.org/name"": ""Manu Sporny"",    ""http://schema.org/url"": {""@id"": ""http://manu.sporny.org/""},    ""http://schema.org/image"": {""@id"": ""http://manu.sporny.org/images/manu.png""}  }'    context <- '{    ""name"": ""http://schema.org/name"",    ""homepage"": {""@id"": ""http://schema.org/url"", ""@type"": ""@id""},    ""image"": {""@id"": ""http://schema.org/image"", ""@type"": ""@id""}  }'  ```    ### Compact and expand:      ```r  (out <- jsonld_compact(doc, context))  ```    ```  {    ""@context"": {      ""name"": ""http://schema.org/name"",      ""homepage"": {        ""@id"": ""http://schema.org/url"",        ""@type"": ""@id""      },      ""image"": {        ""@id"": ""http://schema.org/image"",        ""@type"": ""@id""      }    },    ""image"": ""http://manu.sporny.org/images/manu.png"",    ""name"": ""Manu Sporny"",    ""homepage"": ""http://manu.sporny.org/""  }   ```    ```r  (expanded <- jsonld_expand(out))  ```    ```  [    {      ""http://schema.org/url"": [        {          ""@id"": ""http://manu.sporny.org/""        }      ],      ""http://schema.org/image"": [        {          ""@id"": ""http://manu.sporny.org/images/manu.png""        }      ],      ""http://schema.org/name"": [        {          ""@value"": ""Manu Sporny""        }      ]    }  ]   ```    ### Convert between JSON and RDF:      ```r  cat(nquads <- jsonld_to_rdf(doc))  ```    ```  _:b0 <http://schema.org/image> <http://manu.sporny.org/images/manu.png> .  _:b0 <http://schema.org/name> ""Manu Sporny"" .  _:b0 <http://schema.org/url> <http://manu.sporny.org/> .  ```    ```r  jsonld_from_rdf(nquads)  ```    ```  [    {      ""@id"": ""_:b0"",      ""http://schema.org/image"": [        {          ""@id"": ""http://manu.sporny.org/images/manu.png""        }      ],      ""http://schema.org/name"": [        {          ""@value"": ""Manu Sporny""        }      ],      ""http://schema.org/url"": [        {          ""@id"": ""http://manu.sporny.org/""        }      ]    }  ]   ```    ### Other utilities:      ```r  jsonld_flatten(doc)  ```    ```  [    {      ""@id"": ""_:b0"",      ""http://schema.org/image"": [        {          ""@id"": ""http://manu.sporny.org/images/manu.png""        }      ],      ""http://schema.org/name"": [        {          ""@value"": ""Manu Sporny""        }      ],      ""http://schema.org/url"": [        {          ""@id"": ""http://manu.sporny.org/""        }      ]    }  ]   ```    ```r  cat(jsonld_normalize(doc, algorithm = 'URDNA2015', format = 'application/nquads'))  ```    ```  _:c14n0 <http://schema.org/image> <http://manu.sporny.org/images/manu.png> .  _:c14n0 <http://schema.org/name> ""Manu Sporny"" .  _:c14n0 <http://schema.org/url> <http://manu.sporny.org/> .  ```   """
Semantic web;https://github.com/earthquakesan/fox-py;"""fox-py  ======    Python bindings for FOX - Federated Knowledge Extraction Framework    For installing to virtualenv:  ```  pip install -e git+https://github.com/earthquakesan/fox-py#egg=fox-py  ``` """
Semantic web;https://github.com/wallix/triplestore;"""[![Build Status](https://api.travis-ci.org/wallix/triplestore.svg?branch=master)](https://travis-ci.org/wallix/triplestore)  [![Go Report Card](https://goreportcard.com/badge/github.com/wallix/triplestore)](https://goreportcard.com/report/github.com/wallix/triplestore)  [![GoDoc](https://godoc.org/github.com/wallix/triplestore?status.svg)](https://godoc.org/github.com/wallix/triplestore)     # Triple Store    Triple Store is a library to manipulate RDF triples in a fast and fluent fashion.    RDF triples allow to represent any data and its relations to other data. It is a very versatile concept and is used in [Linked Data](https://en.wikipedia.org/wiki/Linked_data), graphs traversal and storage, etc....    Here the RDF triples implementation follows along the [W3C RDF concepts](https://www.w3.org/TR/rdf11-concepts/). (**Note that reification is not implemented**.). More digestible info on [RDF Wikipedia](https://en.wikipedia.org/wiki/Resource_Description_Framework)    ## Features overview    - Create and manage triples through a convenient DSL  - Snapshot and query RDFGraphs  - **Binary** encoding/decoding  - **Lenient NTriples** encoding/decoding (see W3C Test suite in _testdata/ntriples/w3c_suite/_)  - [DOT](https://en.wikipedia.org/wiki/DOT_(graph_description_language)) encoding  - Stream encoding/decoding (for binary & NTriples format) for memory conscious program   - CLI (Command line interface) utility to read and convert triples files.    ## Library     This library is written using the [Golang](https://golang.org) language. You need to [install Golang](https://golang.org/doc/install) before using it.    Get it:    ```sh  go get -u github.com/wallix/triplestore  ```    Test it:    ```  go test -v -cover -race github.com/wallix/triplestore  ```    Bench it:    ```  go test -run=none -bench=. -benchmem  ```    Import it in your source code:    ```go  import (  	""github.com/wallix/triplestore""  	// tstore ""github.com/wallix/triplestore"" for less verbosity  )  ```    Get the CLI with:    ```  go get -u github.com/wallix/triplestore/cmd/triplestore  ```    ## Concepts    A triple is made of 3 components:        subject -> predicate -> object    ... or you can also view that as:        entity -> attribute -> value    So    - A **triple** consists of a *subject*, a *predicate* and a *object*.  - A **subject** is a unicode string.  - A **predicate** is a unicode string.  - An **object** is a *resource* (or IRI) or a *literal* (blank node are not supported).  - A **literal** is a unicode string associated with a datatype (ex: string, integer, ...).  - A **resource**, a.k.a IRI, is a unicode string which point to another resource.    And    - A **source** is a persistent yet mutable source or container of triples.  - A **RDFGraph** is an **immutable set of triples**. It is a snapshot of a source and queryable .  - A **dataset** is a basically a collection of *RDFGraph*.    You can also view the library through the [godoc](https://godoc.org/github.com/wallix/triplestore)    ## Usage    #### Create triples    Although you can build triples the way you want to model any data, they are usually built from known RDF vocabularies & namespace. Ex: [foaf](http://xmlns.com/foaf/spec/), ...    ```go  triples = append(triples,  	SubjPred(""me"", ""name"").StringLiteral(""jsmith""),   	SubjPred(""me"", ""age"").IntegerLiteral(26),   	SubjPred(""me"", ""male"").BooleanLiteral(true),   	SubjPred(""me"", ""born"").DateTimeLiteral(time.Now()),   	SubjPred(""me"", ""mother"").Resource(""mum#121287""),  )  ```    or dynamically and even shorter with    ```go  triples = append(triples,   	SubjPredLit(""me"", ""age"", ""jsmith""), // String literal object   	SubjPredLit(""me"", ""age"", 26), // Integer literal object   	SubjPredLit(""me"", ""male"", true), // Boolean literal object   	SubjPredLit(""me"", ""born"", time.now()) // Datetime literal object   	SubjPredRes(""me"", ""mother"", ""mum#121287""), // Resource object  )  ```    or with blank nodes and language tag in literal    ```go  triples = append(triples,   	SubjPred(""me"", ""name"").Bnode(""jsmith""),   	BnodePred(""me"", ""name"").StringLiteral(""jsmith""),   	SubjPred(""me"", ""name"").StringLiteralWithLang(""jsmith"", ""en""),  )  ```    #### Create triples from a struct    As a convenience you can create triples from a singular struct, where you control embedding through bnode.    Here is an example.    ```go  type Address struct {  	Street string `predicate:""street""`  	City   string `predicate:""city""`  }    type Person struct {  	Name     string    `predicate:""name""`  	Age      int       `predicate:""age""`  	Size     int64     `predicate:""size""`  	Male     bool      `predicate:""male""`  	Birth    time.Time `predicate:""birth""`  	Surnames []string  `predicate:""surnames""`  	Addr     Address   `predicate:""address"" bnode:""myaddress""` // empty bnode value will make bnode value random  }    addr := &Address{...}  person := &Person{Addr: addr, ....}    tris := TriplesFromStruct(""jsmith"", person)    src := NewSource()  src.Add(tris)  snap := src.Snapshot()    snap.Contains(SubjPredLit(""jsmith"", ""name"", ""...""))  snap.Contains(SubjPredLit(""jsmith"", ""size"", 186))  snap.Contains(SubjPredLit(""jsmith"", ""surnames"", ""...""))  snap.Contains(SubjPredLit(""jsmith"", ""surnames"", ""...""))  snap.Contains(SubjPred(""me"", ""address"").Bnode(""myaddress""))  snap.Contains(BnodePred(""myaddress"", ""street"").StringLiteral(""5th avenue""))  snap.Contains(BnodePred(""myaddress"", ""city"").StringLiteral(""New York""))  ```    #### Equality    ```go  	me := SubjPred(""me"", ""name"").StringLiteral(""jsmith"")   	you := SubjPred(""me"", ""name"").StringLiteral(""fdupond"")     	if me.Equal(you) {   	 	...   	}  )  ```    ### Triple Source    A source is a persistent yet mutable source or container of triples    ```go  src := tstore.NewSource()    src.Add(  	SubjPredLit(""me"", ""age"", ""jsmith""),  	SubjPredLit(""me"", ""born"", time.now()),  )  src.Remove(SubjPredLit(""me"", ""age"", ""jsmith""))  ```    ### RDFGraph    A RDFGraph is an immutable set of triples you can query. You get a RDFGraph by snapshotting a source:    ```go  graph := src.Snapshot()    tris := graph.WithSubject(""me"")  for _, tri := range tris {  	...  }  ```    ### Codec    Triples can be encoded & decoded using either a simple binary format or more standard text format like NTriples, ...    Triples can therefore be persisted to disk, serialized or sent over the network.    For example    ```go  enc := NewBinaryEncoder(myWriter)  err := enc.Encode(triples)  ...    dec := NewBinaryDecoder(myReader)  triples, err := dec.Decode()  ```    Create a file of triples under the lenient NTriples format:    ```go  f, err := os.Create(""./triples.nt"")  if err != nil {  	return err  }  defer f.Close()    enc := NewLenientNTEncoder(f)  err := enc.Encode(triples)    ```     Encode to a DOT graph  ```go  tris := []Triple{          SubjPredRes(""me"", ""rel"", ""you""),          SubjPredRes(""me"", ""rdf:type"", ""person""),          SubjPredRes(""you"", ""rel"", ""other""),          SubjPredRes(""you"", ""rdf:type"", ""child""),          SubjPredRes(""other"", ""any"", ""john""),  }    err := NewDotGraphEncoder(file, ""rel"").Encode(tris...)  ...    // output  // digraph ""rel"" {  //  ""me"" -> ""you"";  //  ""me"" [label=""me<person>""];  //  ""you"" -> ""other"";  //  ""you"" [label=""you<child>""];  //}  ```    Load a binary dataset (i.e. multiple RDFGraph) concurrently from given files:    ```go  path := filepath.Join(fmt.Sprintf(""*%s"", fileExt))  files, _ := filepath.Glob(path)    var readers []io.Reader  for _, f := range files {  	reader, err := os.Open(f)  	if err != nil {  		return g, fmt.Errorf(""loading '%s': %s"", f, err)  	}  	readers = append(readers, reader)  }    dec := tstore.NewDatasetDecoder(tstore.NewBinaryDecoder, readers...)  tris, err := dec.Decode()  if err != nil {  	return err  }  ...  ```    ### triplestore CLI    This CLI is mainly ised for triples files conversion and inspection. Install it with `go get github.com/wallix/triplestore/cmd/triplestore`. Then `triplestore -h` for help.    Example of usage:    ```sh  triplestore -in ntriples -out bin -files fuzz/ntriples/corpus/samples.nt   triplestore -in ntriples -out bin -files fuzz/ntriples/corpus/samples.nt   triplestore -in bin -files fuzz/binary/corpus/samples.bin  ```    ### RDFGraph as a Tree    A tree is defined from a RDFGraph given:     * a specific predicate as an edge   * and considering triples pointing to RDF resource Object     You can then navigate the tree using the existing API calls    	tree := tstore.NewTree(myGraph, myPredicate)  	tree.TraverseDFS(...)  	tree.TraverseAncestors(...)  	tree.TraverseSiblings(...)    Have a look at the [godoc](https://godoc.org/github.com/wallix/triplestore) fro more info     Note that at the moment, constructing a new tree from a graph does not verify if the tree is valid namely no cycle and each child at most one parent. """
Semantic web;https://github.com/opensemanticsearch/solr-ontology-tagger;"""# solr-ontology-tagger  Automatic tagging of documents in an Apache Solr index for faceted search by RDF(S) Ontologies &amp; SKOS thesauri and converter for alternate labels in SKOS thesaurus to Solr synonyms config file. """
Semantic web;https://github.com/linkeddata/gold;"""# gold       [![](https://img.shields.io/badge/project-Solid-7C4DFF.svg?style=flat-square)](https://github.com/solid/solid)  [![Join the chat at https://gitter.im/linkeddata/gold](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/linkeddata/gold?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    `gold` is a reference Linked Data Platform server for the  **[Solid platform](https://github.com/solid/solid-spec)**.    Written in Go, based on  [initial work done by William Waites](https://bitbucket.org/ww/gold).    [![Build Status](https://travis-ci.org/linkeddata/gold.svg?branch=master)](https://travis-ci.org/linkeddata/gold)    ## Installing    ### From docker repository:    ```  sudo docker pull linkeddata/gold  sudo docker run -p ip:port:443 linkeddata/gold  ```  Replace `ip` and `port` with your host computer's IP address and port number.    To check the status of the container, type:    ```  sudo docker ps  ```    `IMPORTANT`: if you want to mount a host directory into the container, you can use the -v parameter:    ```  sudo docker run -p ip:port:443 -v /home/user/data:/data linkeddata/gold  ```    This will mount the host directory, `/home/user/data`, into the container as the `/data/` directory. Doing this will allow you to reuse the data directory without worrying about persistence inside the container.    ### From Github:    1. Setup Go:        * **Mac OS X**: `brew install go`      * **Ubuntu**: `sudo apt-get install golang-go`      * **Fedora**: `sudo dnf install golang`    1. Set the `GOPATH` variable (required by Go):          ```bash        mkdir ~/go        export GOPATH=~/go        ```              (Optionally consider adding `export GOPATH=~/go` to your `.bashrc` or profile).    1. Check that you have the required Go version (**Go 1.4 or later**):          ```        go version        ```              If you don't, please [install](http://golang.org/doc/install) a more recent        version.    1. Use the `go get` command to install the server and all the dependencies:        ```      go get github.com/linkeddata/gold/server      ```        1. Install dependencies:      * **Mac OS X**: `brew install raptor libmagic`      * **Ubuntu**: `sudo apt-get install libraptor2-dev libmagic-dev`      * **Fedora**: `sudo dnf install raptor2-devel file-devel`        1. (Optional) Install extra dependencies used by the tests:        ```      go get github.com/stretchr/testify/assert      ```    ## Running the Server    **IMPORTANT**: Among other things, `gold` is a web server. Please consider  running it as a regular user instead of root. Since gold treats all files  equally, and even though uploaded files are not made executable, it will not  prevent clients from uploading malicious shell scripts.    Pay attention to the data root parameter, `-root`. By default, it will serve  files from its current directory (so, for example, if you installed it from  Github, its data root will be `$GOPATH/src/github.com/linkeddata/gold/`).  Otherwise, make sure to pass it a dedicated data directory to serve, either  using a command-line parameter or the [config file](#configuration).  Something like: `-root=/var/www/data/` or `-root=~/data/`.    1. If you installed it from package via `go get`, you can run it by:      ```    $GOPATH/bin/server -http="":8080"" -https="":8443"" -debug    ```    2. When developing locally, you can `cd` into the repo cloned by `go get`:      ```    cd $GOPATH/src/github.com/linkeddata/gold    ```      And launch the server by:      ```    go run server/*.go -http="":8080"" -https="":8443"" -debug -boltPath=/tmp/bolt.db    ```      Alternatively, you can compile and run it from the source dir in one command:      ```    go run $GOPATH/src/github.com/linkeddata/gold/server/*.go -http="":8080"" -https="":8443"" \      -root=/home/user/data/ -debug -boltPath=/tmp/bolt.db    ```      ## Configuration    You can use the provided `gold.conf-example` file to create your own  configuration file, and specify it with the `-conf` parameter.    ```bash  cd $GOPATH/src/github.com/linkeddata/gold/  cp gold.conf-example server/gold.conf    # edit the configuration file  nano server/gold.conf    # pass the config file when launching the gold server  $GOPATH/bin/server -conf=$GOPATH/src/github.com/linkeddata/gold/server/gold.conf  ```    To see a list of available options:        ~/go/bin/server -help    Some important options and defaults:    * `-conf` - Optional path to a config file.    * `-debug` - Outputs config parameters and extra logging. Default: `false`.    * `-root` - Specifies the data root directory which `gold` will be serving.    Default: `.` (so, likely to be `$GOPATH/src/github.com/linkeddata/gold/`).    * `-http` - HTTP port on which the server listens. For local development,    the default HTTP port, `80`, is likely to be reserved, so pass in an    alternative. Default: `"":80""`. Example: `-http="":8080""`.    * `-https` - HTTPS port on which the server listens. For local development,    the default HTTPS port, `443`, is likely to be reserved, so pass in an    alternative. Default: `"":443""`. Example: `-https="":8443""`.    ## Testing  To run the unit tests (assuming you've installed `assert` via  `go get github.com/stretchr/testify/assert`):    ```  make test  ```    ## Notes    * HOWTO : [Get an example X.509 cert](https://gist.github.com/melvincarvalho/e14753a7137d02d756f19299fed292b4)  * HOWTO : [Login after getting a 401](https://gist.github.com/melvincarvalho/72eaff2fbf1b51a805846320e0bff0cc)  * HOWTO : [Recover an account](https://gist.github.com/melvincarvalho/bcc04e1529dd3a4509892346109b1d37)    ## License  [MIT](http://joe.mit-license.org/) """
Semantic web;https://github.com/BorderCloud/TFT;"""# TFT    TFT (Tester for Triplestore) is a script PHP to pass tests through a sparql endpoint.    # install JMeter for protocol tests  ```  wget http://mirrors.standaloneinstaller.com/apache//jmeter/binaries/apache-jmeter-5.4.1.tgz  tar xvzf apache-jmeter-5.4.1.tgz  mv  apache-jmeter-5.4.1 jmeter  rm apache-jmeter-5.4.1.tgz  ```    ## How to use it ?    You can read the doc here: https://bordercloud.github.io/tft-reports/    ## Usage with Travis Ci    Example of project with Travis Ci and TFT :    * [OpenLink Virtuoso version community 7/stable](https://github.com/BorderCloud/tft-virtuoso7-stable)  * [Blazegraph 2.1.5](https://github.com/BorderCloud/tft-blazegraph)  * [Jena-Fuseki 4.0.0](https://github.com/BorderCloud/tft-jena-fuseki)    ## Usage with Jenkins    Jenkins will be read the reports Junit/XML with this line :    ```  TFT/junit/*junit.xml  ```    ## Usage sparql-auth of Virtuoso  ```  git clone --recursive https://github.com/BorderCloud/TFT.git  cd TFT    #copie tests in a RDF database  ./tft-testsuite -a \                  -t virtuoso \                  -q 'http://database/sparql-auth/' \                  -u 'http://database/sparql-auth/' \                  -l LOGIN -p 'PASS'    #tests Virtuoso  ./tft  \        -t virtuoso \        -q 'http://database/sparql-auth/' \        -u 'http://database/sparql-auth/' \        -tt virtuoso \        -tq http://databasetotest/sparql/ \        -tu http://databasetotest/sparql/ \        -o ./junit \        -r https://marketplace.stratuslab.eu/marketplace/metadata/MvJPyzt00KDfRS-vM5gUEfhlr-R \        --softwareName=""Virtuoso Open-Source Edition""  --softwareDescribeTag=v7.1.1  --softwareDescribe=7.1.1-dev.3211-pthreads \        -l LOGIN -p 'PASSWORD'    #Calculate the score  ./tft-score \        -t virtuoso \        -q 'http://database/sparql-auth/' \        -u 'http://database/sparql-auth/' \        -r https://marketplace.stratuslab.eu/marketplace/metadata/MvJPyzt00KDfRS-vM5gUEfhlr-R \        -l LOGIN -p 'PASSWORD'  ```    ## Read the last score with SPARQL    Example :  ```  SELECT *  WHERE  {  	GRAPH ?graph {         ?service a sd:Service ;                 sd:server ?server ;                 sd:testedBy ?tester ;                 sd:testedDate ?LastDate.         ?server git:name ?serverName ;                 git:describeTag ?serverVersion ;                 git:describe ?serverVersionBuild .         ?tester  git:name ?testerName ;                 git:describeTag ?testerVersion  .  			   ?service sq:scoreTest ?score .  			   ?service sq:totalTest ?total .         }  FILTER(STR(xsd:date(?LastDate)) = STR(xsd:date(NOW())))  }  ```    ## License    TFT (c)2021 by Karima Rafes, BORDERCLOUD    TFT is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.    You should have received a copy of the license along with this work. If not, see http://creativecommons.org/licenses/by-sa/4.0/. """
Semantic web;https://github.com/swirrl/table2qb;"""# table2qb <img src=""https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Tesseract-1K.gif/240px-Tesseract-1K.gif"" align=""right"" height=""139"" alt=""tesseract animation""/>    [![Build Status](https://travis-ci.com/Swirrl/table2qb.svg?branch=master)](https://travis-ci.com/github/Swirrl/table2qb)    ## Build Statistical Linked-Data with CSV-on-the-Web    Create statistical linked-data by deriving CSV-on-the-Web annotations for your data tables using the [RDF Data Cube Vocabulary](https://www.w3.org/TR/vocab-data-cube/).    Build up a knowledge graph from spreadsheets without advanced programming skills or RDF modelling knowledge.    Simply prepare CSV inputs according to the templates and `table2qb` will output standards-compliant CSVW or RDF.    Once you're happy with the results you can adjust the configuration to tailor the URI patterns to your heart's content.    ## Turn Data Tables into Data Cubes     Table2qb expects three types of CSV tables as input:    - observations: a ['tidy data'](http://vita.had.co.nz/papers/tidy-data.pdf) table with one statistic per row (what the standard calls an _observation_)  - components: another table defining the columns used to describe observations (what the standard calls _component properties_ such as _dimensions_, _measures_, and _attributes_)  - codelists: a further set of tables that enumerate and describe the values used in cells of the observation table (what the standard calls _codes_, grouped into _codelists_)    For example, [the ONS says](https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/articles/overviewoftheukpopulation/january2021) that:    > In mid-2019, the population of the UK reached an estimated 66.8 million    This is a single observation value (66.8 million) with two dimensions (date and place) which respectively have two code values (mid-2019 and UK), a single measure (population estimate), and implicitly an attribute for the unit (people).    The [regional-trade example](https://github.com/Swirrl/table2qb/tree/master/examples/regional-trade) goes into more depth. The [colour-coded spreadsheet](./all-colour-coded.ods) should help illustrate how the three types of table come together to describe a cube.    Each of these inputs is processed by it's own pipeline which will output [CSVW](https://w3c.github.io/csvw/metadata/) - i.e. a processed version of the CSV table along with a JSON metadata annotation which describes the translation into RDF. Optionally you can also ask `table2qb` to perform the translation outputting RDF directly that can be loaded into a graph database and queried with SPARQL.    Table2qb also relies on a fourth CSV table for configuration:    - columns: this describes how the observations table should be interpreted - i.e. which components and codelists should be used for each column in the observation tables    This configuration is designed to be used for multiple data cubes across a data collection (so that you can re-use e.g. a ""Year"" column without having to configure anew it each time) to encourage harmonisation and alignment of identifiers.    Ultimately `table2qb` provides a foundation to help you build a collection of interoperable statistical linked open data.    ## Install table2qb    ### Github release    Download the release from [https://github.com/Swirrl/table2qb/releases](https://github.com/Swirrl/table2qb/releases).     Currently the latest is 0.3.0.    Once downloaded, unzip.  The main 'table2qb' executable is in the directory `./target/table2qb-0.3.0` You can add this directory to your `PATH` environment variable, or just run it with the full file path on your system.    To get help on the available commands, type `table2qb help`.    To see the available pipelines (described in more detail below), type `table2qb list`.    To see the required command structure for one of the pipelines (for example the cube-pipeline), type `table2qb describe cube-pipeline`    ### Clojure CLI    Clojure now distributes `clojure` and `cli` command-line programs for running clojure programs. To run `table2qb` through the `clojure` command, first  [install the Clojure CLI tools](https://clojure.org/guides/getting_started). Then create a file `deps.edn` containing the following:       **deps.edn**  ```clojure  {:deps {swirrl/table2qb {:git/url ""https://github.com/Swirrl/table2qb.git""                           :sha ""8c4b22778db0c160b06f2f3b0b3df064d8f8452b""}          org.apache.logging.log4j/log4j-api {:mvn/version ""2.11.0""}          org.apache.logging.log4j/log4j-core {:mvn/version ""2.11.0""}          org.apache.logging.log4j/log4j-slf4j-impl {:mvn/version ""2.11.0""}}   :aliases   {:table2qb    {:main-opts [""-m"" ""table2qb.main""]}}}  ```    You can then run `table2qb` using        clojure -A:table2qb        More details about the `clojure` CLI and the format of the `deps.edn` file can be found [on the Clojure website](https://clojure.org/reference/deps_and_cli)    ## Compiling table2qb    Table2qb is written in Clojure and can be built using [Leiningen](https://leiningen.org/). It is recommended you use [Java 8](https://www.oracle.com/technetwork/java/javase/overview/java8-2100321.html) or later.    `table2qb` can be run through `leiningen` with `lein run` e.g.        lein run list        Alternatively it can be run from an uberjar built with `lein uberjar`. The resulting .jar file in the `target` directory can then be run:        java -jar target/table2qb.jar list    ## How to run table2qb    See [using table2qb](doc/usage.md) for documentation on how to generate RDF data cubes using `table2qb`.    ## Example    The [./examples/employment](./examples/employment) directory provides an example of creating a data cube from scratch with `table2qb`.    ## License    Copyright © 2018 Swirrl IT Ltd.    Distributed under the Eclipse Public License either version 1.0 or (at your option) any later version.    ## Acknowledgements    The development of table2qb was funded by Swirrl, by the UK Office for National Statistics and by the European Union’s Horizon 2020 research and innovation programme under grant agreement No 693849 (the [OpenGovIntelligence](http://opengovintelligence.eu) project). """
Semantic web;https://github.com/ont-app/igraph;"""# <img src=""http://ericdscott.com/NaturalLexiconLogo.png"" alt=""NaturalLexicon logo"" :width=100 height=100/> ont-app/igraph    IGraph defines a protocol which aims to provide a general interface to  a variety of graph-based representations (RDF, datascript, datomic,  loom, ...)    It also defines a `Graph` datatype which implements `IGraph`.    There is a [15-minute video introduction here](https://www.youtube.com/watch?v=BlH__4iNHZE&amp;feature=youtu.be).    ## Contents  - [Dependencies](#h2-dependencies)  - [Motivation](#h2-motivation)  - [The IGraph protocol](#h2-igraph-protocol)    - [Methods summary](#h3-methods-summary)    - [Member access](#Member_access)      - [Normal form](#Normal_form)      - [Tractability](#h4-tractability)      - [`subjects`](#subjects_method)      - [`get-p-o`](#get-p-o_method)      - [`get-o`](#get-o_method)      - [`ask`](#ask_method)      - [`query`](#query_method)      - [`invoke` for arities 0-3](#invoke_method)    - [Content Manipulation](#Content_Manipulation)      - [`mutability`](#mutability_method)      - [The `add-to-graph` multimethod](#add-to-graph)      - [The `remove-from-graph` multimethod](#remove-from-graph)  - [The IGraphImmutable protocol](#IGraphImmutable)      - [`add`](#add_method)      - [`subtract`](#subtract_method)  - [The IGraphMutable protocol](#IGraphMutable)      - [`add!`](#add!_method)      - [`subtract!`](#subtract!_method)  - [The IGraphAccumulateOnly protocol](#IGraphAccumulateOnly)      - [`claim`](#claim_method)      - [`retract`](#retract_method)  - [The IGraphSet protocol](#h2-igraphset-protocol)    - [Methods summary](#h3-igraphset-methods-summary)    - [`union`](#union_method)    - [`intersection`](#intersection_method)    - [`difference`](#difference_method)  - [Traversal](#Traversal)    - [The `traverse` function](#traverse_function)    - [Traversal functions](#Traversal_functions)      - [Context](#h4-context)      - [The queue](#queue)    - [Traversal utilities](#Traversal_utilities)      - [`transitive-closure`](#h4-transitive-closure)      - [`traverse-link`](#h4-traverse-link)      - [`maybe-traverse-link`](#h4-maybe-traverse-link)      - [`traverse-or`](#h4-traverse-or)    - [Traversal composition with `t-comp`](#Traversal_composition)      - [short form](#h4-t-comp-short)      - [long form](#h4-t-comp-long)    - [Using traversal functions as a `p` argument to `invoke`](#traversal-fn-as-p)  - [Cardinality-1 utilites](#cardinality-1_utilities)    - [`unique`](#h3-unique)    - [`flatten-description`](#h3-flatten-description)    - [`normalize-flat-description`](#h3-normalize-flat-description)    - [`assert-unique`](#h3-assert-unique)  - [I/O](#i-o)    - [`write-to-file`](#h3-write-to-file)    - [`read-from-file`](#h3-read-from-file)  - [Other utilities](#Other_utilities)    - [`reduce-spo`](#h3-reduce-spo)  - [Implementations](#h2-implementations)    - [`ont-app.igraph.graph/Graph`](#Graph)      - [Graph creation](#h4-graph-creation)      - [Querying](#h4-querying)    - [sparql-client](#h3-sparql-client)    - [datascript-graph](#h3-datascript-graph)    - [datomic-client](#h3-datomic-client)  - [Acknowledgements](#h2-acknowledgements)  - [Future Work](#h2-future-work)  - [License](#h2-license)  ---    <a name=""h2-dependencies""></a>  ## Dependencies    This is deployed to [clojars](https://clojars.org/ont-app/igraph):    [![Clojars  Project](https://img.shields.io/clojars/v/ont-app/igraph.svg)](https://clojars.org/ont-app/igraph)    ```  [ont-app/igraph ""0.1.7""]  ```    Require thus:  ```  (:require     [ont-app.igraph.core :as igraph] ;; for the IGraph protocol and related stuff    [some.igraph.implementation ...] ;; implements IGraph    )               ```    <a name=""h2-motivation""></a>  ## Motivation    One of the defining characteristics of Clojure is that it revolves  around a minimal set of basic data structures.    I think it can be argued that the collection primitives in Clojure can  be approximately ordered by their degree of expressiveness:    - seq - first/rest  - < set - membership  - < vector - indexable members  - < map - a collection of associations    The conceit of IGraph is that there is room for a new collection  primitive with one higher level of expressiveness:    - < graph - a collection of named relationships between named entities    This is informed to a large degree by the  [RDF](https://www.wikidata.org/wiki/Q54872) model, and aims to align  with [linked data](https://www.wikidata.org/wiki/Q515701) encoded in  RDF, while keeping direct dependencies to a minimum.    <a name=""h2-igraph-protocol""></a>  ## The IGraph protocol    This protocol defines the basic operations over a graph conceived of  as a set of triples S-P-O, where subject `S` and object `O` typically  name entities, and property `P` is a named relation that applies  between those entities.    This is directly inspired by the RDF model, but the requirement that  these identifiers adhere strictly to RDF specifications for URIs, and  that literal values be restricted to a small set of scalars is relaxed  quite a bit.    <a name=""h3-methods-summary""></a>  ### Methods summary    The `IGraph` protocol specifies the following methods:    #### Member access  - `(normal-form g)` -> `{s {p #{o...}...}...}`  - `(subjects g)` -> `(s ...)`, a lazy sequence  - `(get-p-o g s)` -> `{p #{o...} ...}`  - `(get-o g s p)` -> `#{o ...}`  - `(ask g s p o)` -> truthy  - `(query g q)` -> implementation-dependent query results    #### Content manipulation  - `(mutability g)` -> One of `#{::read-only ::immutable ::mutable ::accumulate-only}`    #### `invoke` to support `IFn`  - `(g)` = `(normal-form g)`  - `(g s)` -> {p #{o...} ...}  - `(g s p)` -> #{o ...}  - `(g s p o)` -> truthy    <a name=""Member_access""></a>  ### Member access    <a name=""Normal_form""></a>  #### Normal form    Any implemetation of this protocol, regardless of its _native  representation_ must be expressable in IGraph's `Normal Form`.    As an example, let's start with a graph called 'eg' with four triples:    ```  > (igraph/normal-form eg)  {:john     {:isa #{:person},      :likes #{:beef}},   :mary    {:isa #{:person},     :likes #{:chicken}}}  >  ```    These are facts about two subjects, :john and :mary with two facts  each.    John is a person who likes beef.    Mary is also a person, and likes chicken.      The normal form has three tiers. The ""s-level"" is a map from each  subject to a ""p-level"" `description` of that subject.  The normal form  for descriptions is a map from a property identifier to an ""o-level""  set of objects for said subject and property.    What I'm aiming for here is a form that's  - extremely regular and simple  - lends itself to expressing and thinking about basic set operations  on graphs.    ##### A note on the keyword identifiers used in these examples    To keep things simple and readable, none of the keywords used in these  examples are [namespaced](https://blog.jeaye.com/2017/10/31/clojure-keywords/#namespaced-keywords).     In practice you will probably want to used namespaced keywords, and  some implementations of IGraph, e.g. those that interact directly with  RDF-based representations, will expect them.      <a name=""h4-tractability""></a>  #### Tractability    It is expected that while many implementations of IGraph will be  in-memory data structures of modest size, others might be huge  knowledge bases provided on a server somewhere  ([Wikidata](https://www.wikidata.org/wiki/Q2013), for example). In the  latter case it is always acceptable throw an `::igraph/Intractable`  for any method that warrants it:    ```  (throw (ex-info ""Normal form for Wikidata is intractable""     {:type ::igraph/Intractable}))    ```      <a name=""subjects_method""></a>  #### `subjects`    The `subjects` method must return a lazy sequence of complete set of  subjects in the graph (modulo tractability):    ```  > (igraph/subjects eg)  `(:john :mary)  > (type (igraph/subjects eg))  clojure.lang.LazySeq  >  ```    <a name=""get-p-o_method""></a>  #### `get-p-o`    We must be able to get the p-level description of any subject with  `get-p-o`:    ```  > (igraph/get-p-o eg :john)  {:isa #{:person}, :likes #{:beef}}  >  ```    <a name=""get-o_method""></a>  #### `get-o`    We must be able to get the o-level set of objects for any subject and  predicate with `get-o`:    ```  > (igraph/get-o eg :john :isa)  #{:person}  >  ```     <a name=""ask_method""></a>  #### `ask`    We must be able to test for whether any particular triple is in the  graph with `ask` (any truthy response will do).    ```   > (igraph/ask eg :john :likes :beef)   :beef   > (igraph/ask eg :john :likes :chicken)   nil  >  ```    <a name=""query_method""></a>  #### `query`    We must be able to query the graph using a format appropriate to the  native representation. This example uses the format expected by  `ont-app.igraph.graph/Graph`, described [below](#h4-querying):    ```  > (igraph/query eg [[:?person :isa :person]])  #{{:?person :mary} {:?person :john}}  >  ```    In this case, the result is a set of `binding maps`, mapping  :?variables to values, similar to the result set of a  [SPARQL](https://www.wikidata.org/wiki/Q54871) query.    For comparison, here is a sketch of an equivalent SPARQL query, which  would be appropriate if our IGraph protocol was targeted to a SPARQL  endpoint which we might call `sparql-eg`:    ```  > (query sparql-eg     ""PREFIX : <http://path/to/my/ns#>      SELECT * WHERE      {       ?person a :person     }"")  [{:person :mary} {:person :john}]  >   ```    <a name=""invoke_method""></a>  #### `invoke` for arities 0-3    An instance of IGraph must provide `invoke` implementations as  follows:    Without arguments, it must return Normal Form (or throw an ::igraph/Intractable):    ```  > (eg)  {:john {:isa #{:person}, :likes #{:beef}},   :mary {:isa #{:person}, :likes #{:chicken}}}  >    ```    With a single ""s"" argument, it must treat the argument as the subject  of get-p-o:    ```  > (eg :john)  {:isa #{:person}, :likes #{:beef}},  >  ```    With two arguments ""s"" and ""p"", a set of objects must be returned:    ```  > (eg :mary :likes)  #{:chicken}  >  ```    This will often be the value of `get-o`, but it may also accept as the  ""p"" argument a _traversal function_, described  [below](#traversal-fn-as-p).    With three arguments ""s"" ""p"" and ""o"", the response must be truthy:    ```  > (eg :mary :likes :chicken)  :chicken  >  >  > (eg :mary :likes :beef)  nil  >  ```    This will often be equivalent to `ask`, but again, the ""p"" argument  can be a traversal function, described [below](#traversal-fn-as-p).    <a name=""Content_Manipulation""></a>  ### Content Manipulation    There a several factors to take into account when adding or removing  content from a graph.    Some graphs, (such as a public SPARQL endpoint to which one does not  have UPDATE permissions) may not be subject to modification. Other  native representations (such as a SPARQL endpoint with UPDATE  permissions) might best be treated as mutable graphs.    Naturally, other things being equal, the preferred solution is to use  immutable graphs when it is possible to do so. The examples in this  README will all be applied to immutable graphs.    <a name=""mutability_method""></a>  #### `mutability`    The `mutability` method returns one of the following values  - `::igraph/read-only` - there is no means for altering the contents of    the graph  - `::igraph/immutable` - the graph implements    [IGraphImmutable](#IGraphImmutable)  - `::igraph/mutable` - the graph implements    [IGraphMutable](#IGraphMutable)  - `::igraph/accumulate-only` - the graph implements [IGraphAccumulateOnly](#IGraphAccumulateOnly), the approach used in Datomic    <a name=""add-to-graph""></a>  #### The `add-to-graph` multimethod    IGraph defines a multimethod `add-to-graph`, dispatched on the type of  graph, and a function `triples-format`. This multimethod can inform  mutable, immutable and accumulate-only graphs.    Naturally Normal Form is one possible format:    ```  > (igraph/triples-format {:john {:likes# #{:beef}}})  :normal-form  >  ```    Another possible value is `:vector`, with a subject and at least one  P-O pair:    ```  > (igraph/triples-format [:john :likes :beef])  :vector  > (igraph/triples-format [:john :isa :person :likes :beef])  :vector  >  ```     Finally, we have `:vector-of-vectors`:    ```  > (igraph/triples-format  [[:john :isa :person] [:mary :isa :person]])  :vector-of-vectors  >  ```    Any implementation of IGraph should support adding to the graph in all  of these formats.    <a name=""remove-from-graph""></a>  #### The `remove-from-graph` multimethod    IGraph also defines multimethod `remove-from-graph`, dispatched on the  graph types and a function `triples-removal-format`. This multimethod  can inform both mutable and immutable graphs.    The `triples-removal-format` function returns the same keywords as  `triples-format`, but adds one more: `:underspecified-triple`, a  vector with fewer than 3 elements:    ```  > (igraph/triples-removal-format [:john])  :underspecified-triple  > (igraph/triples-removal-format [:john :likes])  :underspecified-triple  >  ```    `triples-removal-format` assigns the :vector-of-vectors flag to a  vector of either :vector or :underspecified-vector. All  implementations of IGraph should support each of these flags.    This allows us to subtract any format that could also be added, plus  all `[s * *]` or all `[s p *]`.        <a name=""IGraphImmutable""></a>  ## The IGraphImmutable protocol    An add or subtract operation to an immutable graph returns a cheap  copy of the original graph modified per the argument provided.    <a name=""add_method""></a>  ### `add`    Calling `(add g to-add)` must return an immutable graph such that the  graph now contains `to-add`. Any triples in `to-add` which are already  in the graph should be skipped.    See the notes above about the [add-to-graph](#add-to-graph)  multimethod.    Typically adding to a graph in code is most easily expressed using a  vector or a vector of vectors:    ```  > (igraph/normal-form       (igraph/add         eg         [[:chicken :subClassOf :meat]         [:beef :subClassOf :meat]         ]))  {:john {:isa #{:person}, :likes #{:beef}},   :mary {:isa #{:person}, :likes #{:chicken}},   :chicken {:subClassOf #{:meat}},   :beef {:subClassOf #{:meat}}}  >  ```    We can use the Normal Form of one graph to add it to another:    ```  > (meats)  {:chicken {:subClassOf #{meat}}   :beef {:subClassOf #{meat}}}  >  > (igraph/normal-form (add eg (meats)))  {:john {:isa #{:person}, :likes #{:beef}},   :mary {:isa #{:person}, :likes #{:chicken}},   :chicken {:subClassOf #{:beef}},   :beef {:subClassOf #{:beef}}}  >   ```    <a name=""subtract_method""></a>  #### `subtract`    The multimethod `remove-from-graph` supports the `subtract` operation,  dispatched on the type of the graph and `triples-removal-format`,  described [above](#remove-from-graph):    ```  > (igraph/normal-form (igraph/subtract eg [:john]))  {:mary {:isa #{:person}, :likes #{:chicken}}}  >  > (igraph/normal-form (igraph/subtract eg [:john :likes]))  {:john {:isa #{:person}},    :mary {:isa #{:person}, :likes #{:chicken}}}  >  ```    <a name=""IGraphMutable""></a>  ## The IGraphMutable protocol    Some graphs' native representations are implemented as mutable  repositories. To support this, the IGraphMutable protocol provides  methods `add!` and `subtract!`.    The [add-to-graph](#add-to-graph) and  [remove-from-graph](#remove-from-graph) multimethods should still  inform the logic here, and the behavior should be essentially the  same, with the exception that the graph returned is the same object,  mutated as specified.    <a name=""add!_method""></a>  ### `add!`    `(add! g to-add)` -> g, where g is both the argument and return value.    An error should be thrown if `(mutablility g)` != ::igraph/mutable.    <a name=""subtract!_method""></a>  ### `subtract!`    `(subtract! g to-subtract)` -> g, where g is both the argument and  return value.    An error should be thrown if `(mutablility g)` != ::igraph/mutable.    <a name=""IGraphAccumulateOnly""></a>  ## The IGraphAccumulateOnly protocol    A graph whose native representation is based on  [Datomic](https://www.datomic.com/) implements what Datomic calls an  ""Accumulate-only"" approach to adding and removing from a graph. To  support this, the IGraphAccumulateOnly protocol provides methods  `claim` (corresponding to the datomic 'add' operation), and  `retract`. In this scheme the state of the graph can be rolled back to  any point in its history. See the [Datomic  documentation](https://docs.datomic.com/) for details.    The [add-to-graph](#add-to-graph) and  [remove-from-graph](#remove-from-graph) multimethods should still  inform the logic here, and the behavior should be essentially the  same, with the exception that the graph returned now points to the  most recent state of the graph after making the modification. Any  given instantiation of the graph will remain immutable.    <a name=""claim_method""></a>  ### `claim`    `(claim g to-add)` -> g', where g is an append-only graph, and  g' now points to the most recent state of g's  [transactor](https://docs.datomic.com/on-prem/transactor.html).    An error should be thrown if `(mutablility g)` != ::igraph/accumulate-only.    <a name=""retract_method""></a>  ### `retract`    `(retract g to-retract)` -> g', where g is an append-only graph, and  g' now points to the most recent state of g's  [transactor](https://docs.datomic.com/on-prem/transactor.html).    An error should be thrown if `(mutablility g)` != ::igraph/accumulate-only.      <a name=""h2-igraphset-protocol""></a>  ## The IGraphSet protocol    It will make sense for many implementations of IGraph also to  implement the basic set operations, defined in IGraphSet. Set  operations may not be suitable between very large graphs.    For purposes of demonstration, let's assume a second graph `other-eg`:    ```  > (igraph/normal-form other-eg)  {:mary {:isa #{:person}, :likes #{:pork}},   :waldo {:isa #{:person}, :likes #{:beer}}}  >  ```    I think examples of each operation should serve to describe them.    <a name=""h3-igraphset-methods-summary""></a>  ### Methods summary  - `(union g1 g2)` -> A new graph with all triples from both graphs  - `(difference g1 g2)` -> A new graph with triples in g1 not also in    g2  - `(intersection g1 g2)` -> A new graph with only triples shared in    both graphs    <a name=""union_method""></a>  ### `union`  ```  > (igraph/normal-form (igraph/union eg other-eg))  {:john {:isa #{:person}, :likes #{:beef}},   :mary {:isa #{:person}, :likes #{:pork :chicken}},   :waldo {:isa #{:person}, :likes #{:beer}}}  >    ```     <a name=""intersection_method""></a>  ### `intersection`    ```  > (igraph/normal-form (igraph/intersection eg other-eg))   {:mary {:isa #{:person}}  >  ```    <a name=""difference_method""></a>  ### `difference`  ```  > (igraph/normal-form (igraph/difference eg other-eg))  {:john {:isa #{:person}, :likes #{:beef}},    :mary {:likes #{:chicken}}}  >  > (igraph/normal-form (igraph/difference other-eg eg))  {:mary {:likes #{:pork}}, :waldo {:isa #{:person}, :likes #{:beer}}}  >  ```    <a name=""Traversal""></a>  ## Traversal    Clojure and other functional programming languages have a  [reduce](https://clojuredocs.org/clojure.core/reduce) idiom, which  allows the user to create aggregations over a sequence by providing a  ""reducer function"" expressing the relationship between each member of  that sequence and the resulting aggregation.    IGraph defines a `traverse` function to allow the user to create  aggregations over the contents of a graph by providing a `traversal  function`, which is analogous to a reducer function, but is  nessesarily a bit more involved.      - `(traverse g traversal context acc queue)` -> `acc'`  - `(traverse g traversal acc queue)` -> `acc'` ;; default context = {}        ... traversing `g` per the `traversal` function, starting with the      first element of `queue`, possibly informed by `context`.    This function will repeatedly call the `traversal` function until  `queue` is empty, returning the final value for `acc`. Each call to  the traversal function returns modified versions of `context`, `acc`  and `queue`.    To illustrate traversal, let's expand on our `eg` graph by adding some  type structure:    Assume we have a graph called 'eg-with-types':    ```  > (def eg-with-types       (add eg        [[:person :subClassOf :thing]         [:beef :subClassOf :meat]         [:chicken :subClassOf :meat]         [:meat :subClassOf :food]         [:beer :subClassOf :beverage]         [:beverage :subClassOf :consumable]         [:food :subClassOf :consumable]         [:consumable :subClassOf :thing]]))  eg-with-types  > (eg-with-types)  {:consumable {:subClassOf #{:thing}},   :beef {:subClassOf #{:meat}},   :person {:subClassOf #{:thing}},   :beer {:subClassOf #{:beverage}},   :meat {:subClassOf #{:food}},   :food {:subClassOf #{:consumable}},   :beverage {:subClassOf #{:consumable}},   :pork {:subClassOf #{:meat}},   :john {:isa #{:person}, :likes #{:beef}},   :mary {:isa #{:person}, :likes #{:chicken}},   :chicken {:subClassOf #{:meat}}}  ```    Our `eg-with-types` now provides a bit more context for what's going  on with our heroes John and Mary. Note that `:isa` and `:subClassOf`  differ in their _domain_. `:isa` relates an instance to its class,  while `:subClassOf` relates a class to its parent.    <a name=""traverse_function""></a>  ### The `traverse` function    Here's an example of how the `traverse` function works, starting with  a traversal function we'll call `subClassOf*`, which follows and  accumulates all :subClassOf links, starting with an initial queue of  say, `[:meat :beer]`:    ```  > (igraph/traverse eg-with-types subClassOf* {} #{} [:meat :beer])  #{:consumable :beer :meat :food :beverage :thing}  >  ```    The arguments for `traverse` are  - `g` - an invariant graph  - `traversal-fn` - A function `[g c acc q]` -> `[c' acc' q']`,    defining the logic of each step in the traversal  - `context` - (optional) a map holding the traversal history plus    whatever `traversal-fn` may want to track. Default is {}  - `acc` - (optional) accumulates the resulting value of the traversal. Default is `[]`.  - `queue` - the starting queue    <a name=""Traversal_functions""></a>  #### Traversal functions    The traversal function takes 4 arguments and returns a vector of  length 3.    ```  > (subClassOf* eg-with-types {} #{} [:meat])  [{} #{:meat} (:food)]  >  > (subClassOf* eg-with-types {} #{:meat} '(:food))  [{} #{:meat :food} (:consumable)]  >  ```    The first argument is the invariant graph itself.    The second argument (and first element returned) is the context, which  subClassOf* leaves unchanged.  Context is used by `traverse` to avoid  cycles, and will be explained in detail [below](#h4-context). More  sophisticated traversal functions may use the context as a kind of  blackboard.    The third argument (and second element returned) is the value to be  accumulated, identical to its counterpart in the _reduce_ idiom.    The fourth argument (and third element returned) is the traversal  queue. It must be sequential, and may be ordered in any way that makes  sense. An empty queue signals and end of the traversal, at which point  `traverse` will return the value of the accumulator.    Here's a possible definition of subClassOf*:    ```  (defn subClassOf* [g c acc q]    ""Traversal function to accumulate super-classes.""    (let [elt (first q)]      [c                      ;; context is unchanged       (conj acc elt)         ;; updating the accumulator       (reduce conj          (rest q)          (g elt :subClassOf)) ;; adding super-classes to the queue         ]))  ```    <a name=""h4-context""></a>  #### Context    The `context` argument to `traverse` and its traversal function is a  map containing key-values which may inform the course of the  traversal, but are not part of the accumulated value. This will  include:  - `:history` set by `traverse`, and updated to hold all elements    encountered in the course of the traversal. In order to avoid    cycles, any element in the history will be skipped should it ever    re-appear at the head of the queue.      The `traverse` function also supports these optional keys in the  context:    - `:skip?` (optional), a function (fn[x] ...) -> truthy, applicable to    the head of the queue, which will override `:history`.      - `:seek` (optional), a function `(fn [context acc]...)` -> `acc'`. If    specified, this function will be called at the beginning of each    traversal, and if truthy and non-empty, the traversal will end    immediately with that value.    In addition, the traversal function may use the context as a  blackboard to communicate between iterations of the traversal. For  example, you may want to prune and re-order your queue based on a set  of heuristics, details of which are stored in the context.    <a name=""queue""></a>  #### The queue    The `queue` argument must be sequential, but is otherwise  unrestricted. An empty queue signals the end of the traversal, at  which point `traverse` will return the accumulated value.    Note that conj-ing to a vector in the traversal function suggests a  breadth-first traversal, while conj-ing to a seq suggests a  depth-first tranversal.    More sophisticated traversal functions may use the context to inform  logic to prune and re-order the queue to optimize the traversal.    <a name=""Traversal_utilities""></a>  ### Traversal utilities    IGraph provides utilities to express several common types of traversal  functions.    <a name=""h4-transitive-closure""></a>  #### `transitive-closure`    - `(trasitive-closure p)` -> `(fn [g context acc to-visit] ...) ->    [context' acc' queue']`,        This returns a traversal function which will accumulate all _o_    s.t. any _s_ in the queue is associated with _o_ through zero or    more _p_ links.    So in the example above, the `subClassOf*` function could be defined  thus:    ```  (def subClassOf* (igraph/transitive-closure :subClassOf))  ```    <a name=""h4-traverse-link""></a>  ### `traverse-link`    - `(traverse-link p)` -> (fn [g context acc queue] ...) -> [context    acc' []],    The function returned here will accumulate all _o_ s.t. for all _s_ in  _queue_, (g s p o) is truthy:    ```  > (igraph/traverse       eg-with-types       (igraph/traverse-link :isa)       #{}       [:john :mary])  #{:person}  >  ```    <a name=""h4-maybe-traverse-link""></a>  #### `maybe-traverse-link`    - `(maybe-traverse-link p)` -> (fn [g context acc queue] ...) ->    [context acc' []]    Matches 0 or 1 occurrences of _p_:    ```  > (igraph/traverse eg-with-types       (igraph/maybe-traverse-link :isa)       #{}       [:john :mary])  #{:person :john :mary}  >  ```    <a name=""h4-traverse-or""></a>  #### `traverse-or`    - `(traverse-or & ps)` -> (fn [g context acc queue] ...) -> [context    acc' []],    Where _ps_ is one or more traversal functions, merging all of their outputs.    Keyword arguments are interpreted as an implicit `traverse-link`.    ```  > (def subsumed-by (igraph/traverse-or :isa :subClassOf))  subsumed-by  > (igraph/traverse eg-with-types subsumed-by #{} [:john])  #{:person}  >  > (igraph/traverse eg-with-types subsumed-by #{} [:meat])  #{:food}  >  ```    <a name=""Traversal_composition""></a>  ### Traversal composition with `t-comp`  Composition functions are composable with a 'short form' and a 'long  form'.    <a name=""h4-t-comp-short""></a>  #### short form    Short-form composition can be used when the traversal function meets  the following criteria:  - None of the component functions manipulate the traversal context  - Each component function accumulates a sequential value suitable to    serve as the initial queue of the component function that follows    it.    Such functions can be called as a simple vector:    ```  > (def instance-of       (igraph/t-comp [:isa (igraph/transitive-closure :subClassOf)]))  >  > (igraph/traverse eg-with-types instance-of #{} [:john])  #{:person :thing}  >  ```    <a name=""h4-t-comp-long""></a>  #### long form    In cases where you want to compose a traversal function that cannot  meet the criteria above, then instead of passing to `traversal-comp` a  vector of traversal functions, you pass in a map with the following  keys:    ```  { :path  [:<traversal-stage-1> :<traversal-stage-2> ...]     :<traversal-stage-1> {:fn <traversal-fn>                           :doc <docstring> (optional)                           :into <initial accumulator> (default [])                           :local-context-fn <context> (default nil)                           :update-global-context (default nil)                        }     :<traversal-stage-2> ...     ...   }   ```     A call to `(t-comp [:a :b :c])` is equivalent to calling `(t-comp  {:path [:a :b :c]})`.    These parameters should allow you as much control as you need over the  flow of contexts between each stage of traversal, and over the flow of  outputs from any one stage into the input queue of its next stage.    However, most of the time, the short form is sufficient, and at this point,  the long form has not been tested heavily.    ##### the :path parameter    This is a vector of traversal function specifications. Each traversal  function specification must be either:  - A traversal function  - A keyword with an entry in the long-form map  - A keyword eligible as an implicit [traverse-link](#h4-traverse-link)    If the traversal function specification is itself a function, it will  be applied directly.    If the traversal function specification is a keyword, and the t-comp  map has a matching entry for that keyword, it will look for and  interpret a map with the parameters described in the next section.    If the spec is a keyword without an entry in the long-form map, it is  assumed to be a candidate for an implicit traverse-link, i.e. a graph  element in 'p' position in _g_.    ##### traversal specification parameters    - :fn - a traversal function  - :doc (optional) - a docstring  - :into (optional) - a container to which the output should be coerced    (default [])  - :local-context-fn (optional) - a function [global-context] -> `local    context` producing the context for this stage of the traversal.  - :update-local-context (optional) - a function [global-context    local-context] -> `global-context'`, capturing whatever aspects of    the current stage of traversal may be of interest to subsequent    stages.    <a name=""traversal-fn-as-p""></a>  ### Using traversal functions as a `p` argument to `invoke`    Recall that implementations of IGraph should provide `invoke`  functions with 0-3 arguments.    Two of these functions involve specification of a _p_ parameter:    ```  (g s p) -> {<o>...}    (g s p o) -> truthy.  ```    This is informed by a multimethod dispatched on whether _p_ is a  function.    - `(match-or-traverse g s p)` -> #{<o>...}    - `(match-or-traverse g s p o)` -> truthy    A typical declaration for an IGraph implementation will contain  these two method declarations:    ```    #?(:clj clojure.lang.IFn       :cljs cljs.core/IFn)    ...    (invoke [g s p] (igraph/match-or-traverse g s p))    (invoke [g s p o] (igraph/match-or-traverse g s p o))    ...  ```    If the _p_ argument is a function, then _p_ will be expected to match  the signature of a traversal function, and the output of the method  will be the value of its traversal, starting with queue [_s_].    If _p_ is not a function it will be matched directly against elements  of the graph.    So given the traversal functions in the examples above:    ```  > (eg-with-types :beef subClassOf*)  #{:consumable :beef :meat :food :thing}  >  > (eg-with-types :beef subClassOf* :food)  :food  >  > (eg-with-types :john (igraph/t-comp [:likes subClassOf*]))  #{:consumable :beef :meat :food :thing}  >  ```        <a name=""cardinality-1_utilities""></a>  ## cardinality-1 utilites    Requiring normal form to provide a set as its 3rd-tier representation  has the advantage of ensuring that the normal form is as simple and  regular as possible, and makes it easy to think about set operations  over graphs. However, it can be a bit unwieldy when dealing with the  many cases where the descriptive map's keys reliably map to a single  scalar value.    The following utilities are provided to help with this:    - `(unique [x]) -> x` - translates a singleton sequence to its only    value  - `(flatten-description (g s))` Automatically translates the p-o    description into a simple k-v mappings wherever only a single _v_    exists.  - `(normalize-flat-description m)` is the inverse of    `flatten-description`.  - `(assert-unique g s p o) - replaces one singleton object with    another.    <a name=""h3-unique""></a>  ### `unique`    The `unique` function takes a sequence and an optional `on-ambiguity`  argument. Default on-ambiguity throws ex-info of type  `::igraph/Non-unique`.    ```  > (eg-with-types :john :isa)  {:person}  >  > (igraph/unique (eg-with-types :john :isa))  :person  >  > (igraph/unique (eg-with-types :beef subClassOf*))  Execution error (ExceptionInfo) at ont-app.igraph.core/unique$fn (core.cljc:640).  Unique called on non-unique collection  >  > (igraph/unique (eg-with-types :beef subClassOf*)                   first) ;; arbitrary disambiguation  :consumable  ```    Sometimes defining `the` as an alias for `unique` reads better, and is  easier to type:    ```  > (def the igraph/unique)  > (the (eg-with-types :john :isa))  :person  >  ```    <a name=""h3-flatten-description""></a>  ### `flatten-description`    ```  (igraph/flatten-description (eg-with-types :john))  {:isa :person, :likes :beef}  >  > (let [g (igraph/add                eg               [:john :likes :beer :has-vector [1 2 3]])          ]      (igraph/flatten-description (g :john)))  {:isa :person, :likes #{:beef :beer}, :has-vector [1 2 3]}  >  ```    <a name=""h3-normalize-flat-description""></a>  ### `normalize-flat-description`    This is the inverse of `flatten-description`:    ```  > (igraph/normalize-flat-description       {:isa :person, :likes #{:beef :beer}, :has-vector [1 2 3]})  {:isa #{:person}, :likes #{:beef :beer}, :has-vector #{[1 2 3]}}  >  > (let [g (igraph/add                eg                {:john (igraph/normalize-flat-description {:likes :beer})})          ]      (g :john))  {:isa #{:person}, :likes #{:beef :beer}}  >  ```    <a name=""h3-assert-unique""></a>  ### `assert-unique`    We can replace one singleton value with another using `(assert-unique  g s p o) -> g'`:    ```  > (let [g (igraph/assert-unique eg :john :isa :man)]      (g :john))  {:likes #{:beef}, :isa #{:man}}  >  ```    <a name=""i-o""></a>  ## I/O    In general writing the normal form of a graph to a stream and applying  the reader to it on the other end should be fairly  straightforward. Any predicates bearing reader-choking objects will of  course need to be filtered out.    At this point, only the :clj platform is directly supported with a  pair of functions to read/write to the file system.    <a name=""h3-write-to-file""></a>  ### `write-to-file`  `(write-to-file [path g] ...) -> path`    Will write an edn file with the normal form contents of _g_.    <a name=""h3-read-from-file""></a>  ### `read-from-file`    `(read-from-file [g path] ...) -> g'`    Will read the normal form contents of _path_ into _g_.    <a name=""Other_utilities""></a>  ## Other utilities    <a name=""h3-reduce-spo""></a>  ### `reduce-spo`  - `(reduce-spo f acc g)` -> `acc'`, such that _f_ is called on each  triple in _g_.  Where _f_ := `(fn [acc s p o]...) ->  acc'`. Cf. [reduce-kv](https://clojuredocs.org/clojure.core/reduce-kv).    ```  > (defn tally-triples [tally s p o]      (inc tally))  > (igraph/reduce-spo tally-triples 0 eg)  4  ```  <a name=""h2-implementations""></a>  ## Implementations    The `ont-app.igraph.graph` module makes one implementation of IGraph  available without any additional dependencies, and so far there are  three other libraries in the ont-app project which implement this  protocol.    Other implementations are planned, and I'd be interested to learn of  any implementations published by other parties.    <a name=""Graph""></a>  ### `ont-app.igraph.graph/Graph`    The IGraph library comes with `ont-app.igraph.graph`, whose `Graph`  deftype is a very lightweight implementation of IGraph.    Its native representation is just Normal Form. Any hashable object can  technically be provided for any _s_, _p_, or _o_, but best practice is  to keep non-identifiers a the ""o"" level if you want to play easily  with other IGraph implementations.    ```  (require '[ont-app.igraph.graph :as g])  ```    <a name=""h4-graph-creation""></a>  #### Graph creation    Use `make-graph` to create a new graph, with an optional `:contents`  argument.    ```  > (def eg (g/make-graph))  eg  > (eg)  {}  >  > (def eg      (g/make-graph         :contents {:john {:isa #{:person}, :likes #{:beef}},                   :mary {:isa #{:person}, :likes #{:chicken}}}  eg  > (eg)  {:john {:isa #{:person}, :likes #{:beef}},   :mary {:isa #{:person}, :likes #{:chicken}}}  >  ```    The `:contents` argument must be in Normal Form.      <a name=""h4-querying""></a>  #### Querying    Querying is done with a very simple vector-of-triples graph pattern  using keywords starting with "":?"" to serve as variables. It returns an  unordered set of binding maps. This is very minimalistic. Any  selecting, ordering, grouping or aggregation needs to be done  downstream from the call.    ```  > (igraph/query eg [[:?liker :likes :?liked]])  #{{:?liker :john, :?liked :beef}     {:?liker :mary, :?liked :chicken}}  >  ```    Traversal functions can be specified in _p_ position:    ```  > (igraph/query eg-with-types [[:?liker :likes ?liked]                                 [?liked subClassOf* :?liked-class]])  #{{:?liked :beef, :?liked-class :consumable, :?liker :john}    {:?liked :beef, :?liked-class :beef, :?liker :john}    {:?liked :chicken, :?liked-class :food, :?liker :mary}    {:?liked :chicken, :?liked-class :chicken, :?liker :mary}    {:?liked :chicken, :?liked-class :consumable, :?liker :mary}    {:?liked :beef, :?liked-class :meat, :?liker :john}    {:?liked :beef, :?liked-class :food, :?liker :john}    {:?liked :beef, :?liked-class :thing, :?liker :john}    {:?liked :chicken, :?liked-class :thing, :?liker :mary}    {:?liked :chicken, :?liked-class :meat, :?liker :mary}}  >  ```      <a name=""h3-sparql-client""></a>  ### sparql-client    <https://github.com/ont-app/sparql-client>    Implements a mutable IGraph for a [SPARQL  endpoint](https://www.wikidata.org/wiki/Q26261192). Initializtion  requires configuring query and update endpoints, and the query  language is [SPARQL](https://www.wikidata.org/wiki/Q54871).    Keyword identifiers are expected to be namespaced, and rely on the [ont-app/vocabulary](https://github.com/ont-app/vocabulary) library, which uses namespace metadata to intercede between Clojure namespaces and RDF namespaces.    Set operations are not supported.    <a name=""h3-datascript-graph""></a>  ### datascript-graph    <https://github.com/ont-app/datascript-graph>    This implements IGraph for a  [datascript](https://github.com/tonsky/datascript) native  representation, and may as such may need to be initialized with some  schema declarations. Query language is datalog. Immutable, with set  operations.    <a name=""h3-datomic-client""></a>  ### datomic-client    https://github.com/ont-app/datomic-client    This implements IGraph for the [Datomic Client API](https://docs.datomic.com/cloud/client/client-api.html). The query language is datalog. Mutability model is Accumulate Only. Set operations are not supported.    <a name=""h2-future-work""></a>  ## Future work  - Ports to loom, ubergraph, and other graph-oriented libraries   - There will be a regime for providing annotations for reified triples    (for weights and such).  - Ports to table-based representations  - `igraph.graph` will have query planning and indexing  - Some kind of a scheme to bring all the various query formats under a    single tent    <a name=""h2-acknowledgements""></a>  ## Acknowledgements    Thanks to [Ram Krishnan](https://github.com/kriyative) for his  feedback and advice.    <a name=""h2-license""></a>  ## License      Copyright © 2019-21 Eric D. Scott    Distributed under the Eclipse Public License either version 1.0 or (at  your option) any later version.    <table>  <tr>  <td width=75>  <img src=""http://ericdscott.com/NaturalLexiconLogo.png"" alt=""Natural Lexicon logo"" :width=50 height=50/> </td>  <td>  <p>Natural Lexicon logo - Copyright © 2020 Eric D. Scott. Artwork by Athena M. Scott.</p>  <p>Released under <a href=""https://creativecommons.org/licenses/by-sa/4.0/"">Creative Commons Attribution-ShareAlike 4.0 International license</a>. Under the terms of this license, if you display this logo or derivates thereof, you must include an attribution to the original source, with a link to https://github.com/ont-app, or  http://ericdscott.com. </p>   </td>  </tr>  <table> """
Semantic web;https://github.com/banana-rdf/banana-rdf;"""banana-rdf  ==========    [![Build Status](https://secure.travis-ci.org/w3c/banana-rdf.png)](http://travis-ci.org/w3c/banana-rdf) [![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/w3c/banana-rdf?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    The current published version 0.8.6, compiled with scala 2.13.6, is to be found on Maven Central under groupId [net/bblfish/rdf/](https://repo1.maven.org/maven2/net/bblfish/rdf/).    ```scala  val banana = (name: String) => ""net.bblfish.rdf"" %% name % ""0.8.6"" excludeAll (ExclusionRule(organization = ""org.scala-stm""))    //choose the packages you need for your dependencies  val bananaDeps = Seq(""banana"", ""banana-rdf"", ""banana-rdf4j"").map(banana)  ```  Snapshot releases can be found on [sonatype.org](https://oss.sonatype.org/content/repositories/snapshots/net/bblfish/rdf/). To use these you will need at add the sonatype resolver to your sbt build:  ```scala  resolvers += Resolver.sonatypeRepo(""snapshots"")  ```    A Scala3 version is being developed on the [scala-3](https://github.com/banana-rdf/banana-rdf/tree/scala-3) branch of this repository.     An RDF library in Scala  -----------------------    `banana-rdf` is a library for RDF, SPARQL and Linked Data technologies  in Scala.    It can be used with existing libraries without any added cost. There  is no wrapping involved: you manipulate directly the real objects. We  currently support Jena, RDF4J and Plantain, a pure Scala  implementation.    Features  --------    `banana-rdf` emphasizes type-safety and immutability, so it can come  with some cost when the underlying implementation is very mutable (I'm  looking at you, Jena and RDF4J). We try to keep a clear distinction  between the core concepts and the enhanced syntax that Scala can give  us.    [`RDF`](rdf/shared/src/main/scala/org/w3/banana/RDF.scala)  itself is defined as a record of types. Implementations just have to  _plug_ their own types. And because types alone are not enough, we  introduce the  [`RDFOps`](rdf/shared/src/main/scala/org/w3/banana/RDFOps.scala)  typeclass, which defines the mandatory operations that an RDF  implementation must  implement. [`SparqlOps`](rdf/shared/src/main/scala/org/w3/banana/SparqlOps.scala)  does the same for SPARQL.    With `banana-rdf`, you get _Diesel_, a nice DSL to build and navigate  within **pointed graphs** (graphs with a pointer to an inner  node). You also get an abstraction for **graph stores**  ([`GraphStore`](rdf/shared/src/main/scala/org/w3/banana/GraphStore.scala)),   which do not have to be **SPARQL engines**  ([`SparqlEngine`](rdf/shared/src/main/scala/org/w3/banana/SparqlEngine.scala)).   Of course, you can **serialize** and **deserialize**  most of the RDF syntaxes as well as JSON-LD (RDFa will come soon).    `banana-rdf` introduces the concept of **binders**, which let you  bridge the Scala and RDF worlds. Most of the common datastructures are  already available, and you can even map your own classes. Unlike usual  ORM techniques, this does not rely on annotation or reflection.    Until we write thorough documentation, the best place to understand  what you can do is to go through the [test  suite](https://github.com/w3c/banana-rdf/tree/series/0.8.x/rdf-test-suite).    How to start geeking  --------------------    To get going with banana-rdf  and get a feel for how to use it the easiest and  fastest way may well be to use it directly in the Ammonite shell as explained in the  [Scripting with Ammonite wiki page](https://github.com/banana-rdf/banana-rdf/wiki/Scripting-with-Ammonite).    It always helps to have the code available, as there are a lot of useful examples in   the test suite. You only need a recent version of Java, that's all:    ``` bash  $ git clone git@github.com:w3c/banana-rdf.git  $ cd banana-rdf  $ sbt  ```    It's also easy to just build specific target platforms:        ``` bash  $ sbt +banana_js/test    # for javascript only   $ sbt +banana_jvm/test   # for jvm only  ```    ( note: scala-js compilation uses more memory. see [travis.yml](.travis.yml) )    IDE Setup  =========    `banana-rdf` works with both [eclipse](https://www.eclipse.org/) and [IntelliJ IDEA](http://www.jetbrains.com/idea/).    global.sbt  ----------  Independent of your preferred IDE, optionally the add the following line to `~/.sbt/0.13/global.sbt` to prevent the   generation of empty source directories:    ```      unmanagedSourceDirectories in Compile ~= { _.filter(_.exists) }  ```    Eclipse  -------  Eclipse should work ""out of the box"" with the addition of the following global settings:    In `~/.sbt/0.13/global.sbt`:    ```      unmanagedSourceDirectories in Compile ~= { _.filter(_.exists) }  ```    In `~/.sbt/0.13/plugins/build.sbt`    ```      addSbtPlugin(""com.typesafe.sbteclipse"" % ""sbteclipse-plugin"" % ""2.5.0"")  ```    To generate eclipse project files, just run the command:    ``` bash  $ sbt eclipse  ```    IntelliJ IDEA  -------------    IntelliJ IDEA works out of the box since 2016.      Community  =========    For discussions that don't fit in the [issues tracker](https://github.com/w3c/banana-rdf/issues), you may try  either   *  the [w3c banana-rdf mailing list](http://lists.w3.org/Archives/Public/public-banana-rdf/), for longer discussions  *  the [banana-rdf gitter channel](https://gitter.im/banana-rdf/banana-rdf), for quick real time socialising    Code of Conduct  ---------------    **Banana-RDF contributors all agree to follow the [W3C Code of Ethics and Professional Conduct](http://www.w3.org/Consortium/cepc/).**    If you want to take action, feel free to contact Alexandre Bertails <alexandre@bertails.org>. You can also contact W3C Staff as explained in [W3C Procedures](http://www.w3.org/Consortium/pwe/#Procedures).    Licence  -------    This source code is made available under the [W3C Licence](http://opensource.org/licenses/W3C). This is a business friendly license. """
Semantic web;https://github.com/weblyzard/streaming-sparql;"""## Streaming SPARQL  [![Build Status](https://www.travis-ci.org/weblyzard/streaming-sparql.png?branch=master)](https://www.travis-ci.org/weblyzard/streaming-sparql)    Provides a robust, incremental processing of streaming results received from SPARQL servers.   The `StreamingResultSet` iterator yields results as they are received from the server.    ## Javadoc     http://javadoc.io/doc/com.weblyzard.sparql/streaming-sparql/    ## Example code:  ```java  try (StreamingResultSet s = StreamingQueryExecutor.getResultSet(""http://dbpedia.org/sparql"", ""SELECT ?s ?p ?o WHERE { ?s ?p ?o. } LIMIT 5"")) {      while (s.hasNext()) {          System.out.println(""Tupel "" + s.getRowNumber() + "": "" + s.next())      }  }  ```    ## Command line client    Streaming SPARQL also provides a command line client for testing queries.    ### Usage    ```bash  java -jar ./streaming-client-0.0.7-SNAPSHOT.jar  QueryEntitites [URL] [Query]    URL   ... URL to the linked data repository    Query ... The query to perform on the server  ```    ### Example  ```bash  java -jar ./streaming-client-0.0.7-SNAPSHOT.jar http://localhost:8080/rdf4j-sesame/test ""SELECT ?s ?p ?o WHERE { ?s ?p ?o. } LIMIT 5""  ```    ## Background    We have been using Fuseki and RDF4j together with comprehensive result sets (> 100 Mio. tuple) which lead to   instabilities with the native libraries that have been extremely difficult to debug.    Example error messages on the server site have been:    ```  [2017-05-04 19:50:14] Fuseki     WARN  [1450] Runtime IO Exception (client left?) RC = 500 : org.eclipse.jetty.io.EofException        org.apache.jena.atlas.RuntimeIOException: org.eclipse.jetty.io.EofException                                                           ```     ```  [2017-05-04 19:50:14] Fuseki    WARN  (HttpChannel.java:468) (and one from ServletHandler.java:631):  java.io.IOException: java.util.concurrent.TimeoutException: Idle timeout expired: 30001/30000 m  ```    These problems triggered the development of Streaming SPARQL which has proven to be very robust - even for queries that take more than one hour to process and transfer multiple gigabytes of results.  (Note: you will need to call `getResultSet` with a higher timeout to prevent TimeoutExceptions on the server).      ## Compatiblity    Streaming SPARQL is known to work with Jena, OpenRDF, RDF4j and Virtuoso.      ## Changelog    Please refer to the [release](https://github.com/weblyzard/streaming-sparql/releases) page. """
Semantic web;https://github.com/marcelotto/rdf-ex;"""<img src=""rdf-logo.png"" align=""right"" />    # RDF.ex    [![CI](https://github.com/rdf-elixir/rdf-ex/workflows/CI/badge.svg?branch=master)](https://github.com/rdf-elixir/rdf-ex/actions?query=branch%3Amaster+workflow%3ACI)  [![Hex.pm](https://img.shields.io/hexpm/v/rdf.svg?style=flat-square)](https://hex.pm/packages/rdf)  [![Hex Docs](https://img.shields.io/badge/hex-docs-lightgreen.svg)](https://hexdocs.pm/rdf/)  [![Total Download](https://img.shields.io/hexpm/dt/rdf.svg)](https://hex.pm/packages/rdf)  [![License](https://img.shields.io/hexpm/l/rdf.svg)](https://github.com/rdf-elixir/rdf-ex/blob/master/LICENSE.md)      An implementation of the [RDF](https://www.w3.org/TR/rdf11-primer/) data model in Elixir.    The API documentation can be found [here](https://hexdocs.pm/rdf/). For a guide and more information about RDF.ex and it's related projects, go to <https://rdf-elixir.dev>.    Migration guides for the various versions can be found in the [Wiki](https://github.com/rdf-elixir/rdf-ex/wiki).      ## Features    - fully compatible with the RDF 1.1 specification  - support of the [RDF-star] extension  - in-memory data structures for RDF descriptions, RDF graphs and RDF datasets  - basic graph pattern matching against the in-memory data structures with streaming-support  - execution of [SPARQL] queries against the in-memory data structures with the [SPARQL.ex] package or against any SPARQL endpoint with the [SPARQL.Client] package  - RDF vocabularies as Elixir modules for safe, i.e. compile-time checked and concise usage of IRIs  - most of the important XML schema datatypes for RDF literals  - support for custom datatypes for RDF literals, incl. as derivations of XSD datatypes via facets   - sigils for the most common types of nodes, i.e. IRIs, literals, blank nodes and lists  - a description DSL resembling Turtle in Elixir  - implementations for the [N-Triples], [N-Quads] and [Turtle] serialization formats (including the respective RDF-star extensions); [JSON-LD] and [RDF-XML] are available with the separate [JSON-LD.ex] and [RDF-XML.ex] packages  - validation of RDF data against [ShEx] schemas with the [ShEx.ex] package  - mapping of RDF data structures to Elixir structs and back with [Grax]       ## Contributing    There's still much to do for a complete RDF ecosystem for Elixir, which means there are plenty of opportunities to contribute. Here are some suggestions:    - more serialization formats, like [RDFa], [N3], [CSVW], [HDT] etc.  - more XSD datatypes  - improving the documentation    See [CONTRIBUTING](CONTRIBUTING.md) for details.      ## Consulting    If you need help with your Elixir and Linked Data projects, just contact [NinjaConcept](https://www.ninjaconcept.com/) via <contact@ninjaconcept.com>.      ## Acknowledgements    The development of this project was partly sponsored by [NetzeBW](https://www.netze-bw.de/) for [NETZlive](https://www.netze-bw.de/unsernetz/netzinnovationen/digitalisierung/netzlive).    [JetBrains](https://www.jetbrains.com/?from=RDF.ex) supports the project with complimentary access to its development environments.      ## License and Copyright    (c) 2017-present Marcel Otto. MIT Licensed, see [LICENSE](LICENSE.md) for details.      [RDF.ex]:               https://hex.pm/packages/rdf  [JSON-LD.ex]:           https://hex.pm/packages/json_ld  [RDF-XML.ex]:           https://hex.pm/packages/rdf_xml  [SPARQL.ex]:            https://hex.pm/packages/sparql  [SPARQL.Client]:        https://hex.pm/packages/sparql_client  [ShEx.ex]:              https://hex.pm/packages/shex  [Grax]:                 https://hex.pm/packages/grax  [RDF-star]:             https://w3c.github.io/rdf-star/cg-spec  [N-Triples]:            https://www.w3.org/TR/n-triples/  [N-Quads]:              https://www.w3.org/TR/n-quads/  [Turtle]:               https://www.w3.org/TR/turtle/  [N3]:                   https://www.w3.org/TeamSubmission/n3/  [JSON-LD]:              https://www.w3.org/TR/json-ld/  [RDFa]:                 https://www.w3.org/TR/rdfa-syntax/  [RDF-XML]:              https://www.w3.org/TR/rdf-syntax-grammar/  [CSVW]:                 https://www.w3.org/TR/tabular-data-model/  [HDT]:                  http://www.rdfhdt.org/  [SPARQL]:               https://www.w3.org/TR/sparql11-overview/  [ShEx]:                 https://shex.io/ """
Semantic web;https://github.com/kasei/kineo;"""# Kineo    ## A persistent RDF quadstore and SPARQL engine    ### Build    `swift build -c release`    ### Swift Package Manager    You can use the [Swift Package Manager](https://swift.org/package-manager/) to add Kineo to a Swift project by adding it as a dependency in `Package.swift`:    ```swift  .package(name: ""Kineo"", url: ""https://github.com/kasei/kineo.git"", .upToNextMinor(from: ""0.0.91"")),  ```    ### Load data    Create a database file (`geo.db`) and load one or more N-Triples or Turtle files:    ```  % ./.build/release/kineo -q geo.db -d examples/geo-data/geo.ttl load  ```    Specifying `-d FILENAME` will load data from `FILENAME` into the default graph.  Alternatively, data can be loaded into a specific named graph (similarly, a  custom graph name can be used for the query default graph):    ```  % ./.build/release/kineo -q geo.db -g http://example.org/dbpedia examples/geo-data/geo.ttl load  ```    ### Query    Querying of the data can be done using SPARQL:    ```  % cat examples/geo-data/geo.rq  PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>  SELECT  ?s  WHERE {  	?s geo:lat ?lat ;  	   geo:long ?long ;  	FILTER(?long < -120)  	FILTER(?lat >= 34.0)  	FILTER(?lat <= 35.0)  }  ORDER BY ?s    % ./.build/release/kineo -q geo.db query examples/geo-data/geo.rq  Using default graph <file://examples/geo-data/geo.ttl>  1	Result[s: <http://dbpedia.org/resource/Buellton,_California>]  2	Result[s: <http://dbpedia.org/resource/Lompoc,_California>]  3	Result[s: <http://dbpedia.org/resource/Los_Alamos,_California>]  4	Result[s: <http://dbpedia.org/resource/Mission_Hills,_California>]  5	Result[s: <http://dbpedia.org/resource/Orcutt,_California>]  6	Result[s: <http://dbpedia.org/resource/Santa_Barbara_County,_California>]  7	Result[s: <http://dbpedia.org/resource/Santa_Maria,_California>]  8	Result[s: <http://dbpedia.org/resource/Santa_Ynez,_California>]  9	Result[s: <http://dbpedia.org/resource/Solvang,_California>]  10	Result[s: <http://dbpedia.org/resource/Vandenberg_Air_Force_Base>]  ```    ### Kineo API    The Kineo API can be used to create an in-memory or persistent quadstore,  load RDF data into it, and evaluate SPARQL queries over the data:    ```swift  import Foundation  import SPARQLSyntax  import Kineo    let graph = Term(iri: ""http://example.org/default-graph"")  let store = MemoryQuadStore()    let url = URL(string: ""http://kasei.us/about/foaf.ttl"")!  try store.load(url: url, defaultGraph: graph)    let sparql = ""PREFIX foaf: <http://xmlns.com/foaf/0.1/> SELECT * WHERE { ?person a foaf:Person ; foaf:name ?name }""  let q = try SPARQLParser.parse(query: sparql)  let results = try store.query(q, defaultGraph: graph)  for (i, result) in results.bindings.enumerated() {      print(""\(i+1)\t\(result)"")  }  ```    There is also an API that exposes the RDF data in terms of graph vertices and edge traversals:    ```swift  import Foundation  import SPARQLSyntax  import Kineo    let graph = Term(iri: ""http://example.org/default-graph"")  let store = MemoryQuadStore()    let url = URL(string: ""http://kasei.us/about/foaf.ttl"")!  try store.load(url: url, defaultGraph: graph)    let graphView = store.graph(graph)  let greg = graphView.vertex(Term(iri: ""http://kasei.us/about/#greg""))    let knows = Term(iri: ""http://xmlns.com/foaf/0.1/knows"")  let name = Term(iri: ""http://xmlns.com/foaf/0.1/name"")  for v in try greg.outgoing(knows) {      let names = try v.outgoing(name)      if let nameVertex = names.first {          let name = nameVertex.term          print(""Greg know \(name)"")      }  }  ```    ### SPARQL Endpoint    Finally, using the companion [kineo-endpoint](https://github.com/kasei/kineo-endpoint) package,  a SPARQL endpoint can be run allowing SPARQL Protocol clients to access the data:    ```  % kineo-endpoint -q geo.db &  % curl -H ""Accept: application/sparql-results+json"" -H ""Content-Type: application/sparql-query"" --data 'PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#> SELECT ?s ?lat ?long WHERE { ?s geo:lat ?lat ; geo:long ?long } LIMIT 3' 'http://localhost:8080/sparql'  {    ""head"": {      ""vars"": [ ""s"", ""lat"", ""long"" ]    },    ""results"": {      ""bindings"": [        {          ""s"": { ""type"": ""uri"", ""value"": ""http://dbpedia.org/resource/'s-Gravendeel"" },          ""lat"": { ""type"": ""literal"", ""value"": ""5.17833333333333E1"", ""datatype"": ""http://www.w3.org/2001/XMLSchema#float"" },          ""long"": { ""type"": ""literal"", ""value"": ""4.61666666666667E0"", ""datatype"": ""http://www.w3.org/2001/XMLSchema#float"" }        },        {          ""s"": { ""type"": ""uri"", ""value"": ""http://dbpedia.org/resource/'s-Hertogenbosch"" },          ""lat"": { ""type"": ""literal"", ""value"": ""5.17833333333333E1"", ""datatype"": ""http://www.w3.org/2001/XMLSchema#float"" },          ""s"": { ""type"": ""uri"", ""value"": ""http://dbpedia.org/resource/Groesbeek"" },          ""long"": { ""type"": ""literal"", ""value"": ""5.93333333333333E0"", ""datatype"": ""http://www.w3.org/2001/XMLSchema#float"" }        },        {          ""s"": { ""type"": ""uri"", ""value"": ""http://dbpedia.org/resource/'s-Hertogenbosch"" },          ""lat"": { ""type"": ""literal"", ""value"": ""5.1729918E1"", ""datatype"": ""http://www.w3.org/2001/XMLSchema#float"" },          ""long"": { ""type"": ""literal"", ""value"": ""5.306938E0"", ""datatype"": ""http://www.w3.org/2001/XMLSchema#float"" }        }      ]    }  }  ``` """
Semantic web;https://github.com/uzh/katts;"""KATTS  =====    KATTS is a collection of storm operators (bolts) and a configuration facility that enables  a user to write queries in an XML syntax to query a stream of RDF triples.      Information  ===========  - The scripts used for the SSWS/ISWC 2013 publications can be found in src/eval/.  - Information about version numbers can be found in VERSIONS.md                                                                                                                                                                                                           """
Semantic web;https://github.com/drobilla/sord;"""Sord  ====    Sord is a lightweight C library for storing RDF statements in memory.  For more information, see <http://drobilla.net/software/sord>.     -- David Robillard <d@drobilla.net>   """
Semantic web;https://github.com/Callidon/pyHDT;"""# pyHDT    [![Build Status](https://travis-ci.org/Callidon/pyHDT.svg?branch=master)](https://travis-ci.org/Callidon/pyHDT) [![Documentation Status](https://readthedocs.org/projects/pyhdt/badge/?version=latest)](https://callidon.github.io/pyHDT) [![PyPI version](https://badge.fury.io/py/hdt.svg)](https://badge.fury.io/py/hdt)    **pyHDT is joining the RDFlib family as part of the rdflib 6.0 release! The development continues at [rdflib-hdt](https://github.com/RDFLib/rdflib-hdt), and this repository is going into archive.**    Read and query HDT document with ease in Python    [Online Documentation](https://callidon.github.io/pyHDT)    # Requirements    * Python *version 3.6.4 or higher*  * [pip](https://pip.pypa.io/en/stable/)  * **gcc/clang** with **c++11 support**  * **Python Development headers**  > You should have the `Python.h` header available on your system.     > For example, for Python 3.6, install the `python3.6-dev` package on Debian/Ubuntu systems.    Then, install the [pybind11 library](http://pybind11.readthedocs.io/en/stable/)  ```  pip install pybind11  ```    # Installation    Installation in a [virtualenv](https://virtualenv.pypa.io/en/stable/) is **strongly advised!**    ## Pip install (recommended)    ```  pip install hdt  ```    ## Manual installation    ```  git clone https://github.com/Callidon/pyHDT  cd pyHDT/  ./install.sh  ```    # Getting started    ```python  from hdt import HDTDocument     # Load an HDT file.   # Missing indexes are generated automatically, add False as the second argument to disable them  document = HDTDocument(""test.hdt"")    # Display some metadata about the HDT document itself  print(""nb triples: %i"" % document.total_triples)  print(""nb subjects: %i"" % document.nb_subjects)  print(""nb predicates: %i"" % document.nb_predicates)  print(""nb objects: %i"" % document.nb_objects)  print(""nb shared subject-object: %i"" % document.nb_shared)    # Fetch all triples that matches { ?s ?p ?o }  # Use empty strings ("""") to indicates variables  triples, cardinality = document.search_triples("""", """", """")    print(""cardinality of { ?s ?p ?o }: %i"" % cardinality)  for triple in triples:    print(triple)    # Search also support limit and offset  triples, cardinality = document.search_triples("""", """", """", limit=10, offset=100)  # etc ...  ```    # Handling non UTF-8 strings in python    If the HDT document has been encoded with a non UTF-8 encoding the previous code won't work correctly and will result in a `UnicodeDecodeError`.  More details on how to convert string to str from c++ to python [here](https://pybind11.readthedocs.io/en/stable/advanced/cast/strings.html)    To handle this we doubled the API of the HDT document by adding:  - `search_triples_bytes(...)` return an iterator of triples as `(py::bytes, py::bytes, py::bytes)`  - `search_join_bytes(...)` return an iterator of sets of solutions mapping as `py::set(py::bytes, py::bytes)`  - `convert_tripleid_bytes(...)` return a triple as: `(py::bytes, py::bytes, py::bytes)`  - `convert_id_bytes(...)` return a `py::bytes`    **Parameters and documentation are the same as the standard version**    ```python  from hdt import HDTDocument     # Load an HDT file.   # Missing indexes are generated automatically, add False as the second argument to disable them  document = HDTDocument(""test.hdt"")  it = document.search_triple_bytes("""", """", """")    for s, p, o in it:    print(s, p, o) # print b'...', b'...', b'...'    # now decode it, or handle any error    try:      s, p, o = s.decode('UTF-8'), p.decode('UTF-8'), o.decode('UTF-8')    except UnicodeDecodeError as err:      # try another other codecs      pass  ``` """
Semantic web;https://github.com/EIS-Bonn/iRap;"""iRap  =======    Interest-based RDF update propagation framework    iRap is an RDF update propagation framework that propagates only interesting parts of an update from the source to the target dataset.   iRap filters interesting parts of changesets from the source dataset based on graph-pattern-based interest expressions registered by a target dataset user.  This repository provides source code and evaluation material for iRap framework.    Configuration  =========    Interest expressions should be specified in RDF. Basic components of interest expression are:    - **Subscriber:** is an object that identifies a target dataset and preferences associated with it.    - **Interest:** is an object that identifies an interest expression of a `Subscriber` dataset.    iRap interest expression ontology: `@prefix irap: <http://eis.iai.uni-bonn.de/irap/ontology/>`    ## Subscriber    `Subscriber` instance contains the following setting:    * **irap:targetType :** type of dataset as a target to the changes. Valid type are: *TDB, SPARQL_ENDPOINT, and VIRTUOSO_JDBC*    * **irap:targetEndpoint :** path to target dataset. If target type is TDB, then endpoint value is path to TDB folder. If target type is SPARQL_ENDPOINT then irap:targetEndpoint is URI to sparql endpoint (This can be public endpoint for querying only or update enabled endpoint).  VIRTUOSO_JDBC type not supported for this  release.    * **irap:targetUpdateURI :** same as irap:targetEndpoint if target type is TDB and update enabled SPARQL_ENDPOINT. If irap:targetType is query only SPARQL_ENDPOINT, then irap:targetUpdateURI should be a URI to SPARQL Update endpoint.    * **irap:piTrackingMethod :** potentially interesting triples tracking method. Valid methods supported are: LOCAL and LIVE_ON_SOURCE    * **irap:piStorageType :** type of dataset for potentially interesting dataset, if irap:piTrackingMethod is LOCAL    * **irap:piStoreBaseURI :** Path (Endpoint URI) to potentially interesting dataset, if irap:piTrackingMethod is LOCAL and irap:piStorageType is TDB (SPARQL_ENDPOINT, respectively)    ## Interest    `Interest` instance contains the following setting:    * **irap:subscriber :** URI of a subscriber for this interest    * **irap:sourceEndpoint :** endpoint to the source dataset (SPARQL_ENDPOINT)    * **irap:changesetPublicationType :** location of changeset publication. Valid values are: REMOTE and LOCAL    * **irap:changesetBaseURI :** URI to changeset publication location    * **irap:lastPublishedFilename :** last publication file name to compare with last downloaded changesets by iRap    * **irap:bgp :** interest basic graph pattern (BGP) expression    * **irap:ogp :** optional graph pattern (OGP) expression              Executing iRap  =========  In order to execute from source, download the code from the repo  `git clone https://github.com/EIS-Bonn/iRap.git`    - Prepare your interest expression (see `Example interest expression` below).  - If your interest expression contains remote changeset publications (such as DBpedia changesets), edit `lastDownloadDate.dat` and adapt the date according to your target dataset  - Running iRap:  				  		$ git clone https://github.com/EIS-Bonn/iRap.git  		$ cd iRap/irap-core  		$ mvn clean install  		$ mvn exec:java -Dexec.args=""<interest-exp>,<run-mode>""      - `interest-exp` specifies an interest expression RDF file      - `run-mode` specifies how long the changeset manager should run.        * -1 - endless, i.e., run 'forever'       * 0  - one-time, i.e., run until all changesets available are evaluated (DO NOT wait new updates)    		    Example (DBpedia replica)  =========  The following example shows an interest expression for DBpedia remote changesets.    ```  # interest.ttl    @prefix : <http://eis.iai.uni-bonn.de/irap/ontology/> .  @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .  @base <http://eis.iai.uni-bonn.de/irap/ontology/> .    ###  http://eis.iai.uni-bonn.de/irap/resource/Soccer  <http://eis.iai.uni-bonn.de/irap/resource/Soccer> rdf:type :Interest ;                                                                                                    :sourceEndpoint ""http://live.dbpedia.org/sparql"" ;                                                    :lastPublishedFilename ""lastPublishedFile.txt"" ;                                                    :bgp ""?a  a <http://dbpedia.org/ontology/Athlete> .  ?a <http://dbpedia.org/property/goals>  ?goals ."" ;                                                    :ogp ""?a <http://xmlns.com/foaf/0.1/homepage>  ?page ."" ;                                                    :changesetBaseURI ""http://live.dbpedia.org/changesets/"" ;                                                    :changesetPublicationType ""REMOTE"" ;                                                    :subscriber <http://eis.iai.uni-bonn.de/irap/resource/Sport.org> .    ###  http://eis.iai.uni-bonn.de/irap/resource/Sport.org  <http://eis.iai.uni-bonn.de/irap/resource/Sport.org> rdf:type :Subscriber;                                                       :piStoreBaseURI ""sports-pi-tdb"" ;                                                       :piStorageType ""TDB"" ;                                                       :targetType ""TDB"" ;                                                       :targetEndpoint ""sports-tdb"" ;                                                       :piTrackingMethod ""LOCAL"" ;                                                       :targetUpdateURI ""sports-tdb"" .    ```    Dependencies  =========    1. Java 7    Contact  =======  [iRap mailing list](https://groups.google.com/forum/#!forum/irap-ld)    ## License    The source code is under the terms of the [GNU General Public License, version 2](http://www.gnu.org/licenses/gpl-2.0.html). """
Semantic web;https://github.com/newsreader/eso-and-ceo;"""Event and Implied Situation Ontology (ESO) and the Circumstantial Event Ontology for Calamities (CEO)  ========================    This repository contains the Event and Implied Situation Ontology (ESO), developed in Newsreader (www.newsreader-project.eu) and the Circumstantial Event Ontology for Calamities (CEO).      ESO_version2.owl is the stable version of the ontology; the ontology itself is described in ESO_documentation. The folder manual-mappings comprises several files with manually mappings from ESO to FrameNet, SUMO and WordNet.    Further, this repository contains the CEO ontology, which builds upon the metamodel of ESO. CEO is designed for the calamity domain (murders, earthquakes, floodings, evacuations, etc) and allows to infer chains of events based on shared event properties. Current version is version 1.0.     From May 2018 onwards, this repository is further maintained and updated here: https://github.com/RoxaneSegers/CEO-Ontology and here https://github.com/RoxaneSegers/ESO-Ontology.      <a rel=""license"" href=""http://creativecommons.org/licenses/by-sa/4.0/""><img alt=""Creative Commons License"" style=""border-width:0"" src=""https://i.creativecommons.org/l/by-sa/4.0/88x31.png"" /></a><br /><span xmlns:dct=""http://purl.org/dc/terms/"" property=""dct:title"">Event and Implied Situation Ontology (ESO)</span> by <span xmlns:cc=""http://creativecommons.org/ns#"" property=""cc:attributionName"">Roxane Segers and Marco Rospocher</span> is licensed under a <a rel=""license"" href=""http://creativecommons.org/licenses/by-sa/4.0/"">Creative Commons Attribution-ShareAlike 4.0 International License</a>.<br />Based on a work at <a xmlns:dct=""http://purl.org/dc/terms/"" href=""https://github.com/newsreader/eso"" rel=""dct:source"">https://github.com/newsreader/eso</a>. """
Semantic web;https://github.com/tkurz/squebi;"""Squebi  ======    Squebi is a SPARQL editor and SPARQL result visualizer with some nice features:    * customization of SPARQL result visualization  * support for SPARQL 1.1 (update and select)  * bookmarkable uris that define queries and the visualization type  * support for SPARQL query editing (URIs, ontologies and prefixes)  * fast backend switch (quite useful for demo)  * nice GUI    Squebi is in use in the following projects and products:    * [Apache Marmotta](http://marmotta.apache.org)  * [Redlink Dashboard](http://redlink.co)  * [MICO - Media in Context](http://mico-project.eu)  * [Salzburgerland Data Hub](http://data.salzburgerland.com/dataset/events)    Installation  ------------    Squeby uses [bower](http://bower.io/) for dependency management. To get all dependencies, execute `bower install`.  The current version is v1.0.1.    Configuration  -------------    For configuration set your SPARQL endpoints (and additional parameters) by script and include it before the requirejs.    ### selectService : String (required)  The URI of the SPARQL select webservice (used via POST).    ### updateService : String (required)  The URI of the SPARQL update webservice (used via POST).    ### configurable : Boolean  If squebi allows dynamic changes of SPARQL endpoints.    **default: false**    ### automaticQuery : Boolean  If squebi automatically sends the current query after the page is loaded.    **default: true**    ### queryParams : Object  Additional query parameters as property:'value' pairs.    ### updateAllowed : Boolean  If UI allows SPARQL update queries.    **default: true**    ### namespaces : Object  Namespace prefixes as path:'prefix' pairs. Als prefixes that are not defined here will be looked up on prefix.cc.    ### container : String  The identifier (#id or .class) for the container that should be used for the application.    **default: '#squebi'**    ### appLoader : String  The identifier (#id or .class) for the container that is shown before the app is loaded completely. This container is hided on app startup complete.    **default: '#appLoader'**    ### app : String  The uri of the squebi app.    **default: '.'**    ### bower : String  The uri of the bower dependencies.    **default: 'bower_components'**    ### responseMessage : Object  Custom response messages based on http response as 'number':'message' pairs. If no response message is defined, the server response is used for display.    ### samples : List <Object>  A List if sample objects with properties 'name' (what is displayd), 'query', and 'type' (the id of the writer).    ## writers : List <String>  A list that includes the ids of the writers which are enabled.    ## Sample Configuration    ```javascript  SQUEBI = {      selectService : ""https://api.redlink.io/1.0-BETA/data/example/sparql/select"",      updateService : ""https://api.redlink.io/1.0-BETA/data/example/sparql/update"",      queryParams : {          key : ""mykeyasadditionalqueryparameter""      }  };  ```    ## Use Squebi as Webjar  You can use squebi in version 1.0.1 as webjar, too. The webjar is hosted on Maven Central. Put this dependencies to your pom    ```xml  <dependency>      <groupId>com.github.tkurz.webjars</groupId>      <artifactId>squebi</artifactId>      <version>1.0.1</version>  </dependency>  ```    Important: If you want to build your own webjar, please download the required bower dependencies first into the folder `bower_components`. """
Semantic web;https://github.com/ncbo/umls2rdf;"""This project takes a MySQL Unified Medical Language System (UMLS) database and converts the ontologies to RDF using OWL and SKOS as the main schemas.    Virtual Appliance users can review the [documentation in the OntoPortal Administration Guide}(https://ontoportal.github.io/administration/ontologies/handling_umls/).    To use it:    * Specify your database connection conf.py  * Specify the SAB ontologies to export in umls.conf    The umls.conf configuration file must contain one ontology per line. The lines are comma separated tuples where the elements are:    <em>The following list needs updating.</em>  <pre>  (0) SAB  (1) BioPortal Virtual ID. This is optional, any value works.  (2) Output file name  (3) Conversion strategy. Accepted values (load_on_codes, load_on_cuis).  </pre>    Note that 'CCS COSTAR DSM3R DSM4 DXP ICPC2ICD10ENG MCM MMSL MMX MTHCMSFRF MTHMST MTHSPL MTH NDFRT SNM' have no code and should not be loaded on loads_on_codes.    umls2rdf.py is designed to be an offline, run-once process.   It's memory intensive and exports all of the default ontologies in umls.conf in 3h 30min.   The ontologies listed in umls.conf are the UMLS ontologies accessible in [BioPortal](https://bioportal.bioontology.org/).    If you get an error when installing the MySQL-python python library, https://stackoverflow.com/questions/12218229/my-config-h-file-not-found-when-intall-mysql-python-on-osx-10-8 may be of help.    If running a Windows 10 OS with MySQL, the following tips may be of help.    - Install [MySQL 5.5](https://dev.mysql.com/downloads/mysql/5.5.html#downloads) to avoid the InnoDB space [disclaimer](https://www.nlm.nih.gov/research/umls/implementation_resources/scripts/README_RRF_MySQL_Output_Stream.html) by NLM.   - [Python 2.7.x](https://www.python.org/downloads/) should be used to avoid syntax errors on 'raise Attribute'  - For installtion of the MySQLdb module <pre>python -m pip install MySQLdb</pre> is error prone. Install with executable [MySQL-python-1.2.3.win-amd64-py2.7](http://www.codegood.com/archives/129) (last known location).  - Create your RRF subset(s) using mmsys with the MySQL load option, load your database, edit conf.py and umls.py to specifications, run umsl2rdf.py """
Semantic web;https://github.com/owlcs/owlapi;"""# OWLAPI  ======    ## OWL API main repository    The OWL API is a Java API for creating, manipulating and serialising OWL Ontologies.     * The latest version of the API supports OWL 2.    * It is available under Open Source licenses (LGPL and Apache).    The following components are included:    * An API for OWL 2 and an in-memory reference implementation.  * Read and write support for RDF/XML, OWL/XML, Functional syntax, Manchester syntax, Turtle, OBO.  * Write support for KRSS, DL syntax, LaTeX.  * Other formats via RIO integration (NTriples, JSON, etc.).  * Reasoner interfaces for working with reasoners such as FaCT++, HermiT, Pellet, Racer, JFact and Chainsaw.  * See documentation pages on the wiki for more details.    ## Release notes    ## 5.1.18 30 July 2021    ### Features:    *    Specify RioSetting values for Rio renderers #614    ### Bug fixes:    *    Fix sameAs failure when more than 2 entities included #994  *    Fix Trig and rdf/json should include a named graph. #1002  *    Fix ObjectHasSelf rendered wrongly in manchester syntax #1005    ## 5.1.17 6 November 2020    ### Features:    *    Remove @Deprecated annotations for Set based methods in OWLAPI 5 #981  *    Support RDF4J Rio HDT parser #931  *    OWLLiteral for XSD:Long #970    ### Bug fixes:    *    Fix Performance of signature checks during ontology changes #968  *    Fix Error on RIO renderer when expression has 6000 elements #971  *    Fix OWLOntology#datatypesInSignature to include ontology header #965  *    Fix EntitySearcher.getSuperProperties fails when parent is inverse #964  *    Update guava and junit versions  *    Fix OWLParser not ensuring streams are closed on exit #973  *    Error with undeclared classes in domain axioms #962  *    Fix Ontology caches should use weak keys #984    ## 5.1.16 28 July 2020    ### Bug fixes:    *    Fix follow multiple redirects across protocols #954  *    Javadoc fixes for deprecated stream methods #950    ## 5.1.15 02 July 2020    ### Features:    *    Allow creation of tautologies for n-ary axioms #776  *    Configurable fast pruning window size    ### Bug fixes:    *    Fix javadoc for OWLObject::nestedClassExpressions #937  *    Fix classAssertionAxioms with OWLClassExpression fails #930  *    Fix Include ontology annotations in signature #928  *    Fix Unable to set base directive for turtle writers #938  *    Fix OWLAPI accepts IRIs with leading spaces #940  *    Fix SWRL body reordered when structure shared #936  *    Fix roundtrip through OBO changes IRI of owl:versionInfo #947        ## 5.1.14 18 April 2020    ### Features:  *    General modularity classes contributed by Robn Nolte    ### Bug fixes:    *    Fix XSD datatypes are erroneously quoted in OBO writer #918  *    Fix referencingAxioms(OWLPrimitive) misses nested literals #912  *    Fix Empty line in META-INF/services/ files causes exceptions #924  *    Fix OWLOntologyWriterConfiguration does not disable banner comments #904      ## 5.1.13 27 January 2020    ### Bug fixes:    *    Fix OWLEntityRenamer and anonymous individuals #892  *    Fix Builtin annotation properties lost during parsing #895  *    Deal with OWLAnnotationProperty entities in OWLEntityURIConverter class #896      ## 5.1.12 20 October 2019    ### Bug fixes:    *    Implement Allow gzipped imports #887  *    Fix Race condition in Injector #883  *    Jackson update  *    Fix containsReference(OWLEntity) should be deprecated #864  *    Fix referencingAxioms(OWLPrimitive) misses IRI appearances #865  *    Fix Javadoc on applyChange/applyChanges and using the wrong manager #868  *    Fix OWLObjectPropertyExpression#getSimplified() used incorrectly #882  *    Fix Incomplete javadoc on OWLNaryAxiom#asPairwiseAxioms #884  *    Fix OWLNegative*AssertionAxiom#containsAnonymousIndividuals javadoc #885  *    Fix Annotated axiom with anon expression saved incorrectly #881  *    Fix Ontology with relative IRIs is serialized incorrectly #880  *    Fix Ann. annotation with anon individual saved in RDF incorrectly #877  *    Fix Annotations dropped if annotation property is undeclared #875  *    Fix SAXException from AutoIRIMapper at debug logging level #878  *    Ensure isAnonymous is implemented correctly #867  *    Fix Null pointers with imports and relation declarations #859  *    Amend base and escaped characters in Tutle parsing #857  *    Fix Exception when converting obi to obo #860      ## 5.1.11 02 June 2019    ### Features:    *    Add support to load an ontology from classpath #837  *    Implement Allow annotations to be skipped in module extraction #838  *    Add support for custom tags in obo files.    ### Bug fixes:    *    Fix Unescaping characters: OBOFormatParser#handleNextChar() #822  *    Fix IRI PREFIX_CACHE instance uses too much memory #825  *    Fix roundtrip of escaped values #833  *    Fix MaximumNumberOfNamedSuperclasses should count super classes #836  *    Fix OWLDataPropertyAxiom not a subinterface of OWLPropertyAxiom #831  *    Fix HTTP 307 and HTTP 308 redirects are not followed (in 4.x) #821  *    Fix Missing escape character in OBO output #828  *    Fix 5.1.10 Regression in OwlStringTools#translate(..) #829  *    Fixed several incorrect XSD datatype matching patterns.  *    Fix OWLDataFactory::getLiteral error with empty string and integer #846  *    Fix Unnecessary dc prefix added by Manchester syntax parser #845  *    Fix Multiple Ontology Definitions should obey strict parsing #840  *    Fix OWLLogicalEntity is not an OWLAnnotationProperty #847  *    Security: Jackson to 2.9.9  *    Fix Manchester syntax parser crashes on class/property punning #851  *    Fix OBO parser does not support qualifier block #852      ## 5.1.10 04 March 2019    ### Bug fixes:    *    Fix DLExpressivity checker never computes anything #810  *    Jackson version to 2.9.8  *    Fix ensure allValuesFrom axioms are not lost #808  *    Fix HTTP 307 and HTTP 308 redirects are not followed #821  *    Fix OBO renderer stuck with untranslatable axioms on concurrent managers  *    Fix Annotations on DifferentIndividualsAxioms lost #816  *    Fix No roundtrip for IRIs with colons in the fragment or path #817  *    Fix EOFException in CustomTokenizer #813  *    Fix Cyclic imports and missing declarations cause parsing error #798    ## 5.1.9 12 December 2018    ### Bug fixes:    *    Refactor OWLProfile implementations #638  *    Fix Missing user defined datatype violation in literals #639  *    Fix RDFGraph getSubjectsForObjects caught in infinite loop #809    ## 5.1.8 1 December 2018    ### Features:    *    Add OWLClassExpression.isNamed method #790  *    Fix injection problem under OSGi  *    Implement Allow Atomic Decomposition to skip assertions #796  *    Expressivity Checker for EL and FL #500    ### Bug fixes:    *    Fix ReadWriteLock should be injector singleton #785  *    Fix Cyclic import of versioned ontologies fails #788  *    Fix Annotate ontology annotations #791  *    Fix Incorrect documentation for OWLOntologyManager methods. #795  *    DisjointClasses with OWL:Thing produces incorrect axiom #747  *    Fix Concurrent managers with own lock shared with own ontologies #806    ## 5.1.7 2 September 2018    ### Features:    *    PROV and TIME vocabularies    ### Bug fixes:    *    Add representativeInstances() to OWLReasoner #772  *    SWRLRule hash code computed incorrectly  *    Fix OWLOntology with shared structure causes incorrect RDF/XML #780    ## 5.1.6 24 July 2018    ### Features:    *    Remove Guice dependencies  *    Upgrade to jsonld-java version 0.12.0 (performance) #763  *    Allow building with Java 10    ### Bug fixes:    *    Move from Trove4j to HPPC-RT #774  *    Fix incorrect sorting of OBO header tags  *    Fix Line breaks in rdfs:label cause invalid FS output #758  *    Fix AutoIriMapper chooses wrong IRIs. #755    ## 5.1.5 23 April 2018    ### Features:    *    Add an option to represent version build as strings  *    Implement #375 OWLZip reader and writer  *    Allow trimming to size after load to be disabled  *    do not register deprecated oboparser by default #729    ### Bug fixes:    *    Fix SWRL variable IRIs violate URN spec #732  *    default IRI for unnamed ontologies is not valid  *    doubling of # at the end of default namespace  *    IRI should return true for isIRI()  *    Fix OWL/XML writes langString unnecessarily #748  *    Fix importsDeclaration not returning imports for *.obo #727    ## 5.1.4 4 January 2018    ### Features:    *    Support Authorization header in remote loading    ### Bug fixes:    *    Fix Problem saving ontologies in Turtle #719  *    Fix Null pointer in OWLObjectPropertyManager #723  *    Remove com.google.inject and add exclusion of javax.annotation. #720    ## 5.1.3 4 November 2017    Features:    * Performance improvements on parsing of large ontologies.    ## 5.1.2 13 October 2017    ### Features:    *    Accept Headers to include all MIME types with supporting parsers #705  *    Add HasAnnotationValue interface with methods for mapping  *    Optional methods for OWLOntologyChange  *    Implement efficient way to test if an ontology refers entity type #698    ### Bug fixes:    *    Do not output xsd:string when unnecessary #640  *    OWL/XML should reject XML files that are not valid OWL/XML #657  *    getFragment advices an non existent replacement #684  *    OWLObject immutable collections sorted #702  *    Sort imports and imports closure #702  *    Sort namespace prefixes for XML serialization #702  *    OWLOntologyMerger fails with ConcurrentModificationException #673  *    Poor performance in OWLImmutableOntologyImpl.getImportsClosure #696  *    Fix AtomicDecomposition throws Nullpointer #695  *    Fix hashcode and equals on OWLOntology differ #694    ## 5.1.1 25 July 2017    ### Features:    *    Add REPAIR_ILLEGAL_PUNNINGS property to disable fix of illegal punnings  *    Move punning log to warning    ### Bug fixes:    *    Fix Profiles.OWL2_FULL returns Profiles.OWL2_DL #667  *    Fix EntitySearcher.getEquivalentClasses incorrectly returns itself #663  *    OWLEntityRenamer should rename annotation props in ontology annotations  *    Fix blank node ids should be NCNames in the RDF/XML output #689    ## 5.1.0 30 March 2017    ### Features:    *    Add stream support on reasoning interface.  *    Allow SyntacticLocalityModuleExtractor to exclude assertions. #462  *    Explanations: do not fail hard for profile violations  *    Latex formatting improvements  *    ensure ontology annotations are sorted  *    Implement HasApplyChanges.applyChanges returns ChangeApplied #544  *    recompile parsers with javacc 7  *    issue #612 : Add japicmp maven plugin to track API changes  *    Implement Zip with dependencies included for non Maven users #584  *    Dependencies update, move to rdf4j    ### Bug fixes:    *    Fix Relating to inferred axiom generators #646  *    Fix duplication and inverseOf properties in generators #646  *    Fix SimpleRenderer writes InverseOf instead of ObjectInverseOf #647  *    Fix nested annotation bug  *    Fix Turtle parser failure on some bioportal ontology #610  *    Fix Manchester expression parser bug with data cardinalities #609    ## 5.0.5 4 January 2017    ### Bug fixes:    *    Allow supplier for null error messages  *    IsAnonymous, isIndividual, isAxiom, isAnonymousExpression on OWLObject  *    Performance improved in creating OWLOntologyManager  *    Ensure XXE vulnerability is prevented  *    Fix Structural sharing in GCIs causes errors #564  *    Fix Issue with serialization - OWLAPI version 3.5.5 #586  *    Fix Problems compiling version5 for Android v24 Nougat #585  *    Fix Order of RDF triples affects annotation parsing #574  *    Fix Property Axiom Generator should check set size #527  *    Fix Imports not properly loaded after all ontologies removed #580  *    Fix Incomplete Parsing of DifferentIndividuals with distinctMembers #569  *    Fix Apache Harmoony SAX parser not supported #581  *    Fix line separators in `DLSyntaxHTMLStorer` #583  *    Fix IRIs with query string cause OFN unparseable output #570  *    Fix Unqualified data restriction considered qualified #576    ## 5.0.4 16 October 2016    ### Bug fixes:    *    Serializability issues    ## 5.0.3 10 September 2016    ### Bug fixes:    *    Fix OMN Parser mistakes punned class for object property #548  *    Fix OWL/XML format does not support SWRL variable IRIs #535  *    Fix Explicit prefix ignored during serialization #529  *    Fix Saving to OFN loses namespaces due to double default prefix #537  *    Fix DL-Syntax HTML formatter issues #536  *    Fix ManchesterOWLSyntaxRenderer SubPropertyOf render inconsistent #534  *    Fix Slow inferred disjoint generator #542  *    Fix Relative Ontology IRI causes missing declaration #557  *    Fix Invalid Turtle ontologies when hash IRIs are used #543  *    Fix Calling setOntologyDocumentIRI reset imports closure cache #541  *    Fix OWLOntology::annotationPropertiesInSignature has duplicates #555  *    Fix Latex Renderer produces bad LaTeX for property inverses #526  *    Fix ManchesterOWLSyntaxParser doesn't handle the OWL 2 datatypes #556  *    Fix Turtle Renderer doesn't escape periods #525  *    Fix Import IRI not resolved correctly in OBO input #523    ## 5.0.2 7 May 2016    ### Features:    *    Allow banning of parsers #510  *    Allow disabling banner comments  *    Move builders to api to be able to use them in parsers  *    Make AxiomType Comparable    ### Bug fixes:    *    Fix performance regression in String.intern() and improve memory footprint  *    Fix Default prefix overrides explicit prefixes #522  *    Fix Anonymous individuals parsing problems #494  *    Fix Noisy print messages from logging framework #516  *    Fix SWRL rules with incorrect equals() #512  *    Fix OWL 2 Full Profile #508  *    Fix Annotated entities not declared in RDF/XML #511  *    Fix XML literal is not self contained #509    ## 5.0.1 19 March 2016    ### Bug fixes:    *    Fix Fixing punnings fails on dp subDataPropertyOf rdfs:label #505  *    ""Dumping remaining triples"" displayed when no triples left #502  *    Fix Annotations lost on entity renaming of SWRLRules #501  *    Fix Profile validation throws exception on ontology annotations #498  *    Fix RDF/XML parser failing to load ontology containing XMLLiteral #496  *    Fix #489 makeLoadImportRequest log should be a warning #489  *    Fix Anonymous individuals parsing problems #494  *    Fix Saving fails for ontologies containing XMLLiteral #495  *    Fix Anonymous individuals parsing problems #494    ## 5.0.0 28 February 2015    ### Features:    *    Allow overridable and defaultable properties  *    FaCT++ AD implementation from OWLAPITOOLS  *    Add OWLObjectTransformer to replace any part of an ontology  *    Add HasOperands interface  *    Ad componentsWithoutAnnotations() to simplify equalsIgnoreAnnotations  *    Add componentsAnnotationsFirst() to OWLObject  *    Add component() to OWLObject to create a common interface for hashcode  *    Add rdf:langString data type  *    commons-rdf-api integration  *    Add InferenceDepth and convenience methods to OWLReasoner  *    OWLRDFConsumer fills guessed type declaration table with blank nodes  *    Marked methods returning sets where a stream is available as deprecated  *    Update guice version to beta 5  *    Replace Google Optional with Java 8 Optional (#250)  *    OWLProfileViolationVisitorEx returns Optional<T> rather than nulls  *    Add default methods to OWLOntology for direct add/remove of axioms  *    Added a getFormat method to OWLOntology  *    Merged visitor adapters into the interfaces with default methods  *    isAvailable/get pattern in OWLOntologyDocumentSource changed to Optional  *    OWLOntology.getReferencingAxioms() finds any of the OWLPrimitive objects  *    Enabled OWLOntologyManager to build and keep an OntologyConfigurator instance  *    Declaration of save methods on OWLOntology  *    OWLAPI 5 uses Java 8    ## 4.5.20 31 July 2021    ### Features:    *    Specify RioSetting values for Rio renderers #614    ### Bug fixes:    *    Fix sameAs failure when more than 2 entities included #994  *    Ordering of literals to be consistent with OWLAPI 5  *    Fix Trig and rdf/json should include a named graph. #1002  *    Fix ObjectHasSelf rendered wrongly in manchester syntax #1005      ## 4.5.19 7 November 2020    ### Bug fixes:    *    Fix OWLParser not ensuring streams are closed on exit #973  *    Error with undeclared classes in domain axioms #962  *    Fix Ontology caches should use weak keys #984      ## 4.5.18 23 October 2020    ### Bug fixes:    *    Fix Performance of signature checks during ontology changes #968  *    Fix Error on RIO renderer when expression has 6000 elements #971  *    Fix OWLOntology#datatypesInSignature to include ontology header #965  *    Fix Ontology not loaded in case of multiple HTTP redirects #954      ## 4.5.17 02 July 2020    ### Features:    *    Let OBO parser follow redirects  *    Allow creation of tautologies for n-ary axioms #776  *    Configurable fast pruning window size    ### Bug fixes:    *    Fix javadoc for OWLObject::nestedClassExpressions #937  *    Fix classAssertionAxioms with OWLClassExpression fails #930  *    Fix Include ontology annotations in signature #928  *    Fix Unable to set base directive for turtle writers #938  *    Fix OWLAPI accepts IRIs with leading spaces #940  *    Fix SWRL body reordered when structure shared #936  *    Fix roundtrip through OBO changes IRI of owl:versionInfo #947      ## 4.5.16 18 April 2020    ### Bug fixes:    *    Fix XSD datatypes are erroneously quoted in OBO writer #918  *    Fix referencingAxioms(OWLPrimitive) misses nested literals #912  *    Fix Empty line in META-INF/services/ files causes exceptions #924      ## 4.5.15 28 January 2020    ### Bug fixes:    *    Fix OWLEntityRenamer and anonymous individuals #892  *    Fix Builtin annotation properties lost during parsing #895  *    Deal with OWLAnnotationProperty entities in OWLEntityURIConverter class #896      ## 4.5.14 19 October 2019    ### Bug fixes:    *    Implement Allow gzipped imports #887  *    Fix Race condition in Injector #883  *    Jackson update  *    Fix containsReference(OWLEntity) should be deprecated #864  *    Fix Javadoc on applyChange/applyChanges and using the wrong manager #868  *    Fix OWLObjectPropertyExpression#getSimplified() used incorrectly #882  *    Fix Incomplete javadoc on OWLNaryAxiom#asPairwiseAxioms #884  *    Fix OWLNegative*AssertionAxiom#containsAnonymousIndividuals javadoc #885  *    Fix Annotated axiom with anon expression saved incorrectly #881  *    Fix Ontology with relative IRIs is serialized incorrectly #880  *    Fix Ann. annotation with anon individual saved in RDF incorrectly #877  *    Fix Annotations dropped if annotation property is undeclared #875  *    Fix SAXException from AutoIRIMapper at debug logging level #878  *    Ensure isAnonymous is implemented correctly #867  *    Fix Null pointers with imports and relation declarations #859  *    Amend base and escaped characters in Tutle parsing #857    ## 4.5.13 02 June 2019    ### Features:    *    Add support for custom tags in obo files. #848    ### Bug fixes:    *    Fix OWLLogicalEntity is not an OWLAnnotationProperty #847  *    Security: Jackson to 2.9.9  *    Fix Manchester syntax parser crashes on class/property punning #851  *    Fix OBO parser does not support qualifier block #852    ## 4.5.12 06 May 2019    ### Features:    *    Add support to load an ontology from classpath #837  *    Implement Allow annotations to be skipped in module extraction #838    ### Bug fixes:    *    Fix Multiple Ontology Definitions should obey strict parsing #840  *    Fix Unnecessary dc prefix added by Manchester syntax parser #845  *    Fix OWLDataFactory::getLiteral error with empty string and integer #846  *    Fixed several incorrect XSD datatype matching patterns #844    ## 4.5.11 17 April 2019    ### Bug fixes:    *    Fix HTTP 307 and HTTP 308 redirects are not followed (in 4.x) #821  *    Fix Missing escape character in OBO output #828  *    Fix OWLDataPropertyAxiom not a subinterface of OWLPropertyAxiom #831  *    Fix MaximumNumberOfNamedSuperclasses should count super classes #836  *    Fix OWLOntology::getGeneralClassAxioms slow #839  *    Fix roundtrip of escaped values #833    ## 4.5.10 14 March 2019    ### Bug fixes:    *    Fix HTTP 307 and HTTP 308 redirects are not followed (in 4.x) #821  *    Fix Ensure allValuesFrom axioms are not lost #808  *    Fix EOFException in CustomTokenizer #813  *    Fix Cyclic imports and missing declarations cause parsing error #798  *    Fix Unescaping characters: OBOFormatParser#handleNextChar() #822  *    Fix IRI PREFIX_CACHE instance uses too much memory #825      ## 4.5.9 1 February 2019    ### Bug fixes:    *    Jackson version to 2.9.8  *    Fix compatibility with Guava 27 #814  *    Fix OBO renderer stuck with untranslatable axioms on concurrent managers  *    Fix Annotations on DifferentIndividualsAxioms lost #816  *    Fix No roundtrip for IRIs with colons in the fragment or path #817    ## 4.5.8 22 December 2018    ### Features:    *    Refactor OWLProfile implementations #638    ### Bug fixes:    *    Fix Missing user defined datatype violation in literals #639  *    Fix RDFGraph getSubjectsForObjects caught in infinite loop #809  *    Fix DLExpressivity checker never computes anything #810    ## 4.5.7 1 December 2018    ### Features:    *    Add OWLClassExpression.isNamed method #790  *    Fix injection problem under OSGi  *    Expressivity Checker for EL and FL #500    ### Bug fixes:    *    Fix ReadWriteLock should be injector singleton #785  *    Fix Cyclic import of versioned ontologies fails #788  *    Fix Annotate ontology annotations #791  *    Fix Incorrect documentation for OWLOntologyManager methods. #795  *    DisjointClasses with OWL:Thing produces incorrect axiom #747  *    Fix Concurrent managers with own lock shared with own ontologies #806    ## 4.5.6 6 September 2018    ### Bug fixes:    *    OSGi issues fixed    ## 4.5.5 2 September 2018    ### Features:    *    PROV and TIME vocabularies    ### Bug fixes:    *    Prefix splitting at wrong place with percent encoded IRI #737  *    Fix OWLOntology with shared structure causes incorrect RDF/XML #780    ## 4.5.4 26 July 2018    ### Features:    *    Remove Guice dependencies  *    Upgrade to jsonld-java version 0.12.0 (performance) #763  *    Build with Java 10    ### Bug fixes:    *    Move from Trove4j to HPPC-RT #774  *    Literals with no lang and string literals must equal each other  *    Fix OWLLiteral.parseDouble should throw NumberFormatException #764  *    No need for Guice and Guava version updates  *    Amend pom files to build vaild osgi distribution #768  *    Fix incorrect sorting of OBO header tags  *    Fix Line breaks in rdfs:label cause invalid FS output #758  *    Fix AutoIriMapper chooses wrong IRIs. #755    ## 4.5.2 22 April 2018    ### Features:    *    Add an option to represent version build as strings  *    Implement #375 OWLZip reader and writer  *    Signature cache for ontologies  *    Allow trimming to size after load to be disabled  *    do not register deprecated oboparser by default #729    ### Bug fixes:    *    Fix SWRL variable IRIs violate URN spec #732  *    default IRI for unnamed ontologies is not valid  *    doubling of # at the end of default namespace  *    IRI should return true for isIRI()  *    Fix OWL/XML writes langString unnecessarily #748    ## 4.5.1 7 December 2017    ### Features:    *    Support Authorization header in remote loading    ## 4.5.0 13 October 2017    ### Features:    *    Accept Headers to include all MIME types with supporting parsers #705  *    Add HasAnnotationValue interface with methods for mapping  *    Optional methods for OWLOntologyChange  *    Implement efficient way to test if an ontology refers entity type #698    ### Bug fixes:    *    OWL/XML should reject XML files that are not valid OWL/XML #657  *    OWLObject immutable collections sorted #702  *    Sort imports and imports closure #702  *    Sort namespace prefixes for XML serialization #702  *    OWLOntologyMerger fails with ConcurrentModificationException #673  *    getFragment advices an non existent replacement #684  *    Fix Poor performance in OWLImmutableOntologyImpl.getImportsClosure #696    ## 4.3.2 25 July 2017    ### Features:    *    Add REPAIR_ILLEGAL_PUNNINGS property to disable fix of illegal punnings  *    Move punning log to warning    ### Bug fixes:    *    Fix Profiles.OWL2_FULL returns Profiles.OWL2_DL #667  *    Fix EntitySearcher.getEquivalentClasses incorrectly returns itself #663  *    Fix blank node ids should be NCNames in the RDF/XML output #689  *    OWLEntityRenamer should rename annotation props in ontology annotations    ## 4.3.1 27 March 2017    ### Features:    *    Allow SyntacticLocalityModuleExtractor to exclude assertions. #462  *    Explanations: do not fail hard for profile violations    ### Bug fixes:    *    Fix Relating to inferred axiom generators #646  *    Fix duplication and inverseOf properties in generators #646    ## 4.3.0 22 March 2017    ### Features:    *    Implement HasApplyChanges.applyChanges returns ChangeApplied #544  *    Implement Zip with dependencies included for non Maven users #584  *    issue #612 : Add japicmp maven plugin to track API changes  *    dependencies update    ### Bug fixes:    *    Fix OWLLiteral produce different hash codes in version 4 #645  *    Multiple nested annotations fail in RDF/XML #470  *    Parsing of nested anonymous nodes broken in 4.1.1 #478  *    ensure ontology annotations are sorted  *    Fix Turtle parser failure on some bioportal ontology #610  *    Fix Manchester expression parser bug with data cardinalities #609    ## 4.2.8 4 January 2017    ### Bug fixes:    *    Fix Structural sharing in GCIs causes errors #564  *    Fix Issue with serialization - OWLAPI version 3.5.5 #586  *    Fix Order of RDF triples affects annotation parsing #574  *    Fix Property Axiom Generator should check set size #527  *    Fix Imports not properly loaded after all ontologies removed #580  *    Performance improved in creating OWLOntologyManager  *    Fix Incomplete Parsing of DifferentIndividuals with distinctMembers #569  *    Fix Apache Harmoony SAX parser not supported #581  *    Ensure XXE vulnerability does not exist  *    Fix line separators in `DLSyntaxHTMLStorer` #583  *    Fix IRIs with query string cause OFN unparseable output #570  *    Fix Unqualified data restriction considered qualified #576  *    Fix OWLOntologyManager in OWLImmutableOntologyImpl should be @Nullable #568      ## 4.2.7 16 October 2016    ### Bug fixes:    *    Serialization issues      ## 4.2.6 10 September 2016    ### Bug fixes:    *    Fix OMN Parser mistakes punned class for object property #548  *    Fix OWL/XML format does not support SWRL variable IRIs #535  *    Fix Explicit prefix ignored during serialization #529  *    Fix Saving to OFN loses namespaces due to double default prefix #537  *    Fix DL-Syntax HTML formatter issues #536  *    Fix ManchesterOWLSyntaxRenderer SubPropertyOf render inconsistent #534  *    Fix Slow inferred disjoint generator #542  *    Fix Relative Ontology IRI causes missing declaration #557  *    Fix Invalid Turtle ontologies when hash IRIs are used #543  *    Fix Calling setOntologyDocumentIRI reset imports closure cache #541  *    Fix Latex Renderer produces bad LaTeX for property inverses #526  *    Fix ManchesterOWLSyntaxParser doesn't handle the OWL 2 datatypes #556    ## 4.2.5 17 May 2016    ### Bug fixes:    *    Fix Turtle Renderer doesn't escape periods #525  *    Fix Import IRI not resolved correctly in OBO input #523    ## 4.2.4 7 May 2016    ### Bug fixes:    *    Fix Default prefix overrides explicit prefixes #522  *    Fix Anonymous individuals parsing problems #494  *    Fix OWL 2 Full Profile #508  *    Fix Annotated entities not declared in RDF/XML #511  *    Fix XML literal is not self contained #509    ### Features:    *    Allow banning of parsers #510  *    Allow disabling banner comments  *    Fix Noisy print messages from logging framework #516    ## 4.2.3 19 March 2016    ### Bug fixes:    *    Fix Fixing punnings fails on dp subDataPropertyOf rdfs:label #505    ## 4.2.2 15 March 2016    ### Bug fixes:    *    Fix RDF/XML parser failing to load ontology containing XMLLiteral #496  *    Fix Profile validation throws exception on ontology annotations #498  *    Fix Annotations lost on entity renaming of SWRLRules #501  *    Fix ""Dumping remaining triples"" displayed when no triples left #502    ## 4.2.1 5 March 2016    ### Bug fixes:    *    Fix Anonymous individuals parsing problems #494  *    Fix makeLoadImportRequest log should be a warning #489  *    Fix Saving fails for ontologies containing XMLLiteral #495    ## 4.2.0 28 February 2016    ### Features:    *    Allow overridable and defaultable properties    ### Bug fixes:    *    Fix EntitySearcher.getDomains(OWLAnnotationProperty, OWLOntology) returns the range axioms instead of the domain axioms #492    ## 4.1.4 3 February 2016    ### Features:    *    Add OWLObjectTransformer to replace any part of an ontology #487  *    Add OWLLiteralReplacer to replace literals with different values #487  *    Throw error on malformed ontologies when parsing is strict (avoid Error1 classes) #444    ### Bug fixes:    *    Fix RBox axiom types #479  *    Fix Parsing of nested anonymous nodes broken in 4.1.1 #478  *    Fix Error in rdf handling of deep nested annotations  *    Fix EntitySearcher.subPropertiesFilter is broken #486  *    Fix Cyclic imports cause illegal punning #483  *    Fix OWLOntologyManager.copyOntology does not copy imports #480  *    Fix Changes not successfully applied broadcasted as applied #476  *    Add restriction on aduna packages #471  *    Fix BOM removal wrapper is applied to all input streams #212  *    Fix Serialization into incorrect OWL syntax #468  *    Fix Constraint Sesame Versions in OSGi Imports #471  *    Fix ExplanationOrdererImpl.SeedExtractor throws exception #469  *    Fix for Multiple nested annotations fail in RDF/XML #470    ## 4.1.0 25 October 2015    ### Features:    *    Port protege-owlapi concurrency support to the OWL API  *    Enabled the creation of a concurrent manager  *    Part of Add transactional support to OWLOntology #382 : ability to rollback the application of a set of changes if one fails. Added a ChangeApplied.NO_OPERATION to track those changes that did not have any effect (e.g., adding an axiom that is already present or removing an axiom that is not present).  *    ManchesterOWLSyntaxParser could use a convenience method parseClassExpression(String s) #384  *    Add setting to use labels as banner comment or use IRIs as before  *    Add config to switch saving all anon individuals  *    Enforce correct argument types for OWLObjectInverseProperty in default impl  *    Add control flag to structure walking to allow annotations to be walked  *    Reduce space used by OBO Parser  *    update jackson dependency to 2.5.1    ### Bug fixes:    *    OWLImmutableOntologyImpl.asSet() assumes input iterable is coming from a duplicate free collection #404  *    OWLOntologyManagerImpl.getVersions() calls get() on Optional without checking if the contained value is present #401  *    OWLOntologyManagerImpl tests storers with keys rather than canStoreOntology #400  *    setOntologyLoaderConfigurationProvider should accept a javax.inject.Provider #399  *    Cleaned distribution jar  *    Losing Annotations on Annotation Assertion Axioms #406  *    Return types of RemoveAxiom(s) vs. AddAxiom(s) #408  *    Examples file needs review #407  *    BFO fails to load in API version 4.0.X #405  *    Fix RDF/XML renderer does not like properties using urns (or almost urns) #301  *    All anonymous individuals are remapped consistently upon loading. #443  *    Only output node id for anonymous individuals if needed  *    Anon individuals appearing more than once as objects #443  *    Sort annotation assertions on output for manchester syntax #332  *    OWL 2 profile validation mistakes #435  *    [Typedef] created_by don't support space #419  *    ttl namespace abbreviations not preserved on serialization #421  *    Use EntitySearcher.getAnnotationObjects() when interested only in the object of annotation assertion axioms on an entity rather than the annotations on the annotation assertion axioms.  *    convincing owlapi-distribution to export rio dependencies  *    RDF issues testing 4.1.0 RC 1 #412  *    Annotation subproperty/domain/range disappear after save/load cycle (punning enhancement) #351      ## 4.0.2 17 April 2015    ### Features:    *    Simple OWL/FSS IRI sniffing for AutoIRIMapper.  *    Fix #373 Individuals in SWRL rules get extra type  *    XZ compression support  *    Sorting of FunctionalSyntaxObjectRenderer  *    Sort untyped IRI Annotation Assertions, general axioms.  *    Sort axiom Annotations, if they are output using the translateAnnotations method. Negative property assertions not yet ordered; neither are nary axioms and class expressions.  *    Add legacy files  to the OSGI distribution for  version 3.5 backwards compatibility.  *    Some reasoners may not support certain types of reasoning. They may also not support OWLReasoner::isEntailed, and hence may return false for all calls to  OWLReasoner::isEntailmentCheckingSupported.  *    Support preserving, transferring, and adding annotations during OBO Macro expansion  *    Add Extensions to link document formats and common file extensions  *    Optimise annotation lookup for module extraction enrichment  *    Change strategy for MapPointer storage: do all initial insertions in a list, and turn it into a THashSet upon first call to trimToSize or any get/size calls  *    Add benchmarking util to load ontology then dump to an hprof file for use with MAT  *    Improve conversion from OWL to OBO, add test case    ### Bug fixes:    *    Fix PriorityCollectionSorting should be fixed at manager creation time #395  *    Fix OWLOntologyManagerImpl.getVersions() calls get() on Optional without checking if the contained value is present #401  *    Fix XMLLiterals should be output as XML, not quoted XML #333  *    Fix Should RDF/XML Renderer include annotation assertions from the entire closure? #371  *    Fix Using a PriorityCollection for IRI mappers (and perhaps other things) is confusing #386  *    Fix THashSets got left using default load factor (0.50) after construction is finished. #380  *    Fix RDF Consumer failing to guess property types #322  *    Fix NonMappingOntologyIRIMapper confuses extra OWLOntologyIRIMapper implementations #383  *    Fix Data property with xsd:integer range parsed as Object property (RDF/XML source) #378  *    Fix imports in OWL Manchester Notation fail #347  *    Fix ELProfile incorrectly rejects DisjointClasses axioms. #343  *    Fix OWLRDFConsumer fills guessed type declaration table with blank nodes #336  *    Fix #338 OWL-API: DOCTYPE declaration missing in XML serialization  *    Fix #337 Axiom Equality short-circuits are in the wrong place  *    Fixed bug when rendering anonymous individual in Manchester OWL syntax.  *    Reverted incorrect setting of all packaging types to bundle    ## 4.0.1 22 November 2014    ### Features:    *    trimToSize available to cut ontology internals collections to minimal size after loading (Simon Spero)  *    various memory optimizations and caching of rarely used values removed (Simon Spero)  *    use of Trove collections for ontology internals  *    OBOFormat: Configuration option to skip validation before writing OBO file #290    ### Bug fixes:    *    fix a multithread bug revealed by owlapitools tests  *    fix #299 roundtrip errors on TriX and various other syntaxes  *    fix #292 SWRL rules saved by older versions of OWLAPI/Protege lose annotations  *    fix #288 Errors with object property assertions with inverseOf properties  *    fix #289 Manchester syntax errors: inverse() not recognized, SWRL rules rendered twice  *    fix #287 Error parsing RDF/Turtle with triple quoted literals - mismatching single and double quotes  *    fix #281 OntologyIRIShortFormProvider does wrong shortform generation      ## 4.0.0 10 September 2014    Supported Java versions: Java 7, Java 8    ### Features:    *    added HasAnnotationPropertiesInSignature to uniform treatment of annotation properties  *    added missing EntitySearcher methods for negative property assertions  *    Use Trove collections for ontology internals  *    improved performance of OWLAxiomImpl.equals  *    Transform functions from Collection<OWLOntoloy> and Collection<OWLOntologyID> to Collection<IRI>  *    PMD critical violations addressed  *    fix ServiceLoader use to be OSGi compatible  *    create osgidistribution: osgidistribution is a jar with embedded dependencies and including the compatibility module. It addresses the issues due to the OWLAPI dependencies not being wrapped in OSGi bundles separately.  *    enabled OWLOntologyManager to build and keep a loader configuration  *    OBO 1.2 ontologies cannot be parsed by the 1.4 parser. Added 1.2 parser from OWLAPITOOLS to compatibility package.  *    added saveOntology() methods to OWLMutableOntology  *    Add a copy/move ontology method to OWLOntologyManager #12  *    Search introduced to replace forSuperPosition, forSubPosition, ignoreAnnotations booleans  *    Imports.INCLUDED, Imports.EXCLUDED introduced instead of boolean arguments #156  *    introduced ChangeApplied to simplify code around applying changes and remove null warnings  *    Introduction of Searcher with transform functions  *    OWLOntology now exposes a new method for searching for axioms referring an entity, abstracting all various getAxiomsBy... methods.  *    Refactor OWLSignature for uniform imports closure use  *    Refactored OWLAxiomCollection for uniform use of import closure  *    Added targeted parsing to allow one and only one parser to be tried on a document input source. Takes into account MIME type as well.  *    Use MultiMap from Guava throughout V4.0.0 #153  *    OBO parser updated and replaced with oboformat 5.5 release  *    @Nonnull annotations and warnings  *    Centralised and uniformed sax parser factories  *    OWLOntologyID to use Optional #160  *    Refactored property hierarchy to remove generics.  *    Added an OWLParser implementation to enable actual use of DL parser  *    Added a pairwise visitor, changed storage strategy for nary axioms. The pairwise visitor interface is a functional interface that is applied to all distinct pairs in a collection of objects.  *    Added asPairwiseAxioms to all nary axiom types  *    Added HasPriority annotation and comparator for prioritizable objects  *    Added PriorityCollection for managing injected collections  *    Added ServiceLoader adapter for Guice injection  *    Added ManchesterOWLSyntaxParser interface to api  *    OWLAPITOOLS profiles and fixers included  *    Added Profiles enumeration and supported OWL profile for known reasoners.  *    Dependency injection enabled for fixers.  *    Added Guice module for creating OWLOntologyManager and OWLDataFactory  *    Introduced OWLOntologyBuilder to allow swapping OWLOntology implementations via Guice  *    Service loader module in api to be used independently of the owlapi-impl module  *    Add RDFa parser and use static META-INF/services files for owlapi-rio  *    Add JSON-LD support  *    Add rdf:langString from RDF 1.1  to OWL/RDF vocabulary.  *    Better BOM treatment through Apache BOMInputStream  *    Use SLFJ for logging  *    Guava 17  *    Guice 4.0-beta  *    Added jsr305 for Nonnull and Nullable annotations  *    Fix build and javadoc for Java 8   *    Do not raise violations from entities used only in declarations.  *    Do not add declarations for illegal punnings when saving ontologies #112  *    Do not cache any literals in OWLDataFactoryInternalsImpl. Overall improvement of about 9% when parsing Gene Ontology in FSS.  *    Implement Default prefix manager in prefix ontology format objects cannot be overriden #9  *    Implement automatic retries when loading from URLs, managed through OWLOntologyLoaderConfiguration #66  *    Switched OWLDataFactory IllegalArgumentExceptions to NullPointerExceptions #131  *    Handle annotations on punned types when rendering RDF #183  *    Add OWLOntologyManagerFactory and code to allow use of RioOWLRDFParser through an injector  *    Convert RioParserImpl to stream statements through wherever possible  *    Default datatype in RDFLiteral to PLAIN_LITERAL.  *    Made generated classes package protected  *    Rename OWLObjectRenderer to FunctionalSyntaxObjectRenderer  *    OWLReasoner change lists generic  *    Moved OWLOntologyFormat implementation to formats package  *    Performance improvement for rdf/xml rendering  *    Optimised functional renderer and prefix manager  *    feature: OBO parser replaced with oboformat.  *    OWL2Datatype implements HasIRI  *    Add XSD IRIs simple parsing support #56  *    GZip read/write ability  *    Manchester OWL syntax cleanup    ### Bug fixes:    *    Fix #278 AutoIRIMapper is not namespace aware  *    Direct imports result not updated correctly after manual load #277  *    fix #275 DL Syntax rendering of disjoint classes missing comma  *    fix #271 OWLOntology.getXXXInSignature(boolean) and similar methods  *    fix #270 Add OWLOntology.getReferencingAxioms(OWLPrimitive)  *    fix #268 Add documentation to OWLOntologyID to clarify the relationship between isAnonymous() and getOntologyIRI()  *    fix #267 Consider adding containsXInSignature methods that do not have an imports flag to OWLOntology.  *    fix #254 OWLAsymmetricObjectPropertyAxiom not rendered in DL Syntax  *    fix OWLDocumentFormat as interface #258 #259  *    fix #260 and fix #261 data and object cardinality are quantified restricitons  *    fix #255 PrefixOWLOntologyFormat is missing from the compatibility module  *    fix #253 StructuralReasoner.getSameIndividuals does not behave as advertised  *    Fixed #198 A SubClassOf B will not parse  *    Fixed IRI with a space: %20 escape #146  *    Old OWLEntityCollector brought forward as DeprecatedOWLEntityCollector  *    Fixed serialization warnings reported in #163  *    Use AtomicLong for concurrency support in gzip document sources  *    Moved transitive object property type to rbox  *    Set turtle default prefix slash aware #121   *    Fixed #116 import statement erroneously names anonymous ontology  *    Added test for roundtripping annotated SWRL rules and datatype definitions  *    Fixed annotations on datatype definition axioms fixed in OWL/XML  *    Fixed #20 and #60 use zipped buffer for StreamDocumentBase  *    Fixed #71 Misleading message on resolving illegal IRI  *    Fixed variable rendering in Manchester OWL Syntax to preserve ns.  *    Fixed #46 Round tripping error in functional syntax  *    Fixed #40 RDFGraph.getSortedTriplesForSubject throws exception in Java7  *    Fixed infinite recursion on cyclic import structures when declaration does not match location.    ## 3.4.10 18 January 2014    ### Features:    *    Improved feature 32 implementation (simple SWRL variables rendering)  *    javadoc warnings and errors removed to allow building with Java 8  *    refactored OWLAxiomVisitorExAdapter for general use  *    performance improvement for rdf/xml  *    Renamed variables, added explicit flush. Fixes #67 slow functional sytnax renderer on Java 6  *    default, final and abstract classes changed to allow interface verification  *    add oraclejdk8 early access build to travis  *    update oboformat class names to sync with oboformat project  *    make OBOFormatException extend OWLParserException    ### Bug fixes:    *    Fixes #72 Manchester syntax roundtrip of doubles and SWRL rules fails  *    Fixes #24 Manchester Syntax writer generates unparsable file  *    simple renderer writes duplicate annotations  *    Optimised functional renderer and prefix manager  *    Fix #63 null pointer exception loading an ontology and fix #64 empty ontology returned  *    Fixes #63 OBO Ontology Parsing throws NullPointer  *    updated OSGi settings for oboformat, module name, dependencies  *    fix bundle configuration for oboformat    ## 3.4.9 25 November 2013    ### Features:    *    Added default methods and return values for a few visitor adapters  *    #57 Reimplement NodeID so that it doesn't use an inner class  *    #56 parse things like xsd:string easily  *    OBO parser replaced with oboformat. Original code at: https://code.google.com/p/oboformat/    ### Bug fixes:    *    Fixed wrong naming of gzip file source and target and added stream only versions.  *    Restore UnparseableOntologyException interface  *    Added back deprecated parseConstant for compatibility with Pellet CLI code. Warning: it has bugs.      ## 3.4.8 02 November 2013    ### Features:    *    GZip read/write ability. Compressed gzip input and output streams, but make sure to close the stream in the calling code.  *    OSGI dependency removed  *    Added Namespaces.inNamespace  *    Made ParserException to extend OWLParserException, and OWLParserException a runtime exception.  *    Tidying up Manchester OWL Syntax parser code  *    Added to the documentation on getOWLDatatype(OWLDataFactory)  *    Added a convenience method to convert an OWL2Datatype object to an OWLDatatype.  *    mvn install and mvn deploy now make aggregate sources available.  *    Added more well known namespaces, removed non-explanatory comments  *    Added RDFa Core Initial Context prefixes and other well known prefixes and prefix names.  *    GITHUB-10 : Add RDFa Core Initial Context prefixes  *    Extend Namespaces enum with common prefixes    ### Bug fixes:    *    Fixes #40 Fixes #7 RDFGraph.getSortedTriplesForSubject throws exception in Java7 Refactored TripleComparator into RDFTriple and RDFNode as Comparable  *    DLQueryExample did not print the StringBuilder with the answers  *    Fixes #50 Null anonymous individual parsing ObjectHasValue from OWL/XML  *    Fixes SF 313 Man Syntax Parser gives facets unexpected datatype  *    Fixes SF 101 Manchester OWL Syntax Parser doesn't do precedence correctly  *    Fixes #43 Functional Syntax Parser does not support comments  *    Fixes #41 Parsing error of SWRL atom.  *    Fixes #46 Round tripping error in functional syntax  *    Fixes #37 setAddMissingTypes logging if disabled  *    Missed serialization transients and tests, serialization bug fixes  *    Patch IRI.create(String,String) to match IRI.create(String)    ## 3.4.5 25 July 2013    ### Features:    *    Refactored DefaultPrefixManager to expose addPrefixes  *    Updated pom files for OSGI compatible artifacts.  *    Allow override of DefaultPrefixManager in PrefixOWLOntologyFormat  *    Add CollectionFactory.getExpectedThreads  *    Reduce expected threads from 16 to 8 to improve general performance    ### Bug fixes:    *    literal parsing error was stopping HermiT builds with 3.4.4.  *    errors in OSGI support were stopping Protege from using the feature.  *    Fix #370 Typo in website/htdocs/reasoners.html.  *    Fixed a bug with large integer values being parsed as negative ints.  *    Fix infinite recursion on cyclic import structures.  *    Reverted the use of ThreadLocal as this causes memory leaks in webapps.  *    Bug #343 SWRL roundtrip in manchester syntax    ## 3.4.4 19 May 2013    Features and updates implemented:    *    feature 103 - Add methods to get EntityType names.  *    Updated IRI prefix caching to use thread local caches rather than synchronized caches.  *    added test for annotation retrieval for anonymous individuals.  *    WeakIndexCache vulnerability fixed.  *    Improved redirect following.  *    Added method to OWLOntologyLoaderConfiguration to disable cross protocol redirects on URLConnections.  *    Added parameter to OWLOntologyFormat for temp files, default disabled.  *    Update all JavaCharStream versions to fix the UTF BOM issue.  *    Travis Integration (on github).  *    Update KE interface with getBlocker method.  *    OSGI compatible build.  *    Removed IRI.toString() call when checking blank nodes.  *    Change logging of remaining triples to use less memory.  *    updates for javadoc.  *    Update Mockito to 1.9.5.    ### Bug fixes:    *    Bug #369 IRI.create(String) uses / and # to look for namespace and fragments.   *    Bug #348 ManchesterOWLSyntax round trip problem with disjoint classes.  *    Bug #269 single anonymous individual split in two on save to RDF.  *    Bug #319 redirects from http to https are not followed.  *    Bug #367 Serving ontologies from github as well.  *    Bug 4 (github) Missing method in OWLIndividual.  *    Bug 359 Incorrect Javadoc in OWL2DatatypeImpl and redundant javadoc.  *    Bug 360 - Axiom annotations are not parsed correctly.  *    Bug 364 Null pointer checking for containment of anonymous ontologies.  *    Bug #363 Turtle parser fails when loading valid Turtle document.  *    Bug 362 - owlapi files in temp folder.  *    Bug 357 duplicate axioms saving in OWL functional syntax.  *    Bug 355 - added assertions. Ability to parse 1+e7 as 10000000.  *    Bug 355 - Turtle Parser + Integer format Problem.  *    OntologyAlreadyExist bug when calls are made too fast.      ## 3.4.3 26 Jan 2013    Features implemented:    *    Updated OWLFunctionalSyntaxFactory with more methods.  *    Removed deprecated method calls.  *    Test package broken up in smaller packages.  *    Web site updated and added to version management.    ### Bug fixes:    *    Changed VersionInfo message default to provide more information.  *    Bug 354 fixed: There is AnnotationChange, but no AnnotationChangeData  *    Bug 353 fixed: SimpleRenderer renders SubDataPropertyOfAxiom wrong.  *    Bug 351 fixed: QNameShortFormProvider swapped prefix and iri.  *    Bug 352 fixed: SimpleRenderer does not escape double quotes in literals.      ## 3.4.2 4 December 2012    Features implemented:    *    Repository migrated to Git.  *    Fixed memory leak with OWLObject signatures never being released once cached.  *    Improved Serializable implementation.  *    Made timeout property functional.  *    3585007 	Make OWLAxiomChange.isAdd().  *    3579488 	Clean documentation page.  *    3578004 	Javadoc OWLRDFVocabulary.  *    3578003 	Add OWLOntologyManager.removeOntology(OntologyID).  *    3578002 	Add new constructor for SetOntologyID.  *    3576182 	Bump minimum java version to 1.6.  *    3575834 	Improvements to OWLOntologyManager and related contrib.  *    3566810 	Lack of support in OWLOntologyManager for version IRIs.  *    3521809 	KRSS parser throws Error instead of RuntimeException.    ### Bug fixes:    *    Some OWLLiterals cannot be optimised: -0.0^^xsd:float, 01^^xsd:int (optimisation with primitive types makes reasoners queasy in combination with W3C tests).  *    3590243 	hashCode for literals inconsistent.  *    3590084     misuse of XMLUtils.getNCNameSuffix().  *    3581575 	OWLOntologyDocumentAlreadyExistsException in OWL API 3.4.1.  *    3580114 	IRI isAbsolute method.  *    3579862 	OWLDataFactory.getRDFPlainLiteral creates new objects.  *    3579861 	rdf:PlainLiteral is not in the allowed OWL2EL datatypes.    ## 3.4.1 16 September 2012    ### Features:    *    3578004    OWLRDFVocabulary Javadoc  *    3575834    Improvements to OWLOntologyManager and related contrib    ### Bug fixes:    *    3463200    rdf/functional round trip problem (anonymous individuals)  *    3341637    round trip problem with owl functional or rdf/xml syntax  *    3576182    Bump minimum java version to 1.6 (Fixed the bugs, moved the request to feature requests)  *    3497161    Manchester syntax imports  *    3403855    Imports of multiple ont docs containing the same ont fails  *    3491516    unhelpful Manchester parse exception  *    3536150    TypeOntologyHandler/TPVersionIRIHandler overwrite ontologyID  *    3314432    Manchester OWL parser does not handle data ranges in rules  *    3186250    annotation assertion lost when annotation prop undeclared  *    3309666    [Turtle] Parsing turtle files containing relative IRIs  *    3562296    Switch version 3.4.1 to 3.4.1-SNAPSHOT  *    3560287    Is QNameShortFormProvider deprecated?  *    3566820    Missing Imports are not reported  *    3306980    file in Manchester OWL Syntax with BOM doesn't parse  *    3559116    Wrong VersionInfo in OWL-API 3.4 release  *    2887890    Parsing hasValue restrictions with punned properties fails  *    3178902    RDF/XML round trip problem with 3-way equivalent classes  *    3174734    ManSyntax fails to read ontology with single data property  *    3440117    Turtle parser doesn't handle qnames with empty name part    ## 3.4 12 August 2012    This version restructures the Maven modules into six modules: api, impl, parsers,   tools, apibinding and contract. Code is unchanged, only its organization in   the SVN is changed.    To split the dependencies accordingly, DefaultExplanationOrderer is in   contract and is only a shell for ExplanationOrdererImpl, which does not   depend on OWLManager directly. This enables the tools module to be   used with a different apibinfing without recompilation.    ### Bug fixes:    *    3554073 	Manchester Syntax Parser won't parse DisjointUnionOf  *    3552028 	DataFactory returns integer instead of double restrictions  *    3550607 	property cycle detection does not work  *    3545194 	OWLEquivalentObjectProperties as SubObjectPropertyOfAxioms  *    3541476 	OWLRDFConsumer.getErrorEntity counter is not thread safe  *    3541475 	Remove shared static in OWLOntologyXMLNamespaceManager  *    3535046 	Use ArrayList instead of TreeSet in TurtleRenderer  *    3532600 	Use AtomicInteger in OWLOntologyID for counter    ### 3.3 15 May 2012    This version wraps together some minor bug fixes and some performance improvements for loading time and memory footprint for large ontologies.  Maven support has been much improved, thanks to Thomas Scharrenbach's efforts and Peter Ansell's feedback and suggestions.    ### Features:    *    Performance improvements at loading time    ### Bug fixes:    *    OBO Parser updated to be more compliant with the latest draft of the OBO syntax and semantics spec.  *    OBO Parser doesn't expand XREF values  (3515525)  *    Integrating with OpenRDF  (3512217)  *    maven should use snapshot versions during development  (3511755)  *    junit should be in maven test scope  (3511754)  *    mvn clean install requires gpg key  (3511732)  *    OWLOntologyManager.contains(IRI) bug (3497086)  *    Functional and Manchester syntax writers prefix problem  (3479677)  *    SyntacticLocalityModuleExtractor and SubAnnotationPropertyOf  (3477470)  *    Ignored Imports List uses the wrong IRIs  (3472712)  *    OWLLiteralImpl corrupts non-ascii unicode (3452932)  *    OWLOntologyXMLNamespaceManager QName problem  (3449316)  *    MultiImportsTestCase assumes incorrect working directory  (3448125)  *    Mismatch between HasKeyTestCase and corresponding resource  (3447280)  *    functional Renderer: OWLObjectRenderer swaps SWRL variables (3442060)  *    maven build fails because of dependency issue  (3440757)  *    manchester owl syntax doesn't handle rules with anon classes  (3421317)  *    RDFTurtleFormatter doesn't escape '\'  (3415108)  *    RDF consumer does not check for all annotated axiom triples  (3405822)  *    mis-parsing/mis-serialization of haskey (3403359)  *    Literal not a builtin  (3305113)  *    OWL API throws NullPointerException loading ontology  (3302982)    ## 3.2.1 22 July 2011    This version of the API is released under both LGPL v3 and Apache license v2; developers are therefore free to choose which one to use.    ### Features:    *    Tutorial code has been added following the OWLED 2011 tutorial (subpackage tutorialowled2011);  *    A visitor to determine whether a set of axioms is a Horn-SHIQ ontology has been added (HornAxiomVisitorEx.java).    ### Bug fixes:    *    Some test files and extra modules had not had their copyright notices updated to the new license; this has now been fixed.    ## 3.2.3 27 May 2011    This version of the API is released under both LGPL v3 and Apache license v2; developers are therefore free to choose which one to use.    ### Bug fixes:    *    MAVEN support has been fixed (3296393).  *    Some minor problems with rdfs:Literal have been fixed (3305113).  *    Parsers and renderers for various languages have had some corner case errors fixed (3302982, 3300090, 3207844, 3235181, 3277496, 3294069, 3293620, 3235198).  *    An error in OWLOntologyManager.removeAxioms() which was throwing ConcurrentModificationExceptions when called on a specific axiom type has been fixed (3290632).  *    OWLOntologyLoaderConfiguration has been introduced to provide ontology wise parsing configuration (strict and lax parsing modes now enabled) (3203646) More options coming up.  *    Parsers no longer leave streams open when a parsing error is detected (3189947).      ## 3.2.2 17 February 2011    ### Bug fixes:    *    In RDF based serialisations, type triples for entities that don't have ""defining axioms"" don't get added even if the renderer is instructed to type undeclared entities (related to 3184131). Fixed.  *    The RDF parser would not recognise XSD datatypes that aren't OWL 2 Datatypes even in lax parsing mode (related to 3184131). Fixed.  *    OWLOntologyManager.createOntology() methods don't set the document IRI of the created ontologies as adverstised if there aren't any ontology IRI mappers installed (3184878). Fixed.      ## 3.2.1 4 February 2011    ### Bug fixes:    *    Issues with -INF as serialisation for -infinity for floating point literals. Fixed.  *    Null pointer exception can be thrown by OBO parser (3162800). Fixed.  *    OWLOntologyManager.loadOntology() can throw unchecked exceptions (IllegalArgumentException) (3158293). Fixed.  *    OWLOntologyAlreadyExistsException get wrapped as as a parse exception by the OBO parser. (3165446). Fixed.  *    OWLRDFConsumer (OWL RDFParser) used owl:Datatype instead of rdfs:Datatype. Fixed.  *    OWLOntology.getDisjointUnionAxioms() does not work as expected. (3165000). Fixed.  *    Anonymous ontologies do not get typed as owl:Ontology during saving in RDF. (3158177). Fixed.  *    OWLOntology hashCode is not implemented properly. (3165583). Fixed.    ## 3.2.0 14 January 2011    This version of the API includes various bug fixes and performance enhancements since version 3.1.0    ### Bug fixes:    *    Various round tripping problems for various syntaxes (3155509, 3154524, 3149789, 3141366, 3140693, 3137303, 3121903)  *    Plain Literals are rendered incorrectly. Fixed.  *    Cyclic imports cause an OntologyAlreadyExistsException to be thrown. Fixed.  *    OWLAxiom.getAnnotatedAxiom() does not merge annotations.  Fixed.  *    OWLObject.getSignature() returns an unmodifiable collection. Fixed.  *    Various problems with character encodings (3068076, 3077637, 3096546, 3140693). Fixed.  *    IRI.create resulted in a memory leak.  Fixed.  *    Imports by location does not work. Fixed.  *    Various problems with anonymous individuals (2998616, 2943908, 3073742). Fixed.  *    Dublin Core Vocabulary is built into OWL API parsers.  Fixed.  *    Files are left open on parse errors.  Fixed.    ## 3.1.0 20 August 2010    This version of the API includes various bug fixes and enhancements since version 3.0.0.    ### Features:    Changes to the representation of literals:    Please note that there is a slight incompatibility between version 3.1.0 and version 3.0.0.  This is due to some changes  in the way that literals are represented and handled in the API.  There was a disparity between how they are described  in the OWL 2 specification and how they are represented in version 3.0.0 of the API.  The changes bring 3.1.0 inline  with the OWL 2 specification.  The changes are as follows: OWLStringLiteral and OWLTypedLiteral have been removed and  replaced OWLLiteral.  In version 3.0.0 OWLLiteral was a super-interface of OWLStringLiteral and OWLTypedLiteral.  Clients therefore need to replace occurrences of OWLStringLiteral and OWLTypedLiteral with OWLLiteral.  Method calls  need not be changed - methods on OWLStringLiteral and OWLTypedLiteral have corresponding methods on OWLLiteral.  Note  that all literals are now typed.  What used to be regarded as OWLStringLiterals are now OWLLiterals with the datatype  rdf:PlainLiteral.  Although this change introduces a slight backward incompatibility with the previous version of the  API we believe that the handling of literals in 3.1.0 is much cleaner and follows the specification more closely.    Changes to the OWLReasoner interface:    The OWLReasoner interface has been updated.  There are now methods which allow for fine-grained control over reasoning  tasks.  For example, it is now possible to request that a reasoner just classifies the class hierarchy, when in   version 3.0.0 the prepare reasoner method caused realisation to occur as well.  The prepareReasoner method has been removed from  the interface due to the huge amount of confusion it caused (it was not necessary to call this method to get correct  results, but the name suggested that it was necessary).  Clients should update their code to replace any calls to  prepareReasoner with appropriate calls to precomputeInferences.    JavaDoc for various methods has been cleaned up.    ### Bug fixes:    *    Annotations on Declaration axioms were not save.  Fixed.  *    OWLObjectMaxCardinality restrictions incorrectly resulted in a profile violation of the OWL2RL profile. Fixed.  *    OWLOntology.containsAxiom() failed various round trip tests. Fixed.  *    OWLObjectPropertyExpression.getInverses() did not include properties that were asserted to be inverses of themselves. Fixed.  *    Writing large rdf:Lists can cause a stack overflow on saving. Fixed.  *    System.out is closed when using SystemOutDocumentTarget. Fixed.  *    An OWLOntologyAlreadyExists exception is thrown when an imports graph contains multiple imports of the same ontology. Fixed.  *    OWLReasoner.getSubclasses() is missing declarations in the throws list for runtime exceptions that get thrown. Fixed.  *    Parsing RDF graphs containing blank nodes with NodeIDs is broken.  Fixed.  *    Some methods on OWLOntology return sets of object that change with ontology changes. Fixed.  *    Double dashes (--) are not escaped in XML comments output by the XMLWriter. Fixed.  *    The API sometimes prints to System.err.  Fixed.  *    The functional syntax writer writes out annotation assertions twice. Fixed.  *    Ontologies with DataSomeValuesFrom restrictions are incorrectly considered non-OWL2QL. Fixed.  *    Ontologies with Functional data properties are incorrectly considered non-OWL2RL. Fixed.  *    OWL2Datatype.XSD_NAME has the wrong regex pattern. Fixed.  *    RDF rendering of GCIs with multiple elements is broken. Fixed.  *    SubObjectPropertyOf axioms with annotations and property chains don't get parsed correctly from RDF.  Fixed.  *    StructuralReasoner sometimes crashes when determining superclasses. Fixed.  *    SimpleRenderer renders declarations incorrectly.  Fixed.  *    Object property characteristics are not answered correctly.  Fixed.  *    OWLOntology.getReferencingAxioms gives mutable and incorrect results.  Fixed.  *    StructuralReasoner call OWLClassExpression.asOWLClass() on anonymous classes. Fixed.  *    SKOSVocabulary is not up to date.  Fixed.  *    Regular expression for xsd:double has extra white space. Fixed.  *    Structural reasoner sometimes misses subclasses of owl:Thing. Fixed.  *    OWLOntologyManager.getImportsClosure() fails on reload. Fixed.  *    PrefixOWLOntologyFormat has not methods to remove prefixes.  Fixed.  *    RDF/XML rendering of unicode characters is ugly. Fixed.  *    Turtle parser does parse shared blank nodes correctly. Fixed.    ## 3.0.0 28 January 2010    Version 3.0.0 is incompatible with previous releases.  Many interface names have been changed in order to acheive a  close alignment with the names used in the OWL 2 Structural Specification and Functional Style Syntax.  (See http://www.w3.org/TR/2009/REC-owl2-syntax-20091027/)    The opportunity to clean up method names was also taken.      ## 2.2.0  17 April 2008    ### Features:    *    Added support for building using ANT  *    OWL 1.1 namespaces changed to OWL 2.  Old ontologies that are written using the owl11 namespace will still load, but will be converted to use the owl2 namespace.  *    Updated the RDF parser and RDF rendere to support AllDisjointClasses and AllDisjointProperties  *    Added the ability to save ontologies in Turtle.  *    Added the ability to load ontologies that are written in Turtle  *    Added explanation code contributed by Clark & Parsia  *    Added a KRSS renderer (contributed by Olaf Noppens)  *    Added a new, more comprehensive KRSS parser (contributed by Olaf Noppens).  This parser can parser the version of the KRSS syntax that is used by Racer.  *    Added the ability to specify a connection timeout for URL connections via a system property (owlapi.connectionTimeOut) - the default value for the timeout is 20 seconds.  *    Added a method to OWLOntologyManager to clear all registered URI mappers  *    Added a method to OWLOntologyManager so that imports can be obtained by an imports declaration.  *    Added a convenience method to OWLOntologyManager to add a set of axioms to an ontology without having to create the AddAxiom changes  *    Added a makeLoadImportsRequest method on OWLOntologyManager which should be used by parsers and other loaders in order to load imports  *    Added the ability to set an option for silent missing imports handling on OWLOntologyManager.  When this option is set,  exceptions are not thrown when imports cannot be found or cannot be loaded.  It is possible to set a listeners that gets informed when an import cannot be found, so that the exception doesn't get lost entirely.  *    Added the ability to add a ontology loader listener to OWLOntologyManager.  The listener gets informed when the loading process for an ontology starts and finishes (which ontology is being loaded, from where and whether it was successfully loaded etc.).  *    Added a method to OWLReasonerFactory to obtain the human readable name of the reasoner that a factory creates.  *    Added a convenience method to OWLOntology to obtain all referenced entities  *    Added convenience methods to OWLEntity that check whether the entity is an OWLClass, OWLObjectProperty, OWLDataProperty, OWLIndividual or OWLDatatype.  Also added asXXX to obtain an entity in its more specific form.  *    Added convenience methods to OWLDataFactory for creating disjoint class axioms and equivalent classes axioms.  *    Added a general purpose renderer interface for OWLObjects  *    Added an OWLInconsistentOntologyException to the inference module.  *    Added SKOS core to the list of well known namespaces  *    Added a SKOS vocabulary enum  *    Added methods to the OWLOntologyManager interface, so that ontologies can be saved to an output target as well as a URI.  Added implementations of OWLOntologyOutputTarget to enable writing directly to OutputStreams and Writers.  *    Added a StringOutputTarget for writing ontologies into a buffer that can be obtained as a string.  *    Added some new input sources:  StreamInputSource, ReaderInputSource, FileInputSource  *    RDF Parser. Made the classExpression translator selector more intelligent so that when properties aren't typed as either object or data properties, other triples are examined to make the appropriate choice.  *    OWLRestrictedDataRangeFacetVocabulary.  Added methods to obtain facets by their symbolic name (e.g. >=)  *    BidirectionalShortFormProvider.  Added a method to obtain all short forms cached by the provider.  *    Added an option to turn tabbing on/off when rendering Manchester Syntax  *    Added more documentation for the method which adds ontology URI mappers  *    Improved error handling when loading ontologies: For errors that have nothing to do with parse errors e.g. unknown host exceptions, the factory will rethrow the error at the earliest opportunity rather than trying all parsers.  *    Updated parser to throw ManchesterOWLSyntaxOntologyParserException which is a more specific type of OWLParserException  *    Updated the BidirectionalShortFormProviderAdapter with functionality to track ontology changes and update the rendering cache depending on whether entities are referenced or not.  *    Added a latex renderer for rendering ontology axioms in a latex format  *    Added the ability to parse ontologies written in ManchesterOWLSyntax  *    Added URIShortFormProvider as a general purpose interface for providing short forms for URIs. Changed SimpleShortFormProvider to use the SimpleURIShortFormProvider as a base  *    Made the toString rendering of the default implementation pluggable via the ToStringRenderer singleton class.  *    Added some convenience methods to the OWLDataFactory to make creating certain types of objects less tedious.  Specifically: ObjectIntersectionOf, ObjectUnionOf, ObjectOneOf and DataOneOf can now be created using methods that take a variable number of arguments (OWLDescriptions, OWLIndividuals or OWLConstants as appropriate).  Also, added convenience methods that create typed literals directly from Java Strings, ints, doubles, floats and booleans.  For example, createOWLTypedConstant(3) will create a typed literal with a lexical value of ""3"" and a datatype of xsd:integer.  Added convenice methods for creating entity annotations without manually having to create OWLAnnotation objects.  *    Added a getAxiomType method on to the OWLAxiom interface for convenience.  *    Added functionality to the debugging module for ordering explanations  *    Added generics to the inferred axiom generator API  *    Added a new constructor to OWLOntologyNamespaceManager so that it is possible to override the ontology format that is used as a hint when generating namespaces.  *    Added a dlsyntax renderer module that can renderer axioms etc. in the traditional dlsyntax using unicode for the dlsyntax symbols.  *    Modified the RDFXMLNamespaceManager to select the minimal amount of entities for which namespaces need to be generated.  Namespaces are only generated for classes in OWLClassAssertionAxioms, and properties in OWLObjectPropertyAssertionAxioms and OWLDataPropertyAssertionAxioms.  This basically corresponds to the places where valid QNames are needed for entities.  *    Added code to add declarations for ""dangling entities"".  If an RDF graph contains  <ClsA> <rdfs:type> <owl:Class> and ClsA has not been referenced by any other axioms then this would have been dropped by the parser - this has been changed so that declaration axioms are added to the ontology in such cases.  (Hopefully, the OWL 1.1 spec will be updated to do something like this in the mapping to RDF graphs).  *    Added a utility class, AxiomSubjectProvider, which given an axiom returns an object which is regarded to be the ""subject"" of the axioms.  For example given SubClassOf(ClsA ClsB), ClsA is regarded as being the subject.  *    Modified the ontology URI short form provider to provide nicer looking short forms.  *    Added a convenience method to get the individuals that have been asserted to be an instance of an OWLClass.  *    Commons lang is no longer used in the API because it had been replaced with a lightweight utility class in order to escape strings.  *    Removed the fragments module and replaced it with the profiles module.  The EL++ profile is currently implemented.  *    Added support for extended visitors that can return objects in the visit method.  *    Turned off logging in the RDF parser classes by default.    ### Bug fixes:    *    The getOntologyURIs method on AutoURIMapper would return physical rather than logic URIs. Fixed.  *    Namespaces for annotation URIs weren't generated. Fixed.  *    Removing a subclass axiom from an ontology cause the axiom to be added to the ontology as a GCI. Fixed.  *    When parsing an ontology, the accept types has been set to include RDF/XML.  This means that ontologies can be parsed correctly from servers that are configured to return RDF or HTML depending on the request type.  *    OWL/XML writer has been modified to write the datatype URI attribute name correctly.  Previously the name was written as ""Datatype"", however it should be ""datatypeURI"".  *    OWL/XML parser. Modified the literal handler to parse literals using the correct datatype URI attribute name (was ""Datatype"" and should have been ""datatypeURI"").  *    The constructor that required a manager in BidirectionalShortFormProviderAdapter did not rebuild the cache. Fixed.  *    Unqualified cardinality restrictions were rendered out as qualified cardinality restrictions. Fixed.  *    Saving an ontology would fail if the necessary directories did not exist. Fixed.  *    Rendering anonymous property inverses in OWL/XML was incorrect. Fixed.  *    Label and Comment annotations in the functional syntax weren't parsed properly, they were parsed as regular annotations. Fixed.  *    In the OWLXMLParserHandler, no handler for negative data property assertions was registered. Fixed.  *    Annotations that have anonymous individuals as values weren't rendered correctly. Fixed.  *    RDFXMLOntologyStorer and RDFXMLRenderer always used the ontology format that is obtainable from the manager, regardless of whether or not a custom ontology format was specified - fixed.  *    Rules that contained individual or data value objects couldn't be rendered. Fixed.  *    Declaration axioms were automatically added for data properties whether an ontology contained declaredAs triples or not. Fixed.  *    Anonymous properties weren't rendered correcty. Fixed.  *    RDF rendering for sub property axioms whose sub property is a property chain used an old rendering.  The rendering now complies with the latest OWL 2 specification.  Ontologies that use the old rendering can still be parsed.  *    RDF lists were reordered on rendering. Fixed."""
Semantic web;https://github.com/levelgraph/levelgraph;"""LevelGraph&nbsp;&nbsp;&nbsp;[![Dependency Status](https://david-dm.org/levelgraph/levelgraph.svg?theme=shields.io)](https://david-dm.org/levelgraph/levelgraph)  ===========    ![Logo](https://raw.githubusercontent.com/levelgraph/levelgraph/master/logo.png)    [![NPM](https://nodei.co/npm/levelgraph.png)](https://nodei.co/npm/levelgraph/)    __LevelGraph__ is a Graph Database. Unlike many other graph database,  __LevelGraph__ is built on the uber-fast key-value store  [LevelDB](http://code.google.com/p/leveldb/) through the powerful  [LevelUp](https://github.com/rvagg/node-levelup) library.  You can use it inside your node.js application or in any  IndexedDB-powered Browser.    __LevelGraph__ loosely follows the __Hexastore__ approach as presented in the article:  [Hexastore: sextuple indexing for semantic web data management  C Weiss, P Karras, A Bernstein - Proceedings of the VLDB Endowment,  2008](https://sci-hub.se/10.14778/1453856.1453965).  Following this approach, __LevelGraph__ uses six indices for every triple,  in order to access them as fast as it is possible.    __LevelGraph__ was presented in the paper [Graph databases in the  browser: using LevelGraph  to explore New Delhi - A. Maccioni, M. Collina - Proceedings of the VLDB Endowment, 2016](http://www.vldb.org/pvldb/vol9/p1469-maccioni.pdf).    Check out a [slideshow](http://nodejsconfit.levelgraph.io)  that introduces you to LevelGraph by  [@matteocollina](http://twitter.com/matteocollina) at  http://nodejsconf.it.    Also, give the [LevelGraph Playground](http://wileylabs.github.io/levelgraph-playground) to get a quick feel for adding JSON-LD and N3/Turtle documents to a filter-able subject, predicate, object table. The `db` variable in the browser console is very useful for checking out the full power of LevelGraph.    **LevelGraph** is an **OPEN Open Source Project**, see the <a href=""#contributing"">Contributing</a> section to find out what this means.      ## Table of Contents    * [Install](#install)  * [Usage](#usage)    * [Get and Put](#get-and-put)      * [Triple Properties](#triple-properties)      * [Limit and Offset](#limit-and-offset)      * [Reverse Order](#reverse-order)      * [Updating](#updating)      * [Multiple Puts](#multiple-puts)    * [Deleting](#deleting)    * [Searches](#searches)      * [Search Without Streams](#search-without-streams)      * [Triple Generation](#triple-generation)      * [Limit and Offset](#limit-and-offset-1)    * [Filtering](#filtering)    * [Putting and Deleting through Streams](#putting-and-deleting-through-streams)    * [Generate batch operations](#generate-batch-operations)    * [Generate levelup query](#generate-levelup-query)  * [Navigator API](#navigator-api)  * [LevelUp integration](#levelup-integration)  * [Browserify](#browserify)  * [RDF support](#rdf-support)  * [Extensions](#extensions)  * [TODO](#todo)  * [Contributing](#contributing)  * [Credits](#credits)  * [Contributors](#contributors)  * [LICENSE - ""MIT License""](#license---mit-license)      ## Install  ### On Node.js    ```  npm install levelgraph level-browserify --save  ```    At the moment it requires node v0.10.x, but the port to node v0.8.x  should be straighforward.  If you need it, just open a pull request.    ### In the Browser    Just download  [levelgraph.min.js](https://github.com/levelgraph/levelgraph/blob/master/build/levelgraph.min.js)  and you are done!    Alternatively, you can use [browserify](http://browserify.org/).    ## Usage    The LevelGraph API remains the same for Node.js and the browsers,  however the initialization change slightly.    Initializing a database is very easy:  ```javascript  var level = require(""level-browserify"");  var levelgraph = require(""levelgraph"");    // just use this in the browser with the provided bundle  var db = levelgraph(level(""yourdb""));  ```    ### Get and Put    Inserting a triple in the database is extremely easy:  ```javascript  var triple = { subject: ""a"", predicate: ""b"", object: ""c"" };  db.put(triple, function(err) {    // do something after the triple is inserted  });  ```    Retrieving it through pattern-matching is extremely easy:  ```javascript  db.get({ subject: ""a"" }, function(err, list) {    console.log(list);  });  ```    It even supports a Stream interface:  ```javascript  var stream = db.getStream({ predicate: ""b"" });  stream.on(""data"", function(data) {    console.log(data);  });  ```    #### Triple Properties    LevelGraph supports adding properties to triples with very  little overhead (apart from storage costs). It is very easy:  ```javascript  var triple = { subject: ""a"", predicate: ""b"", object: ""c"", ""someStuff"": 42 };  db.put(triple, function() {    db.get({ subject: ""a"" }, function(err, list) {      console.log(list);    });  });  ```    #### Limit and Offset    It is possible to implement pagination of get results by using  `'offset'` and `'limit'`, like so:    ```javascript  db.get({ subject: ""a"", limit: 4, offset: 2}, function(err, list) {    console.log(list);  });  ```    #### Reverse Order    It is possible to get results in reverse lexicographical order  using the `'reverse'` option. This option is only supported by  `get()` and `getStream()` and not available in `search()`.    ```javascript  db.get({ predicate: ""b"", reverse: true }, function (err, list) {    console.log(list);  });  ```      #### Updating    __LevelGraph__ does not support in-place update, as there are no  constraint in the graph.  In order to update a triple, you should first delete it:  ```javascript  var triple = { subject: ""a"", predicate: ""b"", object: ""c"" };  db.put(triple, function(err) {    db.del(triple, function(err) {      triple.object = 'd';      db.put(triple, function(err) {        // do something with your update      });    });  });  ```    #### Multiple Puts    __LevelGraph__ also supports putting multiple triples:  ```javascript  var triple1 = { subject: ""a1"", predicate: ""b"", object: ""c"" };  var triple2 = { subject: ""a2"", predicate: ""b"", object: ""d"" };  db.put([triple1, triple2],  function(err) {    // do something after the triples are inserted  });  ```    ### Deleting    Deleting is easy too:  ```javascript  var triple = { subject: ""a"", predicate: ""b"", object: ""c"" };  db.del(triple, function(err) {    // do something after the triple is deleted  });  ```      ### Searches    __LevelGraph__ also supports searches:  ```javascript  db.put([{    subject: ""matteo"",    predicate: ""friend"",    object: ""daniele""  }, {    subject: ""daniele"",    predicate: ""friend"",    object: ""matteo""  }, {    subject: ""daniele"",    predicate: ""friend"",    object: ""marco""  }, {    subject: ""lucio"",    predicate: ""friend"",    object: ""matteo""  }, {    subject: ""lucio"",    predicate: ""friend"",    object: ""marco""  }, {    subject: ""marco"",    predicate: ""friend"",    object: ""davide""  }], function () {      var stream = db.searchStream([{      subject: ""matteo"",      predicate: ""friend"",      object: db.v(""x"")    }, {      subject: db.v(""x""),      predicate: ""friend"",      object: db.v(""y"")    }, {      subject: db.v(""y""),      predicate: ""friend"",      object: ""davide""    }]);      stream.on(""data"", function(data) {      // this will print ""{ x: 'daniele', y: 'marco' }""      console.log(data);    });  });  ```    #### Search Without Streams    It also supports a similar API without streams:  ```javascript  db.put([{   //...  }], function () {      db.search([{      subject: ""matteo"",      predicate: ""friend"",      object: db.v(""x"")    }, {      subject: db.v(""x""),      predicate: ""friend"",      object: db.v(""y"")    }, {      subject: db.v(""y""),      predicate: ""friend"",      object: ""davide""    }], function(err, results) {      // this will print ""[{ x: 'daniele', y: 'marco' }]""      console.log(results);    });  });  ```    #### Triple Generation    It also allows to generate a stream of triples, instead of a solution:  ```javascript    db.search([{      subject: db.v(""a""),      predicate: ""friend"",      object: db.v(""x"")    }, {      subject: db.v(""x""),      predicate: ""friend"",      object: db.v(""y"")    }, {      subject: db.v(""y""),      predicate: ""friend"",      object: db.v(""b"")    }], {      materialized: {        subject: db.v(""a""),        predicate: ""friend-of-a-friend"",        object: db.v(""b"")      }    }, function(err, results) {      // this will print all the 'friend of a friend triples..'      // like so: {      //   subject: ""lucio"",      //   predicate: ""friend-of-a-friend"",      //   object: ""daniele""      // }    });  ```    #### Limit and Offset    It is possible to implement pagination of search results by using  `'offset'` and `'limit'`, like so:    ```javascript  db.search([{      subject: db.v(""a""),      predicate: ""friend"",      object: db.v(""x"")    }, {      subject: db.v(""x""),      predicate: ""friend"",      object: db.v(""y"")    }], { limit: 4, offset: 2 }, function(err, list) {      console.log(list);  });  ```    ### Filtering    __LevelGraph__ supports filtering of triples when calling `get()`   and solutions when calling `search()`, and streams are supported too.    It is possible to filter the matching triples during a `get()`:  ```javascript  db.get({      subject: 'matteo'    , predicate: 'friend'    , filter: function filter(triple) {        return triple.object !== 'daniele';      }  }, function process(err, results) {    // results will not contain any triples that    // have 'daniele' as object  });  ```    Moreover, it is possible to filter the triples during a `search()`  ```javascript  db.search({      subject: 'matteo'    , predicate: 'friend'    , object: db.v('x')    , filter: function filter(triple) {        return triple.object !== 'daniele';      }  }, function process(err, solutions) {    // results will not contain any solutions that    // have { x: 'daniele' }  });  ```    Finally, __LevelGraph__ supports filtering full solutions:  ```javascript  db.search({      subject: 'matteo'    , predicate: 'friend'    , object: db.v('x')  }, {      filter: function filter(solution, callback) {        if (solution.x !== 'daniele') {          // confirm the solution          callback(null, solution);        } else {          // refute the solution          callback(null);        }      }  }, function process(err, solutions) {    // results will not contain any solutions that    // have { x: 'daniele' }  });  ```    Thanks to solultion filtering, it is possible to implement a negation:  ```javascript  db.search({      subject: 'matteo'    , predicate: 'friend'    , object: db.v('x')  }, {      filter: function filter(solution, callback) {        db.get({            subject: solution.x          , predicate: 'friend'          , object: 'marco'        }, function (err, results) {          if (err) {            callback(err);            return;          }          if (results.length > 0) {            // confirm the solution            callback(null, solution);          } else {            // refute the solution            callback();          }        });      }  }, function process(err, solutions) {    // results will not contain any solutions that    // do not satisfy the filter  });  ```    The heavier method is filtering solutions, so we recommend filtering the  triples whenever possible.      ### Putting and Deleting through Streams    It is also possible to `put` or `del` triples from the store  using a `Stream2` interface:    ```javascript  var t1 = { subject: ""a"", predicate: ""b"", object: ""c"" };  var t2 = { subject: ""a"", predicate: ""b"", object: ""d"" };  var stream = db.putStream();    stream.write(t1);  stream.end(t2);    stream.on(""close"", function() {    // do something, the writes are done  });  ```    ### Generate batch operations    You can also generate a `put` and `del` batch, so you can  manage the batching yourself:    ```javascript  var triple = { subject: ""a"", predicate: ""b"", object: ""c"" };    // Produces a batch of put operations  var putBatch = db.generateBatch(triple);    // Produces a batch of del operations  var delBatch = db.generateBatch(triple, 'del');  ```    ### Generate levelup query    Return the leveldb query for the given triple.    ```js  var query = db.createQuery({ predicate: ""b""});  leveldb.createReadStream(query);  ```    ## Navigator API    The Navigator API is a fluent API for LevelGraph, loosely inspired by  [Gremlin](http://markorodriguez.com/2011/06/15/graph-pattern-matching-with-gremlin-1-1/)  It allows to specify how to search our graph in a much more compact way and navigate  between vertexes.    Here is an example, using the same dataset as before:  ```javascript      db.nav(""matteo"").archIn(""friend"").archOut(""friend"").        solutions(function(err, results) {        // prints:        // [ { x0: 'daniele', x1: 'marco' },        //   { x0: 'daniele', x1: 'matteo' },        //   { x0: 'lucio', x1: 'marco' },        //   { x0: 'lucio', x1: 'matteo' } ]        console.log(results);      });  ```    The above example match the same triples of:  ```javascript      db.search([{        subject: db.v(""x0""),        predicate: 'friend',        object: 'matteo'      }, {        subject: db.v(""x0""),        predicate: 'friend',        object: db.v(""x1"")      }], function(err, results) {        // prints:        // [ { x0: 'daniele', x1: 'marco' },        //   { x0: 'daniele', x1: 'matteo' },        //   { x0: 'lucio', x1: 'marco' },        //   { x0: 'lucio', x1: 'matteo' } ]        console.log(results);      });  ```    It allows to see just the last reached vertex:  ```javascript      db.nav(""matteo"").archIn(""friend"").archOut(""friend"").        values(function(err, results) {        // prints [ 'marco', 'matteo' ]        console.log(results);      });  ```    Variable names can also be specified, like so:  ```javascript  db.nav(""marco"").archIn(""friend"").as(""a"").archOut(""friend"").archOut(""friend"").as(""a"").        solutions(function(err, friends) {      console.log(friends); // will print [{ a: ""daniele"" }]  });  ```    Variables can also be bound to a specific value, like so:  ```javascript  db.nav(""matteo"").archIn(""friend"").bind(""lucio"").archOut(""friend"").bind(""marco"").        values(function(err, friends) {    console.log(friends); // this will print ['marco']  });  ```    A materialized search can also be produced, like so:  ```javascript  db.nav(""matteo"").archOut(""friend"").bind(""lucio"").archOut(""friend"").bind(""marco"").        triples({:          materialized: {          subject: db.v(""a""),          predicate: ""friend-of-a-friend"",          object: db.v(""b"")        }      }, function(err, results) {      // this will return all the 'friend of a friend triples..'    // like so: {    //   subject: ""lucio"",    //   predicate: ""friend-of-a-friend"",    //   object: ""daniele""    // }      console.log(results);  });  ```    It is also possible to change the current vertex:  ```javascript  db.nav(""marco"").archIn(""friend"").as(""a"").go(""matteo"").archOut(""friend"").as(""b"").        solutions(function(err, solutions) {       //  solutions is: [{     //    a: ""daniele"",     //    b: ""daniele""     //   }, {     //     a: ""lucio"",     //     b: ""daniele""     //   }]    });  ```    ## LevelUp integration    LevelGraph allows to leverage the full power of all  [LevelUp](https://github.com/rvagg/node-levelup) plugins.    Initializing a database with LevelUp support is very easy:  ```javascript  var levelup = require(""level"");  var levelgraph = require(""levelgraph"");  var db = levelgraph(levelup(""yourdb""));  ```    ### Usage with SubLevel    An extremely powerful usage of LevelGraph is to partition your  LevelDB with [SubLevel](http://npm.im/level-sublevel):    ```javascript  var levelup = require(""level"");  var sublevel = require(""level-sublevel"");  var levelgraph = require(""levelgraph"");  var db = sublevel(levelup(""yourdb""));  var graph = levelgraph(db.sublevel('graph'));  ```    ## Browserify    You can use [browserify](https://github.com/substack/node-browserify) to bundle your module and all the dependencies, including levelgraph, into a single script-tag friendly js file for use in webpages. For the convenience of people unfamiliar with browserify, a pre-bundled version of levelgraph is included in the build folder.    Simply `require(""levelgraph"")` in your browser modules and use [level.js](https://github.com/maxogden/level.js) instead of `level`:    ```javascript  var levelgraph = require(""levelgraph"");  var leveljs = require(""level-js"");  var levelup = require(""levelup"");  var factory = function (location) { return new leveljs(location) };    var db = levelgraph(levelup(""yourdb"", { db: factory }));  ```    ### Testling    Follow the [Testling install instructions](https://github.com/substack/testling#install) and run `testling` in the levelgraph directory to run the test suite against a headless browser using level.js    ## RDF support    __LevelGraph__ does not support out of the box loading serialized RDF or storing it. Such functionality is provided by extensions:  * [LevelGraph-N3](https://github.com/levelgraph/levelgraph-n3) - __N3/Turtle__  * [LevelGraph-JSONLD](https://github.com/levelgraph/levelgraph-jsonld) - __JSON-LD__    ## Extensions    You can use multiple extensions at the same time. Just check if one depends on another one  to nest them in correct order! *(LevelGraph-N3 and LevelGraph-JSONLD are  independent)*    ```javascript  var lg = require('levelgraph');  var lgN3 = require('levelgraph-n3');  var lgJSONLD = require('levelgraph-jsonld');    var db = lgJSONLD(lgN3(lg(""yourdb"")));  // gives same result as  var db = lgN3(lgJSONLD(lg(""yourdb"")));  ```    ## TODO    There are plenty of things that this library is missing.  If you feel you want a feature added, just do it and __submit a  pull-request__.    Here are some ideas:    * [x] Return the matching triples in the search results.  * [x] Support for Query Planning in search.  * [x] Added a Sort-Join algorithm.  * [ ] Add more database operators (grouping, filtering).  * [x] Browser support    [#10](https://github.com/levelgraph/levelgraph/issues/10)  * [ ] Live searches    [#3](https://github.com/levelgraph/node-levelgraph/issues/3)  * Extensions    * [ ] RDFa    * [ ] RDF/XML    * [ ] Microdata    ## Contributing    LevelGraph is an **OPEN Open Source Project**. This means that:    > Individuals making significant and valuable contributions are given commit-access to the project to contribute as they see fit. This project is more like an open wiki than a standard guarded open source project.    See the [CONTRIBUTING.md](https://github.com/levelgraph/levelgraph/blob/master/CONTRIBUTING.md) file for more details.    ## Credits    *LevelGraph builds on the excellent work on both the LevelUp community  and the LevelDB and Snappy teams from Google and additional contributors.  LevelDB and Snappy are both issued under the [New BSD Licence](http://opensource.org/licenses/BSD-3-Clause).*    ## Contributors    LevelGraph is only possible due to the excellent work of the following contributors:    <table><tbody>  <tr><th align=""left"">Matteo Collina</th><td><a href=""https://github.com/mcollina"">GitHub/mcollina</a></td><td><a href=""https://twitter.com/matteocollina"">Twitter/@matteocollina</a></td></tr>  <tr><th align=""left"">Jeremy Taylor</th><td><a  href=""https://github.com/jez0990"">GitHub/jez0990</a></td></tr>  <tr><th align=""left"">Elf Pavlik</th><td><a href=""https://github.com/elf-pavlik"">GitHub/elf-pavlik</a></td><td><a href=""https://twitter.com/elfpavlik"">Twitter/@elfpavlik</a></td></tr>  <tr><th align=""left"">Riceball LEE</th><td><a href=""https://github.com/snowyu"">GitHub/snowyu</a></td><td></td></tr>  <tr><th align=""left"">Brian Woodward</th><td><a href=""https://github.com/doowb"">GitHub/doowb</a></td><td><a href=""https://twitter.com/doowb"">Twitter/@doowb</a></td></tr>  <tr><th align=""left"">Leon Chen</th><td><a href=""https://github.com/transcranial"">GitHub/transcranial</a></td><td><a href=""https://twitter.com/transcranial"">Twitter/@transcranial</a></td></tr>  </tbody></table>    ## LICENSE - ""MIT License""    Copyright (c) 2013-2017 Matteo Collina and LevelGraph Contributors    Permission is hereby granted, free of charge, to any person  obtaining a copy of this software and associated documentation  files (the ""Software""), to deal in the Software without  restriction, including without limitation the rights to use,  copy, modify, merge, publish, distribute, sublicense, and/or sell  copies of the Software, and to permit persons to whom the  Software is furnished to do so, subject to the following  conditions:    The above copyright notice and this permission notice shall be  included in all copies or substantial portions of the Software.    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,  EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES  OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND  NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT  HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,  WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR  OTHER DEALINGS IN THE SOFTWARE. """
Semantic web;https://github.com/read-write-web/rww-play;"""rww-play   ========    This is an implementation in Play of a number of tools to build a Read-Write-Web server using Play2.x and akka.  It is very early stages at present and it implements sketches of the following    * A [CORS](http://www.w3.org/TR/cors/) proxy  * An initial implementation of [Linked Data Basic Profile](http://www.w3.org/2012/ldp/wiki/Main_Page)    This currently works in the [TLS branch of the bblfish fork of Play 2.x](https://github.com/bblfish/Play20), which comes with TLS support and a few more patches.    We use [Travis CI](http://travis-ci.org/) to verify the build: [![Build Status](https://travis-ci.org/read-write-web/rww-play.png)](http://travis-ci.org/read-write-web/rww-play)        Getting going  -------------      * You need Java 7 at least - the official Oracle JVM or another one based on [the GPLed code](http://openjdk.java.net/): removing the dependency on Oracle's JVM will require [publishing of the GPLed java security libs](http://stackoverflow.com/questions/12982595/openjdk-sun-security-libs-on-maven)  * clone [this project](https://github.com/stample/rww-play)     ```bash   $ git clone git://github.com/stample/rww-play.git   ```     In the `rww-play` home directory, run the `build` bash script. It will download a precompiled tuned   version of play, build the application, and run it. (If there is no remotely downloadable version  it will build it from source in the `Play20` directory.)    ```bash  $ ./build  ```  Some network config.  --------------------    Download the JavaScript apps by running    ```bash  $ ./install-app.sh  ```    To start Play in secure mode with lightweight client certificate verification (for WebID)  In file:  conf/application.conf  set the smtp parameters: host= and user=  of your mail provider server.    In file:  /etc/hosts  add host names for the subdomains you will create, e.g. :  127.0.0.1 jmv.localhost  127.0.0.1 jmv1.localhost  127.0.0.1 jmv2.localhost    Installing RWW apps  ----------  The RWW apps are stored in other git repositories.  One can run the script `./install-app.sh` to install or update the RWW apps that we ship with the platform.  Check the script content, it is simply a git clone. ( If installing on a public server make sure the proxy  url is set. )    Running  -------  To start Play in secure mode with lightweight client certificate verification (for WebID); that is, a self-signed certificate:    ```bash   $ Play20/play   [RWWeb] $ idea with-sources=yes	// if you want to run intelliJ   [RWWeb] $ eclipse with-source=true	// if you want to run eclipse Scala IDE   [RWWeb] $ compile   [RWWeb] $ ~run -Dhttps.port=8443 -Dhttps.trustStore=noCA -Dakka.loglevel=DEBUG -Dakka.debug.receive=on -Drww.root.container.path=test_ldp    ```  Then you can direct your browser to:  [https://localhost:8443/2013/](https://localhost:8443/2013/)    If you want to have multiple users on your server, it is best to give each user a subdomain for JS security. This can  be had by starting the server with the following attributes.       For subdomains on your local machine you will need to edit `/etc/hosts` for each server. For  machines on the web you can just assign all domains to the same ip address.    ```bash  [RWWeb] $ run -Dhttps.port=8443 -Dhttps.trustStore=noCA -Drww.subdomains=true -Dhttp.hostname=localhost -Drww.subdomains=true -Dsmtp.password=secret  ```    You can the create yourself a subdomain by pointing your browser to the root domain:  [https://localhost:8443/](https://localhost:8443/). This will lead you to the account creation   page, which will allow you to create subdomains on your server. An e-mail will be sent to   your e-mail address for verification ( but you will be able to find the link in the logs   if the e-mail server is not set up).       Documentation  -------------    Further documentation can be found on the [rww-play wiki](https://github.com/stample/rww-play/wiki).    Licence  -------       Copyright 2013-2014 Henry Story       Licensed under the Apache License, Version 2.0 (the ""License"");     you may not use this file except in compliance with the License.     You may obtain a copy of the License at          [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)       Unless required by applicable law or agreed to in writing, software     distributed under the License is distributed on an ""AS IS"" BASIS,     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.     See the License for the specific language governing permissions and     limitations under the License. """
Semantic web;https://github.com/GeoKnow/LinkedGeoData;"""## Welcome to LinkedGeoData: Providing OpenStreetMap data as RDF  LinkedGeoData (LGD) is an effort to add a spatial dimension to the Web of Data / Semantic Web. LinkedGeoData uses the information collected by the OpenStreetMap project and makes it available as an RDF knowledge base according to the Linked Data principles. It interlinks this data with other knowledge bases in the Linking Open Data initiative.    The project web site can be found [here](http://linkedgeodata.org).  If you are running [Ubuntu](http://www.ubuntu.com) then this repository contains everything you need to transform OpenStreetMap data to RDF yourself.  For other systems please consider contributing adaptions of the existing scripts.    ### Contributions Welcome  The docker-based architecture is aimed at making it easy to contribute new or alternative components that can sit side-by-side with the core of the system - which  is the a virtual knowledge graph view over an OSM database.  Please open issues for discussion.    Examples include but are not limited to:    * A more modern Linked Data Interface that displays GeoSPARQL geometries out-of-the-box  * Nicer SPARQL interface (YASGUI)  * Another data virtualization engine in order to ease comparision with the already integrated ones  * Updates to the existing OSM-RDF mappings, proposals about how this system could be improved.  * Proposals for a better IRI schema. For example, the 'triplify' in the IRIs is archaic. Migration to the pattern used by [Sophox](https://wiki.openstreetmap.org/wiki/Sophox) seems very worthwhile. Because of the virtual knowledge graph approach there should be no problem to use the legacy approach in parallel.  * General proposals for architecture improvements (e.g. config options in the docker setup to improve modularity)    Dockerfiles for services such as a Linked Data or SPARQL interfaces should be designed to allow configuration of the target SPARQL endpoint(s), ideally via the docker environment.    ### How It Works    The architecture shown in the image below. The docker setup is located in the [linkedgeodata-docker](linkedgeodata-docker) folder.    ![LGD Dockerized Architecture Overview](docs/assets/images/lgd-architecture-2021-01-18.png)    * This project first uses Osmosis to initialize a 'raw' OpenStreetMap postgis database (using simple schema) from a `.osm.pdf` file.  * Then, this database is extended with additional tables containing RDF mapping - and interlinking - information. Also, helper views are provided for simplifying access to the integrated information.  * Further, a nominatim setup (based on osm2pgqsl) is performed to further enrich the initial database osm data.  * A set of RDB2RDF mappings is provided that enables running SPARQL queries over the postgis database. The SPARQL-2-SQL rewriting engine we use is Sparqlify.  * Dumps are generated by simply running preconfigured SPARQL queries.        ### Debian package now available!  Technically, LinkedGeoData is set of SQL files, database-to-rdf (RDB2RDF) mappings, and bash scripts.  The actual RDF conversion is carried out by the SPARQL-to-SQL rewriter [Sparqlify](https://github.com/AKSW/Sparqlify).  You can [view the Sparqlify Mappings for LinkedGeoData here](https://raw.github.com/GeoKnow/LinkedGeoData/master/linkedgeodata-core/src/main/resources/org/aksw/linkedgeodata/sml/LinkedGeoData-Triplify-IndividualViews.sml).  Therefore, if you want to install the LinkedGeoData debian package, you also  Sparqlify one.    For the latest version of LinkedGeoData package, perform the following steps to set up the package source:    Register the repo        echo 'deb http://cstadler.aksw.org/repos/apt precise main contrib non-free' | sudo tee /etc/apt/sources.list.d/cstadler.aksw.org.list    Import the public key        wget -qO - http://cstadler.aksw.org/repos/apt/conf/packages.precise.gpg.key  | sudo apt-key add -    Now you can install LinkedGeoData using        sudo apt-get update      sudo apt-get install linkedgeodata      You can download and install packages manually, however installing their dependencies requires more work:  * [sparqlify-cli Debian Package](http://cstadler.aksw.org/repos/apt/pool/main/s/sparqlify-cli/)  * [linkedgeodata-nominatim Debian Package](http://cstadler.aksw.org/repos/apt/pool/main/l/linkedgeodata-nominatim-v2.5.1/)  * [linkedgeodata Debian Package](http://cstadler.aksw.org/repos/apt/pool/main/l/linkedgeodata/)      After installing these packages, the following essential commands will be available:  * `lgd-createdb` (provided by linkedgeodata) - Sets up a complete database ready for subsequent RDF conversion  * `lgd-createdb-snapshot` (provided by linkedgeodata) - Same as above, however using a different schema (experimental; not recommended)  * `sparqlify-tool` (provided by sparqlify) - Runs SPARQL queries on a relational (LGD) database  * Have a look at the [section for additional tools](#additional-tools)    Read the section on data conversion for their documentation.    ### Alternative set up  In [/bin](https://github.com/GeoKnow/LinkedGeoData/tree/master/linkedgeodata-cli/bin) you find the following setup helper scripts which are aimed at easing the LinkedGeoData setup directly from source; without a debian package:    * `[lgd-apt-install-ubuntu-16.04.sh](linkedgeodata-cli/bin/lgd-apt-install-ubuntu-16.04.sh)`: Installs all required system packages using apt (postgres, postgis, java, and several dependencies required for (building) nominatim)    The following scripts are just helpers to build and/or install the Sparqlify debian package. Mainly intended for development.    * `lgd-build-and-install-dependencies.sh`: Builds a Sparqlify debian package from source and installs it.  * `lgd-download-and-install-dependecies.sh`: Simply downloads and installs the latest Sparqlify debian package.      ### Do it yourself data conversion  This section describes how to create and query a LinkedGeoData database. After you installed the LinkedGeoData scripts, you need to obtain an OpenStreetMap dataset which you want to load.  Note: Make sure to read the section on database tuning when dealing with larger datasets!    As for obtaining datasets, a very good source for OSM datasets in bite-size chunks is [GeoFabrik](http://download.geofabrik.de). For full dumps, refer to the [planet downloads](http://planet.openstreetmap.org/).    In [/bin](https://github.com/GeoKnow/LinkedGeoData/tree/master/linkedgeodata-cli/bin) you find several scripts. Essentially they are designed to work both from a cloned LinkedGeoData Git repo and wrapped up as a debian package.  All of them are configured via `lgd.conf.dist`. You can override the default settings without changing this file by creating a `lgd.conf` file.  If you installed the debian package, instead of the lgd.conf.dist file, the file /etc/sparqlify/sparqlify.conf` is used.  If you are using the following scripts from the git repo, invoke them with `./scriptname.sh` (i.e. don't forget the `./` and `.sh`).    * (`lgd-createdb-snapshot`: An experimental, but possibly much faster, version of the `lgd-createdb` script. Probably the `lgd-createdb` command will eventually refer to this version.)    * `lgd-createdb`: Creates and loads an LGD database    * -h  postgres host name    * -d  postgres database name    * -U  postgres user name    * -W  postgres password (will be added to ~/.pgpass if not exists)    * -f  .pbf file to load (other formats currently not supported)    Example:        wget http://download.geofabrik.de/europe/monaco-latest.osm.pbf      lgd-createdb -h localhost -d lgd -U postgres -W mypwd -f monaco-latest.osm.pbf    The reason we chose Monaco for the example is simply that it is a small file (> 10MB).      * `sparqlify-tool`: This is a small wrapper for `sparqlify` command that adds a simple profile system for convenience.    * -P  profile name. Settings will be loaded from such a file (see below) and can be overridden by further options.    * -h  database host name    * -d  database name    * -U  database user name    * -W  database password (will be added to ~/.pgpass if not exists)    * -Q  SPARQL query string or named query      Here is an example of a profile file, which is assumed to be located at `/etc/sparqlify/profiles.d/lgd-example.conf`.  This file will be deployed when installing the linkedgeodata debian package.            dbHost=""localhost""          dbName=""lgd""          dbUser=""postgres""          dbPass=""postgres""          mappingFile=""/usr/share/linkedgeodata/sml/LinkedGeoData-Triplify-IndividualViews.sml /usr/share/linkedgeodata/sml/LinkedGeoData-Nominatim.sml""      A named query is just a SPARQL query that is referenced by a name.  The mapping of a name to a SPARQL is configured via `/etc/sparqlify/sparqlify.conf`.    Currently, the following named queries exist:    * `ontology`: Creates an N-Triple output with all classes and properties  * `dump`: Create a full dump of the database    Examples:            sparqlify-tool -P lgd-example ontology          sparqlify-tool -P lgd-example dump          sparqlify-tool -h localhost -d lgd -U postgres -W mypwd -Q 'Construct { ?s ?p ?o } { ?s a <http://linkedgeodata.org/ontology/Pub> . ?s ?p ?o }'          sparqlify-tool -P lgd-example -Q 'Select * { ?s ?p ?o . Filter(?s = <http://linkedgeodata.org/triplify/node2028098486>) }'    Again, note that Sparqlify is still in development and the supported features are a bit limited right now - still, basic graph patterns and equal-constraints should be working fine.      ### Additional tools    * `lgd-osm-replicate-sequences`: Convert a timestamp to a sequence ID. This is similar to [mazdermind's replicate sequences tool](https://github.com/MaZderMind/replicate-sequences), however, our version does not require a local index. Instead, our tools combines binary search with linear interpolation: First, the the two most recent state.txt files from the given repository url are fetched, then the time differnce is computed, and based on linear interpolation a sequence id close to the given timetstamp is computed. This process is repeated recursively.  ```bash  lgd-osm-replicate-sequences -u ""http://planet.openstreetmap.org/replication/hour/"" -t ""2017-05-28T15:00:00Z""    # The above command from the debian package is a wrapper for:    java -cp linkedgeodata-debian/target/linkedgeodata-debian-*-jar-with-dependencies.jar \      ""org.aksw.linkedgeodata.cli.command.osm.CommandOsmReplicateSequences"" \      -u ""http://planet.openstreetmap.org/replication/hour/"" -t ""2017-05-28T15:00:00Z""  ```  The output is a (presently subset) of the appropriate state.txt file whose timestamp is strictly less than that given as the argument.  ```  sequenceNumber=41263  timestamp=2017-05-28T14\:00\:00Z  ```  Note, that the timestamp format is compatible with `osmconvert`, which can check for the most recent data item in a osm data file. Hence,  these tools can be combined in order to find the state.txt file from which to proceed with replication.  ```bash  timestamp=`osmconvert --out-timestamp ""data.osm.pbf""`  lgd-osm-replicate-sequences -u ""url-to-repo"" -t ""$timestamp""  ```    ```bash  # Use the -d option to option the (d)uration between the most recently published files  lgd-osm-replicate-sequences -u ""http://planet.openstreetmap.org/replication/day/"" -d  # This yields simply the output (possibly off by a few seconds)  # 86400  ```    ### Postgresql Database Tuning  It is recommended to tune the database according to [these recommendations](http://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server). Here is a brief summary:  Edit `/etc/postgresql/9.1/main/postgresql.conf` and set the following properties:            shared_buffers       = 2GB #recommended values between 25% - 40% of available RAM, setting assumes 8GB RAM          effective_cache_size = 4GB #recommended values between 50% - 75% of available RAM, setting assumes 8GB RAM          checkpoint_segments  = 256          checkpoint_completion_target = 0.9          autovacuum = off # This can be re-enabled once loading has completed            work_mem             = 256MB (This memory is used for sorting, so each user may use this amount of memory for his sorts; You may want to use a significantly lower value if there are many connections doing sorts)          maintainance_work_mem = 256MB      Furthermore, allow more shared memory, otherwise postgres won't start:  Append the following line to `/etc/sysctl.conf`:            #Use more shared memory max          kernel.shmmax=4294967296            # Note: The amount (specified in bytes) for kernel.shmmax must be greater than the shared_buffers settings obove          #4GB = 4294967296          #8GB = 8589934592    Make the changes take effect:            sudo sysctl -p          sudo service postgresql restart    ## License  The content of this project are licensed under the [GPL v3 License](https://github.com/GeoKnow/LinkedGeoData/blob/master/LICENSE).   """
Semantic web;https://github.com/cygri/prefix.cc;"""# prefix.cc Source Code    This is the source code to the [prefix.cc](http://prefix.cc/) web site  operated by Richard Cyganiak ([richard@cyganiak.de](mailto:richard@cyganiak.de), [@cygri](http://twitter.com/cygri)).      ## Requirements    * Apache with `mod_rewrite` enabled  * PHP (recommended version: 7.2)  * MySQL      ## Setting up a test site    1. Clone the repository into a local directory    2. Set up a virtual host `prefixcc.local` with that directory as document root    3. Create a new MySQL database:            echo CREATE DATABASE prefixcc | mysql -u root     4. Set up database tables:            mysql -u root prefixcc < db_schema/schema.sql    5. Make a copy of the default configuration:            cp default.config.php config.php    6. Edit database credentials in `config.php` if necessary    7. Import prefixes from the public prefix.cc site:            php tools/csv-import.php http://prefix.cc/popular/all.file.csv | mysql -u root prefixcc    8. Go to [http://prefixcc.local/](http://prefixcc.local/) and you should have a functional site! """
Semantic web;https://github.com/apache/marmotta;"""# Apache Marmotta    This repository contains the source code for [Apache Marmotta](https://marmotta.apache.org/)    [![Maven Central](https://maven-badges.herokuapp.com/maven-central/org.apache.marmotta/marmotta-core/badge.svg)](https://maven-badges.herokuapp.com/maven-central/org.apache.marmotta/marmotta-core/)  [![Docker Image](https://images.microbadger.com/badges/version/apache/marmotta.svg)](https://hub.docker.com/r/apache/marmotta/)  [![Docker Image layers](https://images.microbadger.com/badges/image/apache/marmotta.svg)](https://microbadger.com/images/apache/marmotta)  [![License](http://img.shields.io/:license-apache-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0.html)      ## Building the Source Distribution    Apache Marmotta uses Maven to build, test, and install the software. A basic  build requires downloading and installing Maven and then running:        mvn clean install    This will compile, package and test all Apache Marmotta modules and install  it in your local Maven repository. In case you want to build your own  projects based on some of the libraries provided by Apache Marmotta, this  usually suffices.    The default loglevel for most unit and integration tests executed during the  build is INFO. To change the loglevel for either more or less output, you  can pass the loglevel as system property:        mvn clean install -Droot-level=TRACE|DEBUG|INFO|WARN|ERROR    Note that some of the integration tests start up parts of the Marmotta  platform during execution. The log level for these tests cannot be  changed, as Marmotta is taking over the log configuration in these  cases.    ## Building, Running and Deploying the Wep Application    Apache Marmotta also includes a default configuration for building a Java  Web Application that can be deployed in any Java Application Server. To  build the web application, first run        mvn clean install    in the project root. Then change to the launchers/marmotta-webapp directory  and run        mvn package    This will create a marmotta.war file in the target/ directory. You can  deploy this archive to any Java Application Server by copying it into  its deployment directory; [more details](http://marmotta.apache.org/installation.html).    Alternatively, you can directly startup the Apache Marmotta Web Application  from Maven with a default configuration suitable for development. To try this  out, run        mvn tomcat7:run    wait until the system is started up and point your browser to `http://localhost:8080`    When developing it is sometimes useful to always start with a clean confi-  guration of the system. Therefore, you can also start up the web application  as follows:        mvn clean tomcat7:run -Pcleanall    This command will remove any existing configuration directory before startup.    ## Building the Standalone Installer    The build environment also offers to automatically build an installer package  that guides users through the installation with an easy-to-use installation  wizard. The installer is based on izPack and dynamically assembled when  building the package. To build the installer, first run        mvn clean install    in the project root. Then change to the launchers/marmotta-installer directory  and run        mvn package -Pinstaller    The build process will automatically create an appropriate installer confi-  guration from the Maven dependencies through the Apache Marmotta refpack  build plugin.    The installer can then be tried out by running        java -jar target/marmotta-installer-x.x.x.jar      ## Building a Docker image    Marmotta also comes with support for creating a Docker images that you can use for development   or testing:    1. Locate at the root of the source repository  2. Build image: `docker build -t marmotta .`  3. Run the container: `docker run -p 8080:8080 marmotta`  4. Access Marmotta at [localhost:8080/marmotta](http://localhost:8080/marmotta) (IP address may      be different, see information bellow).    An official images is [available from Docker Hub](https://hub.docker.com/r/apache/marmotta/) as   an automated build, so you just need to pull it from there to replace the second step above:         docker pull apache/marmotta      ## Building with a Clean Repository    Sometimes it is useful to check if the build runs properly on a clean local  repository, i.e. simulate what happens if a user downloads the source and  runs the build. This can be achieved by running Maven as follows:        mvn clean install -Dmaven.repo.local=/tmp/testrepo    The command changes the local repository location from ~/.m2 to the  directory passed as argument      ## Simulating a Release    To test the release build without actually deploying the software, we have  created a profile that will deploy to the local file system. You can  simulate the release by running        mvn clean deploy -Pdist-local,marmotta-release,installer    Please keep in mind that building a release involves creating digital  signatures, so you will need a GPG key and a proper GPG configuration to run  this task.    Read more about [our release process](https://wiki.apache.org/marmotta/ReleaseProcess).   """
Semantic web;https://github.com/lambdamusic/Ontospy;"""# Ontospy    Python library and command-line interface for inspecting and visualizing RDF models.      #### Links    -   [Pypi](https://pypi.org/project/ontospy/)  -   [Github](https://github.com/lambdamusic/ontospy)  -   [Docs](http://lambdamusic.github.io/Ontospy/)  -   [YouTube Video](https://youtu.be/MkKrtVHi_Ks)    # Description    Ontospy is a lightweight Python library and command line tool for inspecting and visualizing vocabularies encoded using W3C Semantic Web standards, that is, RDF or any of its dialects (RDFS, OWL, SKOS).    The basic workflow is simple: load a graph by instantiating the `Ontospy` class with a file containing RDFS, OWL or SKOS definitions. You get back a object that lets you interrogate the ontology. That's all!    The same functionalities are accessible also via a command line application (`ontospy`). This is an interactive environment (like a repl) that allows to load ontologies from a local repository, interrogate them and cache them so that they can be quickly reloaded for inspection later on.    [![Downloads](https://pepy.tech/badge/ontospy)](https://pepy.tech/project/ontospy)      ## Generating ontology documentation    Ontospy can be used to generate HTML documentation for an ontology pretty easily. E.g. see the [Schema.org](https://lambdamusic.github.io/ontospy-examples/schema_org_topbraidttl/index.html) ontology, or [FOAF](https://lambdamusic.github.io/ontospy-examples/foafrdf/index.html) ontology.    This functionality relies on a module called _ontodocs_, which used to be maintained as a separate library but is now distributed with ontospy as an add-on:    -   `pip install ontospy[FULL]`    For more examples of the kind of documentation that can be generated out-of-the-box, [take a look at this page](https://lambdamusic.github.io/ontospy-examples/index.html).    ## Status    [![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity)    I have little time to spend on this project these days, so I'm mainly focusing on bug fixes and maintenance. Happy to review PRs if you want to add more functionalities!     ## Development    ```  # git clone the repo first  $ mkvirtualenv ontospy  $ pip install -r requirements.txt  $ pip install -e .  ```    ## Documentation    http://lambdamusic.github.io/Ontospy/ """
Semantic web;https://github.com/protegeproject/sparql-dl-api;"""# sparq-dl-api  A query engine for SPARQL-DL.  Originally written by Derivo Systems        http://www.derivo.de/en/resources/sparql-dl-api.html """
Semantic web;https://github.com/amgadmadkour/sparti;"""# Query-Centric Semantic Partitioning (SPARTI)    * Adaptively partition based on query-workload  * Precompute Bloom join between the most frequent triples joins (MF-TJ) combinations  * Partition related properties based on a greedy algorithm and a cost model  * Current version is implemented to run over <b>Apache Spark</b>"""
Semantic web;https://github.com/opencube-toolkit/OpenCube-Expander;"""OpenCube Expander  ===============    The role of the OpenCube Expander component is:  + to search for compatible cubes and   + to create a new expanded cube by merging two compatible cubes.     ###How it works    he OpenCube Expander is developed as a separate component of the OpenCube toolkit and is part of the �Data Expanding� lifecycle step. It supports the identification of compatible cubes stored either at the native triple store or at remote SPARQL end-points. The Expander can be initialized by creating a widget.    Widget configuration for use with the native triple store:    ```  {{#widget: CubeSelection|asynch='true'}}  ```    Widget configuration for use with the remote SPARQL end-point containing data for the 2011 Irish Census:    ```  {{#widget:CubeSelection| sparqlService='<http://data.cso.ie/sparql>'| asynch='true' }}  ```       ###Functionality    The functionality of the OpenCube Expander is based:    + On the links (dimensionValueCompatible and MeasureCompatible) created by the OpenCube Compatibility Explorer in order to detect external compatible cubes  + On the aggregations (across a dimension and across a hierarchy) to detect compatible pre-computed aggregate cubes. The links enable the fast detection of the compatible cubes since no complex computations are made.       At the beginning the component presents the structure of the cube: i) the cube dimensions, ii) the values for each dimension and ii) the cube measures. Then the user can search for compatible cubes based on the following operations:    + **Add measure**. This operation identifies and presents cubes that are compatible to add new measures to the original cubes i.e. associated cubes using the property MeasureCompatible.  + **Add value to dimension**. In this case the user selects an expansion dimension and the operation identifies and presents compatible cubes that can be used to add new values to the selected dimension i.e. associated cubes using the property dimensionValueCompatible.  + **Add hierarchy**. This operation identifies and presents cubes that are compatible to add a hierarchy to the original cube i.e. pre-computed aggregations across a hierarchy created by the OpenCube Aggregator (for simplicity reasons this functionality has been integrated to the OpenCube OLAP Browser).  + **Add dimension**. This operation identifies and presents cubes that are compatible to add a dimension to the original cube i.e. pre-computed aggregations across a dimension created by the OpenCube Aggregator (for simplicity reasons this functionality has been integrated to the OpenCube OLAP Browser).        """
Semantic web;https://github.com/cumulusrdf/cumulusrdf;"""<img src=""https://cloud.githubusercontent.com/assets/7569632/9528013/c54e198e-4cf2-11e5-864a-eb1f92433ee4.png"" width=""120"" height=""120""/>  <img src=""https://cloud.githubusercontent.com/assets/7569632/9528225/cbd9ae52-4cf3-11e5-9993-bf8c56dff5e1.png"" width=""500"" height=""120""/>        **CumulusRDF** (see Wikipedia[1] for details on ""Cumulus"") is an RDF store on a cloud-based architecture. CumulusRDF provides a REST-based API with CRUD operations to manage RDF data. The current version uses Apache Cassandra as storage backend.        To use CumulusRDF for your project, see the [GettingStarted](https://github.com/cumulusrdf/cumulusrdf/wiki/GettingStarted) wiki page!        ## Features  * By means of Apache Cassandra [2] CumulusRDF offers a highly scalable RDF store for write-intensive applications  * CumulusRDF acts as a Linked Data server  * It allows for fast and lightweight evaluation of triple pattern queries  * It has full support for triple and quad storage  * CumulusRDF comprises a SesameSail [3] implementation, see [CodeExamples](https://github.com/cumulusrdf/cumulusrdf/wiki/CodeExamples) wiki page.  * Further, CumulusRDF contains a SPARQL1.1 endpoint.     Please see our [Features](https://github.com/cumulusrdf/cumulusrdf/wiki/Features) as well as our [Roadmap](https://github.com/cumulusrdf/cumulusrdf/wiki/Roadmap) wiki page for further information.     ## Quick start using docker  If you have docker you can get up and running quickly by using the following commands.    ```  $ git clone git@github.com:cumulusrdf/cumulusrdf.git  $ docker build -t cumulusrdf .  $ docker run -d --name cumulusrdf -p 9090:9090 cumulusrdf  ```    CumulusRDF is now available on http://localhost:9090/cumulus    ## Want to contribute?  We welcome any kind of contribution to cumulusRDF. In particular, we have a developer mailing list. Feel free to sign up via the web interface or by emailing at cumulusrdf-dev-list+subscribe@googlegroups.com.     ## Support  You can sign up to the CumulusRDF mailing list via the web interface or by emailing at cumulusrdf-list+subscribe@googlegroups.com.     ## People Involved  CumulusRDF is developed by the Institute of Applied Informatics and Formal Desciption Methods (AIFB) as well as by developers from our community. The developers are (in random order)    ### Active Contributors  * Andreas Wagner   * <a href=""https://github.com/aharth"" target=""_new"">Andreas Harth</a>    * Sebastian Schmidt    * <a href=""https://github.com/agazzarini"" target=""_new"">Andrea Gazzarini</a>    * <a href=""https://github.com/fzancan"" target=""_new"">Federico Zancan</a>    * Yongtao Ma    * <a href=""https://github.com/kamir"" target=""_new"">Mirko Kämpf</a>      ### Previous Contributors    * Günter Ladwig    * Steffen Stadtmüller    * Felix Obenauer      ## Publications   ##### <a href=""http://iswc2011.semanticweb.org/fileadmin/iswc/Papers/Workshops/SSWS/Ladwig-et-all-SSWS2011.pdf"" target=""_new"">CumulusRDF: Linked Data Management on Nested Key-Value Stores</a>        ```Java  @Inproceedings{ladwig2011cldmonks,  author = {Guenter Ladwig and Andreas Harth}  title = {CumulusRDF: Linked Data Management on Nested Key-Value Stores},  booktitle = {Proceedings of the 7th International Workshop on   Scalable Semantic Web Knowledge Base Systems (SSWS2011)  at the 10th International Semantic Web Conference (ISWC2011)},  month = {October},  year = {2011}  }  ```    ##### <a href=""http://ribs.csres.utexas.edu/nosqlrdf/nosqlrdf_iswc2013.pdf"" target=""_new"">NoSQL Databases for RDF: An Empirical Evaluation</a>        ```Java  @incollection{  year={2013},  isbn={978-3-642-41337-7},  booktitle={The Semantic Web – ISWC 2013},  volume={8219},  series={Lecture Notes in Computer Science},  editor={Alani, Harith and Kagal, Lalana and Fokoue, Achille and Groth, Paul and Biemann, Chris and Parreira, JosianeXavier and Aroyo, Lora and Noy, Natasha and Welty, Chris and Janowicz, Krzysztof},  doi={10.1007/978-3-642-41338-4_20},  title={NoSQL Databases for RDF: An Empirical Evaluation},  url={http://dx.doi.org/10.1007/978-3-642-41338-4_20},  publisher={Springer Berlin Heidelberg},  author={Cudré-Mauroux, Philippe and Enchev, Iliya and Fundatureanu, Sever and Groth, Paul and Haque, Albert and Harth, Andreas and Keppmann, FelixLeif and Miranker, Daniel and Sequeda, JuanF. and Wylot, Marcin},  pages={310-325}  }  ```  ## Acknowledgements         CumulusRDF was supported by the German Federal Ministry of Economics and Technology in the iZEUS project.      The CumulusRDF logo was kindly provided by <a href=""https://github.com/danieleliberti"" target=""_new"">Daniele Liberti</a>.          -------------------------  [1] http://en.wikipedia.org/wiki/Cumulus     [2] http://cassandra.apache.org     [3] http://openrdf.org """
Semantic web;https://github.com/IBCNServices/StreamingMASSIF;"""# StreamingMASSIF    This is the implementation of the *StreamingMASSIF* platform, a streaming extension of the MASSIF platform.    StreamingMASSIF allows to perform cascading reasoning by combining various components. In its standard configuration it allows to filter meaningful events from a datastream through RDF Stream Processing, abstract the selection through DL reasoning and perform Complex Event Processing ontop of these abstraction.    Check the [wikipage](https://github.com/IBCNServices/StreamingMASSIF/wiki) for a more in depth explanation on how to use Streaming MASSIF!    How to cite [Streaming MASSIF](https://www.mdpi.com/1424-8220/18/11/3832):  ```  @article{bonte2018streaming,    title={Streaming MASSIF: Cascading Reasoning for Efficient Processing of IoT Data Streams},    author={Bonte, Pieter and Tommasini, Riccardo and Della Valle, Emanuele and De Turck, Filip and Ongenae, Femke},    journal={Sensors},    volume={18},    number={11},    pages={3832},    year={2018},    publisher={Multidisciplinary Digital Publishing Institute}  }  ```    How to cite [MASSIF](https://link.springer.com/article/10.1007/s10115-016-0969-1):  ```  @article{bonte2017massif,    title={The MASSIF platform: a modular and semantic platform for the development of flexible IoT services},    author={Bonte, Pieter and Ongenae, Femke and De Backere, Femke and Schaballie, Jeroen and Arndt, D{\""o}rthe and Verstichel, Stijn and Mannens, Erik and Van de Walle, Rik and De Turck, Filip},    journal={Knowledge and Information Systems},    volume={51},    number={1},    pages={89--126},    year={2017},    publisher={Springer}  }  ```    ## Building and running MASSIF    ### Requirements:    - Java 9+  - Maven2    ### Build  To build the MASSIF app, call `mvn` and get the compiled `.jar`.  ```shell  mvn clean compile assembly:single  mv target/massif-jar-with-dependencies.jar .  ```    To build the MASSIF classes (e.g. for usage in higher level apps), call `mvn` to compile and install the project in the local repository, then add the package to the higher level app dependencies (see `pom.xml` snippet below):    ```shell  mvn install -Dmaven.test.skip=true   ```  ```xml  <dependency>      <groupId>be.ugent.idlab</groupId>      <artifactId>massif</artifactId>      <version>0.0.1</version>  </dependency>  ```    ### Run  To run MASSIF, call the compiled `.jar` from the command line as follows:  ```shell  java -jar -Dlog4j.configurationFile=webfiles/log4j2.xml massif-jar-with-dependencies.jar  ```  Calling this command will return something like this on the CLI:  ```shell  21:27:32.313 [main] INFO  idlab.massif.run.Run - MASSIF STARTING  21:27:32.403 [main] INFO  idlab.massif.run.Run - MASSIF Listening on port 9000  21:27:32.403 [main] INFO  idlab.massif.run.Run - Access the MASSIF GUI on  localhost:9000 or register a configuration on localhost:9000/register  21:27:32.414 [main] INFO  idlab.massif.run.Run - MASSIF is ONLINE  ```  Run options:  * `-p` : TCP port on which massif listens, default: `9000`    GUI:  * The MASSIF GUI is available on http://localhost:9000    ### REST API  The MASSIF allow for direct management through GET/POST calls.  Here are the most important path:  * `<massif_url>/register`: register a configuration  * `<massif_url>/stop` : stop a certain query  * `<massif_url>/configs` : get all registered configs    ### REST API call examples with the [cURL](https://linuxize.com/post/curl-post-request/) tool  List of active configs:  ```  curl -X GET --verbose \    --url ""http://127.0.0.1:9000/configs""  ```  * Note: returns `{}` if there are no active config.    Stop a config (pre-requisite:  config ID, see previous command):  ```  curl -X POST --verbose \    -H ""Content-Type: application/json"" \    -d '19' \    --url ""http://127.0.0.1:9000/stop""  ```  * Note: returns `ok`    Send a config:  ```  curl -X POST --verbose \    -H ""Content-Type: application/json"" \    -d '{""configuration"":{""0"":[1],""1"":[]},""components"":{""0"":{""type"":""Source"",""impl"":""kafkaSource"",""kafkaServer"":""127.0.0.1:9092"",""kafkaTopic"":""backblaze_smart""},""1"":{""type"":""Sink"",""impl"":""httpGetSinkCombined"",""path"":""1"",""config"":""""}}}' \    --url ""http://127.0.0.1:9000/register""  ```  * Note: returns the config ID    Get a component running metrics (pre-requisite: component ID, see previous commands):  ```  curl -X GET --verbose \    --url ""http://127.0.0.1:9000/monitor/1""  ```     """
Semantic web;https://github.com/KNowledgeOnWebScale/unshacled;"""# UnSHACLed    A visual editor for RDF constraints  currently supporting the visual notations [ShapeUML](https://w3id.org/imec/unshacled/spec/shape-uml/20210118) and [ShapeVOWL](https://w3id.org/imec/unshacled/spec/shape-vowl/20210118/)  and import/export/validation of [SHACL](https://www.w3.org/TR/shacl/) constraints.    The [previous version](https://github.com/oSoc19/unshacled) of UnSHACLed was implemented during the Open Summer of Code 2019 in Belgium under the MIT license.  This is an updated version which adds support for different visual notations to interact visually with RDF constraints.    ## Contents  1. [Overview](#Overview)  2. [Setup](#Setup)  3. [Contribute](#Contribute)    ## Overview  At the time of writing this editor supports SHACL, future support for ShEx is envisioned. This editor makes abstraction of specific constraint languages and exposes concepts in a simple visual interface.    ### Functionalities  - [x] Drag and drop to rearrange the visualized shapes  - [x] Add, remove and edit shapes, constraints and relationships  - [x] View and edit namespaces and prefixes  - [x] Import SHACL files in JSON and Turtle  - [x] Export SHACL files in JSON and Turtle  - [x] Import data files in JSON and Turtle  - [x] View and edit data files in JSON format  - [x] Validate data files    ### Concepts  An [internal model](#Model) is used to represent shapes which can be edited in the browser. Using existing shape files requires these to be imported and [translated to this model](#Translation) before use. Editing is done in a [visual editor](#Interface).    ## Setup  To start the application, run the following commands:  1. Install dependencies  ```  npm install  ```  2. Compile and hot-reload for development  ```  npm run serve  ```    The documentation can be generated in `/docs` using the following command:  ```  npm run docs  ```    ### Useful while developing: testing and linting  ```  npm run test // Run tests  npm run lint // Check and fix code style  ```    ### Compile and minify for production  Execute this command, then move the contents of `/dist` into the `gh-pages` branch. The application will be automatically deployed to [UnSHACLed.com](https://unshacled.com).  ```  npm run build  ```    ## Contribute  This section contains information to help contribute to this project.  For more information about the project structure, the internal model et cetera, please consult [the wiki of the previous version](https://github.com/oSoc19/unshacled/wiki/Home).    ### Linting  To ensure code style consistency we use [ESLint](https://eslint.org/) and [Prettier](https://prettier.io/) which are configured in `.eslintrc.js`.    ### Testing  Testing is done with [Jest.js](https://jestjs.io/) and [Vue Jest](https://github.com/vuejs/vue-jest). Unit tests are kept in the same directory as the classes they test and share the same filename but with extension e.g. `somefile.js` and `somefile.test.js`. All tests can be executed using the following command:   ```  npm run test  ```    ### Documentation  Make sure to document your code in [JSDoc style](https://jsdoc.app/). Documentation is generated using the command:   ```  npm run docs  ```    ```  /* This comment should appear in the HTML documentation. */  // This is just a comment and should not be added to the HTML documentation.  ``` """
Semantic web;https://github.com/AKSW/AutoSPARQL;"""## Introduction    AutoSPARQL TBSL is a graphical user interface, which allows to answer natural  language queries over RDF knowledge bases. It is based on algorithms  implemented in the DL-Learner Semantic Web machine learning framework.    ## Requirements  * Java 7 or higher  * Maven 3 or higher  * Git  * an **IPv6-capable internet connection** (you can use a free IPv6 tunnel service like Miredo or Teredo if your connection does not support it natively)    ## Warning  AutoSPARQL TBSL is a research prototype that is not actively developed anymore.  As such, getting it to work may need some effort and depends on the availability of several other online services.  Feel free to create issues if you encounter problems and please share your fixes using pull requests.    ## Installation and Execution  1. clone the git repository  2. run `./compile` and then `./run` (Linux, Mac) or `compile.bat` and then `run.bat` (Windows)  * Manually: Do `mvn install -N` in the folders in that order: autosparql, commons, algorithm-tbsl. Then go into autosparql-tbsl and run `mvn jetty:run`.    Then, go into your browser and access `http://localhost:8080` and click on the link to the application.    ## Error Reporting  If you encounter errors, please look at the issues if the problem is already reported.  If not, please create a single issue including the command line output.  If the error occurs during compilation, please use `./createcompillelog` instead of `./compile` to create the compile log.  Feel free to create issues if you encounter problems and please share your fixes using pull requests.    ## Adding your own Dataset  Using your own datasource instead of DBpedia or Oxford is nontrivial.  It needs several days of work in addition to the time needed to familiarize yourself with the code base.    You need to:    - fork AutoSPARQL TBSL  - add your own domain dependent lexicon as an LTAG grammar at algorithm-tbsl/src/main/resources/tbsl/lexicon (see http://pub.uni-bielefeld.de/publication/2002961 and http://pub.uni-bielefeld.de/publication/2278529 as well as the existing files in that folder)  - add your knowledge base:   1. as a local model to algorithm-tbsl/src/main/resources/models/yourmodel (preferred for small knowledge bases as it is much faster and more reliable)   2. as a SPARQL (version 1.0 is enough) endpoint along with a SOLR server instance  - create a singleton for your knowledge base (see package org.aksw.autosparql.tbsl.algorithm.knowledgebase)  - extend org.aksw.autosparql.tbsl.algorithm.learning.TBSL with TbslYourKnowledgeBase  - finally, in case the dataset isn't a private model, please do a pull request so that it can be integrated in the main project    ## Modules and Packages  | Maven Module      | Package                             | Purpose       |  |-------------------|-------------------------------------|---------------|  | autosparql-parent |                  -                  | Parent Module |  | algorithm-tbsl    | org.aksw.autosparql.tbsl.algorithm  | Algorithm     |  | commons           | org.aksw.autosparql.commons         | Utilities     |  | autosparql-tbsl   | org.aksw.autosparql.tbsl.gui.vaadin | Web Interface | """
Semantic web;https://github.com/albertmeronyo/SPARQL2Git;"""# SPARQL2Git"""
Semantic web;https://github.com/angelo-v/groovyrdf;"""# groovyrdf [![CircleCI](https://circleci.com/gh/angelo-v/groovyrdf/tree/master.svg?style=svg)](https://circleci.com/gh/angelo-v/groovyrdf/tree/master) [![Known Vulnerabilities](https://snyk.io/test/github/angelo-v/groovyrdf/badge.svg)](https://snyk.io/test/github/angelo-v/groovyrdf)    Library for building and processing RDF data with Groovy. groovyrdf helps you to build and consume RDF and Linked Data ""the groovy way"".    Version 0.2.7    ## Install     ### Maven    ```xml  <dependency>    <groupId>de.datenwissen.util</groupId>    <artifactId>groovyrdf</artifactId>    <version>0.2.7</version>    <type>pom</type>  </dependency>  ```    ### Gradle    ```groovy  compile 'de.datenwissen.util:groovyrdf:0.2.7'  ```    ### Ivy    ```xml  <dependency org='de.datenwissen.util' name='groovyrdf' rev='0.2.7'>    <artifact name='groovyrdf' ext='pom' ></artifact>  </dependency>  ```    ## Usage    Please take a look at the [user guide] on how to use groovyrdf.    [user guide]: http://angelo-v.github.com/groovyrdf/    ## Contact    Please contact me for any questions & feedback: [angelo.veltens@online.de](mailto:angelo.veltens@online.de)    ## Release Notes    Version 0.2.2 - 0.2.7    - security & dependency updates    Version 0.2.1    - adding WebIDs to resources    Version 0.2    - reading & processing RDF data from linked data resources    Version 0.1    - building RDF data    ## License    Copyright (c) 2017, Angelo Veltens    All rights reserved.    Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:    - Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.  - Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.    THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. """
Semantic web;https://github.com/njh/redstore;"""RedStore  ========  Nicholas J. Humfrey <njh@aelius.com>    For the latest version of RedStore, please see:  <http://www.aelius.com/njh/redstore/>      What is RedStore ?  ------------------  RedStore is a lightweight RDF triplestore written in C using the [Redland] library.    It has a HTTP interface and supports the following W3C standards:    * [SPARQL 1.0 Query]  * [SPARQL 1.1 Protocol for RDF]  * [SPARQL 1.1 Graph Store HTTP Protocol]  * [SPARQL 1.1 Service Description]    Features  --------    * Built-in HTTP server  * Mac OS X app available  * Supports a wide range of RDF formats  * Only runtime dependancy is [Redland].  * Compatible with rdfproc command line tool for offline operations  * Unit and integration test suite.    Usage  -----      redstore [options] [<name>]         -p <port>       Port number to run HTTP server on (default 8080)         -b <address>    Bind to specific address (default all)         -s <type>       Set the graph storage type (default hashes)         -t <options>    Storage options         -n              Create a new store / replace old (default no)         -f <filename>   Input file to load at startup         -F <format>     Format of the input file (default guess)         -v              Enable verbose mode         -q              Enable quiet mode      Start RedStore on port 8080, bound to localhost, using a new sqlite store:        redstore -p 8080 -b localhost -n -s sqlite    Load a URI into the triplestore:        curl --data uri=http://example.com/file.rdf http://localhost:8080/load    Add a file to the triplestore:        curl -T foaf.rdf 'http://localhost:8080/data/foaf.rdf'    Add a file to the triplestore with full URI specified:        curl -T foaf.rdf 'http://localhost:8080/data/?graph=http://example.com/foaf.rdf'    Add a file to the triplestore with type specified:        curl -T foaf.ttl -H 'Content-Type: application/x-turtle' 'http://localhost:8080/data/foaf.rdf'     You can delete graphs with in the same manner, using the DELETE HTTP verb:        curl -X DELETE 'http://localhost:8080/data/foaf.rdf'    Query using the [SPARQL Query Tool]:        sparql-query http://localhost:8080/sparql 'SELECT * WHERE { ?s ?p ?o } LIMIT 10'      Requirements  ------------    The minimum required versions of the [Redland] RDF Libraries are:    - [raptor2-2.0.4]  - [rasqal-0.9.27]  - [redland-1.0.14]      Installation  ------------  RedStore uses a standard automake build process:        ./configure      make      make install      Supported Storage Modules  -------------------------    You can use any of the [Redland Storage Modules] that supports contexts:    - [hashes] (Default)  - [mysql]  - [memory]  - [postgresql]  - [sqlite]  - [virtuoso]      License  -------    This program is free software: you can redistribute it and/or modify  it under the terms of the [GNU General Public License] as published by  the Free Software Foundation, either version 3 of the License, or  (at your option) any later version.    This program is distributed in the hope that it will be useful,  but WITHOUT ANY WARRANTY; without even the implied warranty of  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the  GNU General Public License for more details.        [Redland]:                     http://librdf.org/  [Redland Storage Modules]:     http://librdf.org/docs/api/redland-storage-modules.html  [SPARQL Query Tool]:           http://github.com/tialaramex/sparql-query  [GNU General Public License]:  http://www.gnu.org/licenses/gpl.html    [SPARQL 1.0 Query]:                     http://www.w3.org/TR/rdf-sparql-query/  [SPARQL 1.1 Protocol for RDF]:          http://www.w3.org/TR/sparql11-protocol/  [SPARQL 1.1 Graph Store HTTP Protocol]: http://www.w3.org/TR/sparql11-http-rdf-update/  [SPARQL 1.1 Service Description]:       http://www.w3.org/TR/sparql11-service-description/    [raptor2-2.0.4]:               http://download.librdf.org/source/raptor2-2.0.4.tar.gz  [rasqal-0.9.27]:               http://download.librdf.org/source/rasqal-0.9.27.tar.gz  [redland-1.0.14]:              http://download.librdf.org/source/redland-1.0.14.tar.gz    [hashes]:                      http://librdf.org/docs/api/redland-storage-module-hashes.html  [mysql]:                       http://librdf.org/docs/api/redland-storage-module-mysql.html  [memory]:                      http://librdf.org/docs/api/redland-storage-module-memory.html  [postgresql]:                  http://librdf.org/docs/api/redland-storage-module-postgresql.html  [sqlite]:                      http://librdf.org/docs/api/redland-storage-module-sqlite.html  [virtuoso]:                    http://librdf.org/docs/api/redland-storage-module-virtuoso.html """
Semantic web;https://github.com/Robsteranium/csvwr;"""# CSV on the Web R Package (csvwr) <img src=""man/figures/logo.png"" align=""right"" height=""139"" />    [![build](https://github.com/Robsteranium/csvwr/actions/workflows/r.yml/badge.svg)](https://github.com/Robsteranium/csvwr/actions/workflows/r.yml)  [![pkgdown](https://github.com/Robsteranium/csvwr/actions/workflows/pkgdown.yml/badge.svg)](https://github.com/Robsteranium/csvwr/actions/workflows/pkgdown.yml)    Read and write csv tables annotated with metadata according to the ""CSV on the Web"" standard (CSVW).    The [csvw model for tabular data](https://w3c.github.io/csvw/syntax/) describes how to annotate a group of csv tables to ensure they are interpreted correctly.    This package uses the [csvw metadata schema](https://w3c.github.io/csvw/metadata/) to find tables, identify column names and cast values to the correct types.    The aim is to reduce the amount of manual work needed to parse and prepare data before it can be used in analysis.      ## Usage    ### Reading CSVW    You can use `csvwr` to read a csv table with json annotations into a data frame:    ```r  library(csvwr)    # Parse a csv table using json metadata :  csvw <- read_csvw(""data.csv"", ""metadata.json"")    # To extract the parsed table (with syntactic variable names and typed-columns):  csvw$tables[[1]]$dataframe  ```    Alternatively, you can jump straight to the parsed table in one call:    ```  read_csvw_dataframe(""data.csv"", ""metadata.json"")  ```    ### Writing CSVW    You can also prepare annotations for a data frame:    ```r  # Given a data frame (saved as a csv)  d <- data.frame(x=c(""a"",""b"",""c""), y=1:3)  write.csv(d, ""table.csv"", row.names=FALSE)    # Derive a schema  s <- derive_table_schema(d)    # Create metadata (as a list)  m <- create_metadata(tables=list(list(url=""table.csv"", tableSchema=s)))    # Serialise the metadata to JSON  j <- jsonlite::toJSON(m)    # Write the json to a file  cat(j, file=""metadata.json"")  ```    For a complete introduction to the library please see the `vignette(""read-write-csvw"")`.      ## Installation    You'll need to use devtools to install this package from github:    ```r  install.packages(""devtools"")  devtools::install_github(""Robsteranium/csvwr"")  ```    ## Contributing    ### Roadmap    Broadly speaking, the objectives are as follows:    - parse csvw, creating dataframes with specified names and types (mostly implemented)  - connecting associated csv tables and json files according to the conventions set out in the csvw standard (partly implemented)  - support for validating a table according to a metadata document (a little implemented)  - support for multiple tables (mostly implemented)  - tools for writing csvw metadata, given an R data frame (partly implemented)  - vignettes and documentation (mostly implemented)  - scripts for running the most useful tools from the command line (not yet implemented)    It's not an urgent objective for the library to perform csv2rdf or csv2json translation although some support for csv2json is provided as this is used to test that the parsing is done correctly.    In terms of the csvw test cases provided by the standard, the following areas need to be addressed (in rough priority order):    - datatypes (most of simple datatypes and some complex ones are supported, but there are more types and constraints too)  - validations (there are a lot of these 😊)  - propagation of inherited properties  - http retrieval (`readr::read_csv` (and indeed `utils::read.csv`) accepts URIs, but the spec also involves link, dialect, and content-type headers)  - referential integrity (a foundation for this is in place)  - json nesting    ### Testing    The project currently incorporates two main parts of the [csvw test](https://w3c.github.io/csvw/tests/) suite:    - [Parsing with JSON output](https://github.com/Robsteranium/csvwr/blob/master/tests/testthat/test-csvw-parsing-json.R)  - [Validation](https://github.com/Robsteranium/csvwr/blob/master/tests/testthat/test-csvw-validation.R)    In each case, we're running only that subset of test entries that can be expected to pass given that part of the standard that has thus far been implemented. Some entries will be skipped (either permanently or) while other priorities are implemented.    You can find out what needs to be implemented next by widening the subset to include the next entry.    During development, you may find it convenient to recreate one of the test entries for exploration. There is a convenience function in [tests/csvw-tests-helpers.R](https://github.com/Robsteranium/csvwr/blob/master/tests/csvw-tests-helpers.R). This isn't exported by the package so you'll need to evaluate it explicitly. You can then use it as follows:    ```r  run_entry_in_dev(16) # index number in the list of entries  run_entry_in_dev(id=""manifest-json#test023"") # identifier for the test  ```    There are also some more [in-depth unit tests](https://github.com/Robsteranium/csvwr/blob/master/tests/testthat/test-parsing.R) written for this library.    ### Workflow    You can use `devtools::load_all()` (`CTRL + SHIFT + L` in RStudio) to load updates and `testthat::test_local()` (`CTRL + SHIFT + T`) to run the tests.    In order to check the vignettes, you need to do `devtools::install(build_vignettes=T)`. Then you can open e.g. `vignette(""read-write-csvw"")`.    ## License    GPL-3    To discuss other licensing terms, please [get in contact](mailto:csvw@infonomics.ltd.uk).    ## Other CSVW tools    There's another R implementation of csvw in the package [rcsvw](https://github.com/davideceolin/rcsvw).    If you're interested in csvw more generally, then the [RDF::Tabular](https://github.com/ruby-rdf/rdf-tabular/) ruby gem provides one of the more robust and comprehensive implementations, supporting both translation and validation.    If you're specifically interested in validation, take a look at the [ODI](https://theodi.org/)'s [csvlint](https://github.com/Data-Liberation-Front/csvlint.rb) which implements csvw and also the [OKFN](https://okfn.org/)'s [frictionless data table schemas](https://specs.frictionlessdata.io/).    If you want rdf translation, then you might like to check out [Swirrl](https://www.swirrl.com/)'s [csv2rdf](https://github.com/Swirrl/csv2rdf/) and also [table2qb](https://github.com/swirrl/table2qb) which generates csvw annotations from csv files to describe [RDF Data Cubes](https://www.w3.org/TR/vocab-data-cube/). """
Semantic web;https://github.com/kbss-cvut/jopa;"""# JOPA - Java OWL Persistence API    [![Build Status](https://kbss.felk.cvut.cz/jenkins/buildStatus/icon?job=jopa-stable)](https://kbss.felk.cvut.cz/jenkins/job/jopa-stable)    JOPA is a Java OWL persistence framework aimed at efficient programmatic access to OWL2 ontologies and RDF graphs in Java. The system is based  on integrity constraints [1] in OWL that JOPA uses to establish the contract between a JOPA-enabled Java application and  an OWL ontology. The system architecture and API is similar to JPA 2.1, see [2].    Notable changes:    * **0.17.0** - Support for SPARQL-based entity attributes and Criteria API. See the [wiki](https://github.com/kbss-cvut/jopa/wiki) for more details.  * **0.15.0** - Support for multilingual String attributes. See the [wiki](https://github.com/kbss-cvut/jopa/wiki/Multilingual-String-Attributes) for more info  * **0.14.0** - Support for the Semantic Object Query Language (SOQL). See the [wiki](https://github.com/kbss-cvut/jopa/wiki/Semantic-Object-Query-Language) for more info  * **0.13.0** - Support for RDF simple literals (language-less strings), access to lexical form of literals, updated named graph handling  * **0.12.0** - Support for `EntityManager.getReference`  * **0.11.0** - Support for Java 8 Streams in Query API and RDF4J repository configuration file  * **0.10.0** - Jena OntoDriver implementation  * **0.9.0** - Single class inheritance support  * **0.8.0** - Mapped superclass support    ### Main Features    * Object-ontological mapping (OOM) based on integrity constraints,  * Explicit access to inferred knowledge,  * Access to unmapped properties and individual's types,  * Transactions,  * Separate storage access layer - Jena, OWLAPI, RDF4J (Sesame) drivers are available.    #### Object-ontological mapping based on integrity constraints    Similarly to ORM, OOM enables to map ontological constructs to constructs of an object-oriented programming language and vice versa.    More specifically, OOM in JOPA maps (using the JLS [3] terminology):    | Ontology   | OO Language    |  | ---------- | -------------- |  | OWL Class  | Reference type |  | Object property | Reference type member |  | Data property | Primitive type member (+ String, Date) |  | Annotation property | Reference or primitive type member |  | Class assertions | Reference type instance or @Types record |    All this means that individuals belonging to an OWL class can be retrieved as instances of a (Java) class.    #### Inheritance    * Support for mapped superclasses was added in version __0.8.0__.  * Support for single inheritance was added in version __0.9.0__.    #### Explicit access to inferred knowledge    A member annotated with the `@Inferred` annotation represents a field whose values are retrieved using a reasoner. As such,  they can for example contain values of a inverse object property (like in [our Jedi example](https://github.com/kbss-cvut/jopa-examples/blob/master/example02-jopa-owlapi/src/main/java/cz/cvut/kbss/jopa/example02/Example.java)).    There are limitations to this: JOPA requires explicit class assertion to be able to load individual as instance of a class.  And, values of inferred members are read-only. These restrictions have pragmatic reasons - if the knowledge is inferred, it  cannot be directly modified/removed. Therefore, it would make no sense to remove object property value, if it was inferred.    #### Access to unmapped properties and individual's types    OOM is not meant to completely capture the ontological model. It would not even make much sense. One of the main features  of JOPA is its ability to work with knowledge which is not part of the object model. This is done using members annotated  with `@Types` and `@Properties`. `@Types` field contains all OWL classes whose instance the particular individual represented by  an object is. `@Properties` field contains values of properties not mapped by object model. This way, the application gets (although limited)  access to unmapped property values (e.g. values of newly added properties), without the need to adjust the object model and recompile.    #### Transactions    JOPA supports object-level transactions. In addition, it makes transactional change visible to the transaction that made them.  This means that when you add an instance of some class during a transaction and then list all instances of that class (during the same  transaction), you'll see the newly added instance as well. This is a feature not usually seen even in large triple stores.    There are some limitations to this approach. Currently, pending changes are not taken into account when doing inference.  Also, the current version of Sesame OntoDriver is not able to include pending changes into results of SPARQL queries.    #### Separate storage access layer    Similarly to JPA and JDBC driver, JOPA sits on top of an OntoDriver instance, which provides access to the underlying storage.  There are two main reasons for such split - first, it decouples storage-specific API usage from the more generic OOM core.  Second, it enables the application to switch the underlying storage with as little as 2-3 lines of configuration code. Nothing else  needs to be modified.    Supported storages:    * Jena (since **0.10.0**)  * OWLAPI  * Sesame/RDF4J    ### Not Supported, yet    JOPA currently does not support two important features - multiple inheritance and referential integrity.    Single inheritance (and mapped superclasses) have been supported since version 0.9.0 (0.8.0 resp.), multiple inheritance is currently in planning.    As for referential integrity, this for example means that removing an instance that is referenced by another instance should  not be possible. Such feature is vital for object-oriented application, but not compatible with the open-world nature of ontologies.  Design possibilities and their implications are currently being studied.    Other missing/planned stuff can be found in [TODO.md](TODO.md) and in the GitHub issue tracker.    ## Modules    The whole framework consists of several modules:    * _JOPA API_ - definition of the JOPA API, similar to JPA,  * _OntoDriver API_ - API of the storage access layer,  * _JOPA Implementation_ - persistence provider implementation,  * _OntoDriver Sesame_ - OntoDriver implementation for RDF4J (Sesame)-accessed storages,  * _OntoDriver OWLAPI_ - OntoDriver implementation for OWLAPI-accessed files,  * _Ontodriver Jena_ - OntoDriver implementation for Jena-based storages,  * _OWL2Java_ - generates JOPA entities based on integrity constraints in input ontology (see [Example01](https://github.com/kbss-cvut/jopa-examples/tree/master/example01-jopa-sesame-owl2java)),  * _JOPA Maven plugin_ - Maven plugin for object model generation (using OWL2Java).    Other modules represent integration tests and various utilities.    ## Documentation    Check out the [Wiki](https://github.com/kbss-cvut/jopa/wiki) for general information about JOPA, explanation of its features and their usage.  We will be gradually building up its content.    Javadoc of the latest stable version is available at [https://kbss.felk.cvut.cz/jenkins/job/jopa-stable/javadoc/index.html?overview-summary.html](https://kbss.felk.cvut.cz/jenkins/job/jopa-stable/javadoc/index.html?overview-summary.html).    For more practical examples of JOPA features, see the JOPA examples repository at [https://github.com/kbss-cvut/jopa-examples](https://github.com/kbss-cvut/jopa-examples).      ## Usage    JOPA examples can be found in a separate repository at [https://github.com/kbss-cvut/jopa-examples](https://github.com/kbss-cvut/jopa-examples).    A simplistic demo of JOPA ([https://github.com/kbss-cvut/jopa-examples/tree/master/eswc2016](https://github.com/kbss-cvut/jopa-examples/tree/master/eswc2016)) is running at [http://onto.fel.cvut.cz/eswc2016](http://onto.fel.cvut.cz/eswc2016).    A more mature project using JOPA as its persistence provider can be found at [https://github.com/kbss-cvut/reporting-tool](https://github.com/kbss-cvut/reporting-tool).  It is a safety occurrence reporting tool developed for the aviation industry as part of the INBAS project ([https://www.inbas.cz](https://www.inbas.cz)).  A live demo of it is running at [https://www.inbas.cz/reporting-tool-public](https://www.inbas.cz/reporting-tool-public).    JOPA is also used in the [BINBAS project](https://www.inbas.cz/web/binbas) [6] and, more recently, in [TermIt](https://github.com/kbss-cvut/termit) - a SKOS vocabulary manager.    Note that JOPA requires Java 8 or later.    ## Getting JOPA    There are two ways of getting JOPA for your project:    * Clone repository/download zip and build it with Maven,  * Use a Maven dependency from the [Maven central repository](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22cz.cvut.kbss.jopa%22).    Basically, the _jopa-impl_ module and one of the OntoDriver implementations is all that is needed:    ```xml  <dependencies>      <dependency>          <groupId>cz.cvut.kbss.jopa</groupId>          <artifactId>jopa-impl</artifactId>      </dependency>      <dependency>          <groupId>cz.cvut.kbss.jopa</groupId>          <artifactId>ontodriver-jena</artifactId>          <!-- OR <artifactId>ontodriver-owlapi</artifactId> -->          <!-- OR <artifactId>ontodriver-sesame</artifactId> -->      </dependency>  </dependencies>  ```    ## More Info    More information about JOPA can be found for example in articles [4], [5] and at [https://kbss.felk.cvut.cz/web/kbss/jopa](https://kbss.felk.cvut.cz/web/kbss/jopa).    JOPA build status and code metrics can be found at:    * KBSS Jenkins [https://kbss.felk.cvut.cz/jenkins](https://kbss.felk.cvut.cz/jenkins),  * KBSS SonarQube [https://kbss.felk.cvut.cz/sonarqube](https://kbss.felk.cvut.cz/sonarqube).    ### Performance    A performance comparison of JOPA and other object-triple mapping libraries can be found at [https://kbss.felk.cvut.cz/web/kbss/otm-benchmark](https://kbss.felk.cvut.cz/web/kbss/otm-benchmark).    A comprehensive comparison - feature and performance - of object-triple mapping libraries is presented in [8].    ## Related    Some related libraries:    * [JB4JSON-LD](https://github.com/kbss-cvut/jb4jsonld) - Serialization and deserialization of POJOs into JSON-LD.  * [JOPA-Spring-transaction](https://github.com/ledsoft/jopa-spring-transaction) - Declarative Spring transactions (using the `@Transactional` annotation) with JOPA.  * [Reporting Tool](https://github.com/kbss-cvut/reporting-tool) - Real-life use case of JOPA.  * [TermIt](https://github.com/kbss-cvut/termit) - A more complex and up-to-date use case of JOPA.    ## References      * [1] J. Tao and E. Sirin, J. Bao, D. L. McGuinness, Integrity Constraints in OWL, The Twenty-Fourth AAAI Conference on Artiﬁcial Intelligence, 2010, available online at [http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1931/2229](http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1931/2229)  * [2] JSR 338 [http://jcp.org/en/jsr/detail?id=338](http://jcp.org/en/jsr/detail?id=338)  * [3] The Java Language Specification, Java SE 8 Edition [https://docs.oracle.com/javase/specs/jls/se8/html/index.html](https://docs.oracle.com/javase/specs/jls/se8/html/index.html)  * [4] P. Křemen and Z. Kouba: Ontology-Driven Information System Design. IEEE Transactions on Systems, Man, and Cybernetics: Part C, 42(3):334–344, May 2012 [https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6011704](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6011704)  * [5] M. Ledvinka and P. Křemen: JOPA: Accessing Ontologies in an Object-oriented Way. In Proceedings of the 17th International Conference on Enterprise Information Systems. Porto: SciTePress - Science and Technology Publications, 2015, p. 212-222. ISBN 978-989-758-096-0. [http://www.scitepress.org/DigitalLibrary/PublicationsDetail.aspx?ID=p/CdcFwtlFM=&t=1](http://www.scitepress.org/DigitalLibrary/PublicationsDetail.aspx?ID=p/CdcFwtlFM=&t=1)  * [6] Ledvinka, M.; Křemen, P.; Kostov, B. JOPA: Efficient Ontology-based Information System Design In: The Semantic Web: ESWC 2016 Satellite Events. Cham: Springer International Publishing AG, 2016. pp. 156-160. 9989. ISSN 0302-9743. ISBN 978-3-319-47601-8. [ESWC 2016 Demo](http://2016.eswc-conferences.org/sites/default/files/papers/Accepted%20Posters%20and%20Demos/ESWC2016_DEMO_JOPA.pdf)  * [7] Ledvinka, M.; Křemen, P.; Kostov, B.; Blaško, M. SISel: Aviation Safety Powered by Semantic Technologies In: Data a znalosti 2017. Plzeň: Západočeská univerzita v Plzni, 2017. pp. 77-82. ISBN 978-80-261-0720-0. [https://daz2017.kiv.zcu.cz/data/DaZ2017-Sbornik-final.pdf](https://daz2017.kiv.zcu.cz/data/DaZ2017-Sbornik-final.pdf)  * [8] M. Ledvinka and  P. Křemen: A comparison of object-triple mapping libraries Semantic Web, 2019, doi: [10.3233/SW-190345](http://dx.doi.org/10.3233/SW-190345)    ## License    LGPLv3 """
Semantic web;https://github.com/ad-freiburg/QLever;"""# QLever      [![Build Status via Docker](https://github.com/ad-freiburg/QLever/actions/workflows/docker.yml/badge.svg)](https://github.com/ad-freiburg/QLever/actions/workflows/docker.yml)  [![Build Status via G++10/Clang++11](https://github.com/ad-freiburg/QLever/actions/workflows/cmake.yml/badge.svg)](https://github.com/ad-freiburg/QLever/actions/workflows/cmake.yml)        QLever (pronounced ""Clever"") is a SPARQL engine that can efficiently index and query very large knowledge graphs with up to 100 billion triples on a single standard PC or server.  In particular, QLever is fast for queries that involve large intermediate or final results, which are notoriously hard for engines like Blazegraph or Virtuoso.  QLever also supports search in text associated with the knowledge base, as well as SPARQL autocompletion.  [Here are demos of QLever](http://qlever.cs.uni-freiburg.de) on a variety of large knowledge graphs, including the complete Wikidata and OpenStreetMap.  Those demos also feature QLever's context-sensitiv autocompletion, which makes SPARQL query construction so much easier.    QLever was first described and evaluated in this [CIKM'17  paper](http://ad-publications.informatik.uni-freiburg.de/CIKM_qlever_BB_2017.pdf).  QLever has developed a lot since then.  Qlever's autocompletion functionality and some other new features are described and evaluated in [this paper](https://ad-publications.cs.uni-freiburg.de/ARXIV_sparql_autocompletion_BKKKS_2021.pdf).  If you use QLever in your work, please cite those papers.  QLever supports standard SPARQL 1.1 constructs like:  LIMIT, OFFSET, ORDER BY, GROUP BY, HAVING, COUNT, DISTINCT, SAMPLE, GROUP_CONCAT, FILTER, REGEX, LANG, OPTIONAL, UNION, MINUS, VALUES, BIND.  Predicate paths and subqueries are also supported.  The SERVICE keyword is not yet supported.  We aim at full SPARQL 1.1 support and we are almost there (except for SPARQL Update operations, which are a longer-term project).    # Quickstart    For easy step-by-step instructions on how to build an index using QLever and  then start a SPARQL endpoint using that index, see our [Quickstart Guide](docs/quickstart.md).  This will take you through a simple example dataset ([120 Years of Olympics](https://github.com/wallscope/olympics-rdf), with 1.8M triples)  as well as a very large dataset ([the complete Wikidata](https://www.wikidata.org), with 16 billion triples as of 30.09.2021).    # Advanced feature and more in-depth information    QLever's [advanced features are described here](docs/advanced_features.md).    For more in-depth information, see the various other `.md` files in [this folder](docs). """
Semantic web;https://github.com/AKSW/QuitDiff;"""# Quit Diff    ## Requirements    For using QuitDiff you need to have python version 3 installed.    To install the required packages use pip:        pip install -r requirements.txt    ## Use as `git-difftool`    Add one of the following sections to you `~/.gitconfig` (in your home directory) or `.git/config` (in your git working directory).        [difftool ""quitdiff""]          cmd = quitdiff.py --local=\""$LOCAL\"" --remote=\""$REMOTE\"" --merged=\""$MERGES\"" --base=\""$BASE\""          [difftool ""quitdiff-sparql""]          cmd = quitdiff.py --diffFormat sparql --local=\""$LOCAL\"" --remote=\""$REMOTE\"" --merged=\""$MERGES\"" --base=\""$BASE\""        [difftool ""quitdiff-eccrev""]          cmd = quitdiff.py --diffFormat eccrev --local=\""$LOCAL\"" --remote=\""$REMOTE\"" --merged=\""$MERGES\"" --base=\""$BASE\""    The git diff tool can then called with one of the following commands        $ git difftool -t quitdiff      $ git difftool -t quitdiff HEAD~0..HEAD~2    ## Use as `git-diff`    Add the following sections to you `~/.gitconfig` (in your home directory) or `.git/config` (in your git working directory).        [diff ""quitdiff""]          command = quitdiff.py    and the following to `.gitattributes`  (in your git working directory).        *.nq diff=quitdiff      *.trig diff=quitdiff      *.nt diff=quitdiff      *.ttl diff=quitdiff      *.rdf diff=quitdiff    git diff can then called with one of the following commands        $ git diff      $ git diff HEAD~0..HEAD~2      # Command line parameters  This tool can be used for git-diff or as git-difftool    ## git-diff:    if using as git-diff, the parameters are: `path old-file old-hex old-mode new-file new-hex new-mode`    ## git-difftool:  https://git-scm.com/docs/git-difftool  * $LOCAL is set to the name of the temporary file containing the contents of the diff pre-image and  * $REMOTE is set to the name of the temporary file containing the contents of the diff post-image.  * $MERGED is the name of the file which is being compared.  * $BASE is provided for compatibility with custom merge tool commands and has the same value as $MERGED.    * local is the old version  * remote is the new version    # License    Copyright (C) 2017 Natanael Arndt <http://aksw.org/NatanaelArndt> and Norman Radtke <http://aksw.org/NormanRadtke>    This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3 of the License, or (at your option) any later version.    This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.    You should have received a copy of the GNU General Public License along with this program; if not, see <http://www.gnu.org/licenses>.  Please see [LICENSE](LICENSE.txt) for further information. """
Semantic web;https://github.com/blake-regalia/linked-data.syntaxes;"""# Syntax highlighting for Linked Data developers    Each syntax highlighter in this package covers the *entire* grammar specification for its language. This provides very high-resolution scopes and immediately shows invalid syntax using special alert highlighting and [all tokens are inspectable](https://superuser.com/questions/848836/how-do-i-see-what-the-current-scope-is-in-sublimetext).    ### Install:  Available on [Package Control](https://packagecontrol.io/packages/LinkedData) as `LinkedData` .    Alternatively, you can download the `.sublime-package` file (or the source code archives) from the [Releases](https://github.com/blake-regalia/linked-data.syntaxes/releases).    #### Features:   - Highly-resolution scoping allows for very detailed color schemes.   - Malformed syntax detection. Expected token(s) are [inspectable via scope name](https://superuser.com/questions/848836/how-do-i-see-what-the-current-scope-is-in-sublimetext).   - Auto-completion and validation for prefix mappings registered on [prefix.cc](http://prefix.cc).    #### Currently supported languages:   - [SPARQL 1.1](https://www.w3.org/TR/sparql11-query/) and [SPARQL*](https://blog.liu.se/olafhartig/2019/01/10/position-statement-rdf-star-and-sparql-star/)   - [RDF 1.1 Turtle](https://www.w3.org/TR/turtle/) (TTL), [Turtle*](https://blog.liu.se/olafhartig/2019/01/10/position-statement-rdf-star-and-sparql-star/), and [RDF 1.1 TriG](https://www.w3.org/TR/trig/)   - [ShExC 2.1](https://shex.io/shex-semantics/#shexc)   - [RDF 1.1 N-Triples](https://www.w3.org/TR/n-triples/) (NT) and [RDF 1.1 N-Quads](https://www.w3.org/TR/n-quads/) (NQ)   - [Notation3](https://www.w3.org/TeamSubmission/n3/) (N3)    #### Currently supported platforms:   - Sublime Text 3    #### Currently supported color themes:   - Macaron Dark   - Macaron Light (in beta)    #### *Planned langauage support*:   - OWL Manchester   - OWL Functional-Style   - RDFa   - JSON-LD    #### *Planned platform support*:   - Atom   - CodeMirror   - Emacs   - minted (LaTeX)   - ~Ace~    #### *Planned color theme support*   - *Suggestions?*      ### Activating the Light Color Scheme  This package ships with two color schemes which were designed specifically for the high-resolution scopes that the syntax highlighting definitions create. By default, this package will use the [Macaron Dark](#macaron-dark) color scheme. If you prefer to use [Macaron Light](#macaron-light), you'll need to create a settings file to override the syntaxes defined below:    First, create a new file in Sublime and paste these contents into it:  ```json  // These settings will override both User and Default settings for the specific LinkedData syntaxes  {  	""color_scheme"": ""Packages/LinkedData/macaron-light.sublime-color-scheme""  }  ```    Next, save this file as `LinkedData.sublime-settings` under the `User/` folder in the [Sublime Text 3 Packages directory](https://stackoverflow.com/a/49967132/1641160). That is, the path should end with: `[...]/Packages/User/LinkedData.sublime-settings` .    Finally, create a new symbolic link to this file for each syntax. The files should be in the same `User` subdirectory as the other file:    For Linux and Mac, open terminal in this directory and run:  ```bash  ln -s LinkedData.sublime-settings n-triples.sublime-settings  ln -s LinkedData.sublime-settings n-quads.sublime-settings  ln -s LinkedData.sublime-settings turtle.sublime-settings  ln -s LinkedData.sublime-settings trig.sublime-settings  ln -s LinkedData.sublime-settings notation3.sublime-settings  ln -s LinkedData.sublime-settings shex.sublime-settings  ln -s LinkedData.sublime-settings sparql.sublime-settings  ```    For Windows, open command prompt in this directory and run:  ```cmd  mklink n-triples.sublime-settings  LinkedData.sublime-settings  mklink n-quads.sublime-settings    LinkedData.sublime-settings  mklink turtle.sublime-settings     LinkedData.sublime-settings  mklink trig.sublime-settings       LinkedData.sublime-settings  mklink notation3.sublime-settings  LinkedData.sublime-settings  mklink shex.sublime-settings       LinkedData.sublime-settings  mklink sparql.sublime-settings     LinkedData.sublime-settings  ```    This will override the default color scheme when any of these syntaxes are loaded in the current view.      ---    ## Previews:    ### Macaron Dark    #### Turtle:  ![Turtle Preview](doc/preview/macaron-dark/turtle.png)    #### SPARQL:  ![SPARQL Preview](doc/preview/macaron-dark/sparql.png)    #### ShExC:  ![ShExC Preview](doc/preview/macaron-dark/shex.png)    ### Macaron Light    #### Turtle:  ![Turtle Preview](doc/preview/macaron-light/turtle.png)    #### SPARQL:  ![SPARQL Preview](doc/preview/macaron-light/sparql.png)    #### ShExC:  ![ShExC Preview](doc/preview/macaron-light/shex.png)   """
Semantic web;https://github.com/rdfostrich/ostrich;"""# OSTRICH  _Offset-enabled TRIple store for CHangesets_    [![Build Status](https://travis-ci.org/rdfostrich/ostrich.svg?branch=master)](https://travis-ci.org/rdfostrich/ostrich)  [![Docker Automated Build](https://img.shields.io/docker/automated/rdfostrich/ostrich.svg)](https://hub.docker.com/r/rdfostrich/ostrich/)  [![DOI](https://zenodo.org/badge/97819866.svg)](https://zenodo.org/badge/latestdoi/97819866)    **OSTRICH** is an _RDF triple store_ that allows _multiple versions_ of a dataset to be stored and queried at the same time.    The store is a hybrid between _snapshot_, _delta_ and _timestamp-based_ storage,  which provides a good trade-off between storage size and query time.  It provides several built-in algorithms to enable efficient iterator-based queries _at_ a certain version, _between_ any two versions, and _for_ versions. These queries support limits and offsets for any triple pattern.    Insertion is done by first inserting a dataset snapshot, which is encoded in [HDT](rdfhdt.org).  After that, deltas can be inserted, which contain additions and deletions based on the last delta or snapshot.    More details on OSTRICH can be found in our [**journal**](https://rdfostrich.github.io/article-jws2018-ostrich/) or [demo](https://rdfostrich.github.io/article-demo/) articles.    ## Building    OSTRICH requires ZLib, Kyoto Cabinet and CMake (compilation only) to be installed.    Compile:  ```bash  $ mkdir build  $ cd build  $ cmake ..  $ make  ```    ## Running    The OSTRICH dataset will always be loaded from the current directory.    ### Tests  ```bash  build/ostrich_test  ```    ### Query  ```bash  build/ostrich-query-version-materialized patch_id s p o  build/ostrich-query-delta-materialized patch_id_start patch_id_end s p o  build/ostrich-query-version patch_id_start s p o  ```    ### Insert  ```bash  build/ostrich-insert [-v] patch_id [+|- file_1.nt [file_2.nt [...]]]*  ```    Input deltas must be sorted in SPO-order.    ### Evaluate  Only load changesets from a path structured as `path_to_patch_directory/patch_id/main.nt.additions.txt` and `path_to_patch_directory/patch_id/main.nt.deletions.txt`.  ```bash  build/ostrich-evaluate path_to_patch_directory patch_id_start patch_id_end  ```  CSV-formatted insert data will be emitted: `version,added,durationms,rate,accsize`.    Load changesets AND query with triple patterns from the given file on separate lines, with the given number of replications.  ```bash  build/ostrich-evaluate path_to_patch_directory patch_id_start patch_id_end patch_to_queries/queries.txt s|p|o nr_replications  ```  CSV-formatted query data will be emitted (time in microseconds) for all versions for the three query types: `patch,offset,limit,count-ms,lookup-mus,results`.    ## Docker    Alternatively, OSTRICH can be built and run using Docker.    ### Build  ```bash  docker build -t ostrich .  ```    Instead of building the container yourself, you can use the pre-built image from [DockerHub](https://hub.docker.com/r/rdfostrich/ostrich/).  ```bash  docker pull rdfostrich/ostrich  ```    ### Test  ```bash  docker run --rm -it --entrypoint /opt/patchstore/build/ostrich_test ostrich  ```    ### Query  ```bash  docker run --rm -it --entrypoint /opt/ostrich/build/ostrich-query-version-materialized ostrich patch_id s p o  docker run --rm -it --entrypoint /opt/ostrich/build/ostrich-delta-version-materialized ostrich patch_id_start patch_id_end s p o  docker run --rm -it --entrypoint /opt/ostrich/build/ostrich-query-version ostrich s p o  ```    ### Insert  ```bash  docker run --rm -it --entrypoint /opt/ostrich/build/ostrich-insert ostrich [-v] patch_id [+|- file_1.nt [file_2.nt [...]]]*  ```    ### Evaluate    Only load changesets from a path structured as `path_to_patch_directory/patch_id/main.nt.additions.txt` and `path_to_patch_directory/patch_id/main.nt.deletions.txt`.  ```bash  docker run --rm -it -v path_to_patch_directory:/var/patches ostrich /var/patches patch_id_start patch_id_end  ```    Load changesets AND query with triple patterns from the given file on separate lines, with the given number of replications.  ```bash  docker run --rm -it -v path_to_patch_directory:/var/patches -v patch_to_queries:/var/queries ostrich /var/patches patch_id_start patch_id_end /var/queries/queries.txt s|p|o nr_replications  ```    Enable debug mode:  ```bash  docker run --rm -it -v path_to_patch_directory:/var/patches -v patch_to_queries:/var/queries -v path_to_crash_dir:/crash --privileged=true ostrich --debug /var/patches patch_id_start patch_id_end /var/queries/queries.txt s|p|o nr_replications  ```    ## Compiler variables  `PATCH_INSERT_BUFFER_SIZE`: The size of the triple parser buffer during patch insertion. (default `100`)    `FLUSH_POSITIONS_COUNT`: The amount of triples after which the patch positions should be flushed to disk, to avoid memory issues. (default `500000`)    `FLUSH_TRIPLES_COUNT`: The amount of triples after which the store should be flushed to disk, to avoid memory issues. (default `500000`)    `KC_MEMORY_MAP_SIZE`: The KC memory map size per tree. (default `1LL << 27` = 128MB)    `KC_PAGE_CACHE_SIZE`: The KC page cache size per tree. (default `1LL << 25` = 32MB)    `MIN_ADDITION_COUNT`: The minimum addition triple count so that it will be stored in the db. Changing this value only has effect during insertion time. Lookups are compatible with any value. (default `200`)    ## Cite    If you are using or extending OSTRICH as part of a scientific publication,  we would appreciate a citation of our [article](https://rdfostrich.github.io/article-jws2018-ostrich/).    ```bibtex  @article{taelman_jws_ostrich_2018,    author = {Taelman, Ruben and Vander Sande, Miel and Van Herwegen, Joachim and Mannens, Erik and Verborgh, Ruben},    title = {Triple Storage for Random-Access Versioned Querying of RDF Archives},    journal = {Journal of Web Semantics},    year = {2018},    month = aug,    url = {https://rdfostrich.github.io/article-jws2018-ostrich/}  }  ```    ## License  This software is written by [Ruben Taelman](http://rubensworks.net/) and colleagues.    This code is copyrighted by [Ghent University – imec](http://idlab.ugent.be/)  and released under the [MIT license](http://opensource.org/licenses/MIT). """
Semantic web;https://github.com/mdesalvo/RDFSharp;"""# RDFSharp [![NuGet Badge](https://buildstats.info/nuget/RDFSharp)](https://www.nuget.org/packages/RDFSharp) [![codecov](https://codecov.io/gh/mdesalvo/RDFSharp/branch/master/graph/badge.svg?token=wtP1B77d3e)](https://codecov.io/gh/mdesalvo/RDFSharp)    RDFSharp has a modular API made up of 4 layers:     <b><a href=""https://github.com/mdesalvo/RDFSharp/releases/download/v2.26.0/RDFSharp.Model-2.26.0.pdf"">RDFSharp.Model</a></b>  <ul>      <li>Create and manage <b>RDF models</b> (resources, literals, triples, graphs, namespaces, ...)</li>      <li>Exchange them using standard <b>RDF formats</b> (N-Triples, TriX, Turtle, RDF/Xml)</li>      <li>Create and validate <b>SHACL shapes</b> (shape graphs, shapes, targets, constraints, reports, ...)</b></li>  </ul>    <b><a href=""https://github.com/mdesalvo/RDFSharp/releases/download/v2.26.0/RDFSharp.Store-2.26.0.pdf"">RDFSharp.Store</a></b>  <ul>      <li>Create and manage <b>RDF stores</b> for context-aware modeling of RDF data (contexts, quadruples, ...)</li>      <li>Exchange them using standard <b>RDF formats</b> (N-Quads, TriX)</li>  </ul>    <b><a href=""https://github.com/mdesalvo/RDFSharp/releases/download/v2.26.0/RDFSharp.Query-2.26.0.pdf"">RDFSharp.Query</a></b>  <ul>      <li>Create and execute <b>SPARQL queries</b> on graphs, stores, federations and <i>SPARQL endpoints</i></li>      <li>Create and execute <b>SPARQL operations</b> on graphs, stores and <i>SPARQL UPDATE endpoints</i></li>  </ul>    <b><a href=""https://github.com/mdesalvo/RDFSharp/releases/download/v2.26.0/RDFSharp.Semantics-2.26.0.pdf"">RDFSharp.Semantics</a></b>  <ul>      <li>Create and manage <b>OWL-DL ontologies</b> (classes, restrictions, properties, facts, assertions, annotations, ...)</li>      <li>Validate them against a wide set of intelligent semantic rules analyzing <b>T-BOX</b> and <b>A-BOX</b></li>      <li>Create and execute <b>SWRL reasoners</b> with forward-chaining materialization of ontology inferences</li>      <li>Create and manage <b>SKOS schemes</b> (concepts, collections, relations, annotations, labels, ...)</li>  </ul> """
Semantic web;https://github.com/Callidon/sparql-engine;"""# sparql-engine  [![build package](https://github.com/Callidon/sparql-engine/actions/workflows/test.yaml/badge.svg?branch=master)](https://github.com/Callidon/sparql-engine/actions/workflows/test.yaml)  [![codecov](https://codecov.io/gh/Callidon/sparql-engine/branch/master/graph/badge.svg)](https://codecov.io/gh/Callidon/sparql-engine) [![npm version](https://badge.fury.io/js/sparql-engine.svg)](https://badge.fury.io/js/sparql-engine) [![JavaScript Style Guide](https://img.shields.io/badge/code_style-standard-brightgreen.svg)](https://standardjs.com)    An open-source framework for building SPARQL query engines in Javascript/Typescript.    [Online documentation](https://callidon.github.io/sparql-engine/)    **Main features**:  * Build a [SPARQL](https://www.w3.org/TR/2013/REC-sparql11-overview-20130321/) query engine on top of any data storage system.  * Supports [the full features of the SPARQL syntax](https://www.w3.org/TR/sparql11-query/) by *implementing a single class!*  * Support for all [SPARQL property Paths](https://www.w3.org/TR/sparql11-query/#propertypaths).  * Implements advanced *SPARQL query rewriting techniques* for transparently optimizing SPARQL query processing.  * Supports [full text search queries](#full-text-search).  * Supports [Custom SPARQL functions](#custom-functions).  * Supports [Semantic Caching](#enable-caching), to speed up query evaluation of reccurent patterns.  * Supports the [SPARQL UPDATE protocol](https://www.w3.org/TR/2013/REC-sparql11-update-20130321/).  * Supports Basic [Federated SPARQL queries](https://www.w3.org/TR/2013/REC-sparql11-federated-query-20130321/) using **SERVICE clauses**.  * Customize every step of SPARQL query processing, thanks to *a modular architecture*.  * Support for [SPARQL Graph Management protocol](https://www.w3.org/TR/2013/REC-sparql11-update-20130321/#graphManagement).    # Table of contents  * [Installation](#installation)  * [Getting started](#getting-started)    * [Examples](#examples)    * [Preliminaries](#preliminaries)    * [RDF Graphs](#rdf-graphs)    * [RDF Datasets](#rdf-datasets)    * [Running a SPARQL query](#running-a-sparql-query)  * [Enable caching](#enable-caching)  * [Full text search](#full-text-search)  * [Federated SPARQL Queries](#federated-sparql-queries)  * [Custom Functions](#custom-functions)  * [Advanced Usage](#advanced-usage)    * [Customize the pipeline implementation](#customize-the-pipeline-implementation)    * [Customize query execution](#customize-query-execution)  * [Documentation](#documentation)  * [Aknowledgments](#aknowledgments)  * [References](#references)    # Installation    ```bash  npm install --save sparql-engine  ```    # Getting started    The `sparql-engine` framework allow you to build a custom SPARQL query engine on top of any data storage system.    In short, to support SPARQL queries on top of your data storage system, you need to:  * [Implements a subclass of `Graph`](#rdf-graphs), which provides access to the data storage system.  * Gather all your Graphs as a `Dataset` (using your own implementation or [the default one](#rdf-datasets)).  * [Instantiate a `PlanBuilder`](#running-a-sparql-query) and use it to execute SPARQL queries.    ## Examples    As a starting point, we provide you with two examples of integration:  * With [N3.js](https://github.com/rdfjs/N3.js), available [here](https://github.com/Callidon/sparql-engine/tree/master/examples/n3.js).  * With [LevelGraph](https://github.com/levelgraph/levelgraph), available [here](https://github.com/Callidon/sparql-engine/tree/master/examples/levelgraph.js).    ## Preliminaries    ### SPARQL.js algebra and TypeScript    The `sparql-engine` framework use the [`SPARQL.js`](https://github.com/RubenVerborgh/SPARQL.js/) library for parsing and manipulating SPARQL queries as JSON objects. For TypeScript compiltation, we use a custom package [`sparqljs-legacy-type`](https://github.com/Callidon/sparqljs-legacy-type) for providing the types information.     Thus, **if you are working with `sparql-engine` in TypeScript**, you will need to install the [`sparqljs-legacy-type`](https://github.com/Callidon/sparqljs-legacy-type) package.    If want to know why we use a custom types package, see [the discussion of this issue](https://github.com/Callidon/sparql-engine/issues/58).    ### RDF triples representation    This framework represents RDF triples using Javascript Object.  You will find below, in Java-like syntax, the ""shape"" of such object.    ```typescript  interface TripleObject {    subject: string; // The Triple's subject    predicate: string; // The Triple's predicate    object: string; // The Triple's object  }  ```    ### PipelineStage    The `sparql-engine` framework uses a pipeline of iterators to execute SPARQL queries. Thus, many methods encountered in this framework needs to return `PipelineStage<T>`, *i.e.*, objects that generates items of type `T` in a pull-based fashion.    An `PipelineStage<T>` can be easily created from one of the following:  * An **array** of elements of type `T`  * A [**Javascript Iterator**](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Iteration_protocols), which yields elements of type `T`.  * An [**EventEmitter**](https://nodejs.org/api/events.html#events_class_eventemitter) which emits elements of type `T` on a `data` event.  * A [**Readable stream**](https://nodejs.org/api/stream.html#stream_readable_streams) which produces elements of type `T`.    To create a new `PipelineStage<T>` from one of these objects, you can use the following code:  ```javascript  const { Pipeline } = require('sparql-engine')    const sourceObject = // the object to convert into a PipelineStage    const stage = Pipeline.getInstance().from(sourceObject)  ```    Fore more information on how to create and manipulate the pipeline, please refers to the documentation of [`Pipeline`](https://callidon.github.io/sparql-engine/classes/pipelinee.html) and [`PipelineEngine`](https://callidon.github.io/sparql-engine/classes/pipelineengine.html).    ## RDF Graphs    The first thing to do is to implement a subclass of the `Graph` abstract class. A `Graph` represents an [RDF Graph](https://www.w3.org/TR/rdf11-concepts/#section-rdf-graph) and is responsible for inserting, deleting and searching for RDF triples in the database.    The main method to implement is `Graph.find(triple)`, which is used by the framework to find RDF triples matching  a [triple pattern](https://www.w3.org/TR/sparql11-query/#basicpatterns) in the RDF Graph.  This method must return an `PipelineStage<TripleObject>`, which will be consumed to find matching RDF triples. You can find an **example** of such implementation in the [N3 example](https://github.com/Callidon/sparql-engine/tree/master/examples/n3.js).    Similarly, to support the [SPARQL UPDATE protocol](https://www.w3.org/TR/2013/REC-sparql11-update-20130321/), you have to provides a graph that implements the `Graph.insert(triple)` and `Graph.delete(triple)` methods, which insert and delete RDF triple from the graph, respectively. These methods must returns [Promises](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise), which are fulfilled when the insertion/deletion operation is completed.    Finally, the `sparql-engine` framework also let your customize how [Basic graph patterns](https://www.w3.org/TR/2013/REC-sparql11-query-20130321/#BasicGraphPatterns) (BGPs) are evaluated against  the RDF graph. The engine provides a **default implementation** based on the `Graph.find` method and the  *Index Nested Loop Join algorithm*. However, if you wish to supply your own implementation for BGP evaluation, you just have to implement a `Graph` with an `evalBGP(triples)` method.  This method must return a `PipelineStage<Bindings>`. You can find an example of such implementation in the [LevelGraph example](https://github.com/Callidon/sparql-engine/tree/master/examples/levelgraph.js).    You will find below, in Java-like syntax, an example subclass of a `Graph`.  ```typescript    const { Graph } = require('sparql-engine')      class CustomGraph extends Graph {      /**       * Returns an iterator that finds RDF triples matching a triple pattern in the graph.       * @param  triple - Triple pattern to find       * @return An PipelineStage which produces RDF triples matching a triple pattern       */      find (triple: TripleObject, options: Object): PipelineStage<TripleObject> { /* ... */ }        /**       * Insert a RDF triple into the RDF Graph       * @param  triple - RDF Triple to insert       * @return A Promise fulfilled when the insertion has been completed       */      insert (triple: TripleObject): Promise { /* ... */ }        /**       * Delete a RDF triple from the RDF Graph       * @param  triple - RDF Triple to delete       * @return A Promise fulfilled when the deletion has been completed       */      delete (triple: : TripleObject): Promise { /* ... */ }    }  ```    ## RDF Datasets    Once you have your subclass of `Graph` ready, you need to build a collection of RDF Graphs, called a [RDF Dataset](https://www.w3.org/TR/rdf11-concepts/#section-dataset). A default implementation, `HashMapDataset`, is made available by the framework, but you can build your own by subclassing [`Dataset`](https://callidon.github.io/sparql-engine/classes/dataset.html).    ```javascript   const { HashMapDataset } = require('sparql-engine')   const CustomGraph = require(/* import your Graph subclass */)     const GRAPH_A_IRI = 'http://example.org#graph-a'   const GRAPH_B_IRI = 'http://example.org#graph-b'   const graph_a = new CustomGraph(/* ... */)   const graph_b = new CustomGraph(/* ... */)     // we set graph_a as the Default RDF dataset   const dataset = new HashMapDataset(GRAPH_A_IRI, graph_a)     // insert graph_b as a Named Graph   dataset.addNamedGraph(GRAPH_B_IRI, graph_b)  ```    ## Running a SPARQL query    Finally, to run a SPARQL query on your RDF dataset, you need to use the `PlanBuilder` class. It is responsible for parsing SPARQL queries and building a pipeline of iterators to evaluate them.    ```javascript    const { PlanBuilder } = require('sparql-engine')      // Get the name of all people in the Default Graph    const query = `      PREFIX foaf: <http://xmlns.com/foaf/0.1/>      SELECT ?name      WHERE {        ?s a foaf:Person .        ?s foaf:name ?name .      }`      // Creates a plan builder for the RDF dataset    const builder = new PlanBuilder(dataset)      // Get an iterator to evaluate the query    const iterator = builder.build(query)      // Read results    iterator.subscribe(      bindings => console.log(bindings),      err => console.error(err),      () => console.log('Query evaluation complete!')    )  ```    # Enable caching    The `sparql-engine` provides support for automatic caching of Basic Graph Pattern evaluation using the [Semantic Cache algorithm](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1161590). Basically, the cache will save the results of BGPs already evaluated and, when the engine wants to evaluates a BGP, it will look for the largest subset of the BGP in the cache. If one is available, it will re-use the cached results to speed up query processing.    By default, semantic caching is disabled. You can turn it on/off using the `PlanBuilder.useCache` and `PlanBuilder.disableCache` methods, respectively. The `useCache` method accepts an optional parameter, so you can provide your own implementation of the semantic cache. By defaults, it uses an in-memory [LRU cache](https://callidon.github.io/sparql-engine/classes/lrubgpcache.html) which stores up to 500MB of items for 20 minutes.    ```javascript  // get an instance of a PlanBuilder  const builder = new PlanBuilder(/* ... */)    // activate the cache  builder.useCache()    // disable the cache  builder.disableCache()  ```    # Full Text Search    The `sparql-engine` provides a non-standard full text search functionnality,  allowing users to execute [approximate string matching](https://en.wikipedia.org/wiki/Approximate_string_matching) on RDF Terms retrieved by SPARQL queries.  To accomplish this integration, it follows an approach similar to [BlazeGraph](https://wiki.blazegraph.com/wiki/index.php/FullTextSearch) and defines several **magic predicates** that are given special meaning, and when encountered in a SPARQL query, they are interpreted as configuration parameters for a full text search query.    The simplest way to integrate a full text search into a SPARQL query is to use the magic predicate `ses:search` inside of a SPARQL join group. In the following query, this predicate is used to search for the keywords `neil` and `gaiman` in the values binded to the `?o` position of the triple pattern.  ```  PREFIX foaf: <http://xmlns.com/foaf/0.1/>  PREFIX ses: <https://callidon.github.io/sparql-engine/search#>  SELECT * WHERE {    ?s foaf:knows ?o .    ?o ses:search “neil gaiman” .  }  ```  In a way, full text search queries allows users to express more complex SPARQL filters that performs approximate string matching over RDF terms.  Each result is annotated with a *relevance score* (how much it matches the keywords, higher is better) and a *rank* (they represent the descending order of relevance scores). These two values are not binded by default into the query results, but you can use magic predicates to get access to them (see below). Note that the meaning of relevance scores is specific to the implementation of the full text search.    The full list of magic predicates that you can use in a full text search query is:  * `ses:search` defines keywords to search as a list of keywords separated by spaces.  * `ses:matchAllTerms` indicates that only values that contain all of the specified search terms should be considered.  * `ses:minRelevance`and `ses:maxRelevance` limits the search to matches with a minimum/maximum  relevance score, respectively. In the default implementation, scores are floating numbers, ranging from 0.0 to 1.0 with a precision of 4 digits.  * `ses:minRank` and `ses:maxRank` limits the search to matches with a minimum/maximum  rank value, respectively. In the default implementation, ranks are positive integers starting at 0.  * `ses:relevance` binds each term's relevance score to a SPARQL variable.  * `ses:rank` binds each term's rank to a SPARQL variable.    Below is a more complete example, that use most of these keywords to customize the full text search.  ```  PREFIX foaf: <http://xmlns.com/foaf/0.1/>  PREFIX ses: <https://callidon.github.io/sparql-engine/search#>  SELECT ?s ?o ?score ?rank WHERE {    ?s foaf:knows ?o .    ?o ses:search “neil gaiman” .    ?o ses:minRelevance “0.25” .    ?o ses:maxRank “1000” .    ?o ses:relevance ?score .    ?o ses:rank ?rank .    ?o ses:matchAllTerms “true” .  }  ```    To provide a custom implementation for the full text search that is more integrated with your backend,  you simply need to override the `fullTextSearch` method of the `Graph` class.  You can find the full signature of this method in the [relevant documentation](https://callidon.github.io/sparql-engine/classes/graph.html#fullTextSearch).    The `sparql-engine` framework provides a default implementation of this method, which computes relevance scores as the average ratio of keywords matched by words in the RDF terms.  Notice that **this default implementation is not suited for production usage**.  It will performs fine for small RDF datasets, but,   when possible, you should always provides a dedicated implementation that leverages your backend.  For example, for SQL databases, you could use [GIN or GIST indexes](https://www.postgresql.org/docs/12/gin-intro.html).    # Federated SPARQL Queries    The `sparql-engine` framework provides automatic support for evaluating [federated SPARQL queries](https://www.w3.org/TR/2013/REC-sparql11-federated-query-20130321/), using the [`SERVICE` keyword](https://www.w3.org/TR/sparql11-query/#basic-federated-query).    To enable them, you need to set **a Graph Factory** for the RDF dataset used to evaluate SPARQL queries.  This Graph factory is used by the dataset to create new RDF Graph on-demand.  To set it, you need to use the [`Dataset.setGraphFactory`](https://callidon.github.io/sparql-engine/classes/dataset.html#setgraphfactory) method, as detailed below.  It takes *a callback* as parameter, which will be invoked to create a new graph from an IRI.  It's your responsibility to define the graph creation logic, depending on your application.    ```typescript  const { HashMapDataset } = require('sparql-engine')  const CustomGraph = require(/* import your Graph subclass */)    const my_graph = new CustomGraph(/* ... */)    const dataset = new HashMapDataset('http://example.org#graph-a', my_graph)    // set the Graph factory of the dataset  dataset.setGraphFactory(iri => {    // return a new graph for the provided iri    return new CustomGraph(/* .. */)  })  ```    Once the Graph factory is set, you have nothing more to do!  Juste execute your federated SPARQL queries as regular queries, like before!    # Custom Functions    SPARQL allows custom functions in expressions so that queries can be used on domain-specific data.  The `sparql-engine` framework provides a supports for declaring such custom functions.    A SPARQL value function is an extension point of the SPARQL query language that allows URI to name a function in the query processor.  It is defined by an `IRI` in a `FILTER`, `BIND` or `HAVING BY` expression.  To register custom functions, you must create a JSON object that maps each function's `IRI` to a Javascript function that takes a variable number of **RDF Terms** arguments and returns one of the following:  * A new RDF Term (an IRI, a Literal or a Blank Node) in RDF.js format.  * An array of RDF Terms.  * An [Iterable](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Iteration_protocols) or a [Generator](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Generator) that yields RDF Terms.  * The `null` value, to indicates that the function's evaluation has failed.    RDF Terms are represented using the [RDF.js data model](http://rdf.js.org/data-model-spec/).  The [`rdf` subpackage](https://callidon.github.io/sparql-engine/modules/rdf.html) exposes a lot  of utilities methods to create and manipulate RDF.js terms in the context of custom SPARQL functions.    The following shows a declaration of some simple custom functions.  ```javascript  // load the utility functions used to manipulate RDF terms  const { rdf } = require('sparql-engine')    // define some custom SPARQL functions  const customFunctions = {    // reverse a RDF literal    'http://example.com#REVERSE': function (rdfTerm) {      const reverseValue = rdfTerm.value.split("""").reverse().join("""")      return rdf.shallowCloneTerm(rdfTerm, reverseValue)    },    // Test if a RDF Literal is a palindrome    'http://example.com#IS_PALINDROME': function (rdfTerm) {      const result = rdfTerm.value.split("""").reverse().join("""") === rdfTerm.value      return rdf.createBoolean(result)    },    // Test if a number is even    'http://example.com#IS_EVEN': function (rdfTerm) {      if (rdf.termIsLiteral(rdfTerm) && rdf.literalIsNumeric(rdfTerm)) {        const jsValue = rdf.asJS(rdfTerm.value, rdfTerm.datatype.value)        const result = jsValue % 2 === 0        return rdf.createBoolean(result)      }      return terms.createFalse()    }  }  ```    Then, this JSON object is passed into the constructor of your PlanBuilder.    ```javascript  const builder = new PlanBuilder(dataset, {}, customFunctions)  ```    Now, you can execute SPARQL queries with your custom functions!  For example, here is a query that uses our newly defined custom SPARQL functions.    ```  PREFIX foaf: <http://xmlns.com/foaf/0.1/>  PREFIX example: <http://example.com#>  SELECT ?length  WHERE {    ?s foaf:name ?name .      # this bind is not critical, but is here for illustrative purposes    BIND(<http://example.com#REVERSE>(?name) as ?reverse)      BIND(STRLEN(?reverse) as ?length)      # only keeps palindromes    FILTER (!example:IS_PALINDROME(?name))  }  GROUP BY ?length  HAVING (example:IS_EVEN(?length))  ```    # Advanced usage    ## Customize the pipeline implementation    The class `PipelineEngine` (and its subclasses) is the main component used by `sparql-engine` to evaluate all SPARQL operations. It defines basic operations (`map`, `filter`, etc) that can be used  to manipulate intermediate results and evaluate SPARQL queries.    By default, the framework uses an implementation of `PipelineEngine` based on [`rxjs`](https://rxjs-dev.firebaseapp.com/), to implements a SPARQL query execution plan as a pipeline of iterators.  However, **you are able to switch to others implementations** of `PipelineEngine`, using `Pipeline.setInstance`.    ```javascript  const { Pipeline, PipelineEngine } = require('sparql-engine')    class CustomEngine extends PipelineEngine {    // ...  }    // add this before creating a new plan builder  Pipeline.setInstance(new CustomEngine())  // ...  ```    Two implementations of `PipelineEngine` are provided by default.  * `RxjsPipeline`, based on [`rxjs`](https://rxjs-dev.firebaseapp.com/), which provides a pure pipeline approach. This approach is **selected by default** when loading the framework.  * `VectorPipeline`, which materializes all intermediate results at each pipeline computation step. This approach is more efficient CPU-wise, but also consumes a lot more memory.    These implementations can be imported as follows:  ```javascript  const { RxjsPipeline, VectorPipeline } = require('sparql-engine')  ```    ## Customize query execution    A `PlanBuilder` implements a [Builder pattern](https://en.wikipedia.org/wiki/Builder_pattern) in order to create a physical query execution plan for a given SPARQL query.  Internally, it defines [*stages builders*](https://callidon.github.io/sparql-engine/classes/stagebuilder) to generates operators for executing all types of SPARQL operations.  For example, the [`OrderByStageBuilder`](https://callidon.github.io/sparql-engine/classes/orderbystagebuilder.html) is invoked when the `PlanBuilder` needs to evaluate an `ORDER BY` modifier.    If you want to customize how query execution plans are built, you have to implement your own stage builders, by extending existing ones.  Then, you need to configure your plan builder to use them, with the [`use` function](https://callidon.github.io/sparql-engine/classes/planbuilder.html#use).    ```javascript    const { PlanBuilder, stages } = require('sparql-engine')      class MyOrderByStageBuilder extends stages.OrderByStageBuilder {      /* Define your custom execution logic for ORDER BY */    }      const dataset = /* a RDF dataset */      // Creates a plan builder for the RDF dataset    const builder = new PlanBuilder(dataset)      // Plug-in your custom stage builder    builder.use(stages.SPARQL_OPERATION.ORDER_BY, MyOrderByStageBuilder(dataset))      // Now, execute SPARQL queries as before with your PlanBuilder  ```    You will find below a reference table of all stage builders used by `sparql-engine` to evaluate SPARQL queries. Please see [the API documentation](https://callidon.github.io/sparql-engine/classes/stagebuilder) for more details.    **Executors**    | SPARQL Operation | Default Stage Builder | Symbol |  |------------------|-----------------------|--------|  | [Aggregates](https://www.w3.org/TR/sparql11-query/#aggregates) | [AggregateStageBuilder](https://callidon.github.io/sparql-engine/classes/aggregatestagebuilder.html) | `SPARQL_OPERATION.AGGREGATE` |  | [Basic Graph Patterns](https://www.w3.org/TR/sparql11-query/#BasicGraphPatterns) | [BGPStageBuilder](https://callidon.github.io/sparql-engine/classes/bgpstagebuilder.html) | `SPARQL_OPERATION.BGP` |  | [BIND](https://www.w3.org/TR/sparql11-query/#bind) | [BindStageBuilder](https://callidon.github.io/sparql-engine/classes/bindstagebuilder.html) | `SPARQL_OPERATION.BIND` |  | [DISTINCT](https://www.w3.org/TR/sparql11-query/#neg-minus) | [DistinctStageBuilder](https://callidon.github.io/sparql-engine/classes/distinctstagebuilder.html) | `SPARQL_OPERATION.DISTINCT` |  | [FILTER](https://www.w3.org/TR/sparql11-query/#expressions) | [FilterStageBuilder](https://callidon.github.io/sparql-engine/classes/filterstagebuilder.html) | `SPARQL_OPERATION.FILTER` |  | [Property Paths](https://www.w3.org/TR/sparql11-query/#propertypaths) | [PathStageBuilder](https://callidon.github.io/sparql-engine/classes/pathstagebuilder.html) | `SPARQL_OPERATION.PROPERTY_PATH` |  | [GRAPH](https://www.w3.org/TR/sparql11-query/#rdfDataset) | [GraphStageBuilder](https://callidon.github.io/sparql-engine/classes/graphstagebuilder.html) | `SPARQL_OPERATION.GRAPH` |  | [MINUS](https://www.w3.org/TR/sparql11-query/#neg-minus) | [MinusStageBuilder](https://callidon.github.io/sparql-engine/classes/minusstagebuilder.html) | `SPARQL_OPERATION.MINUS` |  | [OPTIONAL](https://www.w3.org/TR/sparql11-query/#optionals) | [OptionalStageBuilder](https://callidon.github.io/sparql-engine/classes/optionalstagebuilder.html) | `SPARQL_OPERATION.OPTIONAL` |  | [ORDER_BY](https://www.w3.org/TR/sparql11-query/#modOrderBy) | [OrderByStageBuilder](https://callidon.github.io/sparql-engine/classes/orderbystagebuilder.html) | `SPARQL_OPERATION.ORDER_BY` |  | [SERVICE](https://www.w3.org/TR/sparql11-query/#basic-federated-query) | [ServiceStageBuilder](https://callidon.github.io/sparql-engine/classes/servicestagebuilder.html) | `SPARQL_OPERATION.SERVICE` |  | [UNION](https://www.w3.org/TR/sparql11-query/#alternatives) | [UnionStageBuilder](https://callidon.github.io/sparql-engine/classes/unionstagebuilder.html) | `SPARQL_OPERATION.UNION` |  | [UPDATE](https://www.w3.org/TR/2013/REC-sparql11-update-20130321/) | [UpdateStageBuilder](https://callidon.github.io/sparql-engine/classes/updatestagebuilder.html) | `SPARQL_OPERATION.UPDATE` |      # Documentation    To generate the documentation in the `docs` director:  ```bash  git clone https://github.com/Callidon/sparql-engine.git  cd sparql-engine  yarn install  npm run doc  ```    # Acknowledgments    This framework is developed since 2018 by many contributors, and we thanks them very much for their contributions to this project! Here is the full list of our amazing contributors.    * [Corentin Marionneau](https://github.com/Slaanaroth) (@Slaanaroth)    * Corentin created the first version of `sparql-engine` during its research internship at the [Laboratoire des Sciences du Numérique de Nantes](https://www.ls2n.fr/) (LS2N). He is now a Web developer at SII Atlantique.  * [Merlin Barzilai](https://github.com/Rintarou) (@Rintarou)    * Merlin designed the first SPARQL compliance tests for the framework during its research internship at the [LS2N](https://www.ls2n.fr/).  * [Dustin Whitney](https://github.com/dwhitney) (@dwhitney)    * Dustin implemented the support for custom SPARQL functions and provided a lot of feedback during the early stages of development.  * [Julien Aimonier-Davat](https://github.com/Lastshot97) (@Lastshot97)    * Julien implemented the support for SPARQL Property Paths evaluation during its research internship at the [LS2N](https://www.ls2n.fr/). He is now a Ph.D. Student at the University of Nantes.  * [Arnaud Grall](https://github.com/folkvir) (@folkvir)    * Arnaud contributed to many bugfixes and provided a lot of feedback throughout the development of the framework. He is now a Software Engineer at SII Atlantique.  * [Thomas Minier](https://github.com/Callidon) (@Callidon)    * Thomas developed the framework during his PhD thesis in the [Team ""Gestion des Données Distribuées""](https://sites.google.com/site/gddlina/) (GDD) and supervise its evolution ever since. He is now a Software Engineer at SII Atlantique.    # References    * [Official W3C RDF specification](https://www.w3.org/TR/rdf11-concepts)  * [Official W3C SPARQL specification](https://www.w3.org/TR/2013/REC-sparql11-query-20130321/) """
Semantic web;https://github.com/wikier/djubby;"""This project is currently **not maintained**, so please use it under your own risk.    # Djubby, a Linked Data frontend for SPARQL endpoints    Djubby is a Linked Data frontend for SPARQL endpoints for the Django Web framework.  It's quite inspired by Richard Cyganiak's [Pubby](http://wifo5-03.informatik.uni-mannheim.de/pubby/),   and with the exception of the HTML style, all the code has beed written from scratch   due the many differences between languages (Java vs. Python) and the frameworks (JavaEE vs. Django).    ![djubby](https://raw.githubusercontent.com/wikier/djubby/master/doc/images/djubby.png)    More information at: https://github.com/wikier/djubby    ## Features    * Provides a Linked Data interface to local or remote SPARQL protocol servers.  * Provides dereferenceable URIs by rewriting URIs found in the SPARQL-exposed dataset into the djubby server's namespace.  * Provides a simple HTML interface showing the data available about each resource.  * Takes care of handling 303 redirects and content negotiation.  * Compatible with the Django Web framework.    ### Planned     * Include a metadata extension to add metadata to the provided data.    ### Limitations    * Only works for SPARQL endpoint that can answer ASK and DESCRIBE queries.  * Multiple dataset support may not work as expected, so it is recommended to simply set up a separate djubby instance for each dataset.    ## Dependencies    * RDFLib >= 2.4.0  * SPARQLWrapper >= 1.3.2  * Django >= 1.1.0  * mimeparse >= 0.1.2    ## Usage    ### Installation        cd lib/      sudo python setup.py install    ## Authors    * [Sergio Fernández](http://www.wikier.org)    ## License    GNU Library or Lesser General Public License (LGPL) v3, http://www.gnu.org/licenses/lgpl.html """
Semantic web;https://github.com/hsolbrig/PyShEx;"""# Python implementation of ShEx 2.0  [![Pyversions](https://img.shields.io/pypi/pyversions/PyShEx.svg)](https://pypi.python.org/pypi/PyShEx)    [![PyPi](https://img.shields.io/pypi/v/PyShEx.svg)](https://pypi.python.org/pypi/PyShEx)      [![DOI](https://zenodo.org/badge/116042298.svg)](https://zenodo.org/badge/latestdoi/116042298)    https://mybinder.org/v2/gh/hsolbrig/pyshex/master      This package is a reasonably literal implementation of the [Shape Expressions Language 2.0](http://shex.io/shex-semantics/).  It can parse and ""execute"" ShExC and ShExJ source.    ## Revisions  * 0.2.dev3 -- added SchemaEvaluator and other tweaks.  There are still some unit tests that fail -- beware  * 0.3.0 -- Fix several issues.  Still does not pass all unit tests -- see `test_manifest.py` for details  * 0.4.0 -- Added sparql_slurper capabilities.   * 0.4.1 -- Resolves several issues with reactome and disease test cases  * 0.4.2 -- Fix issues #13 (missing start) and #14 (Inconsistent shape causes loop)  * 0.4.3 -- Fix issues #16 and #15 and some refactoring  * 0.5.0 -- First cut at returning fail reasons... some work still needed  * 0.5.1 -- Update shexc parser to include multi-line comments and bug fixes  * 0.5.2 -- Issue with installer - missed the parse_tree package  * 0.5.3 -- make sparql_slurper a dependency  * 0.5.4 -- Fixed long recursion issue with blood pressure example  * 0.5.5 -- Fixed zero cardinality issue (#20)  * 0.5.6 -- Added CLI entry point and cleaned up error reporting  * 0.5.7 -- Throw an error on an invalid focus node (#23)  * 0.5.9 -- Candidate for ShEx 2.1  * 0.5.10 -- Fixed evaluator to load files, strings, etc. as ShEx  * 0.5.11 -- Added Collections Flattening graph option to evaluator.  * 0.5.12 -- Added -A option, catch missing start node early  * 0.6.0 -- Added the -ut and -sp options to allow start nodes to be specified by rdf:type or an arbitrary predicate  * 0.6.1 -- Added the ability to supply a SPARQL Query (-sq option)   * 0.7.0 -- Fixes for issues 28, 29 and 30   * 0.7.1 -- Fix issue 26  * 0.7.2 -- Upgrade error reporting  * 0.7.3 -- Report using namespaces, enhance PrefixLib to inject into a module  * 0.7.4 -- Added '-ps', '-pr', '-gn', '-pb' options to CLI  * 0.7.5 -- Fix CLOSED issue in evaluate call (issue 41)  * 0.7.6 -- bump version due to build error    ## Installation  ```bash  pip install PyShEx  ```  Note: If you need to escape single quotes in RDF literals, you will need to install the bleeding edge  of rdflib:  ```bash  pip uninstall rdflib  pip install git+https://github.com/rdflib/rdflib  ```  Unfortunately, however, `rdflib-jsonld` is NOT compatible with the bleeding edge rdflib, so you can't use a json-ld parser in this situation.    ## shexeval CLI  ```bash  > shexeval -h  usage: shexeval [-h] [-f FORMAT] [-s START] [-ut] [-sp STARTPREDICATE]                  [-fn FOCUS] [-A] [-d] [-ss] [-cf] [-sq SPARQL] [-se]                  [--stopafter STOPAFTER] [-ps] [-pr] [-gn GRAPHNAME] [-pb]                  rdf shex    positional arguments:    rdf                   Input RDF file or SPARQL endpoint if slurper or sparql                          options    shex                  ShEx specification    optional arguments:    -h, --help            show this help message and exit    -f FORMAT, --format FORMAT                          Input RDF Format    -s START, --start START                          Start shape. If absent use ShEx start node.    -ut, --usetype        Start shape is rdf:type of focus    -sp STARTPREDICATE, --startpredicate STARTPREDICATE                          Start shape is object of this predicate    -fn FOCUS, --focus FOCUS                          RDF focus node    -A, --allsubjects     Evaluate all non-bnode subjects in the graph    -d, --debug           Add debug output    -ss, --slurper        Use SPARQL slurper graph    -cf, --flattener      Use RDF Collections flattener graph    -sq SPARQL, --sparql SPARQL                          SPARQL query to generate focus nodes    -se, --stoponerror    Stop on an error    --stopafter STOPAFTER                          Stop after N nodes    -ps, --printsparql    Print SPARQL queries as they are executed    -pr, --printsparqlresults                          Print SPARQL query and results    -gn GRAPHNAME, --graphname GRAPHNAME                          Specific SPARQL graph to query - use '' for any named                          graph    -pb, --persistbnodes  Treat BNodes as persistent in SPARQL endpoint  ```    ## Documentation  See: [examples](notebooks) Jupyter notebooks for sample uses      ## General Layout  The root `pyshex` package is subdivided into:    * [shape_expressions_language](pyshex/shape_expressions_language) - implementation of the various sections in  [Shape Expressions Language 2.0](http://shex.io/shex-semantics/).  As an example, [3. Terminology](http://shex.io/shex-semantics/#terminology) is implemented in [p3_terminology.py](pyshex/shape_expressions_language/p3_terminology.py), [5.2 Validation Definition](http://shex.io/shex-semantics/#validation) in [p5_2_validation_definition.py](pyshex/shape_expressions_language/p5_2_validation_definition.py), etc.  * [shapemap_structure_and_language](pyshex/shapemap_structure_and_language) - implementation of [ShapeMap Structure and Language](http://shex.io/shape-map/) (as well as we can understand it)  * [sparql11_query](pyshex/sparql11_query) - required sections from [SPARQL 1.1 Query Language section 17.2](https://www.w3.org/TR/sparql11-query/#operandDataTypes)  * [utils](pyshex/utils) - supporting utilities    The ShEx schema definitions for this package come from [ShExJSG](https://github.com/hsolbrig/ShExJSG)    We are trying to keep the python as close as possible to the (semi-)formal specification.  As an example, the statement:  ```text  Se is a ShapeAnd and for every shape expression se2 in shapeExprs, satisfies(n, se2, G, m)  ```  is implemented in Python as:  ```python          ...  if isinstance(se, ShExJ.ShapeAnd):      return satisfiesShapeAnd(cntxt, n, se)          ...  def satisfiesShapeAnd(cntxt: Context, n: nodeSelector, se: ShExJ.ShapeAnd) -> bool:      return all(satisfies(cntxt, n, se2) for se2 in se.shapeExprs)  ```    ## Dependencies  This package is built using:  * [ShExJSG](https://github.com/hsolbrig/ShExJSG) -- an object representation of the ShEx AST as defined by [ShEx.jsg](https://github.com/shexSpec/shexTest/blob/master/doc/ShExJ.jsg) and compiled through the [PyJSG](https://github.com/hsolbrig/pyjsg) compiler.  * The python [ShExC](https://github.com/shexSpec/grammar/tree/master/parsers/python) compiler -- which transforms the [Shape Expressions Language](http://shex.io/shex-semantics/index.html) into ShExJSG images.  * [rdflib](https://rdflib.readthedocs.io/en/stable/)       ## Conformance    This implementation passes all of the tests in the master branch of [validation/manifest.ttl](https://raw.githubusercontent.com/shexSpec/shexTest/master/validation/manifest.ttl) with the following exceptions:    At the moment, there are 1088 tests, of which:    * 1007 pass  * 81 are skipped - reasons:  1) (52) sht:LexicalBNode, sht:ToldBNode and sht:BNodeShapeLabel test non-blank blank nodes (`rdflib` does not preserve bnode ""identity"")  2) (18) sht:Import Uses ShEx 2.1 IMPORT feature -- not yet implemented (three aren't tagged)  3) (3) Uses manifest shapemap feature -- not yet implemented  4) (2) sht:relativeIRI -- this isn't a real problem, but we havent taken time to deal with this in the test harness  5) (6) `rdflib` has a parsing error when escaping single quotes. (Issue submitted, awaiting release)    As mentioned above, at the moment this is as literal an implementation of the specification as was sensible.  This means, in particular, that we are less than clever when it comes to partition management.    ## Docker    ### Build    ```shell  docker build -t pyshex docker  ```    ### Run    ```shell  docker run --rm -it pyshex -gn '' -ss -ut -pr -sq 'select distinct ?item where{?item a <http://w3id.org/biolink/vocab/Gene>} LIMIT 1' http://graphdb.dumontierlab.com/repositories/ncats-red-kg https://github.com/biolink/biolink-model/raw/master/shex/biolink-modelnc.shex  ```   """
Semantic web;https://github.com/zazuko/rdf-vocabularies;"""# @zazuko/rdf-vocabularies -- Zazuko's Default Ontologies & Prefixes  [![Build Status](https://travis-ci.org/zazuko/rdf-vocabularies.svg?branch=master)](https://travis-ci.org/zazuko/rdf-vocabularies)  [![Coverage Status](https://coveralls.io/repos/github/zazuko/rdf-vocabularies/badge.svg?branch=master)](https://coveralls.io/github/zazuko/rdf-vocabularies?branch=master)  [![npm version](https://badge.fury.io/js/%40zazuko%2Frdf-vocabularies.svg)](https://www.npmjs.com/package/@zazuko/rdf-vocabularies)    This package contains a distribution of the most commonly used RDF ontologies (schema/vocab, whatever you call it)  including their default prefixes, together with a set of utility functions to work with prefixes.    It is extending [RDFa Core Initial Context](http://www.w3.org/2011/rdfa-context/rdfa-1.1) and contains what we consider  commonly used prefixes. Some popular prefixes do not resolve to dereferencable RDF and are thus skipped.    The package is built for use in Node.js projects. We ship N-Quads files of the vocabularies so it could be useful for  other programming languages as well as you do not have to take care of downloading the ontologies yourself.    ## Installation    ```bash  $ npm install @zazuko/rdf-vocabularies  ```    ## Usage    (Read below and take a look at some [examples](./examples.js).)    ### Dataset-as-code modules    All vocabularies published by this package are also exported as JS modules so that then can be imported synchronously (no parsing required) and without additional dependencies when in web app setting (see the `raw-loader` instructions below).    Modules `@rdf-vocabularies/datasets` exports factories which returns an array of quads `Quad` and take RDF/JS `DataFactory` as parameter.    ```javascript  const $rdf = require('rdf-ext')  const { schema } = require('@zazuko/rdf-vocabularies/datasets')    const dataset = $rdf.dataset(schema($rdf))  ```    In a bundled web project it is also possible to directly import a single dataset like `import schema from '@zazuko/rdf-vocabularies/datasets/schema'`. At the time of writing this is not supported by newer versions of node (12-14) but has already been fixed and scheduled for release.    ### Vocabularies Metadata    See [`_index.nq`](./ontologies/_index.nq).    ### `vocabularies()`    The function (`require('@zazuko/rdf-vocabularies').vocabularies(options)`) accepts an optional `options` object:    * `options.only: Array?`, default: `undefined`, a subset of all available prefixes, will only load these.  * `options.factory: RDF/JS DatasetFactory`, default: [`rdf-ext`](https://github.com/rdf-ext/rdf-ext), a dataset  factory abiding by the [RDF/JS Dataset Specification](https://rdf.js.org/dataset-spec/), used to create the  returned datasets.  * `options.stream: Boolean`, default: `false`, whether to return a RDF/JS quad stream instead of regular objects/datasets.    #### Loading all Vocabularies as Datasets    In browser environment this will cause a request for each individual dataset.  It is thus recommended to always only [load the needed ontologies](#loading-only-some-ontologies-as-datasets)  to reduce the unnecessary traffic and save bandwidth.    ```js  const { vocabularies } = require('@zazuko/rdf-vocabularies')    vocabularies()    .then((datasets) => {      /* `datasets` is:      {        ""csvw"": Dataset,        ""sd"": Dataset,        ""ldp"": Dataset,        ""schema"": Dataset,        ""owl"": Dataset,        ""void"": Dataset,        ""sioc"": Dataset,        ""foaf"": Dataset,        ""time"": Dataset,        ""dcat"": Dataset,        ""oa"": Dataset,        ""gr"": Dataset,        ""rdf"": Dataset,        ""cc"": Dataset,        ""ssn"": Dataset,        ""rr"": Dataset,        ""rdfa"": Dataset,        ""org"": Dataset,        ""sosa"": Dataset,        ""dc11"": Dataset,        ""skos"": Dataset,        ""dqv"": Dataset,        ""prov"": Dataset,        ""og"": Dataset,        ""qb"": Dataset,        ""rdfs"": Dataset,        ""dc"": Dataset,        ""ma"": Dataset,        ""vcard"": Dataset,        ""grddl"": Dataset,        ""dcterms"": Dataset,        ""skosxl"": Dataset,        ""wgs"": Dataset,        ""dbo"": Dataset,        ""dbpedia"": Dataset,        ""dbpprop"": Dataset,        ""rss"": Dataset,        ""cnt"": Dataset,        ""vs"": Dataset,        ""hydra"": Dataset,        ""gn"": Dataset,        ""gtfs"": Dataset,        ""geo"": Dataset,        ""geof"": Dataset,        ""geor"": Dataset      }      */    })  ```    #### Loading only some Vocabularies as Datasets    ```js  const { vocabularies } = require('@zazuko/rdf-vocabularies')    vocabularies({ only: ['rdfs', 'owl', 'skos'] })    .then((datasets) => {      /* `datasets` is:      {        ""owl"": Dataset,        ""skos"": Dataset,        ""rdfs"": Dataset      }      */    })  ```    #### Getting a Readable Stream (Quad Stream)    ```js  const { vocabularies } = require('@zazuko/rdf-vocabularies')  const stream = await vocabularies({ stream: true, only: ['rdfs', 'owl', 'skos'] })  ```    ### Using `vocabularies` function in browser    The preferred usage in browser projects is to avoid importing from `@zazuko/rdf-vocabularies` because that will require additional bundling of dynamic n-quads modules.    Instead, import from the partial modules:    * `import { expand } from '@zazuko/rdf-vocabularies/expand'`  * `import { prefixes } from '@zazuko/rdf-vocabularies/prefixes'`  * `import { shrink } from '@zazuko/rdf-vocabularies/shrink'`    The module `@zazuko/rdf-vocabularies/expandWithCheck` requires `rdf-ext` and parses datasets. See the instructions below for examples how to configure the application.    The package's main module can also be used in browser albeit it needs a bundler such as webpack and additional steps to configure it:    The package can be used in browser albeit it needs a bundler such as webpack and additional steps to configure it:    * Enable [dynamic imports](https://medium.com/front-end-weekly/webpack-and-dynamic-imports-doing-it-right-72549ff49234).    In webpack it is done with [@babel/plugin-syntax-dynamic-import](https://babeljs.io/docs/en/babel-plugin-syntax-dynamic-import)  * Extend the bundler setup to have it load the contents of vocabulary files (all n-triples). In    In webpack it can be done with [`raw-loader`](https://github.com/webpack-contrib/raw-loader):          module: {          rules: [            {              test: /\.nq$/,              use: ['raw-loader']            }          ]        }  * Be careful with prefetching chunks. Some applications may generate prefetch links for dynamically loaded chunks.  Some of the ontology files are quite large and their number will grow over time. Hence, it may be desired to exclude  certain chunks from the being eagerly loaded. Check the [wiki](https://github.com/zazuko/rdf-vocabularies/wiki/Example-web-app-config) for examples.    ### Expanding a Prefix    `expand`ing means: `'xsd:dateTime' → 'http://www.w3.org/2001/XMLSchema#dateTime'`.  It is the opposite of [`shrink`](#shrinking-an-iri)ing:    `expand(shrink('http://www.w3.org/2001/XMLSchema#dateTime')) === 'http://www.w3.org/2001/XMLSchema#dateTime'`    There are two ways of expanding a prefix:    * `vocabularies.expand(prefixedTerm: String): String` synchronous        Expand without checks. It is similar to prefix.cc in the sense that prefix.cc would expand      `schema:ImNotInSchemaDotOrg` to `http://schema.org/ImNotInSchemaDotOrg`.    * `vocabularies.expand(prefixedTerm: String, types: Array<String|NamedNode>): Promise<String>` **asynchronous**        Expand with type checks. `types` is an array of strings or NamedNodes. See this example:        ```js      const { expand } = require('@zazuko/rdf-vocabularies')      const Class = expand('rdfs:Class')      const Property = expand('rdf:Property')        // Will return <schema:person> expanded to `http://schema.org/Person`      // iff the dataset contains either:      //   <schema:Person> <rdf:type> <rdfs:Class>      // or      //   <schema:Person> <rdf:type> <rdf:Property>      await expand('schema:Person', [Class, Property])      ```    ### Shrinking an IRI    `shrink`ing means: `'http://www.w3.org/2001/XMLSchema#dateTime' → 'xsd:dateTime'`.  It is the opposite of [`expand`](#expanding-a-prefix)ing:    `shrink(expand('xsd:dateTime')) === 'xsd:dateTime'`    * `vocabularies.shrink(iri: String): String`        **Note**: returns empty string when there is no corresponding prefix. Always check the output      when using `shrink` with user-provided strings.        ```js      const assert = require('assert')      const { shrink } = require('@zazuko/rdf-vocabularies')        assert(shrink('http://www.w3.org/2001/XMLSchema#dateTime') === 'xsd:dateTime')      assert(shrink('http://example.com#nothing') === '')        const iri = 'http://example.com#nothing'      const stringToDisplay = shrink(iri) || iri      console.log(stringToDisplay) // 'http://example.com#nothing'      ```    ### Accessing Prefixes: `vocabularies.prefixes`    Getting an object with prefixes and their base URI:    (Returns [this object](./src/prefixes.ts).)    ```js  const { prefixes } = require('@zazuko/rdf-vocabularies')    console.log(prefixes)  /*   {    v: 'http://rdf.data-vocabulary.org/#',    csvw: 'http://www.w3.org/ns/csvw#',    sd: 'http://www.w3.org/ns/sparql-service-description#',    …  }  */  ```    ### Accessing Data Files from the Package    Accessing the N-Quads files:    ```js  const path = require('path')  console.log(path.resolve(require.resolve('@zazuko/rdf-vocabularies'), '..', 'ontologies', 'skos.nq'))  ```    ### Command line    The package also includes a simple command line interface which forwards the vocabulary datasets to standard output. It can be used in two ways.    By prefix:    ```  rdf-vocab prefix foaf  ```    By namespace URI:    ```  rdf-vocab prefix http://schema.org/  ```    ## Versioning Scheme    This package is [vendoring ontologies](./ontologies/). These will be updated periodically.    This package is versioned using the date at which the data was pulled, e.g. `@zazuko/rdf-vocabularies@2019.04.30`.    Updating the vendored ontologies is achieved using `npm run fetch` in this package.    ## Adding new prefixes    New prefixes can be added by opening a pull request on Github. For new requests, first check if the creator/owner  of the namespace defined a prefix. If not check [prefix.cc](http://prefix.cc/). In case prefix.cc is ambiguous a  discussion should be raised before the pull-requests gets integrated. Last thing to check are the predefined namespaces  in the [DBpedia SPARQL endpoint](http://dbpedia.org/sparql?nsdecl) or other popular RDF resources like  [LOV](https://lov.linkeddata.es/dataset/lov/vocabs). If you find one please refer to it in the pull request.    ### Steps to add a prefix    1. Add an entry in [`src/prefixes.ts`](src/prefixes.ts)  1. If necessary, add an entry to [`overrides.ts`](overrides.ts), similar to the others     * for the `file` option, a `file:` scheme IRI can be used, with path relative to the repository root  1. Run `npm run fetch -- <prefix>` with the prefix passed as parameter.     * multiple prefixes can also be to fetch multiple ontologies  1. Commit changes and submit a PR    ### Project-specific prefixes    It is also possible to add prefix within a project so that it can be used with the functions [`expand`](#expanding-a-prefix) and [`shrink`](#shrinking-an-iri).    ```js  import { prefixes, expand, shrink } from '@zazuko/rdf-vocabularies'    prefixes['foo'] = 'http://example.com/'    // 'http://example.com/bar'  const foobar = expand('foo:bar')    // 'foo:bar'  const prefixed = shrink(foobar)  ``` """
Semantic web;https://github.com/epimorphics/IntervalServer;"""# IntervalServer    IntervalServer repo contains the source code for the Servlet that provides the Interval service at reference.data.gov.uk. e.g.       - `http://reference.data.gov.uk/id/quarter/2016-Q1` [.rdf](http://reference.data.gov.uk/doc/quarter/2016-Q1) [.ttl](http://reference.data.gov.uk/doc/quarter/2016-Q1.ttl) [.json](http://reference.data.gov.uk/doc/quarter/2016-Q1.json)     - `http://reference.data.gov.uk/id/gregorian-interval/2016-06-30T00:00:00/P3M` [.rdf](http://reference.data.gov.uk/id/gregorian-interval/2016-06-30T00:00:00/P3M) [.ttl](http://reference.data.gov.uk/id/gregorian-interval/2016-06-30T00:00:00/P3M.ttl) [.json](http://reference.data.gov.uk/id/gregorian-interval/2016-06-30T00:00:00/P3M.json)    This source code is made available under an [Apache 2](http://www.apache.org/licenses/LICENSE-2.0) open source license.    [Interval URIs](interval-uris.md) provides more information about the interval service and the use of the interval URIs that it supports in statistical data publications.   """
Semantic web;https://github.com/stardog-union/stardog.js;"""# Stardog.js    Universal Javascript fetch wrapper for communicating with the Stardog HTTP server.    [![npm](https://img.shields.io/npm/v/stardog.svg?style=flat-square)](https://www.npmjs.com/package/stardog)    <a href=""http://stardog.com""><img src=""https://d33wubrfki0l68.cloudfront.net/66e9dcff51317cfc11b9f3d4ce2917a11ba81681/543c1/img/stardog-logo-optimized.svg"" style=""margin: 0 auto; display: block; width: 250px""/></a>    ## What is it?    This framework wraps all the functionality of a client for the Stardog DBMS, and provides access to a full set of functions such as executing SPARQL queries, administrative tasks on Stardog, and the use of the Reasoning API.    All the implementation uses the HTTP protocol, since most of Stardog functionality is available using this protocol. For more information, go to the Stardog's [HTTP Programming](http://www.stardog.com/docs/#_network_programming) documentation.    This is a universal library and as such can be used in both the browser and Node.js.    ## Installation    To install stardog.js run:    ```bash  npm install stardog  ```    ## Usage    Stardog.js conforms to the [Universal Module Definition](https://github.com/umdjs/umd) API. To use it in Node.js, simply `require` or `import` it as you would any other Node module. To use it in the browser, you can either:    1. Do the same as you would with Node.js, in which case you'll have to use [webpack](https://webpack.js.org/), [parcel](https://parceljs.org/), [browserify](http://browserify.org/), or some other module bundler,  2. Use [require.js](https://requirejs.org/) or some other module loader, or  3. Directly import the built stardog.js file in your HTML (e.g., `<script src=""./node_modules/stardog/dist/stardog.js""></script>`) and then reference the global `stardogjs` object (e.g., `stardogjs.query.execute(/* . . . */)`).    ## Development    To get started, just clone the project. You'll need a local copy of Stardog to be able to run the tests. For more information on starting the Stardog DB service and how it works, go to [Stardog's documentation](http://docs.stardog.com), where you'll find everything you need to get up and running with Stardog.    Go to [http://stardog.com](http://stardog.com), download and install the database and load the data provided in `data/` using the script in the repository.    1. Start the Stardog server  ```bash  stardog-admin server start  ```  2. Install `stardog.js` dependencies:  ```bash  npm install  ```    ### Running Tests    In order to contribute changes, all test cases must pass. With the Stardog server running, execute the following command to run all test cases in `test/spec`:    ```bash  npm test  ```    To test the cluster commands you will need to first start a Stardog cluster then run the cluster suite. The easiest way to do this is to run docker-compose to start a cluster:    ```bash  docker-compose -f .circleci/docker-compose.yml up  ```    Then run the cluster test suite in `test/cluster`:    ```bash  npm run test:cluster  ```    ### Contributing    Fork, clone and develop, write or amend tests, and then open a PR. All PRs go against ""master"". This project uses [prettier](https://github.com/prettier/prettier) on file commit, so don't worry about style as it'll just get rewritten when you commit your changes.    ### Releasing    If you have publishing rights, BE SURE TO RUN `npm version (major|minor|patch)` IMMEDIATELY BEFORE PUBLISHING. This will ensure that the build is up-to-date and will also (1) bump the version number in package.json accordingly, (2) create a git tag matching the version number, and (3) automatically update the README and the CHANGELOG using our type declarations and data from the stardog.js GitHub repo. For this process to work correctly, you will need to have generated a GitHub OAuth token and assigned it to the `MDCHANGELOG_TOKEN` environment variable (the name of the token is a relic of the fact that this repo once used [mdchangelog](https://www.npmjs.com/package/mdchangelog) to generate changelogs; it now uses a custom script). In order to ensure that this process is followed, there will be a very annoying alert triggered whenever you publish; if you're all set, just ignore the alert.    After releasing, be sure to push to master, including the tags (so that the release is reflected on GitHub).    ## Version/Support Details    Each release of stardog.js is tested against the most recent version of Stardog available at the time of the release. The relationship between versions of stardog.js and versions of Stardog is detailed in the following table:    | stardog.js Version  | Supported Stardog Version(s) |  | ------------------  | ---------------------------- |  | 3.x.x               | 7.x.x                        |  | 2.x.x               | 6.x.x                        |  | 1.x.x*              | 5.x.x                        |  | 0.x.x*              | any version < 5              |    _* = No longer supported_    We support and maintain a particular version of stardog.js only if the corresponding Stardog version(s) is (are) officially supported and maintained. For example, we no longer support v0.x.x of stardog.js, as the corresponding Stardog versions are no longer supported. (That said, later versions of stardog.js will often _mostly_ work with earlier Stardog versions. We just don't test this or make any guarantees to that effect.)    ## Quick Example  ```js  const { Connection, query } = require('stardog');    const conn = new Connection({    username: 'admin',    password: 'admin',    endpoint: 'http://localhost:5820',  });    query.execute(conn, 'myDatabaseName', 'select distinct ?s where { ?s ?p ?o }', 'application/sparql-results+json', {    limit: 10,    reasoning: true,    offset: 0,  }).then(({ body }) => {    console.log(body.results.bindings);  });  ```    <!--- API Goes Here --->  # API    ## <a name=""http"">HTTP</a>    #### <a name=""rdfmimetype"">RdfMimeType</a>    One of the following values:    `'application/ld+json'              | 'text/turtle'              | 'application/rdf+xml'              | 'application/n-triples'              | 'application/n-quads'              | 'application/trig'`  #### <a name=""sparqlmimetype"">SparqlMimeType</a>    One of the following values:    `'application/sparql-results+json'              | 'application/sparql-results+xml'`  #### <a name=""acceptmimetype"">AcceptMimeType</a>    One of the following values:    `RdfMimeType              | SparqlMimeType              | 'text/plain'              | 'text/boolean'              | 'application/json'              | '*/*'`  #### <a name=""explainacceptmimetype"">ExplainAcceptMimeType</a>    One of the following values:    `'text/plain'              | 'application/json'`  #### <a name=""body"">Body</a>    Object with the following values:    - status (`number`)  - statusText (`string`)  - result (`object | string | boolean | null`)  - ok (`boolean`)  - headers (`Headers`)  - body (`any`)    #### <a name=""connectionoptions"">ConnectionOptions</a>    Object with the following values:    - endpoint (`string`)  - username (`string`)  - password (`string`)  - token (`string`)  - meta (`ConnectionMeta`)    #### <a name=""requestconstructor"">RequestConstructor</a>    One of the following values:    `{        new (input: string | Request, init?: RequestInit): Request;      }`  #### <a name=""requestcreator"">RequestCreator</a>    One of the following values:    `({ uri, Request }: { uri: string; Request: Constructor }) => ReturnType`  #### <a name=""connectionmeta"">ConnectionMeta</a>    Object with the following values:    - createRequest (`RequestCreator<RequestConstructor, string | Request>`)  - createHeaders (`(defaults: { headers: Headers; }) => Headers`)    ## <a name=""connection"">Connection</a> (Class)    Constructed with:  - options ([`ConnectionOptions`](#connectionoptions))  ### <a name=""config"">Connection.config(options, meta)</a>    Takes the following params:  - options ([`ConnectionOptions`](#connectionoptions))  - meta ([`ConnectionMeta`](#connectionmeta))    Returns [`void`](#void)  ### <a name=""headers"">Connection.headers()</a>    Returns [`Headers`](#headers)  ### <a name=""uri"">Connection.uri(resource)</a>    Takes the following params:  - resource (`string[]`)    Returns `string`  ## <a name=""server"">server</a>    #### <a name=""shutdown"">`server.shutdown(conn, params)`</a>    Shuts down a Stardog server.     Expects the following parameters:    - conn ([`Connection`](#connection))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""status"">`server.status(conn, params)`</a>    Retrieves general status information about a Stardog server. By  default, also includes status information about all databases on  that server. If `params.databases` is `false`, however, then the  information about databases is omitted.    Expects the following parameters:    - conn ([`Connection`](#connection))    - params (`{ databases?: boolean; }`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""db"">db</a>    #### <a name=""create"">`db.create(conn, database, databaseOptions, options, params)`</a>    Creates a new database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - databaseOptions (`object`)    - options (`{ files: { filename: string}[] }`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""drop"">`db.drop(conn, database, params)`</a>    Deletes a database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""get"">`db.get(conn, database, params)`</a>    Gets an RDF representation of a database. See: https://www.w3.org/TR/sparql11-http-rdf-update/#http-get    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""offline"">`db.offline(conn, database, params)`</a>    Sets a database offline.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""online"">`db.online(conn, database, params)`</a>    Sets a database online.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""optimize"">`db.optimize(conn, database, params)`</a>    Optimizes a database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""list"">`db.list(conn, params)`</a>    Gets a list of all databases on a Stardog server.    Expects the following parameters:    - conn ([`Connection`](#connection))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""size"">`db.size(conn, database, params)`</a>    Gets number of triples in a database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""model"">`db.model(conn, database, options, params)`</a>    Gets the model for a database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - options (`object`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""clear"">`db.clear(conn, database, transactionId, params)`</a>    Clears the contents of a database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""add"">`db.add(conn, database, transactionId, content, options, params)`</a>    Adds data within a transaction.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - content (`string`)    - options ([`TransactionOptions`](#transactionoptions))    - params (`object`)    Returns [`Promise<transaction.TransactionResponse>`](#transactionresponse)    #### <a name=""remove"">`db.remove(conn, database, transactionId, content, options, params)`</a>    Removes data within a transaction.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - content (`string`)    - options ([`TransactionOptions`](#transactionoptions))    - params (`object`)    Returns [`Promise<transaction.TransactionResponse>`](#transactionresponse)    #### <a name=""exportdata"">`db.exportData(conn, database, options, params)`</a>    Exports the contents of a database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - options ({ mimetype: [`RdfMimeType`](#rdfmimetype) })    - params (`{ graphUri: string }`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""options"">options</a>    #### <a name=""getavailable"">`db.options.getAvailable(conn)`</a>    Gets all available database options with their default values.     Expects the following parameters:    - conn ([`Connection`](#connection))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""get"">`db.options.get(conn, database, params)`</a>    Gets set of options on a database.     Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""getall"">`db.options.getAll(conn, database)`</a>    Gets all options on a database.     Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""set"">`db.options.set(conn, database, databaseOptions, params)`</a>    Sets options on a database.     Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - databaseOptions (`object`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""graph"">graph</a>    #### <a name=""doget"">`db.graph.doGet(conn, database, graphUri, accept, params)`</a>    Retrieves the specified named graph    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - graphUri (`string`)    - accept ([`RdfMimeType`](#rdfmimetype))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""doput"">`db.graph.doPut(conn, database, graphData, graphUri, contentType, params)`</a>    Stores the given RDF data in the specified named graph    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - graphData (`string`)    - graphUri (`string`)    - contentType ([`RdfMimeType`](#rdfmimetype))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""dodelete"">`db.graph.doDelete(conn, database, graphUri, params)`</a>    Deletes the specified named graph    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - graphUri (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""dopost"">`db.graph.doPost(conn, database, graphUri, options, params)`</a>    Merges the given RDF data into the specified named graph    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - graphUri (`string`)    - options ({ contentType: [`RdfMimeType`](#rdfmimetype) })    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""transaction"">transaction</a>    #### <a name=""encodings"">Encodings</a>    One of the following values:    `'gzip' |                  'compress' |                  'deflate' |                  'identity' |                  'br'`  #### <a name=""transactionresponse"">TransactionResponse</a> extends [HTTP.Body](#body)    Object with the following values:    - transactionId (`string`)    #### <a name=""transactionoptions"">TransactionOptions</a>    Object with the following values:    - contentType (`HTTP.RdfMimeType`)  - encoding (`Encodings`)    #### <a name=""begin"">`db.transaction.begin(conn, database, params)`</a>    Begins a new transaction.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<TransactionResponse>`](#transactionresponse)    #### <a name=""rollback"">`db.transaction.rollback(conn, database, transactionId, params)`</a>    Rolls back a transaction, removing the transaction and undoing all changes    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - params (`object`)    Returns [`Promise<TransactionResponse>`](#transactionresponse)    #### <a name=""commit"">`db.transaction.commit(conn, database, transactionId, params)`</a>    Commits a transaction to the database, removing the transaction and making its changes permanent.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - params (`object`)    Returns [`Promise<TransactionResponse>`](#transactionresponse)    ## <a name=""icv"">icv</a>    #### <a name=""get"">`db.icv.get(conn, database, params)`</a>    Gets the set of integrity constraints on a given database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""add"">`db.icv.add(conn, database, icvAxioms, options, params)`</a>    Adds integrity constraints to a given database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - icvAxioms (`string`)    - options ({ contentType: [`RdfMimeType`](#rdfmimetype) })    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""remove"">`db.icv.remove(conn, database, icvAxioms, options, params)`</a>    Removes integrity constraints from a given database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - icvAxioms (`string`)    - options ({ contentType: [`RdfMimeType`](#rdfmimetype) })    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""clear"">`db.icv.clear(conn, database, params)`</a>    Removes all integrity constraints from a given database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""convert"">`db.icv.convert(conn, database, icvAxioms, options, params)`</a>    Converts a set of integrity constraints into an equivalent SPARQL query for a given database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - icvAxioms (`string`)    - options ({ contentType: [`RdfMimeType`](#rdfmimetype) })    - params (`{ graphUri: string }`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""validate"">`db.icv.validate(conn, database, constraints, options, params)`</a>    Checks constraints to see if they are valid    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - constraints (`string`)    - options ({ contentType: [`RdfMimeType`](#rdfmimetype) })    - params (`{ graphUri: string }`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""validateintx"">`db.icv.validateInTx(conn, database, transactionId, constraints, options, params)`</a>    Checks constraints to see if they are valid within a transaction    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - constraints (`string`)    - options ({ contentType: [`RdfMimeType`](#rdfmimetype) })    - params (`{ graphUri: string }`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""violations"">`db.icv.violations(conn, database, constraints, options, params)`</a>    Accepts integrity constraints as RDF and returns the violation explanations, if any, as RDF.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - constraints (`string`)    - options ({ contentType: [`RdfMimeType`](#rdfmimetype) })    - params (`{ graphUri: string }`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""violationsintx"">`db.icv.violationsInTx(conn, database, transactionId, constraints, options, params)`</a>    Accepts integrity constraints as RDF and returns the violation explanations, if any, as RDF.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - constraints (`string`)    - options ({ contentType: [`RdfMimeType`](#rdfmimetype) })    - params (`{ graphUri: string }`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""report"">`db.icv.report(conn, database, constraints, options, params)`</a>    Accepts integrity constraints as RDF and returns a validation report.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - constraints (`string`)    - options ({ contentType?: HTTP.RdfMimeType, accept?: [`AcceptMimeType`](#acceptmimetype) })    - params (`{ graphUri: string }`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""reportintx"">`db.icv.reportInTx(conn, database, transactionId, constraints, options, params)`</a>    Accepts integrity constraints as RDF and returns a validation report.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - constraints (`string`)    - options ({ contentType?: HTTP.RdfMimeType, accept?: [`AcceptMimeType`](#acceptmimetype) })    - params (`{ graphUri: string }`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""reasoning"">reasoning</a>    #### <a name=""consistency"">`db.reasoning.consistency(conn, database, options, params)`</a>    Returns if the database is consistent    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - options (`{ namedGraph: string }`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""explaininference"">`db.reasoning.explainInference(conn, database, inference, config, params)`</a>    Provides an explanation for an inference    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - inference (`string`)    - config (`{ contentType: string }`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""explaininconsistency"">`db.reasoning.explainInconsistency(conn, database, options, params)`</a>    Provides the reason why a database is inconsistent, as reported by db.reasoning.consistency    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - options (`{ namedGraph: string }`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""explaininferenceintransaction"">`db.reasoning.explainInferenceInTransaction(conn, database, transactionId, inference, config, params)`</a>    Provides an explanation for an inference within a transaction    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - inference (`string`)    - config ([`TransactionOptions`](#transactionoptions))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""explaininconsistencyintransaction"">`db.reasoning.explainInconsistencyInTransaction(conn, database, transactionId, options, params)`</a>    Provides the reason why a database is inconsistent, as reported by db.reasoning.consistency    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - options (`{ namedGraph: string }`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""schema"">`db.reasoning.schema(conn, database, params)`</a>    Gets the reasoning schema of the database    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""docs"">docs</a>    #### <a name=""size"">`db.docs.size(conn, database, params)`</a>    Retrieves the size of the document store    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""clear"">`db.docs.clear(conn, database, params)`</a>    Clears the document store    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""add"">`db.docs.add(conn, database, fileName, fileContents, params)`</a>    Adds a document to the document store    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - fileName (`string`)    - fileContents (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""remove"">`db.docs.remove(conn, database, fileName, params)`</a>    Removes a document from the document store    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - fileName (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""get"">`db.docs.get(conn, database, fileName, params)`</a>    Retrieves a document from the document store    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - fileName (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""namespaces"">namespaces</a>    #### <a name=""get"">`db.namespaces.get(conn, database)`</a>    Gets a mapping of the namespaces used in a database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""add"">`db.namespaces.add(conn, database, fileOrContents, options)`</a>    Extracts namespaces from an RDF file or RDF string and adds new  and updates existing namespaces in the database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - fileOrContents (`object | string`)    - options ({ contentType?: [`RdfMimeType`](#rdfmimetype) })    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""query"">query</a>    #### <a name=""querytype"">QueryType</a>    One of the following values:    `'select' | 'ask' | 'construct' | 'describe' | 'update' | 'paths' | null`  #### <a name=""propertyoptions"">PropertyOptions</a>    Object with the following values:    - uri (`string`)  - property (`string`)    #### <a name=""additionalhandlers"">AdditionalHandlers</a>    Object with the following values:    - onResponseStart (`(res: Response) => boolean | void`)    #### <a name=""property"">`query.property(conn, database, config, params)`</a>    Gets the values for a specific property of a URI individual.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - config ([`PropertyOptions`](#propertyoptions))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""explain"">`query.explain(conn, database, query, accept, params)`</a>    Gets the query plan generated by Stardog for a given SPARQL query.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - query (`string`)    - accept ([`ExplainAcceptMimeType`](#explainacceptmimetype))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""execute"">`query.execute(conn, database, query, accept, params, additionalHandlers)`</a>    Executes a query against a database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - query (`string`)    - accept ([`AcceptMimeType`](#acceptmimetype))    - params (`object`)    - additionalHandlers ([`AdditionalHandlers`](#additionalhandlers))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""executeintransaction"">`query.executeInTransaction(conn, database, transactionId, query, options, params)`</a>    Executes a query against a database within a transaction.    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - transactionId (`string`)    - query (`string`)    - options ({ accept: [`RdfMimeType`](#rdfmimetype) })    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""list"">`query.list(conn)`</a>    Gets a list of actively running queries.    Expects the following parameters:    - conn ([`Connection`](#connection))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""kill"">`query.kill(conn, queryId)`</a>    Kills an actively running query.    Expects the following parameters:    - conn ([`Connection`](#connection))    - queryId (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""get"">`query.get(conn, queryId)`</a>    Gets information about an actively running query.    Expects the following parameters:    - conn ([`Connection`](#connection))    - queryId (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""storedqueryoptions"">StoredQueryOptions</a>    Object with the following values:    - name (`string`)  - database (`string`)  - query (`string`)  - shared (`boolean`)  - reasoning (`boolean`)  - description (`boolean`)    ## <a name=""stored"">stored</a>    #### <a name=""create"">`query.stored.create(conn, config, params)`</a>    Stores a query in Stardog, either on the system level or for a given database.    Expects the following parameters:    - conn ([`Connection`](#connection))    - config ([`StoredQueryOptions`](#storedqueryoptions))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""list"">`query.stored.list(conn, params)`</a>    Lists all stored queries.    Expects the following parameters:    - conn ([`Connection`](#connection))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""update"">`query.stored.update(conn, config, params, useUpdateMethod)`</a>    Updates a given stored query and creates it if the name does not refer to an existing stored query.    Expects the following parameters:    - conn ([`Connection`](#connection))    - config ([`StoredQueryOptions`](#storedqueryoptions))    - params (`object`)    - useUpdateMethod (`boolean`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""remove"">`query.stored.remove(conn, storedQuery, params)`</a>    Removes a given stored query.    Expects the following parameters:    - conn ([`Connection`](#connection))    - storedQuery (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""graphql"">graphql</a>    #### <a name=""execute"">`query.graphql.execute(conn, database, query, variables, params, additionalHandlers)`</a>    Executes a GraphQL query    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - query (`string`)    - variables (`object`)    - params (`object`)    - additionalHandlers ([`AdditionalHandlers`](#additionalhandlers))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""listschemas"">`query.graphql.listSchemas(conn, database, params)`</a>    Retrieves a list of GraphQL schemas in the database    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""addschema"">`query.graphql.addSchema(conn, database, name, schema, params)`</a>    Adds a GraphQL schema to the database    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - name (`string`)    - schema (`object`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""updateschema"">`query.graphql.updateSchema(conn, database, name, schema, params)`</a>    Updates (or adds if non-existent) a GraphQL schema to the database    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - name (`string`)    - schema (`object`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""getschema"">`query.graphql.getSchema(conn, database, name, params)`</a>    Retrieves a GraphQL schema from the database    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - name (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""removeschema"">`query.graphql.removeSchema(conn, database, name, params)`</a>    Removes a GraphQL schemafrom  the database    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - name (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""clearschemas"">`query.graphql.clearSchemas(conn, database, params)`</a>    Clears all GraphQL schemas in the database    Expects the following parameters:    - conn ([`Connection`](#connection))    - database (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""utils"">utils</a>    #### <a name=""querytype"">`query.utils.queryType(query)`</a>    Returns the QueryType (as a string or null) for the given query.    Expects the following parameters:    - query (`string`)    Returns [`QueryType`](#querytyp)    #### <a name=""mimetype"">`query.utils.mimeType(query)`</a>    Returns the default HTTP `Accept` MIME type for the given query.    Expects the following parameters:    - query (`string`)    Returns [`HTTP.AcceptMimeType`](#acceptmimetyp)    ## <a name=""user"">user</a>    #### <a name=""user"">User</a>    Object with the following values:    - username (`string`)  - password (`string`)  - superuser (`boolean`)    #### <a name=""action"">Action</a>    One of the following values:    `'CREATE' |              'DELETE' |              'READ' |              'WRITE' |              'GRANT' |              'REVOKE' |              'EXECUTE'`  #### <a name=""resourcetype"">ResourceType</a>    One of the following values:    `'db' |              'user' |              'role' |              'admin' |              'metadata' |              'named-graph' |              'icv-constraints'`  #### <a name=""list"">`user.list(conn, params)`</a>    Gets a list of users.    Expects the following parameters:    - conn ([`Connection`](#connection))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""get"">`user.get(conn, username, params)`</a>    Gets all information for a given user.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""create"">`user.create(conn, user, params)`</a>    Creates a new user.    Expects the following parameters:    - conn ([`Connection`](#connection))    - user ([`User`](#user))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""changepassword"">`user.changePassword(conn, username, password, params)`</a>    Changes a user's password.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - password (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""valid"">`user.valid(conn, params)`</a>    Verifies that a Connection's credentials are valid.    Expects the following parameters:    - conn ([`Connection`](#connection))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""enabled"">`user.enabled(conn, username, params)`</a>    Verifies that a user is enabled.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""enable"">`user.enable(conn, username, enabled, params)`</a>    Enables/disables a user.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - enabled (`boolean`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""setroles"">`user.setRoles(conn, username, roles, params)`</a>    Sets roles for a user.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - roles (`string[]`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""listroles"">`user.listRoles(conn, username, params)`</a>    Gets a list of roles assigned to a user.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""assignpermission"">`user.assignPermission(conn, username, permission, params)`</a>    Creates a new permission for a user over a given resource.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - permission ([`Permission`](#permission))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""deletepermission"">`user.deletePermission(conn, username, permission, params)`</a>    Removes a permission for a user over a given resource.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - permission ([`Permission`](#permission))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""permissions"">`user.permissions(conn, username, params)`</a>    Gets a list of permissions assigned to user.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""effectivepermissions"">`user.effectivePermissions(conn, username, params)`</a>    Gets a list of a user's effective permissions.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""superuser"">`user.superUser(conn, username, params)`</a>    Specifies whether a user is a superuser.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""remove"">`user.remove(conn, username, params)`</a>    Deletes a user.    Expects the following parameters:    - conn ([`Connection`](#connection))    - username (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""token"">`user.token(conn)`</a>    Returns a token for the user if the connection is valid.    Expects the following parameters:    - conn ([`Connection`](#connection))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""permission"">Permission</a>    Object with the following values:    - action (`Action`)  - resourceType (`ResourceType`)  - resources (`string[]`)    ## <a name=""role"">role</a>    #### <a name=""create"">`user.role.create(conn, role, params)`</a>    Creates a new role.    Expects the following parameters:    - conn ([`Connection`](#connection))    - role (`{ name: string }`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""list"">`user.role.list(conn, params)`</a>    Lists all existing roles.    Expects the following parameters:    - conn ([`Connection`](#connection))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""remove"">`user.role.remove(conn, role, params)`</a>    Deletes an existing role from the system.    Expects the following parameters:    - conn ([`Connection`](#connection))    - role (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""userswithrole"">`user.role.usersWithRole(conn, role, params)`</a>    Lists all users that have been assigned a given role.    Expects the following parameters:    - conn ([`Connection`](#connection))    - role (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""assignpermission"">`user.role.assignPermission(conn, role, permission, params)`</a>    Adds a permission over a given resource to a given role.    Expects the following parameters:    - conn ([`Connection`](#connection))    - role (`string`)    - permission ([`Permission`](#permission))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""deletepermission"">`user.role.deletePermission(conn, role, permission, params)`</a>    Removes a permission over a given resource from a given role.    Expects the following parameters:    - conn ([`Connection`](#connection))    - role (`string`)    - permission ([`Permission`](#permission))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""permissions"">`user.role.permissions(conn, role, params)`</a>    Lists all permissions assigned to a given role.    Expects the following parameters:    - conn ([`Connection`](#connection))    - role (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""virtualgraphs"">virtualGraphs</a>    #### <a name=""sharedoptions"">SharedOptions</a>    Object with the following values:    - base (`string`)  - mappings.syntax (`string`)  - percent.encode (`boolean`)  - optimize.import (`boolean`)  - query.translation (`'DEFAULT' | 'LEGACY'`)    #### <a name=""rdbmsoptions"">RdbmsOptions</a> extends [SharedOptions](#sharedoptions)    Object with the following values:    - jdbc.url (`string`)  - jdbc.username (`string`)  - jdbc.password (`string`)  - jdbc.driver (`string`)  - parser.sql.quoting (`'NATIVE' | 'ANSI'`)  - sql.functions (`string`)  - sql.schemas (`string`)  - default.mappings.include.tables (`string`)  - default.mappings.exclude.tables (`string`)    #### <a name=""mongooptions"">MongoOptions</a> extends [SharedOptions](#sharedoptions)    Object with the following values:    - mongodb.uri (`string`)    #### <a name=""csvoptions"">CsvOptions</a> extends [SharedOptions](#sharedoptions)    Object with the following values:    - csv.separator (`string`)  - csv.quote (`string`)  - csv.escape (`string`)  - csv.header (`boolean`)  - csv.skip.empty (`boolean`)    #### <a name=""allvgoptions"">AllVgOptions</a>    One of the following values:    `SharedOptions & RdbmsOptions & MongoOptions & CsvOptions`  #### <a name=""mappingsrequestoptions"">MappingsRequestOptions</a>    Object with the following values:    - preferUntransformed (`boolean`)  - syntax (`string`)    #### <a name=""vgmeta"">VgMeta</a>    Object with the following values:    - db (`string`)  - dataSource (`string`)    #### <a name=""list"">`virtualGraphs.list(conn)`</a>    Retrieve a list of virtual graphs    Expects the following parameters:    - conn ([`Connection`](#connection))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""listinfo"">`virtualGraphs.listInfo(conn)`</a>    Retrieve a list of virtual graphs info    Expects the following parameters:    - conn ([`Connection`](#connection))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""add"">`virtualGraphs.add(conn, name, mappings, options, meta)`</a>    Add a virtual graph to the system    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - mappings (`string`)    - options ([`T`](#t))    - meta ([`VgMeta`](#vgmeta))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""update"">`virtualGraphs.update(conn, name, mappings, options, meta)`</a>    Update a virtual graph in the system    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - mappings (`string`)    - options ([`T`](#t))    - meta ([`VgMeta`](#vgmeta))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""remove"">`virtualGraphs.remove(conn, name)`</a>    Remove a virtual graph from the system    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""online"">`virtualGraphs.online(conn, name)`</a>    Bring a virtual graph online    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""available"">`virtualGraphs.available(conn, name)`</a>    Determine if the named virtual graph is available    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""options"">`virtualGraphs.options(conn, name)`</a>    Retrieve a virtual graph's options    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""mappings"">`virtualGraphs.mappings(conn, name, requestOptions)`</a>    Retrieve a virtual graph's mappings    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - requestOptions ([`MappingsRequestOptions`](#mappingsrequestoptions))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""importfile"">`virtualGraphs.importFile(conn, file, fileType, database, importOptions)`</a>    Import a JSON or CSV file into a database via `virtual import`.    Expects the following parameters:    - conn ([`Connection`](#connection))    - file (`object`)    - fileType (`string`)    - database (`string`)    - importOptions (`{            mappings?: string,            properties?: string,            namedGraph?: string,          }`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""storedfunctions"">storedFunctions</a>    #### <a name=""add"">`storedFunctions.add(conn, functions, params)`</a>    Adds one or more stored functions to the server    Expects the following parameters:    - conn ([`Connection`](#connection))    - functions (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""get"">`storedFunctions.get(conn, name, params)`</a>    Retrieves the specified function definition    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""remove"">`storedFunctions.remove(conn, name, params)`</a>    Removes a stored function from the server    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""clear"">`storedFunctions.clear(conn, params)`</a>    Removes all stored functions from the server    Expects the following parameters:    - conn ([`Connection`](#connection))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""getall"">`storedFunctions.getAll(conn, params)`</a>    Retrieves an export of all stored functions on the server    Expects the following parameters:    - conn ([`Connection`](#connection))    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""cluster"">cluster</a>    #### <a name=""info"">`cluster.info(conn)`</a>    Retrieves basic information about a Stardog cluster.    Expects the following parameters:    - conn ([`Connection`](#connection))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""status"">`cluster.status(conn)`</a>    Retrieves detailed status information about a Stardog cluster.    Expects the following parameters:    - conn ([`Connection`](#connection))    Returns [`Promise<HTTP.Body>`](#body)    ## <a name=""datasources"">dataSources</a>    #### <a name=""list"">`dataSources.list(conn)`</a>    Retrieve a list of data sources    Expects the following parameters:    - conn ([`Connection`](#connection))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""listinfo"">`dataSources.listInfo(conn)`</a>    Retrieve a list of data sources info    Expects the following parameters:    - conn ([`Connection`](#connection))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""info"">`dataSources.info(conn, name)`</a>    Retrieve the named data source info    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""add"">`dataSources.add(conn, name, options)`</a>    Add a data source to the system    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - options ([`T`](#t))    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""update"">`dataSources.update(conn, name, options, requestOptions)`</a>    Update the named data source in the system    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - options ([`T`](#t))    - requestOptions (`{ force: boolean }`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""remove"">`dataSources.remove(conn, name, params)`</a>    Remove the named data source from the system    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - params (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""share"">`dataSources.share(conn, name)`</a>    Change a private data source to a shared one    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""online"">`dataSources.online(conn, name)`</a>    Bring the named data source online    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""available"">`dataSources.available(conn, name)`</a>    Determine if the named data source is available    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""options"">`dataSources.options(conn, name)`</a>    Retrieve the named data source options    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""getmetadata"">`dataSources.getMetadata(conn, name, options)`</a>    Retrieve the named data source metadata    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - options (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""updatemetadata"">`dataSources.updateMetadata(conn, name, metadata, options)`</a>    Update the named data source metadata    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - metadata ([`T`](#t))    - options (`object`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""query"">`dataSources.query(conn, name, dataSourceQuery)`</a>    Query data source    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - dataSourceQuery (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""refreshcounts"">`dataSources.refreshCounts(conn, name, tableName)`</a>    Refresh table row-count estimates    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - tableName (`string`)    Returns [`Promise<HTTP.Body>`](#body)    #### <a name=""refreshmetadata"">`dataSources.refreshMetadata(conn, name, tableName)`</a>    Refresh metadata    Expects the following parameters:    - conn ([`Connection`](#connection))    - name (`string`)    - tableName (`string`)    Returns [`Promise<HTTP.Body>`](#body)   """
Semantic web;https://github.com/gtfierro/reasonable;"""# Reasonable    ![Nightly Build](https://github.com/gtfierro/reasonable/workflows/Nightly%20Build/badge.svg)  ![Build](https://github.com/gtfierro/reasonable/workflows/Build/badge.svg)  [![PyPI version](https://badge.fury.io/py/reasonable.svg)](https://badge.fury.io/py/reasonable)    An OWL 2 RL reasoner with reasonable performance    ## Performance    Comparing performance of `reasonable` with [OWLRL](https://github.com/RDFLib/OWL-RL) and [Allegro](https://franz.com/agraph/support/documentation/current/materializer.html). Evaluation consisted of loading Brick models of different sizes into the respective reasoning engine and timing how long it took to produce the materialization. `reasonable` is about 7x faster than Allegro and 38x faster than OWLRL on this workload.    ![benchmark](img/benchmark.png)    ## How to Use    ### Python    To facilitate usage, we use the [pyo3](https://pyo3.rs/) project to generate Python 3.x bindings to this project.  Installing these *should* be as easy as `pip install reasonable`.    See also the [`brickschema`](https://github.com/BrickSchema/py-brickschema) package for working with Brick models. The package provides a generic interface to this reasoner and several others.    Usage looks like:    ```python  import reasonable    # import triples from an rdflib Graph  import rdflib  g = rdflib.Graph()  g.parse(""example_models/ontologies/Brick.n3"", format=""n3"")  g.parse(""example_models/small1.n3"", format=""n3"")    r = reasonable.PyReasoner()  r.from_graph(g)  triples = r.reason()  print(""from rdflib:"", len(triples))    # import triples from files on disk  r = reasonable.PyReasoner()  r.load_file(""example_models/ontologies/Brick.n3"")  r.load_file(""example_models/small1.n3"")  triples = r.reason()  print(""from files:"", len(triples))  ```    ### Rust    See [Rust docs](https://docs.rs/reasonable)    Example of usage from Rust:    ```rust  use ::reasonable::owl::Reasoner;  use std::env;  use std::time::Instant;  use log::info;    fn main() {      env_logger::init();      let mut r = Reasoner::new();      env::args().skip(1).map(|filename| {          info!(""Loading file {}"", &filename);          r.load_file(&filename).unwrap()      }).count();      let reasoning_start = Instant::now();      info!(""Starting reasoning"");      r.reason();      info!(""Reasoning completed in {:.02}sec"", reasoning_start.elapsed().as_secs_f64());      r.dump_file(""output.ttl"").unwrap();  }  ```      ## OWL 2 Rules    Using rule definitions from [here](https://www.w3.org/TR/owl2-profiles/#Reasoning_in_OWL_2_RL_and_RDF_Graphs_using_Rules).    **TODO**: implement RDF/RDFS entailment semantics as described [here](https://www.w3.org/TR/rdf11-mt/)    **Note**: haven't implemented rules that produce exceptions; waiting to determine the best way of handling these errors.    ### Equality Semantics    |Completed| Rule name | Notes |  |---------|----------|-------|  | no     | `eq-ref` | implementation is very inefficient; causes lots of flux       |  | **yes**| `eq-sym` |       |  | **yes**| `eq-trans` |       |  | **yes**| `eq-rep-s` |       |  | **yes**| `eq-rep-p` |       |  | **yes**| `eq-rep-o` |       |  | no     | `eq-diff1` | throws exception |  | no     | `eq-diff2` | throws exception |  | no     | `eq-diff3` | throws exception |    ### Property Axiom Semantics    |Completed| Rule name | Notes |  |---------|----------|-------|  | no        | `prp-ap` |       |  | **yes**   | `prp-dom` |       |  | **yes**   | `prp-rng` |       |  | **yes**   | `prp-fp` |       |  | **yes**   | `prp-ifp` |       |  | **yes**   | `prp-irp` | throws exception |  | **yes**   | `prp-symp` |       |  | **yes**   | `prp-asyp` | throws exception |  | **yes**   | `prp-trp` |       |  | **yes**   | `prp-spo1` |       |  | no        | `prp-spo2` |       |  | **yes**   | `prp-eqp1` |       |  | **yes**   | `prp-eqp2` |       |  | **yes**   | `prp-pdw` | throws exception |  | no        | `prp-adp` | throws exception |  | **yes**   | `prp-inv1` |       |  | **yes**   | `prp-inv2` |       |  | no        | `prp-key` |       |  | no        | `prp-npa1` | throws exception |  | no        | `prp-npa2` | throws exception |    ### Class Semantics    |Completed| Rule name | Notes |  |---------|----------|-------|  | **yes**| `cls-thing` |       |  | **yes**| `cls-nothing1` |       |  | **yes**| `cls-nothing2` | throws exception       |  | **yes**| `cls-int1` |       |  | **yes**| `cls-int2` |       |  | **yes**| `cls-uni` |       |  | **yes**| `cls-com` | throws exception    |  | **yes**| `cls-svf1` |       |  | **yes**| `cls-svf2` |       |  | **yes**| `cls-avf` |       |  | **yes**| `cls-hv1` |       |  | **yes**| `cls-hv2` |       |  | no     | `cls-maxc1` | throws exception       |  | no     | `cls-maxc2` |       |  | no     | `cls-maxqc1` | throws exception       |  | no     | `cls-maxqc2` | throws exception      |  | no     | `cls-maxqc3` |       |  | no     | `cls-maxqc4` |       |  | no     | `cls-oo` |       |    ### Class Axiom Semantics    |Completed| Rule name | Notes |  |---------|----------|-------|  | **yes**| `cax-sco` |       |  | **yes**| `cax-eqc1` |       |  | **yes**| `cax-eqc2` |       |  | **yes**| `cax-dw` | throws exception      |  | no     | `cax-adc` |  throws exception     |    ### Other    - no datatype semantics for now    ## Development Notes    To publish new versions of `reasonable`, tag a commit with the version (e.g. `v1.3.2`) and push the tag to GitHub. This will execute the `publish` action which builds an uploads to PyPi. """
Semantic web;https://github.com/scholtzan/rdf-rs;"""# rdf-rs    > Note: This project is work in progress and currently not stable.    `rdf` is a library for the [Resource Description Framework](https://www.w3.org/RDF/) (RDF) and [SPARQL](https://www.w3.org/TR/rdf-sparql-query/) implemented in Rust.    This project is a way for me to learn Rust and combine it with my interests in semantic web technologies.    ### Usage  Add this to your Cargo.toml:    ```toml  [dependencies]  rdf = ""0.1.4""  ```      ### Basic Examples      RDF triples can be stored and represented in a graph.    ```rust  use rdf::graph::Graph;  use rdf::uri::Uri;  use rdf::triple::Triple;    let mut graph = Graph::new(None);  let subject = graph.create_blank_node();  let predicate = graph.create_uri_node(&Uri::new(""http://example.org/show/localName"".to_string()));  let object = graph.create_blank_node();  let triple = Triple::new(&subject, &predicate, &object);    graph.add_triple(&triple);  ```    RDF graphs can be serialized to a supported format.    ```rust  use rdf::writer::n_triples_writer::NTriplesWriter;  use rdf::writer::rdf_writer::RdfWriter;  use rdf::graph::Graph;  use rdf::uri::Uri;  use rdf::triple::Triple;    let writer = NTriplesWriter::new();    let mut graph = Graph::new(None);  let subject = graph.create_blank_node();  let predicate = graph.create_uri_node(&Uri::new(""http://example.org/show/localName"".to_string()));  let object = graph.create_blank_node();  let triple = Triple::new(&subject, &predicate, &object);    graph.add_triple(&triple);  assert_eq!(writer.write_to_string(&graph).unwrap(),             ""_:auto0 <http://example.org/show/localName> _:auto1 .\n"".to_string());  ```    RDF syntax can also be parsed and transformed into an RDF graph.    ```rust  use rdf::reader::turtle_parser::TurtleParser;  use rdf::reader::rdf_parser::RdfParser;  use rdf::uri::Uri;    let input = ""@base <http://example.org/> .  @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .  @prefix foaf: <http://xmlns.com/foaf/0.1/> .    <http://www.w3.org/2001/sw/RDFCore/ntriples/> rdf:type foaf:Document ;          <http://purl.org/dc/terms/title> \""N-Triples\""@en-US ;          foaf:maker _:art ."";    let mut reader = TurtleParser::from_string(input.to_string());  match reader.decode() {    Ok(graph) => {      assert_eq!(graph.count(), 3);      assert_eq!(graph.namespaces().len(), 2);      assert_eq!(graph.base_uri(), &Some(Uri::new(""http://example.org/"".to_string())))    },    Err(_) => assert!(false)  }  ```    ## Current State    Currently `rdf-rs` provides basic data structures for representing RDF graphs, triples and nodes.  The following formats can be parsed and serialized:    * Turtle  * N-Triples      ## Future Work and Ideas    * Support querying with SPARQL  * Add support for more formats  * More comprehensive `Uri` data structure """
Semantic web;https://github.com/robstewart57/rdf4h;"""rdf4h - An RDF library for Haskell  =====    [![Available on Hackage][badge-hackage]][hackage]  [![License BSD3][badge-license]][license]  [![Build Status][badge-travis]][travis]    [badge-travis]: https://travis-ci.org/robstewart57/rdf4h.png?branch=master  [travis]: https://travis-ci.org/robstewart57/rdf4h  [badge-hackage]: https://img.shields.io/hackage/v/rdf4h.svg  [hackage]: http://hackage.haskell.org/package/rdf4h  [badge-license]: https://img.shields.io/badge/license-BSD3-green.svg?dummy  [license]: https://github.com/robstewart57/rdf4h/blob/master/LICENSE.txt    rdf4h is a library for working with RDF in Haskell.    For details see the GitHub project page:    http://robstewart57.github.io/rdf4h/    Supports GHC versions from 8.0.2 (stackage lts-9) to 8.8.3 (stackage lts-16.0).    ### Development with Nix and direnv    To enter a development environment, you can use [Nix](https://nixos.org/download.html) and [direnv](https://github.com/direnv/direnv) which install all required software, also allowing you to use your preferred shell. Once installed, just run    ```shell  $ direnv allow  ```    and you'll have a working development environment for now and the future whenever you enter this directory.    This development environment allows to use either Stack or Cabal for building the software.    ### RDF formats    The coverage of the W3C RDF standards are:    Format | Parsing | Serialising  --- | --- | ---  NTriples | complete | complete  Turtle | complete | complete  RDF/XML | complete | not supported    These results are produced with version 4.0 of this library.    These tests are run on the W3C unit tests for RDF formats: https://github.com/w3c/rdf-tests.    ### Feature requests    1. The parsers in this library parse large files/strings contents     entirely before generating RDF triples. This doesn't scale for very     large files. Implementing stream based RDF parsers would overcome     this problem, e.g. by creating input streams enabling output     streams in the     [io-streams](http://hackage.haskell.org/package/io-streams) library     to consume triples on-the-fly during parsing. This is discussed     here:     https://github.com/robstewart57/rdf4h/issues/56#issuecomment-497892024 and     https://github.com/robstewart57/rdf4h/issues/44#issuecomment-426054978    2. RDF/XML serialisation of RDF graphs.    ### Running tests    To run all the tests (parsers and the library API):    ```shell  $ git submodule update --init --recursive  $ git submodule foreach git pull origin gh-pages  $ stack test --test-arguments=""--quickcheck-tests 1000""  ```    To run specific parser tests when bug fixing:    ```shell  $ stack test --test-arguments=""--pattern /parser-w3c-tests-ntriples/""  $ stack test --test-arguments=""--pattern /parser-w3c-tests-turtle/""  $ stack test --test-arguments=""--pattern /parser-w3c-tests-xml/""  ```    ### Running benchmarks    To run the bencharks:    ```shell  $ wget https://www.dropbox.com/s/z1it340emcreowj/bills.099.actions.rdf  $ gzip -d bills.099.actions.rdf.gz  $ stack bench  ``` """
Semantic web;https://github.com/robstewart57/hsparql;"""  [![Available on Hackage][badge-hackage]][hackage]  [![License BSD3][badge-license]][license]  [![Build Status][badge-travis]][travis]    [badge-travis]: https://travis-ci.org/robstewart57/hsparql.png?branch=master  [travis]: https://travis-ci.org/robstewart57/hsparql  [badge-hackage]: https://img.shields.io/hackage/v/hsparql.svg  [hackage]: http://hackage.haskell.org/package/hsparql  [badge-license]: https://img.shields.io/badge/license-BSD3-green.svg?dummy  [license]: https://github.com/robstewart57/hsparql/blob/master/LICENSE    ## Introduction    `hsparql` includes a DSL to easily create queries, as well as methods to  submit those queries to a SPARQL server, returning the results as  simple Haskell data structures.    ### Select Queries    Take the following SPARQL query:    ```sparql  PREFIX dbpedia: <http://dbpedia.org/resource/>  PREFIX dbprop: <http://dbpedia.org/property/>  PREFIX foaf: <http://xmlns.com/foaf/0.1/>  SELECT ?name ?page  WHERE {    ?x dbprop:genre dbpedia:Web_browser    ?x foaf:name ?name    ?x foaf:page ?page  }  ```    Can be generated using the following Haskell code:    ```haskell  simpleSelect :: Query SelectQuery  simpleSelect = do      resource <- prefix ""dbpedia"" (iriRef ""http://dbpedia.org/resource/"")      dbpprop  <- prefix ""dbprop"" (iriRef ""http://dbpedia.org/property/"")      foaf     <- prefix ""foaf"" (iriRef ""http://xmlns.com/foaf/0.1/"")        x    <- var      name <- var      page <- var        triple_ x (dbpprop .:. ""genre"") (resource .:. ""Web_browser"")      triple_ x (foaf .:. ""name"") name      triple_ x (foaf .:. ""page"") page        selectVars [name, page]  ```      ### Construct Queries    Take the following SPARQL query:    ```sparql  PREFIX dbpedia: <http://dbpedia.org/resource/>  PREFIX dbprop: <http://dbpedia.org/property/>  PREFIX foaf: <http://xmlns.com/foaf/0.1/>  PREFIX example: <http://www.example.com/>  CONSTRUCT {    ?x example:hasName ?name  }  WHERE {    ?x dbprop:genre dbpedia:Web_browser    ?x foaf:name ?name    ?x foaf:page ?page  }  ```    Can be generated using the following Haskell code:    ```haskell  simpleConstruct :: Query ConstructQuery  simpleConstruct = do      resource <- prefix ""dbpedia"" (iriRef ""http://dbpedia.org/resource/"")      dbpprop  <- prefix ""dbprop"" (iriRef ""http://dbpedia.org/property/"")      foaf     <- prefix ""foaf"" (iriRef ""http://xmlns.com/foaf/0.1/"")      example  <- prefix ""example"" (iriRef ""http://www.example.com/"")        x    <- var      name <- var      page <- var        construct <- constructTriple x (example .:. ""hasName"") name        triple_ x (dbpprop .:. ""genre"") (resource .:. ""Web_browser"")      triple_ x (foaf .:. ""name"") name      triple_ x (foaf .:. ""page"") page        return ConstructQuery { queryConstructs = [construct] }  ```    ### Describe Queries    Take the following SPARQL query:    ```sparql  DESCRIBE <http://dbpedia.org/resource/Edinburgh>  ```    Can be generated using the following Haskell code:    ```haskell  simpleDescribe :: Query DescribeQuery  simpleDescribe = do      resource <- prefix ""dbpedia"" (iriRef ""http://dbpedia.org/resource/"")      uri <- describeIRI (resource .:. ""Edinburgh"")      return DescribeQuery { queryDescribe = uri }  ```    ### Ask Queries    Take the following SPARQL query:    ```sparql  PREFIX dbprop: <http://dbpedia.org/property/>  ASK { ?x dbprop:genre <http://dbpedia.org/resource/Web_browser> }  ```    Can be generated using the following Haskell code:    ```haskell  simpleAsk :: Query AskQuery  simpleAsk = do      resource <- prefix ""dbpedia"" (iriRef ""http://dbpedia.org/resource/"")      dbprop  <- prefix ""dbprop"" (iriRef ""http://dbpedia.org/property/"")        x <- var      ask <- askTriple x (dbprop .:. ""genre"") (resource .:. ""Web_browser"")        return AskQuery { queryAsk = [ask] }  ```    ## Output Types    ### Select Queries    `SELECT` queries generate a set of sparql query solutions. See:  http://www.w3.org/TR/rdf-sparql-XMLres/    ```haskell  selectExample :: IO ()  selectExample = do    (Just s) <- selectQuery ""http://dbpedia.org/sparql"" simpleSelect    putStrLn . take 500 . show $ s  ```      Here's the respective type:    ```haskell  selectQuery :: EndPoint -> Query SelectQuery -> IO (Maybe [[BindingValue]])  ```      ### Construct Queries    `CONSTRUCT` queries generate RDF, which is serialized in N3 in this  package. See: http://www.w3.org/TR/rdf-primer/#rdfxml    ```haskell  constructExample :: IO ()  constructExample = do    rdfGraph <- constructQuery ""http://dbpedia.org/sparql"" simpleConstruct    mapM_ print (triplesOf rdfGraph)  ```    Here's the respective type:    ```haskell  constructQuery :: EndPoint -> Query ConstructQuery -> IO MGraph  ```    ### Describe Queries    `DESCRIBE` queries generate RDF, which is serialized in N3 in this  package. See: http://www.w3.org/TR/rdf-sparql-query/#describe    ```haskell  describeExample :: IO ()  describeExample = do    rdfGraph <- describeQuery ""http://dbpedia.org/sparql"" simpleDescribe    mapM_ print (triplesOf rdfGraph  ```    Here's the respective type:    ```haskell  describeQuery :: EndPoint -> Query DescribeQuery -> IO MGraph  ```    ### Ask Queries    `ASK` queries inspects whether or not a triple exists. RDF is an  open-world assumption. See: http://www.w3.org/TR/rdf-sparql-query/#ask    ```haskell  askExample :: IO ()  askExample = do    res <- askQuery ""http://dbpedia.org/sparql"" simpleAsk    putStrLn $ ""result: "" ++ (show (res::Bool))  ```    Here's the respective type:    ```haskell  askQuery :: EndPoint -> Query AskQuery -> IO Bool  ```    ### More examples    Some extra examples can be found in [tests](tests/Database/HSparql/QueryGeneratorTest.hs).    ## TODOs    - Opt for a unified Type representation    This hsparql package and the [RDF4H][1] package use similar, but not    identical, types for triples, namespaces, prefixes and so on. Ideally,    one type representation for such concepts should be adopted for both packages.    - Develop a unified semantic web toolkit for Haskell    Combining the RDF4H and hsparql packages seems like a sensible goal to    achieve, to provide a semantic web toolkit similar to [Jena][2] for Java.      [1]: https://github.com/robstewart57/rdf4h  [2]: http://incubator.apache.org/jena/ """
Semantic web;https://github.com/blazegraph/blazegraph-samples;"""#Welcome to the Blazegraph Samples Project#  The Blazegraph™ Database is our ultra high-performance graph database supporting Blueprints and RDF/SPARQL APIs. It supports up to 50 Billion edges on a single machine and has available enterprise features such as High Availability and Scale-out. It is in production use for Fortune 500 customers such as EMC, Autodesk, and many others.  It powers the Wikimedia Foundation's Wiki Data Query Service.  See the latest [Feature Matrix](http://www.blazegraph.com/product/).    [Sign up](http://eepurl.com/VLpUj) to get the latest news on Blazegraph.      Please also visit us at our: [website](http://www.blazegraph.com), [wiki](https://wiki.blazegraph.com), and [blog](https://wiki.blazegraph.com/).    Find an issue?   Need help?  See [JIRA](https://jira.blazegraph.com) or purchase [Support](https://www.blazegraph.com/buy).    ![image](http://blog.blazegraph.com/wp-content/uploads/2015/07/blazegraph_by_systap_favicon.png)    # blazegraph-samples  Samples for using Blazegraph™    Link to the [Javadoc](https://blazegraph.github.io/blazegraph-samples/apidocs/index.html)    To build everything run:    ```  mvn package  ```    Simple applications demonstrating using Blazegraph for loading/querying data in different modes:    To build a sample, cd in the directory and run:    ```  cd sample-sesame-first  mvn package  ```    1. sample-sesame-first - Sesame API in emmbedded mode    https://wiki.blazegraph.com/wiki/index.php/First_Application_Tutorial    2. 'sample-sesame-embedded' - Sesame API in emmbedded mode    https://wiki.blazegraph.com/wiki/index.php/Sesame_API_embedded_mode    3. 'sample-sesame-remote' - Sesame API in remote mode    https://wiki.blazegraph.com/wiki/index.php/Sesame_API_remote_mode    4. 'sample-blueprints-embedded' - Blueprints API in embedded mode    https://wiki.blazegraph.com/wiki/index.php/Blueprints_API_embedded_mode    5. 'sample-blueprints-remote' - Blueprints API in remote mode    https://wiki.blazegraph.com/wiki/index.php/Blueprints_API_remote_mode    6. 'sample-rdr' - using RDF* and SPARQL* with Blazegraph™    https://wiki.blazegraph.com/wiki/index.php/RDR    7.  'sample-customFunction-embedded'- Custom Embedded Function      https://wiki.blazegraph.com/wiki/index.php/Custom_Function_embedded_mode    8.  'sample-test' - Sample Unit Tests <br>     technical project created for CI system               """
Semantic web;https://github.com/castagna/GeoARQ;"""GeoARQ   ======    GeoARQ uses Lucene Spatial via an ARQ property function to allow to search  nearby a location. All the RDF instances which use the WGS84 geo positioning   RDF vocabulary [1] to represent geographic coordinates are indexed.    This is *experimental* (and unsupported).      How to use it  -------------    This is how you build an index from a Jena Model:        ModelIndexerSubject indexer = new ModelIndexerSubject(""target/lucene"");      indexer.indexStatements(model.listStatements());      indexer.close();    This is how you configure ARQ to use the spatial Lucene index:                IndexSearcher searcher = IndexSearcherFactory.create(""target/lucene"");      GeoARQ.setDefaultIndex(searcher);    This is an example of a SPARQL query using the :nearby property function to find airports close to Bristol (i.e. latitude ~ 51.3000, longitude ~ -2.71000):         PREFIX : <http://example/>      PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>      PREFIX geoarq: <http://openjena.org/GeoARQ/property#>      PREFIX asc: <http://airports.dataincubator.org/schema/> .        SELECT ?label {          ?s a asc:LargeAirport .          ?s rdfs:label ?lavel .          ?s geoarq:nearby (51.3000 -2.71000) .      }    Or, within a bounded box:        PREFIX : <http://example/>      PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>      PREFIX geoarq: <http://openjena.org/GeoARQ/property#>      PREFIX asc: <http://airports.dataincubator.org/schema/> .        SELECT ?label {          ?s a asc:LargeAirport .          ?s rdfs:label ?lavel .          ?s geoarq:within (51.3727 -2.72909 51.3927 -2.70909) .      }      Todo  ----     * Add more tests   * Clean-up the code   * Add support for http://www.w3.org/2003/01/geo/wgs84_pos#lat_long   * Add ability to specify radius (i.e. geoarq:nearby (51.3000 -2.71000 20))   * Add ability to specify number of results within a radius     (i.e. geoarq:nearby (51.3000 -2.71000 20 10))   * Investigate Geohash [2]   * Double check lucene-misc dependency, is it necessary?   * Add ability to index remote SPARQL endpoints?   * ...       [1] http://www.w3.org/2003/01/geo/wgs84_pos#     [2] http://en.wikipedia.org/wiki/Geohash"""
Semantic web;https://github.com/mdesalvo/RDFSharp.Semantics;"""This project is now part of <a href=""https://github.com/mdesalvo/RDFSharp"">RDFSharp</a>! """
Semantic web;https://github.com/paulhoule/infovore;"""Overview  --------    Infovore is an RDF processing system that uses Hadoop to process RDF data  sets in the billion triple range and beyond.  Infovore was originally designed to process  the (old) proprietary Freebase dump into RDF,  but once Freebase came out with an official RDF  dump,  Infovore gained the ability to clean and purify the dump,  making it not just possible  but easy to process Freebase data with triple stores such as Virtuoso 7.    Every week we run Infovore in Amazon Elastic/Map reduce in order to produce a product known as  [:BaseKB](http://basekb.com/).    Infovore depends on the [Centipede](https://github.com/paulhoule/centipede/wiki) framework for packaging  and processing command-line arguments.  The [Telepath](https://github.com/paulhoule/telepath/wiki) project  extends the Infovore project in order to process Wikipedia usage information to produce a product called  [:SubjectiveEye3D](https://github.com/paulhoule/telepath/wiki/SubjectiveEye3D).      Supporting  ----------    It costs several hundreds of dollars per month to process and store files in connection with this work.  Please join <a href=""https://www.gittip.com/"">Gittip</a> and make a <a href=""https://www.gittip.com/paulhoule/"">small weekly donation</a> to keep this data free.      Building  --------    Infovore software requires JDK 7.    mvn clean install    Installing  ----------    The following cantrip, run from the top level ""infovore"" directory, initializes the bash shell  for the use of the ""haruhi"" program,  which can be used to run Infovore applications  packaged in the Bakemono Jar.    source haruhi/target/path.sh    More Information  ----------------    See     https://github.com/paulhoule/infovore/wiki     for documentation and join the discussion group at    https://groups.google.com/forum/#!forum/infovore-basekb         """
Semantic web;https://github.com/sspider/etalis;"""# etalis  Exported from code.google.com/p/etalis    ETALIS is an open source system for Complex Event Processing with two accompanied languages called: ETALIS Language for Events (ELE) and Event Processing SPARQL (EP-SPARQL). ETALIS is based on a declarative semantics, grounded in Logic Programming. Complex events are derived from simpler events by means of deductive rules. Due to its root in logic, ETALIS also supports reasoning about events, context, and real-time complex situations (i.e., Knowledge-based Event Processing). ETALIS stands for Event TrAnsaction Logic Inference System.    ETALIS is implemented in Prolog. The engine runs on many Prolog systems: YAP, SWI, SICStus, XSB, tuProlog and LPA Prolog. Download ETALIS from here. We installed ETALIS and EP-SPARQL on several operating systems including: Windows XP, Vista and 7, Mac OS, Android OS (with tuProlog) and Linux-based systems (Ubuntu, RedHat, SUSE).    Features:    * declarative rule-based language for event processing;  * detection of complex events and reasoning over states (with logic rules);  * classic event operators (e.g., sequence, concurrent conjunction, disjunction, negation etc.). The language supports all operators from Allen's interval algebra (e.g., during, meets, starts, finishes etc.);  * count-based sliding windows;  * event aggregation for count, avg, sum, min, max etc.;  * event filtering, enrichment, projection, translation, and multiplication are supported;  * alarm events to trigger events after certain durations of time or at absolute datimes (working only under SWI Prolog);  * processing of out-of-order events (i.e. events that are delayed due to different circumstances e.g. network anomalies etc.);  * event retraction (revision);  * shared computation plan for evaluation of complex event rules;  * support for Event Processing SPARQL (EP-SPARQL) language.    See the ETALIS Manual for more information about ETALIS.    To evaluate ETALIS we have implemented the Fast Flower Delivery use case created by Opher Etzion and Peter Niblett in their book: Event Processing in Action. Description of the use case can be found in the aforementioned book or from the Event Processing Technical Society website. The use case implementation in ETALIS can be found in examples/flower_delivery or on a corresponding Fast_Flower_Delivery_Use_Case wiki page.    Contact:  Darko Anicic <darko.anicic@fzi.de>: for questions related to ETALIS design, architecture and underlying algorithms;  Paul Fodor <pfodor@cs.stonybrook.edu>: for questions related to implementation of the ETALIS engine or any problems with the execution of the ETALIS programs. """
Semantic web;https://github.com/OpenTriply/YASGUI.legacy;"""IMPORTANT  ======  This is the old, unsupported (but relatively stable) YASGUI repository, still accessible at [http://legacy.yasgui.org][2]    The new YASGUI library is accessible at [http://yasgui.org][1].  For more info, visit [http://about.yasgui.org][3].      [1]: http://yasgui.org    [2]: http://legacy.yasgui.org    [3]: http://about.yasgui.org    [4]: http://yasqe.yasgui.org    [5]: http://yasr.yasgui.org """
Semantic web;https://github.com/magnetik/tac;""""""
Semantic web;https://github.com/clarkparsia/sparql-proxy;"""## Simple SPARQL Endpoint Proxy Servlet    Sparql Endpoint Proxy Servlet - Helps for bypassing CORS issues.    Simple proxy that redirects all elements in the request to a configured SPARQL endpoint. Headers in the request are also redirected.    ### Configuring the proxy    The only setting required to configure the SPARQL Endpoint proxy is `proxy.host` which refers to the host:port info in which the Stardog HTTP service is running, for example:        <init-param>  		<param-name>proxy.host</param-name>  		<param-value>http://localhost:5822</param-value>  	</init-param>    The previous indicats the a Stardog DB is running the HTTP protocol in `http://localhost:5822`, which can be modified to any other information, either a local reference or remote.    ### Building .war file with Ant    To build the proxy war, just execute:        ant clean web-dist        This will generate the `sparql-proxy.war` file that you just need to copy to your app container `webapps` directory.    ### Using the proxy    To use the proxy, simple send the request to it, following the same pattern as you would do with a Stardog DB directly. For instance, to query the DB `gov` directly to Stardog HTTP endpoint you'll do:        curl -X GET ""http://localhost:5822/gov/query?query=select%20*%20where%20%7B%20%3Fs%20%3Fp%20%3Fo%20%7D%20limit%2010""        Using the proxy (with CORS support), the same can be done:        curl -X GET ""http://localhost:8181/sparql-proxy/gov/query?query=select%20*%20where%20%7B%20%3Fs%20%3Fp%20%3Fo%20%7D%20limit%2010""        where:    * `http://localhost:8181/sparql-proxy` is the path for the sparql-proxy (with servlet containter running in port 8181)  * `gov` is the Stardog DB  * `query?query=` points to the encoded query.   """
Semantic web;https://github.com/linkeddata/warp;"""warp  ====    Warp - the linked data file manager """
Semantic web;https://github.com/joepio/atomic-data-browser;"""![Atomic Data Browser](./logo.svg)    [![Discord chat][discord-badge]][discord-url]  [![MIT licensed](https://img.shields.io/badge/license-MIT-blue.svg)](./LICENSE)    Create, share, fetch and model linked [Atomic Data](https://atomicdata.dev)!  This repo consists of three components: a javascript / typescript library, a react library, and a complete GUI: Atomic-Data Browser.    ## [`atomic-data-browser`](data-browser/README.md)    A GUI for viewing, editing and browsing Atomic Data.   Designed for interacting with [atomic-server](https://github.com/joepio/atomic-data-rust/).    **demo on [atomicdata.dev](https://atomicdata.dev)**    https://user-images.githubusercontent.com/2183313/139728539-d69b899f-6f9b-44cb-a1b7-bbab68beac0c.mp4    ```sh  # To run, simply run the following commands:  yarn # install dependencies  yarn bootstrap # symlink ./lib and ./react to ./data-browser  yarn start # run the server!  ```    [→ Read more](data-browser/README.md)    ## [`@tomic/lib`](lib/README.md)    <a href=""https://www.npmjs.com/package\/@tomic/lib"" target=""_blank"">    <img src=""https://img.shields.io/npm/v/@tomic/lib?color=cc3534"" />  </a>  <a href=""https://www.npmjs.com/package/@tomic/lib"" target=""_blank"">    <img src=""https://img.shields.io/npm/dm/@tomic/lib?color=%2344cc10"" />  </a>  <a href=""https://bundlephobia.com/result?p=@tomic/lib"" target=""_blank"">    <img src=""https://badgen.net/bundlephobia/minzip/@tomic/lib"">  </a>    Library with `Store`, `Commit`, `JSON-AD` parsing, and more.    [**docs**](https://joepio.github.io/atomic-data-browser/docs/modules/_tomic_lib.html)    [→ Read more](lib/README.md)    ## [`@tomic/react`](react/README.md)    <a href=""https://www.npmjs.com/package/@tomic/react"" target=""_blank"">    <img src=""https://img.shields.io/npm/v/@tomic/react?color=cc3534"" />  </a>  <a href=""https://www.npmjs.com/package/@tomic/react"" target=""_blank"">    <img src=""https://img.shields.io/npm/dm/@tomic/react?color=%2344cc10"" />  </a>  <a href=""https://bundlephobia.com/result?p=@tomic/react"" target=""_blank"">    <img src=""https://badgen.net/bundlephobia/minzip/@tomic/react"">  </a>      React library with many useful hooks for rendering and editing Atomic Data.    [**demo + template on codesandbox**](https://codesandbox.io/s/atomic-data-react-template-4y9qu?file=/src/MyResource.tsx:0-1223)    [**docs**](https://joepio.github.io/atomic-data-browser/docs/modules/_tomic_react.html)    [→ Read more](react/README.md)    ## Also check out    - [atomic-data-rust](https://github.com/joepio/atomic-data-rs), a rust [**library**](https://crates.io/crates/atomic-lib), [**server**](https://crates.io/crates/atomic-server) and [**cli**](https://crates.io/crates/atomic-cli) for Atomic Data.  - [sign up to the Atomic Data Newsletter](http://eepurl.com/hHcRA1)    ## Contribute    Issues and PR's are welcome!  And join our [Discord][discord-url]!  See [Contribute.md](CONTRIBUTE.md)    [discord-badge]: https://img.shields.io/discord/723588174747533393.svg?logo=discord  [discord-url]: https://discord.gg/a72Rv2P """
Semantic web;https://github.com/frmichel/morph-xr2rml;"""# What is it?    Morph-xR2RML is an implementation of the [xR2RML mapping language](http://i3s.unice.fr/~fmichel/xr2rml_specification.html) that enables the description of mappings from relational or non relational databases to RDF. xR2RML is an extension of [R2RML](http://www.w3.org/TR/r2rml/) and [RML](http://semweb.mmlab.be/rml/spec.html).    Morph-xR2RML comes with connectors for relational databases (MySQL, PostgreSQL, MonetDB) and the MongoDB NoSQL document store.  Two running modes are available:  - the *graph materialization* mode creates all possible RDF triples at once.  - the *query rewriting* mode translates a SPARQL 1.0 query into a target database query and returns a SPARQL answer. It can run as a SPARQL 1.0 endpoint or as a stand-alone application.    Morph-xR2RML was developed by the [I3S laboratory](http://www.i3s.unice.fr/) as an extension of the [Morph-RDB project](https://github.com/oeg-upm/morph-rdb) which is an implementation of R2RML. It is made available under the Apache 2.0 License.    #### SPARQL-to-SQL  The SPARQL-to-SQL rewriting is an adaptation of the former Morph-RDB implementation, it supports SPARQL SELECT and DESCRIBE queries.    #### SPARQL-to-MongoDB  The SPARQL-to-MongoDB rewriting is a fully new component, it supports the SELECT, ASK, CONSTRUCT and DESCRIBE query forms.        ## Publications  [1] F. Michel, L. Djimenou, C. Faron-Zucker, and J. Montagnat. Translation of Relational and Non-Relational Databases into RDF with xR2RML.  In Proceedings of the *11th International Confenrence on Web Information Systems and Technologies (WEBIST 2015)*, Lisbon, Portugal, 2015.    [2] F. Michel, L. Djimenou, C. Faron-Zucker, and J. Montagnat. xR2RML: Relational and Non-Relational Databases to RDF Mapping Language.  Research report, CNRS, 2015. https://hal.archives-ouvertes.fr/hal-01066663    [3] C. Callou, F. Michel, C. Faron-Zucker, C. Martin, J. Montagnat. Towards a Shared Reference Thesaurus for Studies on History of Zoology, Archaeozoology and Conservation Biology. In *Semantic Web For Scientific Heritage (SW4SH), Workshops of the ESWC’15 conference*.    [4] F. Michel, C. Faron-Zucker, and J. Montagnat. A Generic Mapping-Based Query Translation from SPARQL to Various Target Database Query Languages.  In Proceedings of the *12th International Confenrence on Web Information Systems and Technologies (WEBIST 2016)*, Roma, Italy, 2016.    [5] F. Michel, C. Faron-Zucker, and J. Montagnat. Mapping-based SPARQL access to a MongoDB database. Research report, CNRS, 2016.   https://hal.archives-ouvertes.fr/hal-01245883.    [6] F. Michel, C. Faron-Zucker, and J. Montagnat. A Mapping-Based Method to Query MongoDB Documents with SPARQL. In *27th International Conference on Database and Expert Systems Applications (DEXA 2016)*, 2016.      ## Limitations    ##### xR2RML Language support  - The generation of RDF collection and containers is supported in all cases (from a list of values resulting of the evaluation of a mixed syntax path typically, from the result of a join query implied by a referencing object map), except in the case of a regular R2RML join query applied to a relational database: the result of the join SQL query cannot be translated into an RDF collection or container.  - Named graphs are supported although they are not printed out in Turtle which does not support named graphs. It would be quite easy to extend it with a N-Quad or Trig serialization to allow for writing triples in named graphs.    The former limitation on NestedTermMaps was lifted in Sept. 2017. All types of NestedTermMaps are now fully implemented, so that any complex iterations and collection/container nesting can be defined.      ##### Query rewriting   The query rewriting is implemented for RDBs and MongoDB, with the restriction that _no mixed syntax paths be used_. Doing query rewriting with mixed syntax paths is a much more complex problem, that may not be possible in all situations (it would require to ""revert"" expressions such as JSONPath or XPath to retrieve source data base values).    Only one join condition is supported in a referencing object map.    ----------    # Code description    See a detailed [description of the project code and architecture](doc/README_code_architecture.md).    ----------    # Want to try it?    ### Download, Build    Pre-requisite: have **Java SDK 10** installed    You can download the last release or snapshot published in [this repository](https://www.dropbox.com/sh/djnztipsclvcskw/AABT1JagzD4K4aCALDNVj-yra?dl=0).  The latest on-going version is the 1.3.2 snapshot.    Alternatively, you can build the application using [Maven](http://maven.apache.org/): in a shell, CD to the root directory morph-xr2rml, then run the command: `mvn clean package`. A jar with all dependencies is generated in `morph-xr2rml-dist/target`.      ### Run it    The application takes two options: `--configDir` gives the configuration directory and `--configFile` give the configuration file within this directory. Option `--configFile` defaults to `morph.properties`.    Additionally, several parameter given in the configuration file can be overridden using the following options:   - mapping file: `--mappingFile`   - output file : `--output`  - maximum number of triples generated in a single output file: `--outputMaxTriples`      **From a command line interface**, CD to directory morph-xr2rml-dist and run the application as follows:    ```  java -jar target/morph-xr2rml-dist-<version>-jar-with-dependencies.jar \     --configDir <configuration directory> \     --configFile <configuration file within this directory>  ```    Besides, the logger configuration can be overriden by passing the `log4j.configuration` parameter to the JVM:    ```  java -Dlog4j.configuration=file:/path/to/my/log4j.configuration -jar ...  ```    **From an IDE** such as Eclipse or IntelliJ: In project morph-xr2rml-dist locate main class `fr.unice.i3s.morph.xr2rml.engine.MorphRunner`, and run it as a Scala application with arguments `--configDir` and `--configFile`.    ### SPARQL endpoint    To run Morph-xR2RML as a SPARQL endpoint, simply edit the configuration file (see reference) and set the property `sever.active=true`. The default access URL is:  ```  http://localhost:8080/sparql  ```  Property `query.file.path` is ignored and queries can be submitted using either HTTP GET or POST methods as described in the [SPARQL protocol](https://www.w3.org/TR/rdf-sparql-protocol/) recommendation.    For SPARQL SELECT and ASK queries, the XML, JSON, CSV and TSV serializations are supported.    For SPARQL DESCRIBE and CONSTRUCT queries, the supported serializations are RDF/XML, N-TRIPLE, N-QUAD, TURTLE, N3 and JSON-LD.    ### Examples for MongoDB    In directories `morph-xr2rml-dist/example_mongo` and `morph-xr2rml-dist/example_mongo_rewriting` we provide example databases and corresponding mappings. Directory `example_mongo` runs the graph materialization mode, `example_mongo_rewriting` runs the query rewriting mode.    - `testdb_dump.json` is a dump of the MongoDB test database: copy and paste the content of that file into a MongoDB shell window to create the database;  - `morph.properties` provides database connection details;  - `mapping1.ttl` to `mapping4.ttl` contain xR2RML mapping graphs illustrating various features of the language;  - `result1.txt` to `result4.txt` contain the expected result of the mappings 1 to 4;  - `query.sparql` (in directory `example_mongo_rewriting` only) contains a SPARQL query to be executed against the test database.    Edit `morph.properties` and change the database URL, name, user and password with appropriate values.    > _**Note about query optimization**_: the xR2RML xrr:uniqueRef notation is of major importance for query optimization as it allows for self-joins elimination. Check example in `morph-xr2rml-dist/example_taxref_rewriting`.    ### Examples for MySQL    In directories `morph-xr2rml-dist/example_mysql` and `morph-xr2rml-dist/example_mysql_rewriting` we provide example databases and corresponding mappings. Directory `example_mysql` runs the graph materialization mode, `example_mysql_rewriting` runs the query rewriting mode.    - `testdb_dump.sql` is a dump of the MySQL test database. You may import it into a MySQL instance by running command `mysql -u root -p test < testdb_dump.sql`;  - `morph.properties` provides database connection details;  - `mapping.ttl` contains an example xR2RML mapping graph;  - `result.txt` contains the expected result of applying this mapping to that database;  - `query.sparql` (in directory `example_mysql_rewriting` only) contains a SPARQL query to be executed against the test database.    Edit `morph.properties` and change the database url, name, user and password with appropriate values.    ----------    # Configuration file reference  ```  # -- xR2RML mapping file (Mandatory):  # path relative to the configuration directory given in parameter --configDir  mappingdocument.file.path=mapping1.ttl    # -- Server mode: true|false. Default: false  # false: stand-alone application that performs either graph materialization or query rewriting  # true:  SPARQL endpoint with query rewriting  server.active=false    # -- Server port number, ignored when ""server.active=false"". Default: 8080  server.port=8080    # -- Processing result output file, relative to --configDir. Default: result.txt  output.file.path=result.txt    # -- Max number of triples to generate in output file. Default: 0 (no limit)  # If the max number is reached, file name is suffixed with an index e.g. result.txt.0, result.txt.1, result.txt.2 etc.  output.file.max_triples=0    # -- Output RDF syntax: RDF/XML|N-TRIPLE|TURTLE|N3|JSON-LD. Default: TURTLE  # Applies to the graph materialization and the rewriting of SPARQL CONSTRUCT and DESCRIBE queries  output.syntax.rdf=TURTLE    # -- Output syntax for SPARQL result set (SPARQL SELECT and ASK queries): XML|JSON|CSV|TSV. Default: XML  # When ""server.active = true"", this may be overridden by the Accept HTTP header of the request  output.syntax.result=XML    # -- Display the result on the std output after the processing: true|false. Default: true  output.display=false    # -- File containing the SPARQL query to process, relative to --configDir. Default: none.   # Ignored when ""server.active = true""  query.file.path=query.sparql    # -- Database connection type and configuration  no_of_database=1  database.type[0]=MongoDB  database.driver[0]=  database.url[0]=mongodb://127.0.0.1:27017  database.name[0]=test  database.user[0]=user  database.pwd[0]=user      # -- Reference formulation: Column|JSONPath|XPath. Default: Column  database.reference_formulation[0]=JSONPath    # -- Runner factory. Mandatory.  # For MongoDB: fr.unice.i3s.morph.xr2rml.mongo.engine.MorphJsondocRunnerFactory  # For RDBs:    es.upm.fi.dia.oeg.morph.rdb.engine.MorphRDBRunnerFactory  runner_factory.class.name=fr.unice.i3s.morph.xr2rml.mongo.engine.MorphMongoRunnerFactory      # -- URL-encode reserved chars in database values. Default: true  # uricolumn.encode_unsafe_chars_dbvalues=true    # -- URL-encode reserved chars IRI template string. Default: true   # uricolumn.encode_uri=true      # -- Cache the result of previously executed queries for MongoDB. Default: false  # Caution: high memory consumption, to be used for RefObjectMaps only  querytranslator.cachequeryresult=false      # -- Primary SPARQL query optimization. Default: true  querytranslator.sparql.optimize=true    # -- Abstract query optimization: self join elimination. Default: true  querytranslator.abstract.selfjoinelimination=true    # -- Abstract query optimization: self union elimination. Default: true  querytranslator.abstract.selfunionelimination=true    # -- Abstract query optimization: propagation of conditions in a inner/left join. Default: true  querytranslator.abstract.propagateconditionfromjoin=true    ```   """
Semantic web;https://github.com/jpcik/morph-web;"""morph-web  =========    morph-web is a web demonstrator for morph-streams: [https://github.com/jpcik/morph-streams], an RDF stream query processor.    ##Install it yourself    You will need:  * java7  * sbt: www.scala-sbt.org/  * play framework: www.playframework.com/‎    Then you can:    * download the code  * start the application: `sbt run``  * go to a browser to `localhost:9000``  * that's it    ##Use cases    Follow the [tutorial](https://github.com/jpcik/morph-web/wiki/Tutorial:-Morph-streams) to learn about:  * Registering a query  * Pulling data from a registered query  * Pushing data with WebSockets  * Creating R2RML mappings """
Semantic web;https://github.com/Claudenw/JenaSecurity;"""JenaSecurity  ============    JenaSecurity is now part of the Apache Jena project.  ====================================================    Please make any contributions at the Apache Jena website: http://jena.apache.org    package name: jena-security (Jena version < 3)  package name: jena-permissions (Jena version >= 3)    See Apache Jena website for documentation, source and binary packages.    ------    Security (Permissions) wrapper around Jena RDF implementation.    By implementing a SecurityEvaluator users may apply access restricitons to graphs and optionally  to triples within the graphs.    Two implementations are available based on Jena 2.7.4 and another on the upcoming 2.10.0    Documentation is most complete for the 2.10.0 version, however most of that documentation will  also apply to 2.7.4.    Maven Info:  <pre>  Group Id: org.apache.jena  Artifact Id: jena-security  </pre>   """
Semantic web;https://github.com/white-gecko/jekyll-rdf;"""# jekyll-rdf    A [Jekyll plugin](https://jekyllrb.com/docs/plugins/) for including RDF data in your static site.    [![Gem Version](https://badge.fury.io/rb/jekyll-rdf.svg)](https://badge.fury.io/rb/jekyll-rdf)  [![Build Status](https://travis-ci.org/white-gecko/jekyll-rdf.svg?branch=develop)](https://travis-ci.org/white-gecko/jekyll-rdf)  [![Coverage Status](https://coveralls.io/repos/github/white-gecko/jekyll-rdf/badge.svg?branch=develop)](https://coveralls.io/github/white-gecko/jekyll-rdf?branch=develop)    The API Documentation is available at [RubyDoc.info](http://www.rubydoc.info/gems/jekyll-rdf/).    # Contents    1. [Installation](#installation)  2. [Usage](#usage)      1. [Configuration](#configuration)      2. [Building the Jekyll Site](#building-the-jekyll-site)      3. [Defining Templates](#defining-templates)  3. [Parameters and configuration options at a glance](#parameters-and-configuration-options-at-a-glance)      1. [Resource Attributes](#resource-attributes)      2. [Liquid Filters](#liquid-filters)      3. [Plugin Configuration (\_config.yml)](#plugin-configuration-_configyml)  4. [Development](#development)  5. [License](#license)    # Installation    As a prerequisite for *Jekyll RDF* you of course need to install [*Jekyll*](https://jekyllrb.com/).  Please take a look at the installations instructions at https://jekyllrb.com/docs/installation/.    If you already have a working Jekyll installation you can add the the Jekyll-RDF plugin.  Probably you already using [Bundler](https://bundler.io/) and there is a [`Gemfile`](https://bundler.io/gemfile.html) in your Jekyll directory.  Add Jekyll-RDF to the plugins section:    ```  group :jekyll_plugins do      gem ""jekyll-rdf"", '~> 3.0.0.pre.a'      …  end  ```    Replace the version string with the currently available stable release as listed on [rubygems.org](https://rubygems.org/gems/jekyll-rdf).  After updating your `Gemfile` you probably want to run `bundle install` (or `bundle install --path vendor/bundle`) or `bundle update`.    If you are not using a `Gemfile` to manage your jekyll/ruby packages install Jekyll-RDF using `gem`:    ```  gem install jekyll-rdf  ```    If you want to build the plugin from source, please have a look at our [Development](#development) section.    # Usage    This section explains how to use Jekyll-RDF in three steps:    1. [Configuration](#configuration)  2. [Building the Jekyll Site](#building-the-jekyll-site)  3. [Defining Templates](#defining-templates)    All filters and methods to use in templates and configuration options are documented in the section “[Parameters and configuration options at a glance](#parameters-and-configuration-options-at-a-glance)”.    ## Configuration  First, you need a jekyll page. In order to create one, just do:  ```  jekyll new my_page  cd my_page  ```    Further there are some parameters required in your `_config.yml` for `jekyll-rdf`. I.e. the `url` and `baseurl` parameters are used for including the resource pages into the root of the site, the plug-in has to be configured, and the path to the RDF file has to be present.    ```yaml  baseurl: ""/simpsons""  url: ""http://example.org""    plugins:      - jekyll-rdf    jekyll_rdf:      path: ""_data/data.ttl""      default_template: ""default.html""      restriction: ""SELECT ?resourceUri WHERE { ?resourceUri ?p ?o . FILTER regex(str(?resourceUri), 'http://example.org/simpsons')  }""      class_template_mappings:          ""http://xmlns.com/foaf/0.1/Person"": ""person.html""      instance_template_mappings:          ""http://example.org/simpsons/Abraham"": ""abraham.html""  ```    ### Map resources to templates  It is possible to map a specific class (resp. RDF-type) or individual resources to a template.  ```yaml    class_template_mappings:        ""http://xmlns.com/foaf/0.1/Person"": ""person.html""    instance_template_mappings:        ""http://aksw.org/Team"": ""team.html""  ```    A template mapped to a class will be used to render each instance of that class and its subclasses.  Each instance is rendered with its most specific class mapped to a template.  If the mapping is ambiguous for a resource, a warning will be output to your command window, so watch out!    It is also possible to define a default template, which is used for all resources, which are not covered by the `class_template_mappings` or `instance_template_mappings`.    ```yaml    default_template: ""default.html""  ```    ### Restrict resource selection  You can restrict the resources selected to be built by adding a SPARQL query as `restriction` parameter to `_config.yml`. Please use `?resourceUri` as the placeholder for the resulting URIs:  ```yaml    restriction: ""SELECT ?resourceUri WHERE { ?resourceUri <http://www.ifi.uio.no/INF3580/family#hasFather> <http://www.ifi.uio.no/INF3580/simpsons#Homer> }""  ```    There are 3 pre-defined keywords for restrictions implemented:  * `subjects` will load all subject URIs  * `predicates` will load all predicate URIs  * `objects` will load all object URIs    Because some SPARQL endpoints have a built in limit for SELECT queries you can also define a list of resources to be built.  A file `_data/restriction.txt` cool have the following content:    ```  <http://example.org/resourceA>  <http://example.org/resourceB>  <http://example.org/resourceC>  <http://example.org/resourceD>  <http://example.org/resourceE>  ```    In the `_config.yml` you specify the file with the key `restriction_file`.  If both, a `restriction_file` and a `restriction`, are specified JekyllRDF will build pages for the union of the both.    ### Blank Nodes  Furthermore you can decide if you want to render blank nodes or not. You just need to add `include_blank`to `_config.yml`:  ```yaml  jekyll_rdf:    include_blank: true  ```    ### Preferred Language  Finally it is also possible to set a preferred language for the RDF-literals with the option `language`:  ```yaml  jekyll_rdf:    language: ""en""  ```    ## Building the Jekyll Site    Running `jekyll build` will render the RDF resources to the `_site/…` directory. Running `jekyll serve` will render the RDF resources and provide you with an instant HTTP-Server usually accessible at `http://localhost:4000/`.  RDF resources whose IRIs don't start with the configured jekyll `url` and `baseurl` are rendered to the `_site/rdfsites/…` subdirectory.    ## Defining Templates  To make use of the RDF data, create one or more files (e.g `rdf_index.html` or `person.html`) in the `_layouts`-directory. For each resource a page will be rendered. See example below:    ```html  ---  layout: default  ---  <div class=""home"">    <h1 class=""page-heading""><b>{{ page.rdf.iri }}</b></h1>    <p>      <h3>Statements in which {{ page.rdf.iri }} occurs as subject:</h3>      {% include statements_table.html collection=page.rdf.statements_as_subject %}    </p>    <p>      <h3>Statements in which {{ page.rdf.iri }} occurs as predicate:</h3>      {% include statements_table.html collection=page.rdf.statements_as_predicate %}    </p>    <p>      <h3>Statements in which {{ page.rdf.iri }} occurs as object:</h3>      {% include statements_table.html collection=page.rdf.statements_as_object %}    </p>  </div>  ```    ### Template Examples  We included some template examples at  * `test/source/_layouts/rdf_index.html`  * `test/source/_layouts/person.html`    ### Get the IRI of a resource        {{ page.rdf }}    Is the currently rendered resource.        {{ page.rdf.iri }}    Returns the IRI of the currently rendered resource.    To access objects which are connected to the current subject via a predicate you can use our custom liquid filters. For single objects or lists of objects use the `rdf_property`-filter (see [1](#single-objects) and [2](#multiple-objects)).    ### Single Objects  To access one object which is connected to the current subject through a given predicate please filter `page.rdf` data with the `rdf_property`-filter. Example:  ```  Age: {{ page.rdf | rdf_property: '<http://xmlns.com/foaf/0.1/age>' }}  ```    ### Optional Language Selection  To select a specific language please add a second parameter to the filter:  ```  Age: {{ page.rdf | rdf_property: '<http://xmlns.com/foaf/0.1/job>','en' }}  ```    ### Multiple Objects  To get more than one object connected to the current subject through a given predicate please use the filter `rdf_property` in conjunction with a third argument set to `true` (the second argument for the language can be omitted by setting it to `nil`):  ```html  Sisters: <br />  {% assign resultset = page.rdf | rdf_property: '<http://www.ifi.uio.no/INF3580/family#hasSister>', nil, true %}  <ul>  {% for result in resultset %}      <li>{{ result }}</li>  {% endfor %}  </ul>  ```    ### Optional Language Selection  To select a specific language please add a second parameter to the filter:  ```html  Book titles: <br />  {% assign resultset = page.rdf | rdf_property: '<http://xmlns.com/foaf/0.1/currentProject>','de' %}  <ul>  {% for result in resultset %}      <li>{{ result }}</li>  {% endfor %}  </ul>  ```    ### RDF Containers and Collections  To support [RDF Containers](https://www.w3.org/TR/rdf-schema/#ch_containervocab) and [RDF Collections](https://www.w3.org/TR/rdf-schema/#ch_collectionvocab) we provide the `rdf_container` and `rdf_collection` filters.    In both cases the respective container resource resp. head of the collection needs to be identified and then passed through the respective filter.  For containers we currently support explicit instances of `rdf:Bag`, `rdf:Seq` and `rdf:Alt` with the members identified using the `rdfs:ContainerMembershipProperty`s: `rdf:_1`, `rdf:_2`, `rdf:_3` ….  Collections are identified using `rdf:first`, `rdf:rest` and terminated with `L rdf:rest rdf:nil`.  Since the head of a collection needs to be identified you cannot use a blank node there, you can identify it indirectly through the predicate which contains the collection.    Example graph:    ```  @prefix ex: <http://example.org/> .  @prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .    ex:Resource ex:lists ex:List ;              ex:directList (""hello"" ""from"" ""turtle"") .              ex:hasContainer ex:Container .    ex:List rdf:first ""hello"" ;          rdf:rest (""rdf"" ""list"") .    ex:Container a rdf:Bag ;               rdf:_1 ""hello"" ;               rdf:_2 ""rdf"" ;               rdf:_3 ""container"" .  ```    The template for `ex:Resource`:    ```  {% assign list = page.rdf | rdf_collection: '<http://example.org/directList>' %}  <ol>  {% for item in list %}  <li>{{ item }}</li>  {% endfor %}  </ol>    {% assign container = page.rdf | rdf_property: '<http://example.org/hasContainer>' | rdf_container %}  <ul>  {% for item in container %}  <li>{{ item }}</li>  {% endfor %}  </ul>  ```    ### Custom SPARQL Query  We implemented a liquid filter `sparql_query` to run custom SPARQL queries. Each occurence of `?resourceUri` gets replaced with the current URI.  *Caution:* You have to separate query and resultset variables because of Liquids concepts. Example:  ```html  {% assign query = 'SELECT ?sub ?pre WHERE { ?sub ?pre ?resourceUri }' %}  {% assign resultset = page.rdf | sparql_query: query %}  <table>  {% for result in resultset %}    <tr>      <td>{{ result.sub }}</td>      <td>{{ result.pre }}</td>    </tr>  {% endfor %}  </table>  ```    ### Defining Prefixes for RDF  It is possible to declare a set of prefixes which can be used in the `rdf_property` and `sparql_query` liquid-filters.  This allows to shorten the amount of text required for each liquid-filter.  The syntax of the prefix declarations is the same as for [SPARQL 1.1](https://www.w3.org/TR/2013/REC-sparql11-query-20130321/).  Just put your prefixes in a separate file and include the key `rdf_prefix_path` together with a relative path in the [YAML Front Matter](https://jekyllrb.com/docs/frontmatter/) of a file where your prefixes should be used.    For the prefixes the same rules apply as for other variables defined in the YAML Front Matter.  *These variables will then be available to you to access using Liquid tags both further down in the file and also in any layouts or includes that the page or post in question relies on.* (source: [YAML Front Matter](https://jekyllrb.com/docs/frontmatter/)).  This is especially relevant if you are using prefixes in includes.    ### Dealing with Fragment Identifiers  If the URI of a resource contains a [fragment identifier (`#…`)](https://en.wikipedia.org/wiki/Fragment_identifier) the resource can be hosted together with other resources with the same base URI up to the fragment identifier on a single page.  The page will by accessible through the base URI, while in the template the individual URIs with a fragment identifier are accessible through the collection `page.sub_rdf`.    **Example**    In the `_config.yml`:  ```yaml    'instance_template_mappings' :      'http://www.ifi.uio.no/INF3580/simpsons' : 'family.html'  ```    In `_layouts/family.html`:  ```html    {% for member in page.sub_rdf%}      {% include simPerson.html person = member%}    {% endfor %}  ```    The example uses the template `family.html` to render a single page containing every resource whose URI begins with `http://www.ifi.uio.no/INF3580/simpsons#`, was well as the resource `http://www.ifi.uio.no/INF3580/simpsons` itself.  Jekyll-rdf collects all resources with a fragment identifier in their URI (from here on called `subResources`) and passes them through `page.sub_rdf` into the templates of its `superResource` (resources whose base URI is the same as of its `subResources` except for the fragment identifier).    # Parameters and configuration options at a glance    ## Resource Attributes  Every resource returned by one of `jekyll-rdf`s filters is an object that liquid can also handle like a string. They all have the following methods usable in Liquid.    ### Resource.statements_as_subject  Return a list of statements whose subject is the current resource.  The statements in the returned list can be accessed by addressing their positions: `Statement.subject`, `Statement.predicate`, respective `Statement.object`.    ### Resource.statements_as_predicate  Return a list of statements whose predicate is the current resource.  The statements in the returned list can be accessed by addressing their positions: `Statement.subject`, `Statement.predicate`, respective `Statement.object`.    ### Resource.statements_as_object  Return a list of statements whose object is the current resource.  The statements in the returned list can be accessed by addressing their positions: `Statement.subject`, `Statement.predicate`, respective `Statement.object`.    ### Resource.page_url  Return the URL of the page representing this RdfResource.    ### Resource.render_path  Return the path to the page representing this RdfResource. Use it with care.    ### Resource.covered  This method is relevant for rendering pages for IRIs containing a fragment identifier (`http://superresource#anchor`).  This method returns true for the super-resource (`http://superresource`) if it is actually described in the given knowledgebase.    ### Resource.inspect  Returns a verbose String representing this resource.    ## Liquid Filters  ### rdf_get  **Synopsis:** `<resource_iri> | rdf_get`    **Parameters:**  - `<resource_iri>` is a string representing an RDF resource, with prefix (`prefix:name`) or a full IRI (`<http://ex.org/name>`). To reference the resource of the current page use `page.rdf`, `page`, or `nil`.    **Description:** Takes the provided IRI and returns the corresponding RdfResource object from your knowledge base.  On this object you can call the methods as described in the section [Resource](Resource).    **Example:**  ```  {{'<http://www.ifi.uio.no/INF3580/simpsons>' | rdf_get }}  ```    **Result:**  ```html  http://www.ifi.uio.no/INF3580/simpsons  ```    ### rdf_property  **Synopsis:** `<rdf_resource> OR <rdf_resource_string> | rdf_property: <property>, [<lang>] OR [<lang>, <list>] OR [nil, <list>]`    **Parameters:**  - `<rdf_resource>` is an RdfResource. To reference the resource of the current page use `page.rdf`, `page`, or `nil`.  - `<rdf_resource_string>` is a String representing the IRI of `<rdf_resource>`.  - `<property>` is a string representing an RDF predicate, with prefix (`prefix:name`) or a full IRI (`<http://ex.org/name>`).  - `<lang>` is a language tag (e.g. `de`). If this parameter is omitted replace it by `nil`.  - `<list>` is a boolean value (`true`, `false`).    **Description:** Returns the object, of the triple `<rdf_resource> <predicate> ?object`.  The returned object can by any of the kind, resource, literal, or blanknode.    **Example (default):**  ```  {assign resource = '<http://www.ifi.uio.no/INF3580/simpsons#Homer>' | rdf_get }  {{ resource | rdf_property: '<http://xmlns.com/foaf/0.1/job>' }}  ```  **Result:**  ```html  ""unknown""  ```  **Example (string):**  ```  {{ '<http://www.ifi.uio.no/INF3580/simpsons#Homer>' | rdf_property: '<http://xmlns.com/foaf/0.1/job>' }}  ```  **Result:**  ```html  ""unknown""    ```    **Example (with language):**  ```  {assign resource = '<http://www.ifi.uio.no/INF3580/simpsons#Homer>' | rdf_get }  {{ resource | rdf_property: '<http://xmlns.com/foaf/0.1/job>', 'de' }}  ```  **Result:**  ```html  ""unbekannt""  ```    **Example (return as list):**  ```  {assign resource = '<http://www.ifi.uio.no/INF3580/simpsons#Homer>' | rdf_get }  {% assign resultset = resource | rdf_property: '<http://xmlns.com/foaf/0.1/job>', nil, true %}  {% for result in resultset %}  <li>{{ result }}</li>  {% endfor %}  ```  **Result:**  ```html  <li>""unknown""</li>  <li>""unbekannt""</li>  <li>""unbekannter Job 2""</li>  <li>""unknown Job 2""</li>  ```    ### rdf_inverse_property  **Synopsis:** `<rdf_resource> OR <rdf_resource_string>| rdf_inverse_property: <property>, [<list>]`    **Parameters:**  - `<rdf_resource>` is an RdfResource. To reference the resource of the current page use `page.rdf`, `page`, or `nil`.  - `<rdf_resource_string>` is a String representing the IRI of `<rdf_resource>`.  - `<property>` is a string representing an RDF predicate, with prefix (`prefix:name`) or a full IRI (`<http://ex.org/name>`).  - `<list>` is a boolean value (`true`, `false`).    **Description:** Same as rdf_property, but in inverse direction.  It returns the subject, of the triple `?subject <predicate> <rdf_resource>`.  The returned object can by any of the kind, resource, or blanknode.    **Examples (default):**  ```  {assign resource = '<http://www.ifi.uio.no/INF3580/simpsons#Homer>' | rdf_get }  {{ page.rdf | rdf_inverse_property: '<http://www.ifi.uio.no/INF3580/family#hasFather>' }}  ```  **Result:**  ```html  http://www.ifi.uio.no/INF3580/simpsons#Bart  ```    **Examples (string):**  ```  {{ '<http://www.ifi.uio.no/INF3580/simpsons#Homer>' | rdf_inverse_property: '<http://www.ifi.uio.no/INF3580/family#hasFather>' }}  ```  **Result:**  ```html  http://www.ifi.uio.no/INF3580/simpsons#Bart  ```      **Example (as list):**  ```  {assign resource = '<http://www.ifi.uio.no/INF3580/simpsons#Homer>' | rdf_get }  {% assign resultset = resource | rdf_property: '<http://www.ifi.uio.no/INF3580/family#hasFather>', true %}  {% for result in resultset %}  <li>{{ result }}</li>  {% endfor %}  ```  **Result:**  ```html  http://www.ifi.uio.no/INF3580/simpsons#Bart  http://www.ifi.uio.no/INF3580/simpsons#Lisa  http://www.ifi.uio.no/INF3580/simpsons#Maggie  ```    ### sparql_query  **Synopsis:** `<rdf_resource> | sparql_query: <query>` **OR** `<reference_array> | sparql_query: <query>` **OR** `<query> | sparql_query`    **Parameters:**  - `<rdf_resource>` is an RdfResource which will replace `?resourceUri` in the query. To omit this parameter or reference the resource of the current page use `page.rdf`, `page`, or `nil`.  - `<reference_array>` an array containing IRIs as Strings or `rdf_resource`. They will consecutively replace each `?resourceUri_<index>` in your query.  - `<query>` a string containing a SPARQL query.    **Description:** Evaluates `query` on the given knowledge base and returns an array of results (result set).  Each entry object in the result set (result) contains the selected variables as resources or literals.  You can use `?resourceUri` inside the query to reference the resource which is given as `<rdf_resource>`.    **Example (page)**  ```  <!--Rendering the page of resource Lisa -->  {% assign query = 'SELECT ?sub ?pre WHERE { ?sub ?pre ?resourceUri }' %}  {% assign resultset = page | sparql_query: query %}  <table>  {% for result in resultset %}    <tr><td>{{ result.sub }}</td><td>{{ result.pre }}</td></tr>  {% endfor %}  </table>  ```  **Result:**  ```html  <table>  <tr><td>http://www.ifi.uio.no/INF3580/simpsons#TheSimpsons</td><td>http://www.ifi.uio.no/INF3580/family#hasFamilyMember</td></tr>  <tr><td>http://www.ifi.uio.no/INF3580/simpsons#Bart</td><td>http://www.ifi.uio.no/INF3580/family#hasSister</td></tr>  <tr><td>http://www.ifi.uio.no/INF3580/simpsons#Maggie</td><td>http://www.ifi.uio.no/INF3580/family#hasSister</td></tr>  ...  ```    **Example (array)**  ```  {% assign query = 'SELECT ?x WHERE {?resourceUri_0 ?x ?resourceUri_1}' %}  {% assign array = ""<http://www.ifi.uio.no/INF3580/simpsons#Homer>,<http://www.ifi.uio.no/INF3580/simpsons#Marge>"" | split: %}  {% assign resultset = array | sparql_query: query %}  <table>  {% for result in resultset %}    <tr><td>{{ result.x }}</td></tr>  {% endfor %}  </table>  ```  **Result:**  ```  <table>    <tr><td>http://www.ifi.uio.no/INF3580/family#hasSpouse</td></tr>  </table>  ```    **Example (query)**  ```  {% assign query = 'SELECT ?x WHERE {<http://www.ifi.uio.no/INF3580/simpsons#Homer> ?x <http://www.ifi.uio.no/INF3580/simpsons#Marge>}' %}  {% assign resultset = query | sparql_query %}  <table>  {% for result in resultset %}    <tr><td>{{ result.x }}</td></tr>  {% endfor %}  </table>  ```    **Result:**  ```  <table>    <tr><td>http://www.ifi.uio.no/INF3580/family#hasSpouse</td></tr>  </table>  ```    ### rdf_container  **Synopsis:** `<rdf_container_head> **OR** <rdf_container_head_string> | rdf_container`    **Parameters:**  - `<rdf_container_head>` is an RdfResource. To reference the resource of the current page use `page.rdf`, `page`, or `nil`.  - `<rdf_container_head_string>` is a String representing the IRI of `<rdf_container_head>`.    **Description:** Returns an array with resources for each element in the container whose head is referenced by `rdf_container_head`.    **Examples:**  ```  {% assign resource = '<http://www.ifi.uio.no/INF3580/simpson-container#Container>' | rdf_get %}  {% assign array = resource | rdf_container %}  {% for item in array %}  {{ item }}  {% endfor %}  ```  ###### Result:  ```html  http://www.ifi.uio.no/INF3580/simpsons#Homer  http://www.ifi.uio.no/INF3580/simpsons#Marge  http://www.ifi.uio.no/INF3580/simpsons#Bart  http://www.ifi.uio.no/INF3580/simpsons#Lisa  http://www.ifi.uio.no/INF3580/simpsons#Maggie    ```    **Examples: (string)**  ```  {% assign array = '<http://www.ifi.uio.no/INF3580/simpson-container#Container>' | rdf_container %}  {% for item in array %}  {{ item }}  {% endfor %}  ```  ###### Result:  ```html  http://www.ifi.uio.no/INF3580/simpsons#Homer  http://www.ifi.uio.no/INF3580/simpsons#Marge  http://www.ifi.uio.no/INF3580/simpsons#Bart  http://www.ifi.uio.no/INF3580/simpsons#Lisa  http://www.ifi.uio.no/INF3580/simpsons#Maggie    ```    ### rdf_collection  **Synopsis:** `<rdf_collection_head> OR <rdf_collection_head_string> | rdf_collection` **OR** `<rdf_resource> | rdf_collection: ""<property>""`    **Parameters:**  - `<rdf_collection_head>` is an RdfResource. To reference the resource of the current page use `page.rdf`, `page`, or `nil`.  - `<rdf_collection_head_string>` is a String representing the IRI of `<rdf_collection_head>`.  - `<rdf_resource>` is an RdfResource. To reference the resource of the current page use `page.rdf`, `page`, or `nil`.  - `<property>` is a string representing an RDF predicate, with prefix (`prefix:name`) or a full IRI (`<http://ex.org/name>`).    **Description:** Returns an array with resources for each element in the collection whose head is referenced by `rdf_collection_head`.  Instead of directly referencing a head it is also possible to specify the property referencing the collection head.    **Example (specify head resource):**  ```  {% assign resource = '<http://www.ifi.uio.no/INF3580/simpson-collection#Collection>' | rdf_get %}  {% assign array = resource | rdf_collection %}  {% for item in array %}  {{ item }}  {% endfor %}  ```  **Result:**  ```html  http://www.ifi.uio.no/INF3580/simpsons#Homer  http://www.ifi.uio.no/INF3580/simpsons#Marge  http://www.ifi.uio.no/INF3580/simpsons#Bart  http://www.ifi.uio.no/INF3580/simpsons#Lisa  http://www.ifi.uio.no/INF3580/simpsons#Maggie  ```  **Example (specify head string):**  ```  {% assign array = '<http://www.ifi.uio.no/INF3580/simpson-collection#Collection>' | rdf_collection %}  {% for item in array %}  {{ item }}  {% endfor %}  ```  **Result:**  ```html  http://www.ifi.uio.no/INF3580/simpsons#Homer  http://www.ifi.uio.no/INF3580/simpsons#Marge  http://www.ifi.uio.no/INF3580/simpsons#Bart  http://www.ifi.uio.no/INF3580/simpsons#Lisa  http://www.ifi.uio.no/INF3580/simpsons#Maggie  ```  **Example (specify via property):**  ```  {% assign resource = '<http://www.ifi.uio.no/INF3580/simpsons>' | rdf_get %}  {% assign array = resource | rdf_collection: ""<http://www.ifi.uio.no/INF3580/simpsons#familycollection>"" %}  {% for item in array %}  {{ item }}  {% endfor %}  ```  **Result:**  ```html  http://www.ifi.uio.no/INF3580/simpsons#Homer  http://www.ifi.uio.no/INF3580/simpsons#Marge  http://www.ifi.uio.no/INF3580/simpsons#Bart  http://www.ifi.uio.no/INF3580/simpsons#Lisa  http://www.ifi.uio.no/INF3580/simpsons#Maggie  ```    ## Plugin Configuration (\_config.yml)  |Name|Parameter|Default|Description|Example|  |---	|---	|---	|---	|---	|  |path|Relative path to the RDF-File|no default|Specifies the path to the RDF file from where you want to render the website|```path: ""rdf-data/simpsons.ttl""```|  |remote|Section to specify a remote data source|no default|Has to contain the `endpoint` key. The `remote` paramter overrides the `path` parameter.||  |remote > endpoint|SPARQL endpoint to get the data from|no default|Specifies the URL to the SPARQL endpoint from where you want to render the website|```remote: endpoint: ""http://localhost:5000/sparql/""```|  |language|Language-Tag as String|no default|Specifies the preferred language when you select objects using our Liquid filters|```language: ""en""```|  |include_blank|Boolean-Expression|false|Specifies whether blank nodes should also be rendered or not|```include_blank: true```|  |restriction|SPARQL-Query as String or subjects/objects/predicates|no default|Restricts the resource-selection with a given SPARQL-Query to the results bound to the special variable `?resourceUri` or the three keywords `subjects` (only subject URIs), `objects`, `predicates`|```restriction: ""SELECT ?resourceUri WHERE { ?resourceUri <http://www.ifi.uio.no/INF3580/family#hasFather> <http://www.ifi.uio.no/INF3580/simpsons#Homer> }""```|  |restriction_file|File of resources to be rendered|no default|Restricts the resource-selection to the list of resources in the file|```restriction_file: _data/restriction.txt```|  |default_template|Filename of the default RDF-template in _layouts directory|no default|Specifies the template-file you want Jekyll to use to render all RDF resources|```default_template: ""rdf_index.html""```|  |instance_template_mappings|Target URI as String : filename of the template as String|no default|Maps given URIs to template-files for rendering an individual instance|```instance_template_mappings: ""http://www.ifi.uio.no/INF3580/simpsons#Abraham"": ""abraham.html""```|  |class_template_mappings|Target URI as String : filename of the template as String|no default|Maps given URIs to template-files for rendering all instances of that class|```class_template_mappings: ""http://xmlns.com/foaf/0.1/Person"": ""person.html""```|    # Development    ## Installation from source  To install the project with the git-repository you will need `git` on your system. The first step is just cloning the repository:  ```  git clone git@github.com:white-gecko/jekyll-rdf.git  ```  A folder named `jekyll-rdf` will be automatically generated. You need to switch into this folder and compile the ruby gem to finish the installation:  ```  cd jekyll-rdf  gem build jekyll-rdf.gemspec  gem install jekyll-rdf -*.gem  ```    ## Run tests  ```  bundle exec rake test  ```    ## Test page  Everytime the tests are executed, the Jekyll page inside of `test/source` gets processed. Start a slim web server to watch the results in web browser, e.g. Pythons `SimpleHTTPServer` (Python 2, for Python 3 it's `http.server`):  ```  cd test/source/_site  python -m SimpleHTTPServer 8000  ```    ## Build API Doc  To generate the API Doc please navigate to `jekyll-rdf/lib` directory and run  ```  gem install yard  yardoc *  ```    The generated documentation is placed into `jekyll-rdf/lib/doc` directory.    # License  jekyll-rdf is licensed under the [MIT license](https://github.com/DTP16/jekyll-rdf/tree/master/LICENSE). """
Semantic web;https://github.com/AKSW/IGUANA;"""[![GitLicense](https://gitlicense.com/badge/dice-group/IGUANA)](https://gitlicense.com/license/dice-group/IGUANA)  ![Java CI with Maven](https://github.com/dice-group/IGUANA/workflows/Java%20CI%20with%20Maven/badge.svg)[![BCH compliance](https://bettercodehub.com/edge/badge/AKSW/IGUANA?branch=master)](https://bettercodehub.com/)  [![Codacy Badge](https://api.codacy.com/project/badge/Grade/9668460dd04c411fab8bf5ee9c161124)](https://www.codacy.com/app/TortugaAttack/IGUANA?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=AKSW/IGUANA&amp;utm_campaign=Badge_Grade)  [![Project Stats](https://www.openhub.net/p/iguana-benchmark/widgets/project_thin_badge.gif)](https://www.openhub.net/p/iguana-benchmark)      # IGUANA    <img src = ""https://github.com/dice-group/IGUANA/raw/develop/images/IGUANA_logo.png"" alt = ""IGUANA Logo"" width = ""400"" align = ""center"">    ## ABOUT      Semantic Web is becoming more important and it's data is growing each day. Triple stores are the backbone here, managing these data.  Hence it is very important that the triple store must scale on the data and can handle several users.   Current Benchmark approaches could not provide a realistic scenario on realistic data and could not be adjustet for your needs very easily.  Additionally Question Answering systems and Natural Language Processing systems are becoming more and more popular and thus needs to be stresstested as well.  Further on it was impossible to compare results for different benchmarks.     Iguana is an an Integerated suite for benchmarking read/write performance of HTTP endpoints and CLI Applications.</br>  which solves all these issues.   It provides an enviroment which ...      + ... is highly configurable  + ... provides a realistic scneario benchmark  + ... works on every dataset  + ... works on SPARQL HTTP endpoints  + ... works on HTTP Get & Post endpoints  + ... works on CLI applications  + and is easily extendable      For further Information visit    [iguana-benchmark.eu](http://iguana-benchmark.eu)     [Documentation](http://iguana-benchmark.eu/docs/3.3/)      # Getting Started    # Prerequisites     You need to install Java 11 or greater.  In Ubuntu you can install these using the following commands    ```  sudo apt-get install java  ```    # Iguana Modules    Iguana consists of two modules    1. **corecontroller**: This will benchmark the systems   2. **resultprocessor**: This will calculate the Metrics and save the raw benchmark results     ## **corecontroller**    The **corecontroller** will benchmark your system. It should be started on the same machine the  is started.    ## **resultprocessor**    The **resultprocessor** will calculate the metrics.  By default it stores its result in a ntriple file. But you may configure it, to write the results directly to a Triple Store.   On the processing side, it calculates various metrics.    Per run metrics:  * Query Mixes Per Hour (QMPH)  * Number of Queries Per Hour (NoQPH)  * Number of Queries (NoQ)  * Average Queries Per Second (AvgQPS)    Per query metrics:  * Queries Per Second (QPS)      * Number of successful and failed queries      * result size      * queries per second      * sum of execution times    You can change these in the Iguana Benchmark suite config.    If you use the [basic configuration](https://github.com/dice-group/IGUANA/blob/master/example-suite.yml), it will save all mentioned metrics to a file called `results_{{DATE_RP_STARTED}}.nt`      # Setup Iguana    ## Download  Please download the release zip **iguana-x.y.z.zip** from the newest release available [here](https://github.com/dice-group/IGUANA/releases/latest):    ```  mkdir iguana  wget https://github.com/dice-group/IGUANA/releases/download/v3.3.0/iguana-3.3.0.zip  unzip iguana-3.3.0.zip  ```      It contains the following files:    * iguana.corecontroller-X.Y.Z.jar  * start-iguana.sh  * example-suite.yml    # Run Your Benchmarks    ## Create a Configuration    You can use the [basic configuration](https://github.com/dice-group/IGUANA/blob/master/example-suite.yml) we provide and modify it to your needs.  For further information please visit our [configuration](http://iguana-benchmark.eu/docs/3.2/usage/configuration/) and [Stresstest](http://iguana-benchmark.eu/docs/3.0/usage/stresstest/) wiki pages. For a detailed, step-by-step instruction please attend our [tutorial](http://iguana-benchmark.eu/docs/3.2/usage/tutorial/).        ## Execute the Benchmark    Use the start script   ```  ./start-iguana.sh example-suite.yml  ```  Now Iguana will execute the example benchmark suite configured in the example-suite.yml file      # How to Cite    ```bibtex  @InProceedings{10.1007/978-3-319-68204-4_5,  author=""Conrads, Felix  and Lehmann, Jens  and Saleem, Muhammad  and Morsey, Mohamed  and Ngonga Ngomo, Axel-Cyrille"",  editor=""d'Amato, Claudia  and Fernandez, Miriam  and Tamma, Valentina  and Lecue, Freddy  and Cudr{\'e}-Mauroux, Philippe  and Sequeda, Juan  and Lange, Christoph  and Heflin, Jeff"",  title=""Iguana: A Generic Framework for Benchmarking the Read-Write Performance of Triple Stores"",  booktitle=""The Semantic Web -- ISWC 2017"",  year=""2017"",  publisher=""Springer International Publishing"",  address=""Cham"",  pages=""48--65"",  abstract=""The performance of triples stores is crucial for applications driven by RDF. Several benchmarks have been proposed that assess the performance of triple stores. However, no integrated benchmark-independent execution framework for these benchmarks has yet been provided. We propose a novel SPARQL benchmark execution framework called Iguana. Our framework complements benchmarks by providing an execution environment which can measure the performance of triple stores during data loading, data updates as well as under different loads and parallel requests. Moreover, it allows a uniform comparison of results on different benchmarks. We execute the FEASIBLE and DBPSB benchmarks using the Iguana framework and measure the performance of popular triple stores under updates and parallel user requests. We compare our results (See https://doi.org/10.6084/m9.figshare.c.3767501.v1) with state-of-the-art benchmarking results and show that our benchmark execution framework can unveil new insights pertaining to the performance of triple stores."",  isbn=""978-3-319-68204-4""  }  ``` """
Semantic web;https://github.com/d2rq/r2rml-kit;"""# r2rml-kit – Export relational databases to RDF with R2RML    **r2rml-kit** is an implementation of W3C's [R2RML](https://www.w3.org/TR/r2rml/) and [Direct Mapping](https://www.w3.org/TR/rdb-direct-mapping/) standards. It can:    - Generate an R2RML mapping by inspecting a relational database schema  - Validate R2RML mapping files  - Dump the contents of a database to an RDF file according to an R2RML mapping  - Dump the contents of a database to an RDF file according to the W3C Direct Mapping  - Access the contents of a database through the Jena API    Besides R2RML, **r2rml-kit** also supports the [D2RQ mapping language](http://d2rq.org/d2rq-language).    **r2rml-kit** is an offshoot of [D2RQ](http://d2rq.org/), based on its abandoned `develop` branch. Unlike D2RQ, it does not support SPARQL, and does not include a server application equivalent to D2RQ's D2R Server.    **r2rml-kit** is currently in pre-alpha stage. It is not yet fully separated from the D2RQ codebase, and many things will not yet work. It does not support R2RML's named graph features. See [`TODO.md`](https://github.com/d2rq/r2rml-kit/blob/master/TODO.md) for a short-term roadmap.    ## Running r2rml-kit    After building with `mvn compile`, you can test-run the various components. Let's assume you have a MySQL database called `mydb` on your machine.    ### Generating a default mapping file    ```./generate-mapping -u root -o mydb.ttl jdbc:mysql:///mydb```    This generates a mapping file `mydb.ttl` for your database.    ### Validating an R2RML mapping    ```./validate mydb.ttl```    This validates the mapping file `mydb.ttl`.    ### Dumping the database    ```./dump-rdf -m mydb.ttl -o dump.nt```    This creates `dump.nt`, a dump containing the mapped RDF in N-Triples format.    ### Running the unit tests    The unit tests can be executed with `mvn test`.    Some unit tests rely on MySQL being present, and require that two databases are created:    1. A database called `iswc` that contains the data from `src/test/resources/example/iswc-mysql.sql`:        echo ""CREATE DATABASE iswc"" | mysql -u root      mysql -u root iswc < src/test/resources/example/iswc-mysql.sql    2. An empty database called `D2RQ_TEST`. """
Semantic web;https://github.com/vastix/blazegraph-service;"""Vert.x Blazegraph Service  =========================    This service provides an asynchronous interface around Blazegraph.    Before use this service, place it into Maven local repo using this task:    ```  $ gradle publishToMavenLocal  ```    If you want to use this service from Maven local, use this in your dependencies part of your `build.gradle` file:    ```  dependencies {      ...      ...      compile 'name.bpdp.vertx:blazegraph-service:1.0.0'      ...      ...  }  ``` """
Semantic web;https://github.com/joshsh/rdfagents;"""RDFAgents is a messaging protocol for real-time, peer-to-peer knowledge sharing on the Semantic Web. It patterned after, and designed to interoperate with Linked Data, but extends Linked Data principles to support a ""word of mouth"" style of information discovery over a variety of communications protocols. It is geared towards ubiquitous Semantic Web computing, lightweight devices with variable network connectivity, and highly reactive, event-driven interfaces to Semantic Web services.    Read more in the [RDFAgents specification](http://fortytwo.net/2011/rdfagents/spec).   """
Semantic web;https://github.com/cavendish-ldp/cavendish;"""# Cavendish  ## LDP on BlazeGraph  Cavendish endeavours to be both an implementation of [LDP](https://www.w3.org/TR/ldp/) and a laboratory for elaborating [Fedora 4](http://duraspace.org/about_fedora) APIs as extensions of the LDP specification. We are also interested in better understanding how the constituent frameworks of a Fedora 4 implementation influence the way its core model is understood- in this case, building a Fedora on a triplestore. Cavendish is built on [BlazeGraph](https://www.blazegraph.com/).  ## Running Cavendish  ```bash  cd cavendish  mvn install  cd cavendish-jetty  JETTY_HOME=file:`pwd`/src/main/webapp  BG_CONFIG=`pwd`/src/main/webapp/WEB-INF/RWStore.properties  JETTY_XML=`pwd`/src/main/resources/jetty.xml  mvn exec:java -Djetty.home=$JETTY_HOME -Dbigdata.propertyFile=$BG_CONFIG -DjettyXml=$JETTY_XML  ```  ## Colophon  > By this Poetical Description, you may perceive, that my ambition is not onely to be Empress, but Authoress of a whole World;  > and that the Worlds I have made, both the Blazing- and the other Philosophical World, mentioned in the first part of this Description, are framed and composed of the most pure, that is, the Rational parts of Matter, which are the parts of my Mind;  > which Creation was more easily and suddenly effected, than the Conquests of the two famous Monarchs of the World.  >  > ... as for the Blazing-world, it having an Empress already, who rules it with great Wisdom and Conduct, which Empress is my dear Platonick Friend;  > I shall never prove so unjust, treacherous and unworthy to her, as to disturb her Government, much less to depose her from her Imperial Throne, for the sake of any other, but rather chuse to create another World for another Friend.  >  > <cite>Margaret Cavendish, in the Epilogue to [The Blazing-World](http://digital.library.upenn.edu/women/newcastle/blazing/blazing.html)</cite> """
Semantic web;https://github.com/sage-org/sage-engine;"""# Sage: a SPARQL query engine for public Linked Data providers  [![Build Status](https://travis-ci.com/sage-org/sage-engine.svg?branch=master)](https://travis-ci.com/sage-org/sage-engine) [![PyPI version](https://badge.fury.io/py/sage-engine.svg)](https://badge.fury.io/py/sage-engine) [![Docs](https://img.shields.io/badge/docs-passing-brightgreen)](https://sage-org.github.io/sage-engine/)    [Read the online documentation](https://sage-org.github.io/sage-engine/)    SaGe is a SPARQL query engine for public Linked Data providers that implements *Web preemption*. The SPARQL engine includes a smart Sage client  and a Sage SPARQL query server hosting RDF datasets using [HDT](http://www.rdfhdt.org/), [postgres](https://www.postgresql.org/), [sqlite](https://www.sqlite.org/), or [hbase](https://hbase.apache.org/)  This repository contains the **Python implementation of the SaGe SPARQL query server**.    SPARQL queries are suspended by the web server after a fixed quantum of time and resumed upon client request. Using Web preemption, Sage ensures stable response times for query execution and completeness of results under high load.    The complete approach and experimental results are available in a Research paper accepted at The Web Conference 2019, [available here](https://hal.archives-ouvertes.fr/hal-02017155/document). *Thomas Minier, Hala Skaf-Molli and Pascal Molli. ""SaGe: Web Preemption for Public SPARQL Query services"" in Proceedings of the 2019 World Wide Web Conference (WWW'19), San Francisco, USA, May 13-17, 2019*.    We appreciate your feedback/comments/questions to be sent to our [mailing list](mailto:sage@univ-nantes.fr) or [our issue tracker on github](https://github.com/sage-org/sage-engine/issues).    # Table of contents    * [Installation](#installation)  * [Getting started](#getting-started)    * [Server configuration](#server-configuration)    * [PostgreSQL configuration](#postgresql-configuration)    * [Data ingestion](#data-ingestion)    * [Starting the server](#starting-the-server)  * [Sage Docker image](#sage-docker-image)  * [Command line utilities](#command-line-utilities)  * [Documentation](#documentation)    # Installation    Installation in a [virtualenv](https://virtualenv.pypa.io/en/stable/) is **strongly advised!**    Requirements:  * Python 3.7 (*or higher*)  * [pip](https://pip.pypa.io/en/stable/)  * **gcc/clang** with **c++11 support**  * **Python Development headers**  > You should have the `Python.h` header available on your system.     > For example, for Python 3.6, install the `python3.6-dev` package on Debian/Ubuntu systems.    ## Installation using pip    The core engine of the SaGe SPARQL query server with [HDT](http://www.rdfhdt.org/) as a backend can be installed as follows:  ```bash  pip install sage-engine[hdt,postgres,hbase]  ```  The SaGe query engine uses various **backends** to load RDF datasets.  The various backends available are installed as extras dependencies. The above command install both the HDT, the PostgreSQL and the HBase backends.    ## Manual Installation using poetry    The SaGe SPARQL query server can also be manually installed using the [poetry](https://github.com/sdispater/poetry) dependency manager.  ```bash  git clone https://github.com/sage-org/sage-engine  cd sage-engine  poetry install --extras ""hdt postgres hbase""  ```  As with pip, the various SaGe backends are installed as extras dependencies, using the  `--extras` flag.    # Getting started    ## Server configuration    A SaGe server is configured using a configuration file in [YAML syntax](http://yaml.org/).  You will find below a minimal working example of such a configuration file.  Full examples are available [in the `config_examples/` directory](https://github.com/sage-org/sage-engine/blob/master/config_examples/example.yaml)    ```yaml  name: SaGe Test server  maintainer: Chuck Norris  quota: 75  max_results: 2000  graphs:  -    name: dbpedia    uri: http://example.org/dbpedia    description: DBPedia    backend: hdt-file    file: datasets/dbpedia.2016.hdt  ```    The `quota` and `max_results` fields are used to set the maximum time quantum and the maximum number of results  allowed per request, respectively.    Each entry in the `graphs` field declare a RDF dataset with a name, description, backend and options specific to this backend.  Different backends are available:  - the `hdt-file` backend allows a SaGe server to load RDF datasets from [HDT files](http://www.rdfhdt.org/). SaGe uses [pyHDT](https://github.com/Callidon/pyHDT) to load and query HDT files.  - the `postgres` backend allows a SaGe server to create, query and update RDF datasets stored in [PostgreSQL](https://www.postgresql.org/). Each dataset is stored in a single table composed of 3 columns; S (subject), P (predicate) and O (object). Tables are created with B-Tree indexes on SPO, POS and OSP. SaGe uses [psycopg2](https://pypi.org/project/psycopg2/) to interact with PostgreSQL.  - the `postgres-catalog` backend uses a different schema than `postgres` to store datasets. Triples terms are mapped to unique identifiers and a dictionary table that is common to all datasets is used to map RDF terms with their identifiers. This schema allows to reduce the space required to store datasets.  - the `sqlite` backend allows a SaGe server to create, query and update RDF datasets stored in [SQLite](https://docs.python.org/3/library/sqlite3.html). Datasets are stored using the same schema as the `postgres` backend.  - the `sqlite-catalog` is another backend for SQLite that uses a dictionary based schema as the `postgres-catalog` backend.  - the `hbase` backend allows a SaGe server to create, query and update RDF datasets stored in [HBase](https://hbase.apache.org/). To have a sorted access on dataset triples, triples are inserted three times in three different tables using SPO, POS and OSP as triples keys. SaGe uses [happybase](https://happybase.readthedocs.io/en/latest/) to interact with HBase.    ## PostgreSQL configuration    This section is optional and can be skipped if you don't use one of the PostgreSQL backends.    To ensure stable performance when using PostgreSQL with SaGe, PostgreSQL needs to be configured. Open the file `postgresql.conf` in the PostgreSQL main directory and apply the following changes in the *Planner Method Configuration* section:  - Uncomment all enable_XYZ options  - Set *enable_indexscan*, *enable_indexonlyscan* and *enable_nestloop* to **on**  - Set all the other enable_XYZ options to **off**    These changes force the PostgreSQL query optimizer to generate the desired query plan for the SaGe resume queries.    ## Data ingestion    Different executables are available to load a RDF file depending on the backend you want to use.    To load a dataset from a HDT file, just declare a new dataset in your configuration file using the `hdt-file` backend.    To load a N-Triples file using one of the `postgres`, `postgres-catalog`, `hbase`, `sqlite` and `sqlite-catalog` backends, first declare a new dataset in your configuration file. For example, to load the file `my_dataset.nt` using the `sqlite` backend, we start by declaring a new dataset named `my_dataset` in our configuration file `my_config.yaml`.    ```yaml  quota: 75  max_results: 10000  graphs:  -    name: my_dataset    uri: http://example.org/my_dataset    backend: sqlite    database: sage-sqlite.db  ```    For each backend, an example that illustrate how to declare a new dataset is available in the [`config_examples/`](https://github.com/sage-org/sage-engine/blob/master/config_examples/example.yaml) directory.    To load a file into a dataset declared using one of the `SQLite` backends, use the following commands:    ```bash  # Create the required SQLite tables to store the dataset  sage-sqlite-init --no-index my_config.yaml my_dataset  # Insert the RDF triples in SQLite  sage-sqlite-put my_dataset.nt my_config.yaml my_dataset  # Create the SPO, OSP and POS indexes  sage-sqlite-index my_config.yaml my_dataset_name  ```    To load a file into a dataset declared using one of the `PostgreSQL` backends, use the following commands:    ```bash  # Create the required PostgreSQL tables to store the dataset  sage-postgres-init --no-index my_config.yaml my_dataset  # Insert the RDF triples in PostgreSQL  sage-postgres-put my_dataset.nt my_config.yaml my_dataset  # Create the SPO, OSP and POS indexes  sage-postgres-index my_config.yaml my_dataset_name  ```    To load a file into a dataset declared using the `hbase` backend, use the following commands:    ```bash  # Create the required HBase tables to store the dataset  sage-hbase-init my_config.yaml my_dataset  # Insert the RDF triples in HBase  sage-hbase-put my_dataset.nt my_config.yaml my_dataset  ```    ## Starting the server    The `sage` executable, installed alongside the SaGe server, allows to easily start a SaGe server from a configuration file using [Uvicorn](https://www.uvicorn.org/), a Python ASGI HTTP Server.    ```bash  # launch Sage server with 4 workers on port 8000  sage my_config.yaml -w 4 -p 8000  ```    The full usage of the `sage` executable is detailed below:  ```  Usage: sage [OPTIONS] CONFIG      Launch the Sage server using the CONFIG configuration file    Options:    -p, --port INTEGER              The port to bind  [default: 8000]    -w, --workers INTEGER           The number of server workers  [default: 4]    --log-level [debug|info|warning|error]                                    The granularity of log outputs  [default:                                    info]    --help                          Show this message and exit.  ```    Once started, you can interact with the SaGe server on http://localhost:8000/docs    # SaGe Docker image    The Sage server is also available through a [Docker image](https://hub.docker.com/r/callidon/sage/).  In order to use it, do not forget to [mount in the container](https://docs.docker.com/storage/volumes/) the directory that contains you configuration file and your datasets.    ```bash  docker pull callidon/sage  docker run -v path/to/config-file:/opt/data/ -p 8000:8000 callidon/sage sage /opt/data/config.yaml -w 4 -p 8000  ```    # Documentation    To generate the documentation, navigate in the `docs` directory and generate the documentation    ```bash  cd docs/  make html  open build/html/index.html  ```    Copyright 2017-2019 - [GDD Team](https://sites.google.com/site/gddlina/), [LS2N](https://www.ls2n.fr/?lang=en), [University of Nantes](http://www.univ-nantes.fr/) """
Semantic web;https://github.com/RDFLib/rdflib;"""![](docs/_static/RDFlib.png)        RDFLib  ======  [![Build Status](https://drone.rdflib.ashs.dev/api/badges/RDFLib/rdflib/status.svg?ref=refs/heads/master)](https://drone.rdflib.ashs.dev/RDFLib/rdflib/branches)  [![Coveralls branch](https://img.shields.io/coveralls/RDFLib/rdflib/master.svg)](https://coveralls.io/r/RDFLib/rdflib?branch=master)  [![GitHub stars](https://img.shields.io/github/stars/RDFLib/rdflib.svg)](https://github.com/RDFLib/rdflib/stargazers)  [![PyPI](https://img.shields.io/pypi/v/rdflib.svg)](https://pypi.python.org/pypi/rdflib)  [![PyPI](https://img.shields.io/pypi/pyversions/rdflib.svg)](https://pypi.python.org/pypi/rdflib)    RDFLib is a pure Python package for working with [RDF](http://www.w3.org/RDF/). RDFLib contains most things you need to work with RDF, including:    * parsers and serializers for RDF/XML, N3, NTriples, N-Quads, Turtle, TriX, Trig and JSON-LD  * a Graph interface which can be backed by any one of a number of Store implementations  * store implementations for in-memory, persistent on disk (Berkeley DB) and remote SPARQL endpoints  * a SPARQL 1.1 implementation - supporting SPARQL 1.1 Queries and Update statements  * SPARQL function extension mechanisms    ## RDFlib Family of packages  The RDFlib community maintains many RDF-related Python code repositories with different purposes. For example:    * [rdflib](https://github.com/RDFLib/rdflib) - the RDFLib core  * [sparqlwrapper](https://github.com/RDFLib/sparqlwrapper) - a simple Python wrapper around a SPARQL service to remotely execute your queries  * [pyLODE](https://github.com/RDFLib/pyLODE) - An OWL ontology documentation tool using Python and templating, based on LODE.    Please see the list for all packages/repositories here:    * <https://github.com/RDFLib>    ## Versions & Releases    * `6.2.0-alpha` current `master` branch  * `6.x.y` current release and support Python 3.7+ only. Many improvements over 5.0.0      * see [Releases](https://github.com/RDFLib/rdflib/releases)  * `5.x.y` supports Python 2.7 and 3.4+ and is [mostly backwards compatible with 4.2.2](https://rdflib.readthedocs.io/en/stable/upgrade4to5.html).    See <https://rdflib.dev> for the release overview.    ## Documentation  See <https://rdflib.readthedocs.io> for our documentation built from the code. Note that there are `latest`, `stable` `5.0.0` and `4.2.2` documentation versions, matching releases.    ## Installation  The stable release of RDFLib may be installed with Python's package management tool *pip*:        $ pip install rdflib    Alternatively manually download the package from the Python Package  Index (PyPI) at https://pypi.python.org/pypi/rdflib    The current version of RDFLib is 6.1.1, see the ``CHANGELOG.md`` file for what's new in this release.    ### Installation of the current master branch (for developers)    With *pip* you can also install rdflib from the git repository with one of the following options:        $ pip install git+https://github.com/rdflib/rdflib@master    or        $ pip install -e git+https://github.com/rdflib/rdflib@master#egg=rdflib    or from your locally cloned repository you can install it with one of the following options:        $ python setup.py install    or        $ pip install -e .    ## Getting Started  RDFLib aims to be a pythonic RDF API. RDFLib's main data object is a `Graph` which is a Python collection  of RDF *Subject, Predicate, Object* Triples:    To create graph and load it with RDF data from DBPedia then print the results:    ```python  from rdflib import Graph  g = Graph()  g.parse('http://dbpedia.org/resource/Semantic_Web')    for s, p, o in g:      print(s, p, o)  ```  The components of the triples are URIs (resources) or Literals  (values).    URIs are grouped together by *namespace*, common namespaces are included in RDFLib:    ```python  from rdflib.namespace import DC, DCTERMS, DOAP, FOAF, SKOS, OWL, RDF, RDFS, VOID, XMLNS, XSD  ```    You can use them like this:    ```python  from rdflib import Graph, URIRef, Literal  from rdflib.namespace import RDFS, XSD    g = Graph()  semweb = URIRef('http://dbpedia.org/resource/Semantic_Web')  type = g.value(semweb, RDFS.label)  ```  Where `RDFS` is the RDFS namespace, `XSD` the XML Schema Datatypes namespace and `g.value` returns an object of the triple-pattern given (or an arbitrary one if multiple exist).    Or like this, adding a triple to a graph `g`:    ```python  g.add((      URIRef(""http://example.com/person/nick""),      FOAF.givenName,      Literal(""Nick"", datatype=XSD.string)  ))  ```  The triple (in n-triples notation) `<http://example.com/person/nick> <http://xmlns.com/foaf/0.1/givenName> ""Nick""^^<http://www.w3.org/2001/XMLSchema#string> .`  is created where the property `FOAF.givenName` is the URI `<http://xmlns.com/foaf/0.1/givenName>` and `XSD.string` is the  URI `<http://www.w3.org/2001/XMLSchema#string>`.    You can bind namespaces to prefixes to shorten the URIs for RDF/XML, Turtle, N3, TriG, TriX & JSON-LD serializations:     ```python  g.bind(""foaf"", FOAF)  g.bind(""xsd"", XSD)  ```  This will allow the n-triples triple above to be serialised like this:   ```python  print(g.serialize(format=""turtle""))  ```    With these results:  ```turtle  PREFIX foaf: <http://xmlns.com/foaf/0.1/>  PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>    <http://example.com/person/nick> foaf:givenName ""Nick""^^xsd:string .  ```    New Namespaces can also be defined:    ```python  dbpedia = Namespace('http://dbpedia.org/ontology/')    abstracts = list(x for x in g.objects(semweb, dbpedia['abstract']) if x.language=='en')  ```    See also [./examples](./examples)      ## Features  The library contains parsers and serializers for RDF/XML, N3,  NTriples, N-Quads, Turtle, TriX, JSON-LD, RDFa and Microdata.    The library presents a Graph interface which can be backed by  any one of a number of Store implementations.    This core RDFLib package includes store implementations for  in-memory storage and persistent storage on top of the Berkeley DB.    A SPARQL 1.1 implementation is included - supporting SPARQL 1.1 Queries and Update statements.    RDFLib is open source and is maintained on [GitHub](https://github.com/RDFLib/rdflib/). RDFLib releases, current and previous  are listed on [PyPI](https://pypi.python.org/pypi/rdflib/)    Multiple other projects are contained within the RDFlib ""family"", see <https://github.com/RDFLib/>.    ## Running tests    ### Running the tests on the host    Run the test suite with `pytest`.  ```shell  pytest  ```    ### Running test coverage on the host with coverage report    Run the test suite and generate a HTML coverage report with `pytest` and `pytest-cov`.  ```shell  pytest --cov  ```    ### Running the tests in a Docker container    Run the test suite inside a Docker container for cross-platform support. This resolves issues such as installing BerkeleyDB on Windows and avoids the host and port issues on macOS.  ```shell  make tests  ```    Tip: If the underlying Dockerfile for the test runner changes, use `make build`.    ### Running the tests in a Docker container with coverage report    Run the test suite inside a Docker container with HTML coverage report.  ```shell  make coverage  ```    ### Viewing test coverage    Once tests have produced HTML output of the coverage report, view it by running:  ```shell  pytest --cov --cov-report term --cov-report html  python -m http.server --directory=htmlcov  ```    ## Contributing    RDFLib survives and grows via user contributions!  Please read our [contributing guide](https://rdflib.readthedocs.io/en/stable/developers.html) to get started.  Please consider lodging Pull Requests here:    * <https://github.com/RDFLib/rdflib/pulls>    You can also raise issues here:    * <https://github.com/RDFLib/rdflib/issues>    ## Support & Contacts  For general ""how do I..."" queries, please use https://stackoverflow.com and tag your question with `rdflib`.  Existing questions:    * <https://stackoverflow.com/questions/tagged/rdflib>    If you want to contact the rdflib maintainers, please do so via:    * the rdflib-dev mailing list: <https://groups.google.com/group/rdflib-dev>  * the chat, which is available at [gitter](https://gitter.im/RDFLib/rdflib) or via matrix [#RDFLib_rdflib:gitter.im](https://matrix.to/#/#RDFLib_rdflib:gitter.im) """
Semantic web;https://github.com/lanthaler/HydraBundle;"""HydraBundle  ==============    Hydra is a lightweight vocabulary to create hypermedia-driven Web APIs. By  specifying a number of concepts commonly used in Web APIs it renders the  creation of generic API clients possible.    This is a [Symfony2](http://www.symfony.com/) bundle which shows how easily  Hydra can be integrated in modern Web frameworks. It acts as a proof of  concept to show how Hydra can simplify the implementation of interoperable  and evolvable RESTful APIs.    ***WARNING: This is highly experimental stuff that isn't ready for  production use yet.***    To participate in the development of this bundle, please file bugs and  issues in the issue tracker or submit pull requests. If you have questions  regarding Hydra in general, join the  [Hydra W3C Community Group](http://bit.ly/HydraCG).    You can find an online demo of this bundle as well as more information about  Hydra on my homepage:  http://www.markus-lanthaler.com/hydra      Installation  ------------    You can install this bundle by running        composer require ml/hydra-bundle dev-master    or by adding the package to your composer.json file directly    ```json  {      ""require"": {          ""ml/hydra-bundle"": ""dev-master""      }  }  ```    After you have installed the package, you just need to add the bundle  to your `AppKernel.php` file:    ```php  // in AppKernel::registerBundles()  $bundles = array(      // ...      new ML\HydraBundle\HydraBundle(),      // ...  );  ```    and import the routes in your `routing.yml` file:    ```yaml  hydra:      resource: ""@HydraBundle/Controller/""      type:     annotation      prefix:   /  ```      Credits  ------------    This bundle heavily uses the  [Doctrine Common project](http://www.doctrine-project.org/projects/common.html)  and is inspired by its  [object relational mapper](http://www.doctrine-project.org/projects/orm.html).  The code generation is based on Sensio's  [SensioGeneratorBundle](https://github.com/sensio/SensioGeneratorBundle). """
Semantic web;https://github.com/ML-Schema/core;"""# ML Schema    ML-Schema is a collaborative, community effort with a mission to develop, maintain, and promote standard schemas for data mining and machine learning algorithms, datasets, and experiment    See the wiki for more details: https://github.com/ML-Schema/core/wiki    **Guidelines**  * Open GitHub issues for any kind of discussion. Selected items will be discussed during telco's. For instance, see how this works for schema.org  * Store all schema in http://www.w3.org/TR/turtle/. This you can load in Protege, Jena,... if you want to edit them locally.  * Use the GitHub Wiki for information relevant to the group and users: https://github.com/ML-Schema/core/wiki  * Include structure (inheritance, value bounds) so arrive at a non-ambiguous schema.  * We use namespace 'mls' and will register this at PURL asap.  * If there is an issue to discuss, first create a GitHub issue, and then label it as 'discuss in call'  Conference calls    **Conference calls**    We do a conference call on Hangout every two weeks, on Monday 13:30 - 14:30 (European time zone)  * Next calls 2015: Nov 9, Nov 23, Dec 7, Dec 21  * Next calls 2016: Jan 4, Jan 18, Feb 1, ...    **Google group**    To receive updates about ML Schema, or contact everybody, please join our Google group: https://groups.google.com/forum/#!forum/mlw3c    When you join, please send a short introduction of yourself, e.g. who are you, what is your interest in ML schema, what would you like to contribute,... """
Semantic web;https://github.com/AKSW/FOX;"""[4]: https://dice-research.org/FOX  [6]: https://fox.demos.dice-research.org/    [![Build Status](https://travis-ci.org/dice-group/FOX.svg?branch=master)](https://travis-ci.org/dice-group/FOX)  [![BCH compliance](https://bettercodehub.com/edge/badge/dice-group/FOX?branch=master)](https://bettercodehub.com/)  [![Project Stats](https://www.openhub.net/p/FOX-Framework/widgets/project_thin_badge.gif)](https://www.openhub.net/p/FOX-Framework)  <!---  [![Codacy Badge](https://api.codacy.com/project/badge/Grade/348e14317ea140cbb98a110c40718d88)](https://www.codacy.com/app/renespeck/FOX?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=dice-group/FOX&amp;utm_campaign=Badge_Grade)  -->    ## FOX - Federated Knowledge Extraction Framework  FOX ([https://dice-research.org/FOX][4]) is a framework that integrates the Linked Data Cloud and makes use of the diversity of NLP algorithms to extract RDF triples of high accuracy out of NL.    In its current version, it integrates and merges the results of Named Entity Recognition tools as well as it integrates several Relation Extraction tools.    ## Requirements  Java 8, Maven 3, graphviz (for JavaDoc only)    ## Documentation:  [documentation](documentation/readme.md).    ## Demo  This version supports multiple languages for NER, NED and RE.    Live Demo: [https://fox.demos.dice-research.org/][6]    ## How to cite    English version with details:    ```Tex  @incollection{    year={2014},    isbn={978-3-319-11963-2},    booktitle={The Semantic Web – ISWC 2014},    volume={8796},    series={Lecture Notes in Computer Science},    title={Ensemble Learning for Named Entity Recognition},    publisher={Springer International Publishing},    author={Ren{\'e} Speck and Axel-Cyrille {Ngonga Ngomo}},  }  ```    The extended version for multiple languages:    ```Tex  @InProceedings{speck2017,     author={Ren{\'e} Speck and Axel-Cyrille {Ngonga Ngomo}},     title={{Ensemble Learning of Named Entity Recognition Algorithms using Multilayer Perceptron for the Multilingual Web of Data}},     booktitle={K-CAP 2017: Knowledge Capture Conference},     year={2017},     pages={4},     organization={ACM}   }   ```    ## License    FOX is licensed under the [GNU Affero General Public License v3.0](LICENSE) (license document is in the application folder).    FOX uses several other libraries. An incomplete list is as follows:  * Illinois NLP Pipeline  (University of Illinois Research and Academic Use License)  * Stanford CoreNLP (GNU GPL Version 2)  * Apache OpenNLP (Apache License, Version 2)  * Balie (GNU GPL Version 2)      ## Bugs  Found a :bug: bug? [Open an issue](https://github.com/dice-group/FOX/issues/new) with some [emojis](http://emoji.muan.co). Issues without emojis are not valid. :trollface: """
Semantic web;https://github.com/tarql/tarql;"""# Tarql: SPARQL for Tables    Tarql is a command-line tool for converting CSV files to RDF using SPARQL 1.1 syntax. It's written in Java and based on Apache ARQ.    **See http://tarql.github.io/ for documentation.**    ## Building    Get the code from GitHub: http://github.com/tarql/tarql    Tarql uses Maven. To create executable scripts for Windows and Unix in `/target/appassembler/bin/tarql`:        mvn package appassembler:assemble    Otherwise it's standard Maven. """
Semantic web;https://github.com/mhausenblas/mrlin;"""# mrlin - MapReduce processing of Linked Data    > ...because it's magic    The basic idea of **mrlin** is to enable **M**ap **R**educe processing of **Lin**ked Data - hence the name. In the following I'm going to show you first to how to use HBase to store Linked Data with RDF, and then how to use Hadoop to run MapReduce jobs.    ## Background    ### Dependencies    * You'll need [Apache HBase](http://hbase.apache.org/) first. I downloaded [`hbase-0.94.2.tar.gz`](http://ftp.heanet.ie/mirrors/www.apache.org/dist/hbase/stable/hbase-0.94.2.tar.gz) and followed the [quickstart](http://hbase.apache.org/book/quickstart.html) up to section 1.2.3. to set it up.  * The mrlin Python scripts depend on:   * [Happybase](https://github.com/wbolster/happybase) to manage HBase; see also the [docs](http://happybase.readthedocs.org/en/latest/index.html) for further details.   * [mrjob](https://github.com/Yelp/mrjob) to run MapReduce jobs; see also the [docs](http://packages.python.org/mrjob/) for further details.    ### Representing RDF triples in HBase  Learn about how mrlin represents [RDF triples in HBase](https://github.com/mhausenblas/mrlin/wiki/RDF-in-HBase).    ### RESTful Interaction with HBase  Dig into [RESTful interactions](https://github.com/mhausenblas/mrlin/wiki/RESTful-interaction) with HBase, in mrlin.    ## Usage    ### Setup  I assume you have HBase installed in some directory `HBASE_HOME` and mrlin in some other directory `MRLIN_HOME`. First let's make sure that Happybase is installed correctly - we will use a [virtualenv](http://pypi.python.org/pypi/virtualenv ""virtualenv 1.8.2 : Python Package Index""). You only need to do this once: go to `MRLIN_HOME` and type:    	$ virtualenv hb    Time to launch HBase and the Thrift server: in the `HBASE_HOME` directory, type the following:    	$ ./bin/start-hbase.sh   	$ ./bin/hbase thrift start -p 9191    OK, now we're ready to launch mrlin - change to the directory `MRLIN_HOME` and first activate the virtualenv we created earlier:    	$ source hb/bin/activate    You should see a change in the prompt to something like `(hb)michau@~/Documents/dev/mrlin$` ... and this means we're good to go!    ### Import RDF/NTriples  To import  [RDF NTriples](http://www.w3.org/TR/rdf-testcases/#ntriples) documents, use the [`mrlin import`](https://raw.github.com/mhausenblas/mrlin/master/mrlin_import.py) script.    First, try to import **a file** from the local filesystem. Note the second parameter (`http://example.org/`), which specifies the target graph URI to import into:    	$ (hb)michau@~/Documents/dev/mrlin$ python mrlin_import.py data/test_0.ntriples http://example.org/    If this works, try to import directly from **a URL** `http://dbpedia.org/data/Galway.ntriples`:    	(hb)michau@~/Documents/dev/mrlin$ python mrlin_import.py http://dbpedia.org/data/Galway.ntriples http://dbpedia.org/  	2012-10-30T08:56:21 Initialized mrlin table.  	2012-10-30T08:56:31 Importing RDF/NTriples from URL http://dbpedia.org/data/Galway.ntriples into graph http://dbpedia.org/  	2012-10-30T08:56:31 == STATUS ==  	2012-10-30T08:56:31  Time to retrieve source: 9.83 sec  	2012-10-30T08:56:31 == STATUS ==  	2012-10-30T08:56:31  Time elapsed since last checkpoint:  0.07 sec  	2012-10-30T08:56:31  Import speed: 1506.61 triples per sec  	2012-10-30T08:56:31 == STATUS ==  	2012-10-30T08:56:31  Time elapsed since last checkpoint:  0.02 sec  	2012-10-30T08:56:31  Import speed: 4059.10 triples per sec  	2012-10-30T08:56:31 ==========  	2012-10-30T08:56:31 Imported 233 triples.    Note that you can also import **an entire directory** (mrlin will look for `.nt` and `.ntriples` files):  	  	(hb)michau@~/Documents/dev/mrlin$ python mrlin_import.py data/ http://example.org/  	2012-10-30T03:55:18 Importing RDF/NTriples from directory /Users/michau/Documents/dev/mrlin/data into graph http://example.org/  	...  	  To reset the HBase table (and remove all triples from it), use the [`mrlin utils`](https://raw.github.com/mhausenblas/mrlin/master/mrlin_utils.py) script like so:    	(hb)michau@~/Documents/dev/mrlin$ python mrlin_utils.py clear    ### Query  In order to query the mrlin datastore in HBase, use the [`mrlin query`](https://raw.github.com/mhausenblas/mrlin/master/mrlin_query.py) script:    	(hb)michau@~/Documents/dev/mrlin$ python mrlin_query.py Tribes  	2012-10-30T04:01:22 Scanning table rdf with filter ValueFilter(=,'substring:Tribes')  	2012-10-30T04:01:22 Key: http://dbpedia.org/resource/Galway - Value: {'O:148': 'u\'""City of the Tribes""\'', 'O:66': 'u\'""City of the Tribes""\'',  ...}  	2012-10-30T04:01:22 ============  	2012-10-30T04:01:22 Query took me 0.01 seconds.    ### Running MapReduce jobs    *TBD*    * setup in virtual env: `source hb/bin/activate` then `pip install mrjob`  * `cp .mrjob.conf ~` before launch  * `source hb/bin/activate`  * run `python mrlin_mr.py README.md` for standalone  * set up [Hadoop 1.0.4](http://ftp.heanet.ie/mirrors/www.apache.org/dist/hadoop/common/hadoop-1.0.4/hadoop-1.0.4.tar.gz) - if unsure follow a [single-node setup](http://orzota.com/blog/single-node-hadoop-setup-2/)  tutorial  * `cp .mrjob.conf ~` before launch if you change settings (!)  * note all changes that were necessary in ` conf/core-site.xml`, `conf/mapred-site.xml`, `conf/hdfs-site.xml`, and `hadoop-env.sh` (provide examples)  * run `python mrlin_mr.py -r hadoop README.md` for local Hadoop       #### Debug    * `tail -f hadoop-michau-namenode-Michael-Hausenblas-iMac.local.log`        ## License    All artifacts in this repository are licensed under [Apache 2.0](http://www.apache.org/licenses/LICENSE-2.0.html) Software License."""
Semantic web;https://github.com/ontop/ontop;"""[![Maven Central](https://img.shields.io/maven-central/v/it.unibz.inf.ontop/ontop.svg)](http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22it.unibz.inf.ontop%22)  [![GitHub license](https://img.shields.io/badge/license-Apache%20License%202.0-blue.svg?style=flat)](http://www.apache.org/licenses/LICENSE-2.0)  [![SourceForge](https://img.shields.io/sourceforge/dm/ontop4obda.svg)](http://sourceforge.net/projects/ontop4obda/files/)  [![Twitter](https://img.shields.io/twitter/follow/ontop4obda.svg?style=social)](https://twitter.com/ontop4obda)    | Branch    | build status  |  |-----------|---------------|  | [master](https://github.com/ontop/ontop/tree/master)  |[![Build Status](https://travis-ci.org/ontop/ontop.svg?branch=master)](https://travis-ci.org/ontop/ontop)|  | [version4](https://github.com/ontop/ontop/tree/version4) |[![Build Status](https://travis-ci.org/ontop/ontop.svg?branch=version4)](https://travis-ci.org/ontop/ontop)|      Ontop  =====    Ontop is a Virtual Knowledge Graph system.  It exposes the content of arbitrary relational databases as knowledge graphs. These graphs are virtual, which means that data remains in the data sources instead of being moved to another database.    Ontop translates [SPARQL queries](https://www.w3.org/TR/sparql11-query/) expressed over the knowledge graphs into SQL queries executed by the relational data sources. It relies on [R2RML mappings](https://www.w3.org/TR/r2rml/) and can take advantage of lightweight ontologies.    Compiling, packing, testing, etc.  --------------------    The project is a [Maven](http://maven.apache.org/) project. Compiling,  running the unit tests, building the release binaries all can be done  using maven.  Currently, we use Maven 3 and Java 8 to build the  project.      Links  --------------------    - [Official Website and Documentation](https://ontop-vkg.org)  - [SourceForge Download](http://sourceforge.net/projects/ontop4obda/files/)  - [Docker Hub](https://hub.docker.com/r/ontop/ontop-endpoint)  - [GitHub](https://github.com/ontop/ontop/)  - [GitHub Issues](https://github.com/ontop/ontop/issues)  - [Google Group](https://groups.google.com/forum/#!forum/ontop4obda)  - [Facebook](https://www.facebook.com/obdaontop/)  - [Twitter](https://twitter.com/ontop4obda)  - [Travis CI](https://travis-ci.org/ontop/ontop)    License  -------    The Ontop framework is available under the Apache License, Version 2.0    ```    Copyright (C) 2009 - 2021 Free University of Bozen-Bolzano      Licensed under the Apache License, Version 2.0 (the ""License"");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at          http://www.apache.org/licenses/LICENSE-2.0      Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an ""AS IS"" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.  ```    All documentation is licensed under the  [Creative Commons](http://creativecommons.org/licenses/by/4.0/)  (attribute)  license. """
Semantic web;https://github.com/antoniogarrote/rdfstore-js;"""#rdfstore-js [![Build Status](https://travis-ci.org/antoniogarrote/rdfstore-js.svg?branch=master)](https://travis-ci.org/antoniogarrote/rdfstore-js) [![Join the chat at https://gitter.im/antoniogarrote/rdfstore-js](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/antoniogarrote/rdfstore-js?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    ## Important Note    Many features present in versions 0.8.X have been removed in the 0.9.X. Some of them, will be added in the next versions, other like the MongoDB backend will be discarded.  Please read this README file carefully to find the current set of features.    ## Table of Contents    - [Introduction](#introduction)  - [Documentation](#documentation)  - [SPARQL support](#sparql-support)  - [Installation](#installation)  - [Building](#building)  - [Tests](#tests)  - [API](#api)  	- [Store creation](#store-creation)  	- [Query execution](#query-execution)  	- [Construct queries RDF Interfaces API](#construct-queries-rdf-interfaces-api)  	- [Loading remote graphs](#loading-remote-graphs)  	- [High level interface](#high-level-interface)  	- [RDF Interface API](#rdf-interface-api)  	- [Default Prefixes](#default-prefixes)  	- [JSON-LD Support](#json-ld-support)  	- [Events API](#events-api)  	- [Custom Filter Functions](#custom-filter-functions)  	- [Persistence](#persistence)  - [Dependencies](#dependencies)  - [Frontend](#frontend)  - [Contributing](#contributing)  - [Author](#author)  - [License](#license)      ## Introduction    rdfstore-js is a pure Javascript implementation of a RDF graph store with support for the SPARQL query and data manipulation language.  ```javascript  var rdfstore = require('rdfstore');    rdfstore.create(function(err, store) {    store.execute('LOAD <http://dbpedia.org/resource/Tim_Berners-Lee> INTO GRAPH <http://example.org/people>', function() {    	store.setPrefix('dbp', 'http://dbpedia.org/resource/');    	store.node(store.rdf.resolve('dbp:Tim_Berners-Lee'),  ""http://example.org/people"", function(err, graph) {    	  var peopleGraph = graph.filter(store.rdf.filters.type(store.rdf.resolve(""foaf:Person"")));    	  store.execute('PREFIX rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\  					 PREFIX foaf: <http://xmlns.com/foaf/0.1/>\  					 PREFIX : <http://example.org/>\  					 SELECT ?s FROM NAMED :people { GRAPH ?g { ?s rdf:type foaf:Person } }',  					 function(err, results) {    					   console.log(peopleGraph.toArray()[0].subject.valueOf() === results[0].s.value);    					 });  	});      });  });  ```    rdfstore-js can be executed in a web browser or can be included as a library in a node.js application. It can also be executed as a stand-alone SPARQL end-point accepting SPARQL RDF Protocol HTTP requests. Go to the bottom of this page to find some application examples using the library.    The current implementation is far from complete but it already passes all the test cases for the SPARQL 1.0 query language and supports data manipulation operations from the SPARQL 1.1/Update version of the language.    Some other features included in the library are the following:    - SPARQL 1.0 support  - SPARQL 1.1/Update support  - Partial SPARQL 1.1 query support  - JSON-LD parser  - Turtle/N3 parser  - W3C RDF Interfaces API  - RDF graph events API  - Custom filter functions  - Browser persistence using IndexedDB    ## Documentation    Documentation for the store can be found [here](http://antoniogarrote.github.com/rdfstore-js/doc/index.html).    ## SPARQL support    rdfstore-js supports at the moment SPARQL 1.0 and most of SPARQL 1.1/Update.  Only some parts of SPARQL 1.1 query have been implemented yet.    This is a list of the different kind of queries currently implemented:    - SELECT queries  - UNION, OPTIONAL clauses  - NAMED GRAPH identifiers  - LIMIT, OFFSET  - ORDER BY clauses  - SPARQL 1.0 filters and builtin functions  - variable aliases  - variable aggregation: MAX, MIN, COUNT, AVG, SUM functions  - GROUP BY clauses  - DISTINCT query modifier  - CONSTRUCT queries  - ASK queries  - INSERT DATA queries  - DELETE DATA queries  - DELETE WHERE queries  - WITH/DELETE/INSERT/WHERE queries  - LOAD queries  - CREATE GRAPH clauses  - DROP DEFAULT/NAMED/ALL/GRAPH clauses  - CLEAR DEFAULT/NAMED/ALL/Graph clauses  - FILTER EXISTS / NOT EXISTS operators  - BIND  - FILTER IN / NOT IN operators      ##Installation    The library can be installed using NPM:    ```bash  $ npm install rdfstore  ```    The library can also be installed via bower using a global module:    ```bash  $ bower install rdfstore  ```    ##Building    Before running the build script, you must install JavaScript dependencies with [npm](https://npmjs.org/doc/install.html) (`npm` is shipped with [node](http://nodejs.org/download/)):    ```bash  $ npm install  ```    The library can be built using gulp:    ```bash  $ gulp  ```    The browser version can be built using the 'browser' gulp target:    ```bash  $ gulp browser  ```    ## Tests    To execute the whole test suite of the library, including the DAWG  test cases for SPARQL 1.0 and the test cases for SPARQL 1.1  implemented at the moment, a gulp target can be executed:    ```bash  $ gulp specs  ```    Additionally, there are some smoke tests for both browser versions that can be found ithe 'spec/browser'' directory.    ## API    This is a small overview of the rdfstore-js API.    ###Store creation    ```javascript  //nodejs only  var rdfstore = require('rdfstore');    // in the browser the rdfstore object  // is already defined    // alt 1  rdfstore.create(function(err, store) {    // the new store is ready  });      // alt 2  new rdfstore.Store(function(err, store) {    // the new store is ready  });  ```    ###Query execution    ```javascript  // simple query execution  store.execute(""SELECT * { ?s ?p ?o }"", function(err, results){    if(!err) {  	// process results  	if(results[0].s.token === 'uri') {  	  console.log(results[0].s.value);  	}    }  });    // execution with an explicit default and named graph    var defaultGraph = [{'token':'uri', 'value': graph1}, {'token':'uri', 'value': graph2}, ...];  var namedGraphs  = [{'token':'uri', 'value': graph3}, {'token':'uri', 'value': graph4}, ...];    store.executeWithEnvironment(""SELECT * { ?s ?p ?o }"",defaultGraph,    namedGraphs, function(err, results) {    if(err) {  	// process results    }  });  ```    ###Construct queries RDF Interfaces API    ```javascript  var query = ""CONSTRUCT { <http://example.org/people/Alice> ?p ?o } \  			 WHERE { <http://example.org/people/Alice> ?p ?o  }"";    store.execute(query, function(err, graph){    if(graph.some(store.rdf.filters.p(store.rdf.resolve('foaf:name')))) {  	nameTriples = graph.match(null,  							  store.rdf.createNamedNode(rdf.resolve('foaf:name')),  							  null);    	nameTriples.forEach(function(triple) {  	  console.log(triple.object.valueOf());  	});    }  });  ```    ###Loading remote graphs    rdfstore-js will try to retrieve remote RDF resources across the network when a 'LOAD' SPARQL query is executed.  The node.js build of the library will use regular TCP sockets and perform proper content negotiation. It will also follow a limited number of redirections.  The browser build, will try to perform an AJAX request to retrieve the resource using the correct HTTP headers. Nevertheless, this implementation is subjected to the limitations of the Same Domain Policy implemented in current browsers that prevents cross domain requests. Redirections, even for the same domain, may also fail due to the browser removing the 'Accept' HTTP header of the original request.  rdfstore-js relies in on the jQuery Javascript library to peform cross-browser AJAX requests. This library must be linked in order to exeucte 'LOAD' requests in the browser.    ```javascript  store.execute('LOAD <http://dbpedialite.org/titles/Lisp_%28programming_language%29>\  			   INTO GRAPH <lisp>', function(err){    if(err) {  	var query = 'PREFIX foaf:<http://xmlns.com/foaf/0.1/> SELECT ?o \  				 FROM NAMED <lisp> { GRAPH <lisp> { ?s foaf:page ?o} }';  	store.execute(query, function(err, results) {  	  // process results  	});    }  })  ```    ###High level interface    The following interface is a convenience API to work with Javascript code instead of using SPARQL query strings. It is built on top of the RDF Interfaces W3C API.    ```javascript  /* retrieving a whole graph as JS Interafce API graph object */    store.graph(graphUri, function(err, graph){    // process graph  });      /* Exporting a graph to N3 (this function is not part of W3C's API)*/  store.graph(graphUri, function(err, graph){    var serialized = graph.toNT();  });      /* retrieving a single node in the graph as a JS Interface API graph object */    store.node(subjectUri, function(err, node) {    //process node  });    store.node(subjectUri, graphUri, function(err, node) {    //process node  });        /* inserting a JS Interface API graph object into the store */    // inserted in the default graph  store.insert(graph, function(err) {}) ;    // inserted in graphUri  store.insert(graph, graphUri, function(err) {}) ;        /* deleting a JS Interface API graph object into the store */    // deleted from the default graph  store.delete(graph, function(err){});    // deleted from graphUri  store.delete(graph, graphUri, function(err){});        /* clearing a graph */    // clears the default graph  store.clear(function(err){});    // clears a named graph  store.clear(graphUri, function(err){});        /* Parsing and loading a graph */    // loading local data  store.load(""text/turtle"", turtleString, function(err, results) {});    // loading remote data  store.load('remote', remoteGraphUri, function(err, results) {});        /* Registering a parser for a new media type */    // The parser object must implement a 'parse' function  // accepting the data to parse and a callback function.    store.registerParser(""application/rdf+xml"", rdXmlParser);  ```    ###RDF Interface API    The store object includes a 'rdf' object implementing a RDF environment as described in the [RDF Interfaces 1.0](http://www.w3.org/TR/rdf-interfaces/) W3C's working draft.  This object can be used to access to the full RDF Interfaces 1.0 API.    ```javascript  var graph = store.rdf.createGraph();  graph.addAction(rdf.createAction(store.rdf.filters.p(store.rdf.resolve(""foaf:name"")),  								 function(triple){ var name = triple.object.valueOf();  												   var name = name.slice(0,1).toUpperCase()  												   + name.slice(1, name.length);  												   triple.object = store.rdf.createNamedNode(name);  												   return triple;}));    store.rdf.setPrefix(""ex"", ""http://example.org/people/"");  graph.add(store.rdf.createTriple( store.rdf.createNamedNode(store.rdf.resolve(""ex:Alice"")),  								  store.rdf.createNamedNode(store.rdf.resolve(""foaf:name"")),  								  store.rdf.createLiteral(""alice"") ));    var triples = graph.match(null, store.rdf.createNamedNode(store.rdf.resolve(""foaf:name"")), null).toArray();    console.log(""worked? ""+(triples[0].object.valueOf() === 'Alice'));  ```    ###Default Prefixes    Default RDF name-spaces can be specified using the *registerDefaultNamespace*. These names will be included automatically in all queries. If the same name-space is specified by the client in the query string the new prefix will shadow the default one.  A collection of common name-spaces like rdf, rdfs, foaf, etc. can be automatically registered using the *registerDefaultProfileNamespace* function.    ```javascript  new Store({name:'test', overwrite:true}, function(err,store){  	store.execute('INSERT DATA {  <http://example/person1> <http://xmlns.com/foaf/0.1/name> ""Celia"" }', function(err){    	   store.registerDefaultProfileNamespaces();    	   store.execute('SELECT * { ?s foaf:name ?name }', function(err,results) {  		   test.ok(results.length === 1);  		   test.ok(results[0].name.value === ""Celia"");  	   });  	});  });  ```    ###JSON-LD Support    rdfstore-js implements parsers for Turtle and JSON-LD. The specification of JSON-LD is still an ongoing effort. You may expect to find some inconsistencies between this implementation and the actual specification.    ```javascript  		jsonld = {  		  ""@context"":  		  {  			 ""rdf"": ""http://www.w3.org/1999/02/22-rdf-syntax-ns#"",  			 ""xsd"": ""http://www.w3.org/2001/XMLSchema#"",  			 ""name"": ""http://xmlns.com/foaf/0.1/name"",  			 ""age"": {""@id"": ""http://xmlns.com/foaf/0.1/age"", ""@type"": ""xsd:integer"" },  			 ""homepage"": {""@id"": ""http://xmlns.com/foaf/0.1/homepage"", ""@type"": ""xsd:anyURI"" },  			 ""ex"": ""http://example.org/people/""  		  },  		  ""@id"": ""ex:john_smith"",  		  ""name"": ""John Smith"",  		  ""age"": ""41"",  		  ""homepage"": ""http://example.org/home/""  		};    store.setPrefix(""ex"", ""http://example.org/people/"");    store.load(""application/ld+json"", jsonld, ""ex:test"", function(err,results) {    store.node(""ex:john_smith"", ""ex:test"", function(err, graph) {  	// process graph here    });  });  ```    ###Events API    rdfstore-js implements an experimental events API that allows clients to observe changes in the RDF graph and receive notifications when parts of this graph changes.  The two main event functions are *subscribe* that makes possible to set up a callback function that will be invoked each time triples matching a certain pattern passed as an argument are added or removed, and the function *startObservingNode* that will be invoked with the modified version of the node each time triples are added or removed from the node.    ```javascript  var cb = function(event, triples){    // it will receive a notifications where a triple matching    // the pattern s:http://example/boogk, p:*, o:*, g:*    // is inserted or removed.    if(event === 'added') {  	console.log(triples.length+"" triples have been added"");    } else if(event === 'deleted') {  	console.log(triples.length+"" triples have been deleted"");    }  }    store.subscribe(""http://example/book"",null,null,null,cb);      // .. do something;    // stop receiving notifications  store.unsubscribe(cb);  ```    The main difference between both methods is that *subscribe* receives the triples that have changed meanwhile *startObservingNode* receives alway the whole node with its updated triples. *startObservingNode* receives the node as a RDF Interface graph object.    ```javascript  var cb = function(node){    // it will receive the updated version of the node each    // time it is modified.    // If the node does not exist, the graph received will    // not contain triples.    console.log(""The node has now ""+node.toArray().length+"" nodes"");  }    // if only tow arguments are passed, the default graph will be used.  // A graph uri can be passed as an optional second argument.  store.startObservingNode(""http://example/book"",cb);      // .. do something;    // stop receiving notifications  store.stopObservingNode(cb);  ```    In the same way, there are *startObservingQuery* and *stopObservingQuery* functions that makes possible to set up callbacks for whole SPARQL queries.  The store will try to be smart and not perform unnecessary evaluations of these query after quad insertion/deletions. Nevertheless too broad queries must be used carefully with the events API.    ###Custom Filter Functions    Custom filter function can be registered into the store using the *registerCustomFunction* function. This function receives two argument, the name of the custom function and the associated implementation. This functions will be available in a SPARQL query using the prefix *custom*.  You can also use a full URI to identify the function that is going to be registered.  The function implementation will receive two arguments, an object linking to the store query filters engine and a list with the actual arguments. Arguments will consist of literal or URIs objects. Results from the function must also be literal or URI objects.    The query filters engine can be used to access auxiliary function to transform literals into JavaScript types using the *effectiveTypeValue* function, boolean values using the *effectiveBooleanValue*, to build boolean literal objects (*ebvTrue*, *ebvFalse*) or return an error with the *ebvError*. Documentation and source code for the *QueryFilters* object n the 'js-query-engine' module can be consulted to find information about additional helper functions.    The following test shows a simple examples of how custom functions can be invoked:    ```javascript  new Store({name:'test', overwrite:true}, function(err,store) {  	store.load(  		'text/n3',  		'@prefix test: <http://test.com/> .\  		 test:A test:prop 5.\  		 test:B test:prop 4.\  		 test:C test:prop 1.\  		 test:D test:prop 3.',  		function(err) {    			var invoked = false;              // instead of 'my_addition_check' a full URI can be used 'http://test.com/my_fns/my_addition_check'  			store.registerCustomFunction('my_addition_check', function(engine,args) {  		// equivalent to var v1 = parseInt(args[0].value), v2 = parseInt(args[1].value);    		var v1 = engine.effectiveTypeValue(args[0]);  		var v2 = engine.effectiveTypeValue(args[1]);    		// equivalent to return {token: 'literal', type:""http://www.w3.org/2001/XMLSchema#boolean"", value:(v1+v2<5)};    		return engine.ebvBoolean(v1+v2<5);  	});    	   store.execute(  				'PREFIX test: <http://test.com/> \  				 SELECT * { ?x test:prop ?v1 .\  							?y test:prop ?v2 .\  							filter(custom:my_addition_check(?v1,?v2)) }',  				function(err) {  				   test.ok(results.length === 3);  		   for(var i=0; i<results.length; i++) {  			test.ok(parseInt(results[i].v1.value) + parseInt(results[i].v2.value) < 5 );  		}  		test.done()  		}  	);    });  });  ```    ###Persistence    The store can be persisted in the browser using IndexedDB as the backend. In order to make the store persistent,  the 'persistent' flag must be set to true in the store creation options.  Additionally, a 'name' option can also be passed for the store. Different persistent instances of the store can be  opened using different names.    ###Controlling the frequency of function yielding    Performance of the store can be improved by reducing the frequency the 'nexTick' mechanism is used to cancel the the calls stack.  You can reduce this frequency by invoking the `yieldFrequency` function on the Store object and passing a bigger number:    ``` javascript  var rdfstore = require('rdfstore')  rdfstore.Store.yieldFrequency(200); // will only yield after 200 invocations of nextTick    ```  If the number is too big a number can produce stack overflow errors during execution. If you find this problem, reduce the value provided for `yieldFrequency`.    ##Dependencies    The library include dependencies to two semantic-web libraries for  parsing:    - [N3.js library](https://github.com/RubenVerborgh/N3.js/), developed    by Ruben Verborgh and released under the MIT license.    - [jsonld](https://github.com/digitalbazaar/jsonld.js), developed by Digital Bazaar and released under the New BSD license.    ##Frontend    A stand-along frontend for the store built using electron has been added in version 0.9.7.  You can build the frontend running the command:    ```bash  $ gulp frontend  ```    The file will be added under the releases directory.    ##Contributing    rdfstore-js is still at the beginning of its development. If you take a look at the library and find a way to improve it, please ping us. We'll be very greatful for any bug report or pull-request.    ## Author    Antonio Garrote, email:antoniogarrote@gmail.com, twitter:@antoniogarrote.      ## License    Licensed under the [MIT License](http://opensource.org/licenses/MIT), copyright Antonio Garrote 2011-2015 """
Semantic web;https://github.com/RMLio/yarrrml-parser;"""# YARRRML Parser    This library allows to convert [YARRRML](https://w3id.org/yarrrml) rules to [RML](http://rml.io) or [R2RML](https://www.w3.org/TR/r2rml/) rules.    ## Install    - `npm i -g @rmlio/yarrrml-parser`    ## Usage    ### CLI    There are two CLI functions, `yarrrml-parser` and `yarrrml-generator`.  Using the `--help` flag will show all possible commands.    #### yarrrml-parser    If you want to generate RML rules from a YARRRML document,  you do the following: `yarrrml-parser -i rules.yml`.    The rules will be written to standard output.  If you want to write them to a file, you can add the `-o` option.    By default, the parser generates RML rules.  If you want to generate R2RML rules add `-f R2RML`.    If you want to use `rr:class` instead of Predicate Object Maps, use the `-c` flag.    You can use multiple input files too: `yarrrml-parser -i rules-1.yml -i rules-2.yml`.  They are converted to a single RML document.  Note that the keys in `prefixes`, `sources`, and `mappings` have to be unique across all files.  `base` can only be set once.  You find an example at [`test/multiple-input-files`](test/multiple-input-files).    You can overwrite external references via the `-e`.  An external reference starts with `_`.  For example, `-e name=John` will replace all occurrences of `$(_name)` with `John`.  Repeat `-e` for multiple references.  When you do not provide a value for an external reference,  the reference will not be replaced.  You find an example in [`test/template-escape`](test/template-escape).  If you want to use for example `$(_name)` as both an external reference and a normal reference,  then you add a `\` for the latter resulting in `$(\_name)` for the latter.    If you want the outputted RML to be pretty, please provide the `-p` or `--pretty` parameter.    #### yarrrml-generator    If you want to generate YARRRML rules from an RML document, you do the following: `yarrrml-generator -i rules.rml.ttl`.  The rules will be written to standard output.  If you want to write them to a file, you can add the `-o` option.    ### Library    `npm i --save @rmlio/yarrrml-parser`    ```javascript  let yarrrml = require('@rmlio/yarrrml-parser/lib/rml-generator');    const yaml = ""[yarrrml string]"";  const y2r = new yarrrml();  const triples = y2r.convert(yaml);    if ( y2r.getLogger().has('error') ) {     const logs = y2r.getLogger().getAll();     ...  }  ```    ## Development    - Clone this repo.  - Install the dependencies via `npm i`  - Update code, if needed.  - Run the tests via `npm test`    - If you make a new test, make sure the (RML) Turtle is 'pretty'. If you're not sure it's pretty, run `./test/prettify_ttl.js`  - Make the [CLI](#cli) (based on the code in the cloned repo)  available system-wide via `npm link` (optional).    ## Docker    Run (from [DockerHub](https://hub.docker.com/repository/docker/rmlio/yarrrml-parser)):    ```bash  docker run --rm -it -v $(pwd)/resources:/data rmlio/yarrrml-parser:latest -i /data/test.yarrr.yml  ```    Build from source:    ```bash  docker build -t yarrrml-parser .  ```    ## License    This code is copyrighted by [Ghent University – imec](http://idlab.ugent.be/) and released under the [MIT license](http://opensource.org/licenses/MIT). """
Semantic web;https://github.com/edmcouncil/rdf-toolkit;"""<img src=""https://spec.edmcouncil.org/fibo/htmlpages/master/latest/img/logo.66a988fe.png"" width=""150"" align=""right""/>    # rdf-toolkit    The `rdf-toolkit` is a command-line 'swiss army knife' tool for reading and writing RDF and OWL files in whatever format.    The primary reason for creating this tool was to have a reference implementation of the toolkit/formatter that   creates the FIBO ontologies as they are stored in the [Github FIBO repository](https://github.com/edmcouncil/fibo)   (which is at this point in time still a private repository). However, this tool is not in any way specific to FIBO,   it can be used with any set of ontologies or for that matter even ""normal"" RDF files.    It currently uses OWLAPI and RDF4J to do the hard work, see [this page](docs/dependencies.md) for more info about those products.    This will be used in a commit-hook to make sure that all RDF files in the repo are stored in the same way.    See for more information about developing rdf-toolkit [this](docs/develop.md) page or [this page](docs/dependencies.md) for information about dependencies.    # Recommended Output Format    The recommended Output Format at this time is RDF/XML because that is the format that the OMG requires for submissions.   The EDM Council develops the FIBO Ontologies and submits them as RDF/XML, serialized by the `rdf-toolkit` to the OMG.   So that is why we also use RDF/XML in Github itself. There are some issues with that and we're working on resolving that,   by either ""fixing"" the RDF/XML output generated by OWLAPI or RDF4J, or by eventually migrating to some other format.   For use in Git we need a format that:    ## Requirements for Git-based Ontology Serialization    - As few 'diff-lines' as possible per 'pull request'  - Relative URIs    - so that Git branch or tag name can become part of the final Ontology Version IRI    - so that dereferencing from tools like Protege, straight to the GitHub repo would work  - Readable (RDF/XML is only readable by the very few)    # Issues    The FIBO JIRA server has a separate project for the rdf-toolkit: https://jira.edmcouncil.org/browse/RDFSER    Please add your issues, bugs, feature requests, requirements or questions as issues on the JIRA site.    # Download    Download the RDF Toolkit [here](https://jenkins.edmcouncil.org/view/rdf-toolkit/job/rdf-toolkit-build/lastSuccessfulBuild/artifact/target/rdf-toolkit.jar)    # Usage    Download the `rdf-toolkit.jar` file mentioned in the Download section above to your computer.    ## Linux or Mac OS X    On Linux or Mac OS X you can execute the rdf-toolkit as follows:    1. Open a Terminal  2. Type the name of the `rdf-toolkit.jar` file on the command prompt and supply the `--help` option:  ```  $ java -jar rdf-toolkit.jar --help  ```    ## Windows    1. Open a Command Shell by going to the Start menu and type `cmd.exe`.  2. Ensure that Java is installed by typing `java -version` on the command line. The Java version should be at least 1.7 (i.e. Java 7).  3. Then launch the rdf-toolkit's help function as follows:  ```  java -jar rdf-toolkit.jar --help  ```    # --help    The current `--help` option gives the following information:    ```  usage: RdfFormatter (rdf-toolkit version 1.11.0)   -bi,--base-iri <arg>                    set IRI to use as base URI   -dtd,--use-dtd-subset                   for XML, use a DTD subset in order to allow prefix-based                                           IRI shortening   -h,--help                               print out details of the command-line arguments for the                                           program   -i,--indent <arg>                       sets the indent string.  Default is a single tab character   -ibi,--infer-base-iri                   use the OWL ontology IRI as the base URI.  Ignored if an                                           explicit base IRI has been set   -ibn,--inline-blank-nodes               use inline representation for blank nodes.  NOTE: this will                                           fail if there are any recursive relationships involving                                           blank nodes.  Usually OWL has no such recursion involving                                           blank nodes.  It also will fail if any blank nodes are a                                           triple subject but not a triple object.   -ip,--iri-pattern <arg>                 set a pattern to replace in all IRIs (used together with                                           --iri-replacement)   -ir,--iri-replacement <arg>             set replacement text used to replace a matching pattern in                                           all IRIs (used together with --iri-pattern)   -lc,--leading-comment <arg>             sets the text of the leading comment in the ontology.  Can                                           be repeated for a multi-line comment   -osl,--override-string-language <arg>   sets an override language that is applied to all strings   -s,--source <arg>                       source (input) RDF file to format   -sd,--source-directory <arg>            source (input) directory of RDF files to format.  This is a                                           directory processing option   -sdp,--source-directory-pattern <arg>   relative file path pattern (regular expression) used to                                           select files to format in the source directory.  This is a                                           directory processing option   -sdt,--string-data-typing <arg>         sets whether string data values have explicit data types,                                           or not; one of: explicit, implicit [default]   -sfmt,--source-format <arg>             source (input) RDF format; one of: auto (select by                                           filename) [default], binary, json-ld (JSON-LD), n3, n-quads                                           (N-quads), n-triples (N-triples), rdf-a (RDF/A), rdf-json                                           (RDF/JSON), rdf-xml (RDF/XML), trig (TriG), trix (TriX),                                           turtle (Turtle)   -sip,--short-iri-priority <arg>         set what takes priority when shortening IRIs: prefix                                           [default], base-iri   -t,--target <arg>                       target (output) RDF file   -tc,--trailing-comment <arg>            sets the text of the trailing comment in the ontology.  Can                                           be repeated for a multi-line comment   -td,--target-directory <arg>            target (output) directory for formatted RDF files.  This is                                           a directory processing option   -tdp,--target-directory-pattern <arg>   relative file path pattern (regular expression) used to                                           construct file paths within the target directory.  This is                                           a directory processing option   -tfmt,--target-format <arg>             target (output) RDF format: one of: json-ld (JSON-LD),                                           rdf-xml (RDF/XML), turtle (Turtle) [default]   -v,--version                            print out version details  ```    * [Sesame serializer documentation](docs/RdfFormatter.md) """
Semantic web;https://github.com/Swirrl/matcha;"""# Matcha    [![Clojars Project](https://img.shields.io/clojars/v/grafter/matcha.alpha.svg)](https://clojars.org/grafter/matcha.alpha) [![cljdoc badge](https://cljdoc.org/badge/grafter/matcha.alpha)](https://cljdoc.org/d/grafter/matcha.alpha)    *WARNING: Alpha Software Subject to Change*    A Clojure DSL to query in memory triple models with a SPARQL like  language.  Matcha provides simple BGP (Basic Graph Pattern) style  queries on in memory graphs of linked data triples.    ![Matcha](https://raw.githubusercontent.com/Swirrl/matcha/master/doc/matcha.jpg ""Matcha"")    Whilst Matcha is intended to query RDF models it can also be used to  query arbitrary clojure data, so long as it consists of Clojure values  stored in 3/tuple vectors, each entity of the triple is assumed to  follow Clojure value equality semantics.    The primary use cases for Matcha are to make handling graphs of RDF  data easy by querying data with SPARQL-like queries.  A typical  workflow is to `CONSTRUCT` data from a backend SPARQL query, and then  use Matcha to query this graph locally.    ## Features    - SPARQL-like BGP queries across multiple triple patterns.  - Parameterised queries using just clojure `let`.  - Ability to index your database, with `index-triples`.  In order to    be queried Matcha needs to have indexed the data; if your data is    unindexed it will index it before running the query, and then    dispose of the index.  This can lead to poor performance when you    want to query the same set of data multiple times.  - Construct graph query results directly into clojure datastructures.  - Support for `VALUES` clauses (unlike in SPARQL we do not yet support    binding arbitrary tuples/tables).  So we only support the    `VALUES ?x { ... }` form.  - Support for `OPTIONAL`s with SPARQL-like semantics.    ## Limitations    The initial implementation is macro heavy.  This means use cases where  you want to dynamically create in memory queries may be more awkward.    Currently there is no support for the following SPARQL-like features:    1. Reasoning on in memory vocabularies with RDFS (maybe OWL)  2. Clojurescript support (planned)    ## Usage    Matcha defines some primary query functions `select`, `select-1`,  `build`, `build-1`, `construct`, `construct-1` and `ask`.    First lets define an in memory database of triples, in reality this  could come from a SPARQL query `CONSTRUCT`, but here we'll just define  some RDF-like data inline.    Triples can be vectors of clojure values or any datastructure that  supports positional destructuring via `clojure.lang.Indexed`, this  allows Matcha to work `grafter.rdf.protocols.Statement` records.  Matcha works with any clojure values in the triples, be they java  URI's, or clojure keywords.    ```clojure  (def friends-db [[:rick :rdfs/label ""Rick""]                   [:martin :rdfs/label ""Martin""]                   [:katie :rdfs/label ""Katie""]                   [:julie :rdfs/label ""Julie""]                     [:rick :foaf/knows :martin]                   [:rick :foaf/knows :katie]                   [:katie :foaf/knows :julie]                     [:rick :a :foaf/Person]                   [:katie :a :foaf/Person]                   [:martin :a :foaf/Person]])  ```    Now we can build our query functions:    ### General Query Semantics    There are two main concepts to Matcha queries.  They typically define:    1. a projection, which states what variables to return to your Clojure  program, and the datastructure they should be returned in.  2. a Basic Graph Pattern (BGP), that defines the pattern of the graph     traversal.    BGPs have some semantics you need to be aware of:    - Clojure symbols beginning with a `?` are treated specially as query    variables.  - Other symbols are resolved to their values.    ### `build`    `build` always groups returned solutions into a sequence of clojure  maps, where the subjects are grouped into maps, and the maps are  grouped by their properties. If a property has multiple values they  will be rolled up into a set, otherwise they will be a scalar value.    Each map returned by `build` typically represents a resource in the  built graph, which is projected into a sequence of maps, with  potentially multi-valued keys.    It takes a binding for `?subject` of the map, a map form specifying  the projection of other property/value bindings a `bgp` and a  database.    ``` clojure  (build ?person         {:foaf/knows ?friends}         [[?person :foaf/knows ?friends]]         friends-db)    ;; => ({:grafter.rdf/uri :rick, :foaf/knows #{:martin :katie}}  ;;     {:grafter.rdf/uri :katie, :foaf/knows :julie}  ```    NOTE: `:foaf/knows` is projected into a set of values for `:rick`, but  a single scalar value for `:katie`.    The `?subject` is by default associated with the key  `:grafter.rdf/uri`. If you wish to specify this key yourself you can  by providing a key/value pair as the subject: e.g. substituting  ?person for `[:id ?person]` changes the return values like so:    ``` clojure  (build [:id ?person]         {:foaf/knows ?friends}         [[?person :foaf/knows ?friends]]           friends-db)  ;; => ({:id :rick, :foaf/knows #{:martin :katie}}  ;;     {:id :katie, :foaf/knows :julie}  ```    Because `build` knows it is always returning a sequence of maps, it  will remove any keys corresponding to unbound variables introduced  through optionals.  This is unlike `construct`.    ### `select`    `select` compiles a query from your arguments, that returns results as a  sequence of tuples. It is directly analagous to SPARQL's `SELECT` query.    The `bgp` argument is analagous to a SPARQL `WHERE` clause and should be  a BGP.    When called with one argument, `select` projects all `?qvar`s used in the  query.  This is analagous to `SELECT *` in SPARQL:    ```clojure  (def rick-knows    (select      [[:rick :foaf/knows ?p2]      [?p2 :rdfs/label ?name]]))    (rick-knows friends-db)  ;; => [""Martin"" ""Katie""]  ```    When called with two arguments `select` expects the first argument to be a  vector of variables to project into the solution sequence.    ```clojure  (def rick-knows (select [?name]                    [[:rick :foaf/knows ?p2]                     [?p2 :rdfs/label ?name]]))    (rick-knows friends-db)  ;; => [""Martin"" ""Katie""]  ```    There is also `select-1` which is just like `select` but returns just  the first solution.    ### `construct`    NOTE: if you're using you `construct` to return maps, you should first  consider using `build` which fixes some issues present in common  `construct` usage.    `CONSTRUCT`s allow you to construct arbitrary clojure data structures  directly from your query results, and position the projected query  variables where ever you want within the projected datastructure  template.    Args:   * `construct-pattern`: an arbitrary clojure data structure. Results     will be projected into the `?qvar` ""holes"".   * `bgps`: this argument is analagous to a SPARQL `WHERE` clause and should be     a BGPs.   * `db-or-idx`: A matcha ""database"".    When called with two arguments `construct` returns a query function  that accepts a `db-or-idx` as its only argument. When called, the  function returns a sequence of matching tuples in the form of the  `construct-pattern`.    ```clojure  (construct {:grafter.rdf/uri :rick              :foaf/knows {:grafter.rdf/uri ?p                           :rdfs/label ?name}}    [[:rick :foaf/knows ?p]     [?p :rdfs/label ?name]])    ;; => (fn [db-or-idx] ...)  ```    When called with 3 arguments, queries the `db-or-idx` directly, returning a  sequence of results in the form of the `construct-pattern`.    ```clojure  (construct {:grafter.rdf/uri :rick              :foaf/knows {:grafter.rdf/uri ?p                           :rdfs/label ?name}}    [[:rick :foaf/knows ?p]     [?p :rdfs/label ?name]]    friends-db)    ;; => {:grafter.rdf/uri :rick  ;;     :foaf/knows #{{:grafter.rdf/uri :martin, :rdfs/label ""Martin""}  ;;                   {:grafter.rdf/uri :katie, :rdfs/label ""Katie""}}}  ```    Maps in a projection that contain the special key of  `:grafter.rdf/uri` trigger extra behaviour, and cause the query  engine to group solutions by subject, and merge values into clojure  sets.  For example in the above query you'll notice that `foaf:knows`  groups its solutions.  If you don't want these maps to be grouped,  don't include the magic key `:grafter.rdf/uri` in the top level  projection.    There is also `construct-1` which is just like `construct` but returns  only the first solution.    See the [unit  tests](https://github.com/Swirrl/matcha/blob/ae2449483d5a7849ac60a3e5b6a29e459d74ad8e/test/grafter/matcha/alpha_test.clj#L113)  for more examples, including examples that use Matcha with Grafter  Statements and vocabularies.    ### `ask`    `ask` is the only query that doesn't specify an explicit projection.  It accepts a BGP, like the other query types and returns a boolean  result if there were any matches found.    ```clojure  (def any-triples? (ask [[?s ?p ?o]])    (any-triples? friends-db) ;; => true  ```    ### Parameterising queries    You can parameterise Matcha queries simply by adding a lexical binding or wrapping a function call over your Matcha query.  For example    ```clojure  (defn lookup-friends [person-id database]    (->> database         (construct {:grafter.rdf/uri ?friend                     :name ?name}                     [[person-id :foaf/knows ?friend]                      [?friend :rdfs/label ?name]]))    (lookup-friends :rick friends-db)    ;; => [{:grafter.rdf/uri :martin, :name ""Martin""}  ;;     {:grafter.rdf/uri :katie, :name ""Katie""}]  ```    ### OPTIONALs    We support SPARQL-like `OPTIONAL`s in all query types with the following syntax:    ```clojure  (defn lookup-name [person-id database]    (select [?name]      [[person-id :a :foaf/Person]       (optional [[person :rdfs/label ?name]])       (optional [[person :foaf/name ?name]])]))  ```    ### VALUEs    We support dynamic VALUEs clauses in all query types like so:    ```clojure  (defn lookup-names [person-ids database]    (select [?name]      [(values ?person-id person-ids)       [?person-id :rdfs/label ?name]]))    (lookup-names [:rick :katie] friends-db) ;; => [""Rick"", ""Katie""]  ```    You can also hardcode the values into the query:    ```clojure  (defn lookup-names [person-ids database]    (select [?name]      [(values ?person-id [:rick :katie])       [?person-id :rdfs/label ?name]]))  ```    Any ""flat collection"" (i.e. a `sequential?` or a `set?`) is valid  on the right hand side of a `values` binding.    ## Performance    Matcha is intended to be used on modest sizes of data, typically  thousands of triples, and usually no more than a few hundred thousand  triples.  Proper benchmarking hasn't yet been done but finding all  solutions on a database of a million triples can be done on a laptop  in less than 10 seconds.  Query time scaling seems to be roughly  linear with the database size.    ## Avoiding clj-kondo lint errors with matcha macros    Matcha exports some clj-kondo configuration which prevents clj-kondo  warning about unbound variables when using the matcha query macros.    You can import these configs into your project with the following  command:    ```  $ clj-kondo --copy-configs --dependencies --lint ""$(clojure -Spath)""  Imported config to .clj-kondo/grafter/matcha.alpha. To activate, add ""grafter/matcha.alpha"" to :config-paths in .clj-kondo/config.edn.  ```    Then simply add the following to `.clj-kondo/config.edn`:    ```  {:config-paths [""grafter/matcha.alpha""]}  ```    ## Developing Matcha    Matcha uses [`tools.build`](https://clojure.org/guides/tools_build) and  [`tools.deps`](https://clojure.org/guides/deps_and_cli) for builds,  development and testing.    The command:    ```  $ clojure -T:build test  ```    Will run the tests, whilst    ```  $ clojure -T:build build  $ clojure -T:build install  ```    can be used to build and install a jar into your local mvn repository.    However for consuming local Matcha changes in local projects you are  usually better using `tools.deps` `:classpath-overrides`, or creating  a branch and consuming via a `:git/url`.    ## Deploying to Clojars    For [deployments CircleCI is setup](https://github.com/Swirrl/matcha/blob/fafe7478ae605c4cb2a0253714c3bd286e1ca185/.circleci/config.yml#L46-L55)  to automatically deploy tags of the form `vX.Y.Z` where `X.Y.Z` are  `major.minor.patch` numbers.  If you have permissions (i.e. you are  a Swirrl developer) the recommended workflow is to create a new  release of the `main` branch in github with a tag that bumps the  version number appropriately.    _NOTE_: For this step to work you will need appropriate deployment  privileges on clojars.org.    ## License    Copyright © Swirrl IT Ltd 2018    Distributed under the Eclipse Public License either version 1.0 or (at  your option) any later version. """
Semantic web;https://github.com/guiveg/rdfsurveyor;"""RDF Surveyor  ==========  RDF Surveyor is an easy-to-use exploration tool of semantic datasets. It can be plugged in any CORS-enabled SPARQL 1.1 endpoint without requiring any installation.    These are some of the features of RDF Surveyor:    * Intuitive user interface that completely hides the RDF/SPARQL syntax    * It gives an overview of the repository contents, supports class navigation and individual visualization    * No installation required for exploring any RDF dataset    * Works with large datasets such as DBpedia    * Prepared to work with multilingual datasets    * The UI adapts to mobile devices    * RESTful design      Please cite RDF Surveyor as:    > G. Vega-Gorgojo, L. Slaughter, B.M. von-Zernichow, N. Nikolov, D. Roman. Linked Data Exploration with RDF Surveyor. IEEE Access. 7(1):172199-172213, December 2019.      Usage  ==========  RDF Surveyor is a web application developed in Javascript. You can easily deploy it in your web server or just try RDF Surveyor on [http://tools.sirius-labs.no/rdfsurveyor](http://tools.sirius-labs.no/rdfsurveyor)    To begin the exploration of a repository, you only need to copy the URI of the target SPARQL endpoint (and optionally the URI of the named graph). You can also import the URIs of some well-known repositories such as DBpedia with the ""Import configuration"" button.      Help us to improve  ==========  RDF Surveyor is available under an Apache 2 license. Please send us an email to [guiveg@ifi.uio.no](mailto:guiveg@ifi.uio.no) if you use or plan to use RDF Surveyor in a project. Drop us also a message if you have comments or suggestions for improvement.        Screenshots  ==========  Some screenshots of RDF Surveyor:    ![screenshot](/screenshots/config.png ""Config"")    ![screenshot](/screenshots/namespaces.png ""Namespaces"")    ![screenshot](/screenshots/upper.png ""Upper classes"")    ![screenshot](/screenshots/artwork.png ""Artwork class"")    ![screenshot](/screenshots/painting0.png ""The Surrender of Breda individual (1)"")    ![screenshot](/screenshots/painting1.png ""The Surrender of Breda individual (2)"")    ![screenshot](/screenshots/oslo.png ""Oslo"")      Configuration  ==========  You can edit the parameters of the configuration file at `etc/data/config.js`:    * `pagesize`: number of elements per page (default 10)    * `hidemax`: maximum number of elements to show in a list before including a *show more* button (default 8)    * `hidebegin`: element index to begin hiding (default 5)    * `repos`: preloaded configuration of repositories. Each element should have a name, an endpoint URI, and optionally a named graph URI    * `geoenabled`: if true, RDF Surveyor will try to find geographic coordinates for individuals and show a map widget provided by [Leaflet](http://leafletjs.com/).     * `geooptions`: the map widget provided by [Leaflet](http://leafletjs.com/) requires your own `accessToken`. The rest of parameters here should not be changed    * `gaenabled`: if true, RDF Surveyor will log events (requested resource, latency, and number of SPARQL queries) through [Google Analytics](https://www.google.com/analytics/)    * `gaproperty`: you have to provide your own property in order to log Google Analytics events         """
Semantic web;https://github.com/yasarkhangithub/SAFE;"""# SAFE: SPARQL Federation over RDF Data Cubes with Access Control    SAFE, a SPARQL query federation engine that enables decentralised, access to clinical information represented as RDF data cubes with controlled access.    ## Experimental Setup  The experimental setup (i.e. code, datasets, setting, queries) for evaluation of SAFE is described here.    ### Code  The SAFE source code can be checkedout from [SAFE GitHub Page](https://github.com/yasarkhangithub/SAFE/).     ### Datasets  We use two groups of datasets exploring two different use cases, i.e. INTERNAL and EXTERNAL.    The first group of datasets **(INTERNAL)** are collected from the three clinical partners involved in our primary use case. These datasets contain aggregated clinical data represented as RDF data cubes and are privately owned/restricted.    The second group of datasets **(EXTERNAL)** are collected from legacy Linked Data containing sociopolitical and economical statistics (in the form of RDF data cubes) from the **World Bank**, **IMF (International Monetary Fund)**, **Eurostat**, **FAO (Food and Agriculture Organization of the United Nations)** and **TI (Transparency International)**.    External datasets used in evaluation experiments of SAFE can be downloaded from [SAFE External Datasets](https://goo.gl/6s4juv).    ### Settings    Each dataset was loaded into a different SPARQL endpoint on separate physical machines. All experiments are carried out on a local network, so that network cost remains negligible. The machines used for experiments have a 2.60 GHz Core i7 processor, 8 GB of RAM and 500 GB hard disk running a 64-bit Windows 7 OS. Each dataset is hosted as a Virtuoso Open Source SPARQL endpoint hosted physically on separate machines. The details of the parameters used to configure Virtuoso are listed in Table 1 below. Virtuoso version 7.2.4.2 has been used in experiments.    **Table 1:** SPARQL Endpoints Configuration    | SPARQL Endpoint       | Port           | URL  | Virtuoso Config Parameters  |  | ------------- |-------------| -----| -----|  | IMF      | 8890 | {System-IP}:8890/sparql | NoB=680000, MDF=500000, MQM=8G |  | World Bank      | 8891      |   {System-IP}:8891/sparql | NoB=680000, MDF=500000, MQM=8G |  | TI | 8892      |    {System-IP}:8892/sparql | NoB=680000, MDF=500000, MQM=8G |  | Eurostat | 8893      |    {System-IP}:8893/sparql | NoB=680000, MDF=500000, MQM=8G |  | FAO | 8895      |    {System-IP}:8895/sparql | NoB=680000, MDF=500000, MQM=8G |    - *NoB = NumberOfBuffers*  - *MDF = MaxDirtyBuffers*  - *MQM = MaxQueryMem*    ### Queries    A total of 15 queries are designed to evaluate and compare the query federation performance of SAFE against FedX, HiBISCuS and SPLENDID based on the metrics defined. We define ten queries for the federation of EXTERNAL datasets and five for the federation of INTERNAL datasets. Only ten queries (EXTERNAL dataset queries) are made available due to owner restrictions.    Queries used in evaluation experiments of SAFE can be downloaded from [SAFE Queries](https://goo.gl/WCCnx3).     ### Metrics    For each query type we measured (1) the number of sources selected; (2) the average source selection time; (3) the average query execution time; and (4) the number of ASK requests issued to sources.    ## Team    [Yasar Khan](https://www.insight-centre.org/users/yasar-khan)    [Muhammad Saleem](http://aksw.org/MuhammadSaleem.html)    [Aidan Hogan](http://aidanhogan.com/)    [Muntazir Mehdi](https://www.insight-centre.org/users/muntazir-mehdi)    [Qaiser Mehmood](https://www.insight-centre.org/users/qaiser-mehmood)    [Dietrich Rebholz-Schuhmann](https://www.insight-centre.org/users/dietrich-rebholz-schuhmann)    [Ratnesh Sahay](https://www.insight-centre.org/users/ratnesh-sahay) """
Semantic web;https://github.com/ncarboni/awesome-GLAM-semweb;"""## awesome GLAM semweb [![Awesome](https://awesome.re/badge.svg)](https://github.com/ncarboni/Awesome-GLAM-semweb)    A curated list of various semantic web and linked data resources for heritage, humanities and art history practitioners.      The list is an extension of [the semantic web awesome list](https://github.com/semantalytics/awesome-semantic-web) specifically targeted for GLAM (Galleries, Libraries, Archive, Museum). The [the semantic web awesome list](https://github.com/semantalytics/awesome-semantic-web) is the reference for general SM solutions, while this list is specifically target to domain resources which do not belong to the general list (e.g ontologies, specific software widely used within the community, documentation targeting DH practitioners and point of contacts/exchanges). For the purpose of providing to the reader a complete and stand-alone resource, few elements of the [the semantic web awesome list](https://github.com/semantalytics/awesome-semantic-web) will be reported also here.      The list is public and contributions are welcome.      <!-- MarkdownTOC levels=""2,3,4,5"" -->    - [Semantic Web Standards & Recommendation](#semantic-web-standards--recommendation)  	- [RDF](#rdf)  	- [RDFS](#rdfs)  	- [OWL](#owl)  	- [Data Shape](#data-shape)  	- [SPARQL](#sparql)  	- [RDFa](#rdfa)  	- [Linked Data Fragments \(LDF\)](#linked-data-fragments-ldf)  	- [Linked Data Notifications](#linked-data-notifications)  	- [Linked Data Platform](#linked-data-platform)  - [Serialization](#serialization)  - [Ontologies](#ontologies)  	- [CIDOC-CRM](#cidoc-crm)  		- [CIDOC-CRM Official extensions](#cidoc-crm-official-extensions)  		- [CIDOC-CRM Unofficial extensions](#cidoc-crm-unofficial-extensions)  		- [CIDOC-CRM/FRBRoo Tutorials](#cidoc-crmfrbroo-tutorials)  		- [CIDOC-CRM Modelling examples and documentation](#cidoc-crm-modelling-examples-and-documentation)  	- [Gemeinsame Normdatei \(GND\)](#gemeinsame-normdatei-gnd)  	- [Europeana Data Model](#europeana-data-model)  	- [Dublin Core](#dublin-core)  	- [Open Archives Initiative Object Reuse and Exchange \(OAI-ORE\)](#open-archives-initiative-object-reuse-and-exchange-oai-ore)  	- [Encoded Archival Context for Corporate Bodies, Persons, and Families \(EAC-CPF\)](#encoded-archival-context-for-corporate-bodies-persons-and-families-eac-cpf)  	- [ICA Expert Group on Archival Description \(EGAD\)](#ica-expert-group-on-archival-description-egad)  	- [Metadata Object Description Schema \(MADS\)](#metadata-object-description-schema-mads)  	- [BIBFRAME \(Bibliographic Framework Initiative\)](#bibframe-bibliographic-framework-initiative)  	- [BIBO \(Bibliographic Ontology Specification\)](#bibo-bibliographic-ontology-specification)  	- [Resource Description Access Ontology](#resource-description-access-ontology)  	- [PREMIS](#premis)  	- [World Wide Web Consortium \(W3C\)](#world-wide-web-consortium-w3c)  	- [Others](#others)  	- [Where to find ontologies](#where-to-find-ontologies)  - [Mapping tools](#mapping-tools)  	- [X3ML](#x3ml)  	- [Karma](#karma)  	- [Ontop](#ontop)  - [Vocabularies and KOS](#vocabularies-and-kos)  	- [General](#general)  	- [France](#france)  	- [Italy](#italy)  	- [Where to find controlled vocabularies/thesauri](#where-to-find-controlled-vocabulariesthesauri)  - [Vocabulary / KOS Management](#vocabulary--kos-management)  	- [Vocabulary validation & conversion tools](#vocabulary-validation--conversion-tools)  - [Exchange and discussions](#exchange-and-discussions)  	- [Conferences](#conferences)  	- [Conference not specifically on Semantic Web, but with strong ties to the community](#conference-not-specifically-on-semantic-web-but-with-strong-ties-to-the-community)  	- [Discussion groups](#discussion-groups)  	- [Academic Journals](#academic-journals)  - [Knowledge Graph Management](#knowledge-graph-management)  	- [Linked Data Platform \(LDP\)](#linked-data-platform-ldp)  - [Books](#books)  - [Editors](#editors)  	- [TextMate](#textmate)  	- [Sublime Text](#sublime-text)  	- [BBedit](#bbedit)  	- [VIM](#vim)  	- [Emacs](#emacs)  	- [IntelliJ](#intellij)  - [Data Management](#data-management)  	- [OpenRefine Reconciliation services](#openrefine-reconciliation-services)  - [Data Validation](#data-validation)  - [IIIF](#iiif)  - [Misc](#misc)  	- [Prefix](#prefix)  	- [Ontology](#ontology)  		- [Documentation](#documentation)  		- [Management](#management)  	- [Alignment](#alignment)  	- [Conversion](#conversion)  	- [Visualisation](#visualisation)  	- [Images](#images)    <!-- /MarkdownTOC -->      ## Semantic Web Standards & Recommendation    ### RDF    - [RDF 1.1 Primer](https://www.w3.org/TR/rdf11-primer/)  - [RDF 1.1 Semantics](https://www.w3.org/TR/rdf11-mt/)  - [RDF 1.1 Concepts and Abstract Syntax](https://www.w3.org/TR/rdf11-concepts/)  - [RDF 1.1: On Semantics of RDF Datasets](https://www.w3.org/TR/rdf11-datasets/)  - [XSD Datatypes](https://www.w3.org/2011/rdf-wg/wiki/XSD_Datatypes)    ### RDFS    - [RDF Schema 1.1](https://www.w3.org/TR/rdf-schema/)    ### OWL    - [OWL 2 Web Ontology Language Document Overview](https://www.w3.org/TR/owl-overview/)  - [OWL 2 Web Ontology Language Primer](https://www.w3.org/TR/owl-primer/)    ### Data Shape    - [SHACL Core](https://www.w3.org/TR/shacl/)  - [SHACL Advanced](https://www.w3.org/TR/shacl-af/)  - [SHex](http://shex.io)    ### SPARQL    - [SPARQL 1.1 Overview](https://www.w3.org/TR/sparql11-overview/)  - [SPARQL 1.1 Query Language](https://www.w3.org/TR/sparql11-query/)  - [SPARQL 1.1 Update](https://www.w3.org/TR/sparql11-update/)  - [SPARQL 1.1 Service Description](https://www.w3.org/TR/sparql11-service-description/)  - [SPARQL 1.1 Federated Query](https://www.w3.org/TR/sparql11-federated-query/)  - [SPARQL 1.1 Query Results JSON Format](https://www.w3.org/TR/sparql11-results-json/)  - [SPARQL 1.1 Query Results CSV and TSV Formats](https://www.w3.org/TR/sparql11-results-csv-tsv/)  - [SPARQL 1.1 Query Results XML Format (Second Edition)](https://www.w3.org/TR/rdf-sparql-XMLres/)  - [SPARQL 1.1 Entailment Regimes](https://www.w3.org/TR/sparql11-entailment/)  - [SPARQL 1.1 Protocol](https://www.w3.org/TR/sparql11-protocol/)  - [SPARQL 1.1 Graph Store HTTP Protocol](https://www.w3.org/TR/sparql11-http-rdf-update/)    ### RDFa    - [XHTML+RDFa 1.1 - Third Edition](https://www.w3.org/TR/xhtml-rdfa/)  - [RDFa Lite 1.1 - Second Edition](https://www.w3.org/TR/rdfa-lite/)  - [HTML+RDFa 1.1 - Second Edition](https://www.w3.org/TR/html-rdfa/)    ### Linked Data Fragments (LDF)    - [Linked Data Fragments](http://linkeddatafragments.org)    ### Linked Data Notifications    - [Linked Data Notifications](https://www.w3.org/TR/ldn/)    ### Linked Data Platform    - [Linked Data Platform 1.0 Primer](https://www.w3.org/TR/ldp-primer/)  - [Linked Data Platform Best Practices and Guidelines](https://www.w3.org/TR/ldp-bp/)  - [Linked Data Platform 1.0](https://www.w3.org/TR/ldp/)  - [Linked Data Platform 1.0 Test Cases](https://dvcs.w3.org/hg/ldpwg/raw-file/tip/tests/ldp-testsuite.html)      ## Serialization    | Format  | Description | Mime-type |  | :--- | :--- | :---: |  | [Turtle](https://www.w3.org/TR/turtle/) | Terse RDF Triple Language. | `text/turtle`, `application/x-turtle` |  | [TriG](https://www.w3.org/TR/trig/) | Plain text format for serializing named graphs and RDF Datasets. | `application/trig`, `application/x-trig` |  | [JSON-LD](https://json-ld.org/) | JSON-based Serialization for Linked Data. | `application/ld+json` |  | [RDF/JSON](https://www.w3.org/TR/rdf-json/) | RDF 1.1 JSON Alternate Serialization. | `application/rdf+json` |  | [N-Triples](https://www.w3.org/TR/n-triples/) | Line-based syntax for RDF datasets. |  `application/n-triples` |  | [N-Quads](https://www.w3.org/TR/n-quads/) | Line-based syntax for RDF datasets. | `application/n-quads`, `text/x-nquads`, `text/nquads` |  | [Notation3](https://www.w3.org/TeamSubmission/n3/) | Notation3 (N3): A readable RDF syntax. | `text/n3`, `text/rdf+n3` |  | [RDF/XML](https://www.w3.org/TR/REC-rdf-syntax/) | RDF/XML Syntax Specification. | `application/rdf+xml`, `application/xml` |  | [TriX](http://www.hpl.hp.com/techreports/2004/HPL-2004-56.html) | RDF Triples in XML. | `application/trix` |  | [HDT](https://www.w3.org/Submission/2011/03/) | Binary RDF Representation for Publication and Exchange. | `application/x-binary-rdf` |  | [aREF](https://gbv.github.io/aREF/aREF.html) | Another RDF Encoding Form. | |       ## Ontologies    ### CIDOC-CRM    - [Documentation](http://www.cidoc-crm.org/versions-of-the-cidoc-crm): Official website of the CIDOC-CRM  - [RDFS](http://www.cidoc-crm.org/versions-of-the-cidoc-crm) Official version of CIDOC-CRM available in RDF. *No direct link, you can use the latest version available in the page*  - [OWL](http://www.cidoc-crm.org/versions-of-the-cidoc-crm) OWL version of CIDOC-CRM. *No direct link, use the latest version available in the page*.  - [CIDOC-CRM Periodic Table](https://remogrillo.github.io/cidoc-crm_periodic_table/)  Visualize and search the CRM in a user-friendly interface.     #### CIDOC-CRM Official extensions    - [CRMdig](http://www.cidoc-crm.org/crmdig): Model for provenance metadata  - [CRMsci](http://www.cidoc-crm.org/crmsci): Scientific observation model  - [CRMinf](http://www.cidoc-crm.org/crminf/): Argumentation model  - [FRBRoo](http://www.cidoc-crm.org/frbroo): Functional Requirement for Bibliographic Records  - [PRESSoo](http://www.cidoc-crm.org/pressoo/): Modelling of bibliographical information  - [CRMpc](http://www.cidoc-crm.org/versions-of-the-cidoc-crm): Modelling .1 properties in CRM as n-ary relationship. *no direct link download the CRM-PC file from the latest CRM version*  - [CRMgeo](http://www.cidoc-crm.org/crmgeo/): Spatiotemporal model  - [CRMba](http://www.cidoc-crm.org/crmba): Model for archaeological buildings  - [CRMtex](http://www.cidoc-crm.org/crmtex): Model for the study of ancient text  - [CRMarcheo](http://www.cidoc-crm.org/crmarchaeo/): Excavation model    #### CIDOC-CRM Unofficial extensions    - [VIR](http://w3id.org/vir): Model for visual and iconographical representations  - [DOREMUS](http://data.doremus.org/ontology/): Model for describing musical performances and recordings    #### CIDOC-CRM/FRBRoo Tutorials    - [Long Video Tutorial by Stephen Staad](http://old.cidoc-crm.org/cidoc_tutorial/index.html) - Require Flash  - [Short Video Tutorial by George Bruseker](https://youtu.be/lVQFciW7V4I)  - [FRBRoo Tutorial](http://83.212.168.219/FRBR_Tutorial/)    #### CIDOC-CRM Modelling examples and documentation    - [Official Website modelling](http://www.cidoc-crm.org/functional-units) and [Best Practices](http://www.cidoc-crm.org/best_practices)  - [Reference Data Models](https://docs.swissartresearch.net/instruction/) - Ready-Made data patterns for describing Person, Artwork, Documents, Events and more.  - [Consortium for Open Research Data in the Humanities](https://docs.cordh.net) - Basic shared pattern for interoperable CRM  - [Linked Art](https://linked.art) - Art Museum Application Profile for CRM in JSON-LD.  - [DOPHEDA](https://chin-rcip.github.io/collections-model/) - Project of the Canadian Heritage Information Network to foster the development of LOD in heritage institutions, including a Data Model based on CIDOC CRM      ### Gemeinsame Normdatei (GND)    - [GND Ontology](https://d-nb.info/standards/elementset/gnd) for authority files    ### Europeana Data Model    - [EDM](https://pro.europeana.eu/resources/standardization-tools/edm-documentation)    ### Dublin Core    - [DCMI Metadata Terms](http://dublincore.org/documents/dcmi-terms/)    ### Open Archives Initiative Object Reuse and Exchange (OAI-ORE)    - [Vocabulary](http://www.openarchives.org/ore/1.0/vocabulary)    ### Encoded Archival Context for Corporate Bodies, Persons, and Families (EAC-CPF)    - [EAC-CPF Description Ontology for Linked Archival Data](https://labs.regesta.com/progettoReload/wp-content/uploads/2013/10/eac-cpf.html)    ### ICA Expert Group on Archival Description (EGAD)    - [Records in Context - Conceptual Model (RiC-CM)](https://www.ica.org/en/egad-ric-conceptual-model)  - [Records in Context - Ontology](#)    ### Metadata Object Description Schema (MADS)    - [Vocabulary](http://www.loc.gov/standards/mads/rdf/v1.html)    ### BIBFRAME (Bibliographic Framework Initiative)    - [Model Overview](https://www.loc.gov/bibframe/docs/bibframe2-model.html)  - [Vocabulary](http://id.loc.gov/ontologies/bibframe.html)    ### BIBO (Bibliographic Ontology Specification)    - [Vocabulary](http://bibliontology.com)    ### Resource Description Access Ontology    - [RDA Registry](http://www.rdaregistry.info/rgAbout/rdaont/)    ### PREMIS    - [Vocabulary](http://id.loc.gov/ontologies/premis-3-0-0.html) of digital preservation metadata    ### World Wide Web Consortium (W3C)    - [Web Annotation Vocabulary](https://www.w3.org/TR/annotation-vocab/)  - [WGS84](https://www.w3.org/2003/01/geo/) - Basic Geo (WGS84 lat/long) Vocabulary.  - [skos](http://www.w3.org/2004/02/skos/core.html) - SKOS Simple Knowledge Organization System.  - [skos-xl](http://www.w3.org/TR/skos-reference/skos-xl.html) - SKOS Simple Knowledge Organization System eXtension for Labels.  - [vcard](https://www.w3.org/TR/vcard-rdf/) - vCard Ontology - for describing People and Organizations.  - [void](https://www.w3.org/TR/void/) - Describing Linked Datasets with the VoID Vocabulary.  - [time](https://w3c.github.io/sdw/time/) - Time Ontology in OWL.  - [org](https://www.w3.org/TR/vocab-org/) - The Organization Ontology.  - [dqv](http://www.w3.org/ns/dqv#) - Vocabulary for describing quality metadata.  - [PROV-O](https://www.w3.org/TR/prov-o/) - Represent provenance information.    ### Others    - [foaf](http://www.foaf-project.org/) - Friend of a Friend (FOAF) ontology.    - [obo-relations](http://obofoundry.org/ontology/ro.html) - Relation Ontology. Relationship types shared across multiple ontologies.    - [RELATIONSHIP](http://vocab.org/relationship/) - Vocabulary for describing relationships between people.  - [BIO](http://vocab.org/bio/) - Vocabulary for describing biographical information.    - [schema.org](https://schema.org/docs/datamodel.html) - Structured data on the Internet (Google, Microsoft, Yahoo and Yandex).  - [SPAR](http://www.sparontologies.net) - Semantic Publishing and Referencing Ontologies.    - GeoSPARQL ([DOCS](https://www.opengeospatial.org/standards/geosparql)|[RDF](www.opengis.net/ont/geosparql))    - [Creative Commons Rights Expression](https://creativecommons.org/ns)    - [QUDT](http://www.qudt.org) Quantities, Units, Dimensions and Types Ontologies and Vocabularies    - [Ontology of units of measure](http://www.ontology-of-units-of-measure.org) Dimensions and measurements ontology    ### Where to find ontologies     - [Linked Open Vocabularies](https://lov.linkeddata.es/)        ## Mapping tools    Mapping tools for transforming your data (CSV, XML) into RDF    ### X3ML    X3ML is a transformation engine developed by FORTH. It is perfected to work with CIDOC-CRM, however it does work greatly with other ontologies as well. It is available as web application (3M) and a stand alone app (X3ML). In both cases the input file has to be in XML (for transforming a CSV file to XML see [Mr Data Converter](https://shancarter.github.io/mr-data-converter/)).     - [3M](http://139.91.183.3/3M/)  - [X3ML](https://github.com/isl/x3ml)    In order to transform the data it is necessary to create a X3ML declaration and a URI Mapping. Examples of both, together with the necessary commands are available at this addresses:    - [Consortium for Open Research Data in the Humanities](https://docs.cordh.net/tool/mapping/)    ### Karma    [Karma](http://usc-isi-i2.github.io/karma/) is an information integration tool for aggregating, harmonising and transforming diverse data sources (CSV, XML, JSON, KML, Web APIs). The process is driven by an ontology and results in a transformation of the original data in RDF. A graphical user interface help the user map the data and, moreover, it is build to recognize the mapping of data to ontology classes and then uses the ontology to propose a model that ties together these classes. Karma does not only help the user transform the data but it can be used to normalise them too.    ### Ontop        [Ontop](https://ontop.inf.unibz.it) is an application developed by the University of Bolzano for creating a virtual RDF Graph on top of your current data source. Mappings can be easily created using [Protege](http://protege.stanford.edu/) and results are queryable using SPARQL 1.0. Moreover, it support reasoning.              ## Vocabularies and KOS    ### General    - [Getty Art and Architecture Thesaurus](http://vocab.getty.edu/aat/)   - [Getty Union List of Artist Names](http://vocab.getty.edu/ulan/)  - [Thesaurus of Geographic Names](http://vocab.getty.edu/tgn/)  - Iconclass [keyword search](http://www.iconclass.org/rkd/9/) + [help LOD](http://www.iconclass.org/help/lod)  - [CERL Thesaurus](https://data.cerl.org/thesaurus/) for book heritage  - [Library Congress Subject Headings](http://id.loc.gov/authorities/subjects.html)  - [Thesaurus Graphical Materials](http://id.loc.gov/vocabulary/graphicMaterials.html)  - [Nomenclature for Museum Cataloging](https://www.nomenclature.info) + [help LOD](https://www.nomenclature.info/integration.app)    ### France    - [Thésaurus de la désignation des objets mobiliers](http://data.culture.fr/thesaurus/page/ark:/67717/T69)  - [Liste d'autorité Actions pour l'indexation des archives locales](http://data.culture.fr/thesaurus/resource/ark:/67717/T2)  - [Liste d'autorité Contexte historique pour l'indexation des archives locales](http://data.culture.fr/thesaurus/resource/ark:/67717/T4)  - [Liste d'autorité Typologie documentaire pour l'indexation des archives locales](http://data.culture.fr/thesaurus/resource/ark:/67717/T3)  - [Nomenclatures HADOC](http://data.culture.fr/thesaurus/resource/ark:/67717/0404efce-2024-4694-bf80-eba4fc2b336c)  - [Techniques photographiques](http://data.culture.fr/thesaurus/resource/ark:/67717/T61)  - [Thésaurus de la désignation des œuvres architecturales et des espaces aménagés](http://data.culture.fr/thesaurus/resource/ark:/67717/T96)  - [Thésaurus-matières pour l'indexation des archives locales](http://data.culture.fr/thesaurus/resource/ark:/67717/Matiere)  - [Vocabulaire des activités des entités productrices d'archives](http://data.culture.fr/thesaurus/resource/ark:/67717/51232822-adac-4a33-aa14-29e2c701a5ee)  - [Vocabulaire des domaines d'action ou objets des entités productrices d'archives](http://data.culture.fr/thesaurus/resource/ark:/67717/f14e8183-5885-46d6-8fc9-17ebd8f3c27e)  - [Vocabulaire pour les techniques photographiques](http://data.culture.fr/thesaurus/resource/ark:/67717/2012b973-ddb2-4540-a775-9157c3c1d7fd)      ### Italy    - [Thesaurus Portale della Cultural Italiana (PICO)](http://www.culturaitalia.it/pico/thesaurus/4.3/thesaurus_4.3.0.skos.xml#http://culturaitalia.it/pico/thesaurus/4.1%23beni_materiali_della_tradizione_e_del_folklore)  - [Soggettario Biblioteca Nazionale Centrale Firenze](http://thes.bncf.firenze.sbn.it/ricerca.php)    ### China    - [Chinese Iconography Thesaurus (CIT)](https://chineseiconography.org/)    ### Where to find controlled vocabularies/thesauri     - [Bartoc](https://bartoc.org)    ## Vocabulary / KOS Management    - [Skosmos](http://skosmos.org) Access SKOS vocabularies with SPARQL or API  - [VocBench](http://vocbench.uniroma2.it) Web-based, multilingual, collaborative platform for managing OWL, SKOS(/XL) and generic RDF datasets.  - [Ginco](https://github.com/culturecommunication/ginco) Collaborative management and alignment of vocabularies.  - [Opentheso](https://github.com/miledrousset/opentheso) Multilingual collaborative management of KOS  - [iqvoc](https://github.com/innoq/iqvoc) SKOS(-XL) Vocabulary Management System for the Semantic Web.  - [TemaTres](https://www.vocabularyserver.com) Manage, share, publish, and re-use SKOS vocabularies.       ### Vocabulary validation & conversion tools    - [Skosify](https://github.com/NatLibFi/Skosify) Validate, convert and improve SKOS vocabularies  - [qSKOS](https://github.com/cmader/qSKOS) Find quality issues in SKOS vocabularies.  - [SKOS Play](http://labs.sparna.fr/skos-play/) Render and visualise thesaurus, taxonomies or controlled vocabularies. Furthermore, convert Excel spreadsheets into SKOS files.      ## Exchange and discussions     ### Conferences     - [International Semantic Web Conference (ISWC 2019)](http://iswc2020.semanticweb.org)  - [European Semantic Web Conference (ESWC 2019)](https://2020.eswc-conferences.org)  - [CIDOC - ICOM International Committee for Documentation](http://network.icom.museum/cidoc/)  - [Workshop on Humanities in the Semantic Web (WHiSe)](http://whise.kmi.open.ac.uk)  - [Semantic Web in Libraries](http://swib.org)  - [LODLAM Summit](https://lod-lam.net/)    ### Conference not specifically on Semantic Web, but with strong ties to the community    - [International Conference on Theory and Practice of Digital Libraries (TPDL)](http://www.tpdl.eu)  - [International Conference on Metadata and Semantics Research](http://www.mtsr-conf.org/home)  - [Code4Lib](https://code4lib.org/conference/)  - [European Library Automation Group](https://elag.org/)  - [Digital Heritage](#)  - [Europeana](#)      ### Discussion groups    - [CIDOC-CRM SIG Mailing List](http://lists.ics.forth.gr/mailman/listinfo/crm-sig)  - [LODLAM - Linked Open Data in Libraries, Archives & Museum Community Group](https://groups.google.com/forum/#!forum/lod-lam)  - [w3c semantic web Mailing List](https://lists.w3.org/Archives/Public/semantic-web/)  - [w3c Linked Open Data Mailing List](https://lists.w3.org/Archives/Public/public-lod/)  - [GLAM–Wiki initiative](https://en.wikipedia.org/wiki/Wikipedia:GLAM)  - [Wikidata GLAM](https://www.wikidata.org/wiki/Wikidata:GLAM)  - [GLAM Wiki Facebook Group](https://www.facebook.com/groups/GLAMWikiGlobal/)    ### Academic Journals    - [Semantic Web Journal](http://www.semantic-web-journal.net/)  - [Journal of Web Semantics](https://www.journals.elsevier.com/journal-of-web-semantics)  - [International Journal of Web and Semantic Technology](http://www.airccse.org/journal/ijwest/ijwest.html)    ## Knowledge Graph Management    $ - Proprietary    OS - OpenSource    *f* - Free Version      - [Researchspace](https://www.researchspace.org) - (OS) platform for managing, interacting and building entry points (template, graph authoring) for RDF Stores. Specifically targeting GLAM researchers and institutions.  - [Metaphacts](https://metaphacts.com) - (OS)($) platform for managing, interacting and building entry points (template, graph authoring) for RDF Stores.  - [WissKI](http://wiss-ki.eu) - (OS) Drupal-based platform to interact and build entry point for RDF Stores.  - [LinkedDataHub](https://atomgraph.com/products/linkeddatahub/) - (OS) collaborative data and information management for RDF data.   - [GraphDB by Ontotext](https://www.ontotext.com/products/graphdb/) - ($)(*f*) RDF Database for Knowledge Graphs.       ### Linked Data Platform (LDP)    - [fedora](https://duraspace.org/fedora/) - Repository platform with native linked data support.  - [warp](https://github.com/linkeddata/warp) - Warp an LDP file manager.  - [Marmotta](https://github.com/apache/marmotta) - Apache linked data platform implementation.  - [Elda](https://github.com/epimorphics/elda) - Linked data platform from Epimorphics.  - [LDP4j](https://github.com/ldp4j/ldp4j)  - [gold](https://github.com/linkeddata/gold) - Linked Data server for Go.  - [CarbonLDP](https://github.com/CarbonLDP)  - [trellis](https://github.com/trellis-ldp/trellis)    ## Books    - [Linked Data](https://www.manning.com/books/linked-data)  - [Explorer's Guide to the Semantic Web](https://www.manning.com/books/explorers-guide-to-the-semantic-web)  - [Semantic Web Programming](https://www.wiley.com/en-us/Semantic+Web+Programming-p-9781118080603)  - [Semantic Web for the Working Ontologist](http://workingontologist.org/)  - [Programming the Semantic Web](http://shop.oreilly.com/product/9780596153823.do)  - [Building Ontologies with Basic Formal Ontology](https://mitpress.mit.edu/books/building-ontologies-basic-formal-ontology)  - [Structures for Organizing Knowledge: Exploring Taxonomies, Ontologies, and Other Schema](https://www.amazon.com/Structures-Organizing-Knowledge-Taxonomies-Ontologies/dp/1555706991)  - [Validating RDF Data](http://book.validatingrdf.com/)  - [Demystifying OWL for the Enterprise](https://doi.org/10.2200/S00824ED1V01Y201801WBE017)  - [Learning SPARQL](http://learningsparql.com)  - [Knowledge Representation](http://www.jfsowa.com/krbook/)            ## Editors    ### TextMate    - [sparql/turtle extension](https://github.com/peta/turtle.tmbundle)    ### Sublime Text    - [Turtle and SPARQL syntax highlighter](https://github.com/abcoates/sublime-text-turtle-sparql)  - [SPARQL 1.1, Turtle, TriG, N-Triples, N-Quads and Notation3 syntax highlighter](https://github.com/blake-regalia/linked-data.syntaxes)    ### BBedit    - [Turtle syntax highlighter](https://github.com/njh/bbedit-turtle)    ### VIM    - [sparql.vim](https://github.com/vim-scripts/sparql.vim) - SPARQL syntax highlighting.  - [vim-sparql](https://github.com/Omer/vim-sparql)  - [semweb.vim](https://github.com/seebi/semweb.vim)    ### Emacs    - [sparql-mode](https://github.com/ljos/sparql-mode)    ### IntelliJ    - [sparql4idea](https://github.com/mattnathan/sparql4idea) - SPARQL language plugin for IntelliJ IDEA.    ## Data Management    - [Timbuctoo](https://timbuctoo.huygens.knaw.nl) Data management, enrichment and sharing  - [Openrefine](http://openrefine.org) Data cleaning and normalisation    ### OpenRefine Reconciliation services    - [VIAF](https://viaf.org) & [ORCID](https://orcid.org) — OpenRefine reconciliation services for VIAF, ORCID, and Open Library available in [Github](https://github.com/codeforkjeff/conciliator). To make it work, it is necessary the launch a jar file for its use. After that the endpoint is available at: [http://localhost:8080/reconcile/viaf](http://localhost:8080/reconcile/viaf).    - [Geonames](http://www.geonames.org) —  The OpenRefine reconciliation services is available in [GitHub](https://github.com/cmharlow/geonames-reconcile). To make it work, it is necessary to launch a python script. After that the endpoint is available at: [http://0.0.0.0:5000/reconcile](http://0.0.0.0:5000/reconcile).    - [GND](https://lobid.org/gnd) — Reconciliation service offered by [lob-id](https://lobid.org). Endpoint for OpenRefine: [https://lobid.org/gnd/reconcile](http://services.getty.edu/vocab/reconcile/). Possible to ""Add columns from reconciled values"".    - [SNAC](http://snaccooperative.org)— Social Networks and Archival Context. Endpoint for OpenRefine: [http://openrefine.snaccooperative.org](http://services.getty.edu/vocab/reconcile/)- [Nomisma](http://nomisma.org) — Nomisma provide stable digital representations of numismatic concepts. Endpoint for OpenRefine: [http://nomisma.org/apis/reconcile](http://nomisma.org/apis/reconcile).     - [OpenCorporate](https://opencorporates.com/) — Open database of companies. Endpoint for OpenRefine: [https://opencorporates.com/reconcile](https://opencorporates.com/reconcile).    - [Getty Research Institute](https://www.getty.edu/research/tools/vocabularies/obtain/openrefine.html) - OpenRefine reconciliation services for the Getty Vocabularies (ULAN, TGN, AAT).  - [Nomisma](http://nomisma.org) - OpenRefine reconciliation service for Nomisma. Endpoint for OpenRefine: [http://nomisma.org/apis/reconcile](http://nomisma.org/apis/reconcile). Documentation on their [website](https://numishare.blogspot.com/2017/10/nomisma-launches-openrefine.html)  - [Perio.do](https://test.perio.do) - The OpenRefine reconciliation services is available in [GitHub](https://github.com/periodo/periodo-reconciler).  - [Pleiades]() - OpenRefine reconciliation service for Pleiades. Endpoint for OpenRefine: [https://geocollider-sinatra.herokuapp.com/reconcile](https://geocollider-sinatra.herokuapp.com/reconcile). More information [here](http://geocollider-sinatra.herokuapp.com).  - [GODOT](https://godot.date/home) - OpenRefine reconciliation service for GODOT. Endpoint for OpenRefine: [https://godot.date/api/openrefine/reconcile](https://godot.date/api/openrefine/reconcile).    ## Data Validation     - [pySHACL](https://github.com/RDFLib/pySHACL) - a Python validator for SHACL.  - [SHaclEX](https://github.com/weso/shaclex) - Scala implementation of SHEX and SHACL. Possible to use a demo version from a web interface.  - [RDFUnit](http://rdfunit.aksw.org/) - RDF testing suite. Include but not limited to SHACL.  - [dotNetRDF SHACL](http://langsamu.net/shacl) - SHACL procecssor that can check conformance and validate data graphs against shapes graphs.  - [YASHE](http://www.weso.es/YASHE/) -  ShEx editor with examples  - [Shex validator](http://shex.io/webapps/shex.js/doc/shex-simple.html) - Simple Online Validator for ShEx    ## IIIF    - [International Image Interoperability Framework](https://iiif.io/)  - [Awesome IIIF-related resources](https://github.com/IIIF/awesome-iiif)    ## Misc      ### Prefix     - [prefix.cc](https://github.com/cygri/prefix.cc) - Source code to the prefix.cc website.    ### Ontology    #### Documentation    - [LODE](http://www.essepuntato.it/lode) ontology documentation environment.  - [Widoco](https://github.com/dgarijo/Widoco) Ontology documentation (include LODE).    #### Management    - [OntoME](http://ontologies.dataforhistory.org) Ontology Management Environment  - [Grafo](https://gra.fo/) Collaborative and graphical ontology design    ### Alignment    - [SILK](http://silkframework.org) Linked Data Integration Framework.  - [OnAGUI](https://github.com/lmazuel/onagui) Ontology alignment GUI.  - [Alignment API](http://alignapi.gforge.inria.fr) Tool for Expressing, generating and sharing ontology alignments    ### Conversion    - [RDFConvert](https://sourceforge.net/projects/rdfconvert/) - RDFConvert is a simple command-line tool for converting RDF file betweeen different syntax formats.  - [RDF2RDF](http://www.l3s.de/~minack/rdf2rdf/) Java tool to converts RDF files from any format to any format.  - [marc2rdf](https://github.com/DOREMUS-ANR/marc2rdf) Takes as input INTERMARC-XML and UNIMARC-XML files and generates as output RDF.  - [ntcat](https://github.com/cgutteridge/ntcat) Command line tool for concatenating NTriples documents.  - [How to diff RDF](https://www.w3.org/2001/sw/wiki/How_to_diff_RDF)  - [grlc](https://github.com/CLARIAH/grlc) - Web APIs from SPARQL queries.      ### Visualisation    - [Ontology Visualisation](https://github.com/usc-isi-i2/ontology-visualization) Python tool for visualising RDF. Convert rdf to .dot and use Graphviz for constructing a visual representation.    ### Images    - [ImageSnippets](http://www.imagesnippets.com/glam/) - Platform to links RDF descriptions to images   """
Semantic web;https://github.com/owlcs/owlapitools;"""# OWLAPITOOLS 4.1.1  ## Built for OWLAPI version: 4.1.1    ### concurrentimpl  Multithread safe internals for the OWLAPI. Access with ThreadSafeOWLManager.    ### suggestor  An API to simplify recurring tasks that use a reasoner. Uses OWLKnowledgeExplorerReasoner, which works with FaCT++ or JFact.    ### atomicdecomposition  A reasoner independent, self contained implementation of atomic decomposition and modularisation. It is a port of the same tools implemented by Dmitry Tsarkov in FaCT++, but can be used in Java without JNI and without FaCT++. """
Semantic web;https://github.com/gjhiggins/RDFAlchemy;"""RDFAlchemy is an abstraction layer, allowing Python developers to use familiar  *dot notation* to access and update an RDF triplestore.            * RDFAlchemy is an **ORM** (Object Rdf Mapper) for graph data as:      * SQLAlchemy is an **ORM** (Object Relational Mapper) for relalational databases          See the the homepage at http://www.openvest.com/trac/wiki/RDFAlchemy,  and the docs at http://rdfalchemy.readthedocs.org/ for details.    [![Build Status](https://travis-ci.org/gjhiggins/RDFAlchemy.png?branch=master)](https://travis-ci.org/gjhiggins/RDFAlchemy) """
Semantic web;https://github.com/ruby-rdf/rdf-reasoner;"""# RDF::Reasoner    A partial RDFS/OWL/schema.org reasoner for [RDF.rb][].    [![Gem Version](https://badge.fury.io/rb/rdf-reasoner.png)](https://badge.fury.io/rb/rdf-reasoner)  [![Build Status](https://github.com/ruby-rdf/rdf-reasoner/workflows/CI/badge.svg?branch=develop)](https://github.com/ruby-rdf/rdf-reasoner/actions?query=workflow%3ACI)  [![Coverage Status](https://coveralls.io/repos/ruby-rdf/rdf-reasoner/badge.svg?branch=develop)](https://coveralls.io/github/ruby-rdf/rdf-reasoner?branch=develop)  [![Gitter chat](https://badges.gitter.im/ruby-rdf/rdf.png)](https://gitter.im/ruby-rdf/rdf)    ## Description    Reasons over RDFS/OWL vocabularies and schema.org to generate statements which are entailed based on base RDFS/OWL rules along with vocabulary information. It can also be used to ask specific questions, such as if a given object is consistent with the vocabulary ruleset. This can be used to implement [SPARQL Entailment][] Regimes and [RDF Schema][] entailment.    ## Features    * Entail `rdfs:subClassOf` generating an array of terms which are ancestors of the subject.  * Entail `rdfs:subPropertyOf` generating an array of terms which are ancestors of the subject.  * Entail `rdfs:domain` and `rdfs:range` adding `rdf:type` assertions on the subject or object.  * Inverse `rdfs:subClassOf` entailment, to find descendant classes of the subject term.  * Inverse `rdfs:subPropertyOf` entailment, to find descendant properties of the subject term.  * Entail `owl:equivalentClass` generating an array of terms equivalent to the subject.  * Entail `owl:equivalentProperty` generating an array of terms equivalent to the subject.  * `domainCompatible?` determines if a particular resource is compatible with the domain definition of a given predicate, based on the intersection of entailed subclasses with the property domain.  * `rangeCompatible?` determines if a particular resource is compatible with the range definition of a given predicate, based on the intersection of entailed subclasses or literal types with the property domain.  * adds `entail` and `lint` commands to the `rdf` command line interface    Domain and Range entailment include specific rules for schema.org vocabularies.    * A plain literal is an acceptable value for any property.  * If `resource` is of type `schema:Role`, `resource` is domain acceptable if any other resource references `resource` using the same property.  * If `resource` is of type `schema:Role`, it is range acceptable if it has the same property with an acceptable value.  * If `resource` is of type `rdf:List` (must be previously entailed), it is range acceptable if all members of the list are otherwise range acceptable on the same property.    ### Limiting vocabularies used for reasoning    As loading vocabularies can dominate processing time, the `RDF::Vocabulary.limit_vocabs` method can be used to set a specific set of vocabularies over which to reason. For example:        RDF::Vocabulary.limit_vocabs(:rdf, :rdf, :schema)    will limit the vocabularies which are returned from `RDF::Vocabulary.each`, which is used for reasoning and other operations over vocabularies and terms.        ## Examples  ### Determine super-classes of a class        require 'rdf/reasoner'        RDF::Reasoner.apply(:rdfs)      term = RDF::Vocabulary.find_term(""http://xmlns.com/foaf/0.1/Person"")      term.entail(:subClassOf)        # => [          foaf:Agent,          http://www.w3.org/2000/10/swap/pim/contact#Person,          geo:SpatialThing,          foaf:Person        ]    ### Determine sub-classes of a class        require 'rdf/reasoner'        RDF::Reasoner.apply(:rdfs)      term = RDF::Vocab::FOAF.Person      term.entail(:subClass) # => [foaf:Person, mo:SoloMusicArtist]    ### Determine if a resource is compatible with the domains of a property        require 'rdf/reasoner'      require 'rdf/turtle'        RDF::Reasoner.apply(:rdfs)      graph = RDF::Graph.load(""etc/doap.ttl"")      subj = RDF::URI(""https://rubygems.org/gems/rdf-reasoner"")      RDF::Vocab::DOAP.name.domain_compatible?(subj, graph) # => true    ### Determine if a resource is compatible with the ranges of a property        require 'rdf/reasoner'      require 'rdf/turtle'        RDF::Reasoner.apply(:rdfs)      graph = RDF::Graph.load(""etc/doap.ttl"")      obj = RDF::Literal(Date.new)      RDF::Vocab::DOAP.created.range_compatible?(obj, graph) # => true    ### Perform equivalentClass entailment on a graph        require 'rdf/reasoner'      require 'rdf/turtle'        RDF::Reasoner.apply(:owl)      graph = RDF::Graph.load(""etc/doap.ttl"")      graph.entail!(:equivalentClass)    ### Yield all entailed statements for all entailment methods        require 'rdf/reasoner'      require 'rdf/turtle'        RDF::Reasoner.apply(:rdfs, :owl)      graph = RDF::Graph.load(""etc/doap.ttl"")      graph.enum_statement.entail.count # >= graph.enum_statement.count    ### Lint an expanded graph        require 'rdf/reasoner'      require 'rdf/turtle'        RDF::Reasoner.apply(:rdfs, :owl)      graph = RDF::Graph.load(""etc/doap.ttl"")      graph.entail!      messages = graph.lint      messages.each do |kind, term_messages|        term_messages.each do |term, messages|          options[:output].puts ""#{kind}  #{term}""          messages.each {|m| options[:output].puts ""  #{m}""}        end      end    ## Command-Line interface  The `rdf` command-line interface is extended with `entail` and `lint` commands. `Entail` can be used in combination, with `serialize` to generate an output graph representation including entailed triples.    ## Dependencies    * [Ruby](https://ruby-lang.org/) (>= 2.6)  * [RDF.rb](https://rubygems.org/gems/rdf) (~> 3.2)    ## Mailing List    * <https://lists.w3.org/Archives/Public/public-rdf-ruby/>    ## Authors    * [Gregg Kellogg](https://github.com/gkellogg) - <https://greggkellogg.net/>    ## Contributing    * Do your best to adhere to the existing coding conventions and idioms.  * Don't use hard tabs, and don't leave trailing whitespace on any line.    Before committing, run `git diff --check` to make sure of this.  * Do document every method you add using [YARD][] annotations. Read the    [tutorial][YARD-GS] or just look at the existing code for examples.  * Don't touch the `.gemspec`, `VERSION` or `AUTHORS` files. If you need to    change them, do so on your private branch only.  * Do feel free to add yourself to the `CREDITS` file and the corresponding    list in the the `README`. Alphabetical order applies.  * Do note that in order for us to merge any non-trivial changes (as a rule    of thumb, additions larger than about 15 lines of code), we need an    explicit [public domain dedication][PDD] on record from you,    which you will be asked to agree to on the first commit to a repo within the organization.    Note that the agreement applies to all repos in the [Ruby RDF](https://github.com/ruby-rdf/) organization.    ## License    This is free and unencumbered public domain software. For more information,  see <https://unlicense.org/> or the accompanying {file:UNLICENSE} file.    [Ruby]:             https://ruby-lang.org/  [RDF]:              https://www.w3.org/RDF/  [YARD]:             https://yardoc.org/  [YARD-GS]:          https://rubydoc.info/docs/yard/file/docs/GettingStarted.md  [PDD]:              https://unlicense.org/#unlicensing-contributions  [SPARQL]:           https://en.wikipedia.org/wiki/SPARQL  [SPARQL Query]:     https://www.w3.org/TR/2013/REC-sparql11-query-20130321/  [SPARQL Entailment]:https://www.w3.org/TR/sparql11-entailment/  [RDF 1.1]:          https://www.w3.org/TR/rdf11-concepts  [RDF.rb]:           https://www.rubydoc.info/github/ruby-rdf/rdf/  [RDF Schema]:       https://www.w3.org/TR/rdf-schema/  [Rack]:             https://rack.github.io/ """
Semantic web;https://github.com/OpenTriply/YASGUI;"""# YASGUI    Useful links:    - User documentation: https://triply.cc/docs/yasgui  - Developer documentation: https://triply.cc/docs/yasgui-api  - Documentation Github repository (feel free to add a PR for improvements): https://github.com/TriplyDB/Documentation    ## Installation    Below are instructions on how to include Yasgui in your project. If you only want to install Yasr or Yasqe, replace yasgui in the commands below    ### npm    ```sh  npm i @triply/yasgui  ```    ### Yarn    ```sh  yarn add @triply/yasgui  ```    ## Local development    #### Installing dependencies    Run `yarn install`.    #### Running Yasgui locally    To develop locally, run `yarn run dev`    #### Compiling Yasgui    Run `yarn run build`. It'll store the transpiled js/css files in the `build` directory    ## License    This software is written by Triply.    This code is released under the MIT license. """
Semantic web;https://github.com/goerlitz/rdffederator;"""# About #    SPLENDID provides a federation infrastructure for distributed, linked RDF data sources. SPARQL queries are executed transparently across a set of pre-configured SPARQL endpoints. Automatic source selection and query optimization is based on statistical information provided by VoiD descriptions.    This is an ongoing research project at the Institute for Web Science and Technologies, University of Koblenz, Germany. Currently, the software is offered as a stable alpha version. New features and updates may be added over time.    # Quick Start #      * check out the source code from svn (eclipse project)  > > `svn checkout http://rdffederator.googlecode.com/svn/trunk/ splendid`    * compile sources    * run `SPLENDID.sh` or `SPLENDID.bat` with a repository configuration and a SPARQL query file as parameters, e.g.  > > `./SPLENDID.sh SPLENDID-config.n3 eval/queries/cd/`    # Customizing SPLENDID federation #      * choose a set of data sources    * generate voiD statistics for all data sources      * download data source dump      * 1st option: run voiD generator shell script on N-Triples file  > > > `$>scripts/generate_void_description.sh dump.nt void.n3`      * 2nd option: run Java voiD generator on subject-sorted RDF file  > > > `$>scripts/run_voidgen dump_sorted_by_subject`    * add `fed:member` definition for each data source to the federation configuration (see `SPLENDID-config.n3`)      * `rep:repositoryType` is always ""west:VoidRepository""      * `fed:voidDescription` requires an URI pointing to the voiD file      * `void:sparqlEndpoint` is the URI of the source's SPARQL endpoint (overrides the endpoint definition in the voiD file)    * run SPLENDID with your custom configuration    # Technical Overview #    [Presentation at Consuming Linked Open Data Workshop (ISWC 2011):](http://www.slideshare.net/OlafGoerlitz/splendid-9858478)   """
Semantic web;https://github.com/yahoo/anthelion;"""# nutch-anth  Anthelion is a Nutch plugin for focused crawling of semantic data.  The project is an open-source project released under the Apache License 2.0.    Note: This project contains the complete Nutch 1.6 distribution. The plugin itself can be found in /src/plugin/parse-anth    Table of Contents  -----------------  * [Nutch-Anthelion Plugin](#nutch-anthelion plugin)    * [Plugin Overview] (#plugin-overview)    * [Usage and Development] (#usage-and-development)    * [Some Results] (#some-results)    * [3rd Party Libraries] (#3rd-party-libraries)  * [Anthelion](#anthelion)   * [References](#references)    Nutch-Anthelion Plugin  ---------  The plugin uses an online learning approach to predict data-rich web pages based on the context of the page as well as using feedback from the extraction of metadata from previously seen pages [1].    ### Plugin Overview    To perform the focused crawling the plugin implements three extensions:    1. **AnthelionScoringFilter** (implements the ScoringFilter interface): wraps around the Anthelion online classifier to classify newly discovered outlinks, as relevant or not. This extension gives score to each outlink, which is then used in the Generate stage, i.e., the URLs for the next fetch cycle are selected based on the score. This extension also pushes feedback to the classifier for the already parsed web pages. The online classifier can be configured and tuned (see [Usage and Development](#usage and development)).    2. **WdcParser** (implements the Parser interface): This extension parses the web page content and tries to extract semantic data. The parser is adaptation of an already existing Nutch parser plugin implemented in [2]. The parser is based on the [any23 library](https://any23.apache.org/) and is able to extract Microdata, Microformats and RDFa annotation from HTML. The extracted triples are stored in the *Content* field.    3. **TripleExtractor** (implements the IndexingFilter interface): This extension stores new fields to the index that can be later used for querying.    An overview of the complete crawling process using the Anthelion plugin is given in the following figure.    <p align=""center"">    <img src=""https://github.com/yahoo/anthelion/blob/master/documentation/architecture.png?raw=true"" alt=""Anthelion Architecture""/>  </p>      ### Usage and Development    As mentioned in the beginning of the document this project contains the complete Nutch 1.6 code, including the plugin. If you download the complete project, there is no need for any changes and settings. If you want to download only the plugin, please download only the nutch-anth.zip from the root of the folder and go to step 2 of the configuration. If you want to contribute to the plugin and/or want to use the sources with another version of Nutch, please follow the following instructions:    1. Download and copy the /src/plugin/parse-anth folder into your Nutch's plugins directory.    2. Enable the plugin in conf/nutch-site.xml by adding *parse-anth* in the *plugin.includes* property.    3. Copy the properties from nutch-anth.xml to conf/nutch-site.xml.    	3.1. Download the baseline.properties file and set the property *anth.scoring.classifier.PropsFilePath* conf/nutch-site.xml to point to the file. This file contains all configurations for the online classifier.    4. In order for ant to compile and deploy the plugin you need to edit the src/plugin/build.xml, by adding the following line in the *deploy* target:  	```xml  	<ant dir=""parse-anth"" target=""deploy""/>  	```  5. Add the following lines in conf/parse-plugins.xml:  	```xml  	<mimeType name=""text/html"">  			<plugin id=""parse-anth"" />  		</mimeType>  	  	        <mimeType name=""application/xhtml+xml"">  			<plugin id=""parse-anth"" />  		</mimeType>  	```  6. Add the following line in the *alias* property in conf/parse-plugins.xml:  	  	```xml  	<alias name=""parse-anth"" extension-id=""com.yahoo.research.parsing.WdcParser"" />  	```  7. Copy the *lib* folder into the root of the Nutch distribution.    8. Run `mvn package` inside the *anthelion* folder. This will create the jar ""Anthelion-1.0.0-jar-with-dependencies.jar"". Copy the jar to src/plugin/parse-anth/lib.    9. Add the following field in conf/schema.xml (also add it to the Solr schema.xml, if you are using Solr):  	```xml  	<field name=""containsSem"" type=""text_general"" stored=""true"" indexed=""true""/>  	```  10. Run ant in the root of your folder.    ### Some Results    In order to evaluate the focused crawler we measure the precision of the crawled pages, i.e., the ratio of the number of crawled web pages that contain semantic data and the total number of crawled web pages.  So far, we have evaluated using three different seeds sample, and several different configurations. An overview is given in the following table.    <table border=0 cellpadding=0 cellspacing=0 width=532 style='border-collapse:   collapse;table-layout:fixed;width:532pt'>   <col width=65 style='width:65pt'>   <col width=77 style='mso-width-source:userset;mso-width-alt:3285;width:77pt'>   <col width=65 span=2 style='mso-width-source:userset;mso-width-alt:2773;   width:65pt'>   <col class=xl65535 width=65 style='mso-width-source:userset;mso-width-alt:   2773;width:65pt'>   <col width=65 span=2 style='mso-width-source:userset;mso-width-alt:2773;   width:65pt'>   <col class=xl65535 width=65 style='mso-width-source:userset;mso-width-alt:   2773;width:65pt'>   <tr height=15 style='height:15.0pt'>    <td rowspan=2 height=30 class=xl65 width=65 style='height:30.0pt;width:65pt'>#seeds</td>    <td rowspan=2 class=xl68 width=77 style='width:77pt'>nutch options</td>    <td colspan=3 class=xl65 width=195 style='border-left:none;width:195pt'>standard    scoring</td>    <td colspan=3 class=xl65 width=195 style='border-left:none;width:195pt'>anthelion    scoring</td>   </tr>   <tr height=15 style='height:15.0pt'>    <td height=15 class=xl66 style='height:15.0pt;border-top:none;border-left:    none'>#total pages</td>    <td class=xl66 style='border-top:none;border-left:none'>#sem pages</td>    <td class=xl67 style='border-top:none;border-left:none'>precision</td>    <td class=xl66 style='border-top:none;border-left:none'>#total pages</td>    <td class=xl66 style='border-top:none;border-left:none'>#sem pages</td>    <td class=xl67 style='border-top:none;border-left:none'>precision</td>   </tr>   <tr height=15 style='height:15.0pt'>    <td height=15 class=xl66 align=right style='height:15.0pt;border-top:none'>2</td>    <td class=xl69 style='border-top:none;border-left:none'>-depth 3 -topN 1<span    style='display:none'>5</span></td>    <td class=xl66 align=right style='border-top:none;border-left:none'>17</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>2</td>    <td class=xl67 align=right style='border-top:none;border-left:none'>0.12</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>22</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>7</td>    <td class=xl67 align=right style='border-top:none;border-left:none'>0.32</td>   </tr>   <tr height=15 style='height:15.0pt'>    <td height=15 class=xl66 align=right style='height:15.0pt;border-top:none'>10</td>    <td class=xl69 style='border-top:none;border-left:none'>-depth 8 -topN 1<span    style='display:none'>5</span></td>    <td class=xl66 align=right style='border-top:none;border-left:none'>99</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>2</td>    <td class=xl67 align=right style='border-top:none;border-left:none'>0.02</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>49</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>11</td>    <td class=xl67 align=right style='border-top:none;border-left:none'>0.22</td>   </tr>   <tr height=15 style='height:15.0pt'>    <td height=15 class=xl66 align=right style='height:15.0pt;border-top:none'>1000</td>    <td class=xl69 style='border-top:none;border-left:none'>-depth 4 -topN 1<span    style='display:none'>000</span></td>    <td class=xl66 align=right style='border-top:none;border-left:none'>3200</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>212</td>    <td class=xl67 align=right style='border-top:none;border-left:none'>0.07</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>2910</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>1469</td>    <td class=xl67 align=right style='border-top:none;border-left:none'>0.50</td>   </tr>   <tr height=15 style='height:15.0pt'>    <td height=15 class=xl66 align=right style='height:15.0pt;border-top:none'>1000</td>    <td class=xl70 style='border-top:none;border-left:none'>    <meta charset=utf-8>    <span>-depth 5 -topN 2<span style='display:none'>000</span></span></td>    <td class=xl66 align=right style='border-top:none;border-left:none'>8240</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>511</td>    <td class=xl67 align=right style='border-top:none;border-left:none'>0.06</td>    <td class=xl66 align=right style='border-top:none;border-left:none'>    <meta charset=utf-8>    <span>9781</span></td>    <td class=xl66 align=right style='border-top:none;border-left:none'>7587</td>    <td class=xl67 align=right style='border-top:none;border-left:none'>0.78</td>   </tr>  </table>    The pairwise comparison is given in the following chart:  <p align=""center"">    <img src=""https://github.com/yahoo/anthelion/blob/master/documentation/results_chart.png?raw=true"" alt=""Architecture""/>  </p>    ### 3rd Party Libraries  The Anthelion plugin uses several 3rd party open source libraries and tools.  Here we summarize the tools used, their purpose, and the licenses under which they're released.    1. This project includes the sources of Apache Nutch 1.6 (Apache License 2.0 - http://www.apache.org/licenses/LICENSE-2.0)  	* http://nutch.apache.org/    2. Apache Any23 1.2 (Apache License 2.0 - http://www.apache.org/licenses/LICENSE-2.0)  	* Used for extraction of semantic annotation from HTML.  	* https://any23.apache.org/  	* More information about the 3rd party dependencies used in the any23 library can be found [here](https://any23.apache.org/)      3. The classes com.yahoo.research.parsing.WdcParser and com.yahoo.research.parsing.FilterableTripleHandler are modified versions of existing Nutch plugins (Apache License 2.0 - http://www.apache.org/licenses/LICENSE-2.0)  	* Used for parsing the crawled web pages  	* Hellman et al. [2]; https://www.assembla.com/spaces/commondata/subversion/source/HEAD/extractorNutch    4. For the libraries and tools used in Anthelion, please check the Anthelion [README file] (https://github.com/yahoo/anthelion/blob/master/anthelion/README.md).     Anthelion  ---------  For more details about the Anthelion project please check the Anthelion [README file] (https://github.com/yahoo/anthelion/blob/master/anthelion/README.md).    References  ----------  [1]. Meusel, Robert, Peter Mika, and Roi Blanco. ""Focused Crawling for Structured Data."" Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management. ACM, 2014.    [2]. Hellmann, Sebastian, et al. ""Knowledge Base Creation, Enrichment and Repair."" Linked Open Data--Creating Knowledge Out of Interlinked Data. Springer International Publishing, 2014. 45-69.  	      ###Troubleshooting  (TODO) """
Semantic web;https://github.com/wbsg/ldif;"""LDIF  ====    LDIF translates heterogeneous Linked Data from the Web into a clean, local target representation while keeping track of data provenance.    #### Get Started    To see LDIF in action, please follow these steps:    1. [Download](http://dl.mes-semantics.com/ldif/ldif-0.5.2.zip) the latest release  2. Unzip the archive and change into the extracted directory `ldif-0.5.2`  3. Run LDIF on the Music example            bin/ldif examples/music/light/schedulerConfig.xml        4. Monitor the progress of the scheduled jobs through the status interface, at [http://localhost:5343](http://localhost:5343)  5. Integration results will be written in the working directory, into `integrated_music_light.nq`      #### Learn More    Learn more about LDIF at [http://ldif.wbsg.de/](http://ldif.wbsg.de/).    #### Feedback    For questions and feedback please use the [LDIF Google Group](http://groups.google.com/group/ldif). """
Semantic web;https://github.com/knakk/rdf2rdf;"""## rdf2rdf  CLI tool to convert between different RDF serialization formats.    Primarly made to test and showcase the capabilites of the [rdf package](https://github.com/knakk/rdf).    ## Status    Currently supported input formats: RDF/XML, N-Triples, N-Quads, Turtle.    Currently supported output formats: N-Triples, Turtle.    More formats are coming soon.    ## Installation  Install as you would any other Go package:        go get -u github.com/knakk/rdf2rdf    Provided that `GOPATH/bin` is on your `PATH`, you're good to go.    When the tool has proven stable and complete, I can provide binaries for the most common OS'es for download.    ## Usage  <pre>  rdf2rdf  -------  Convert between different RDF serialization formats.    Usage:  	rdf2rdf -in=input.xml -out=output.ttl    Options:    -h --help      Show this message.    -in            Input file.    -out           Output file.    -stream=true   Streaming mode.    -v=false       Verbose mode (shows progress indicator)    By default the converter is streaming both input and output, emitting  converted triples/quads as soon as they are available. This ensures you can  convert huge files with minimum memory footprint. However, if you have  small datasets you can choose to load all data into memory before conversion.  This makes it possible to sort the data, remove duplicate triples, and  potentially generate more compact Turtle serializations, maximizing predicate  and object lists. Do this by setting the flag stream=false.    Conversion from a quad-format to a triple-format will disregard the triple's  context (graph). Conversion from a triple-format to a quad-format is not  supported.    Input and ouput formats are determined by file extensions, according to  the following table:      Format    | File extension    ----------|-------------------    N-Triples | .nt    N-Quads   | .nq    RDF/XML   | .rdf .rdfxml .xml    Turtle    | .ttl    </pre>"""
Semantic web;https://github.com/sisinflab-swot/cowl;"""# Cowl    *Cowl* is a lightweight C API for working with OWL 2 ontologies, developed by  [SisInf Lab][swot] at the [Polytechnic University of Bari][poliba].    ### Documentation    Documentation and build instructions are available [online][docs].    ### Changelog    See [CHANGELOG.md](CHANGELOG.md)    ### Copyright and License    Copyright (c) 2019-2021 [SisInf Lab][swot], [Polytechnic University of Bari][poliba]    Cowl is distributed under the [Eclipse Public License, Version 2.0][epl2].    [docs]: http://swot.sisinflab.poliba.it/cowl  [epl2]: https://www.eclipse.org/legal/epl-2.0  [poliba]: http://www.poliba.it  [swot]: http://swot.sisinflab.poliba.it """
Semantic web;https://github.com/mmlab/Git2PROV;"""  [![Build Status](https://travis-ci.org/vladistan/Git2PROV.svg?branch=feature%2Funit-tests)](https://travis-ci.org/vladistan/Git2PROV.svg?branch=feature%2Funit-tests)  [![Coverage Status](https://img.shields.io/coveralls/vladistan/Git2PROV.svg)](https://coveralls.io/r/vladistan/Git2PROV)    #Git2PROV  Check out our [One-minute Git2PROV tutorial on Vimeo](http://vimeo.com/70980809)    For an in-depth description of this tool and its creation, we refer to the following paper:    [Git2PROV: Exposing Version Control System Content as W3C PROV](http://www.iswc2013.semanticweb.org/sites/default/files/iswc_demo_32_0.pdf)  by Tom De Nies, Sara Magliacane, Ruben Verborgh, Sam Coppens, Paul Groth, Erik Mannens, and Rik Van de Walle  Published in 2013 in the Poster and Demo Proceedings of the 12th International Semantic Web Conference.    #Disclaimer and License  Git2PROV is a joint work of [Ghent University](http://www.ugent.be/) - [iMinds](http://www.iminds.be/) - [Multimedia Lab](http://mmlab.be/), and the [Data2Semantics](http://www.data2semantics.org/) project of the [VU University Amsterdam](http://www.vu.nl/) .    The people involved are:  * Tom De Nies (Ghent University - iMinds - MMLab)  * Sara Magliacane (VU Amsterdam)  * Ruben Verborgh (Ghent University - iMinds - MMLab)  * Sam Coppens (Ghent University - iMinds - MMLab)  * Paul Groth (VU Amsterdam)  * Erik Mannens (Ghent University - iMinds - MMLab)  * Rik Van de Walle (Ghent University - iMinds - MMLab)    We chose to make Git2PROV open source under [GPL Version 3 license](http://www.gnu.org/licenses/gpl.html) because we believe this will lead it to its full potential, and be of much more value to the Web community than a single isolated instance running on a server somewhere.    So in short, you are free to use and modify Git2PROV for non-commercial purposes, as long as you make your stuff open source as well and you properly credit us. This is most conveniently done by citing the paper mentioned above.    #Installation    Make sure you have node.js and git installed and in the system PATH variable. Then, run:  ```  [sudo] npm install -g git2prov  ```    ## Converting a repository  To convert a single repository, run:        git2prov git_url [serialization]    For example:        git2prov git@github.com:RubenVerborgh/N3.js.git PROV-JSON    ## Running the server  To run the server, use the following command:        git2prov-server [port]    For example:        git2prov-server 8905    Then go to your browser and enter the following url:  http://localhost:8905/    This will give you the [standard Git2PROV homepage](http://git2prov.org).    TO use the service directly, use the following URL:  http://localhost:8905/git2prov?giturl=<your open git repository>&serialization=<your serialization of choice>&[optional parameters]  The OPTIONAL parameters are:    serialization:  * PROV-N (default)  * PROV-JSON  * PROV-O  * PROV-XML    shortHashes  * true ---> This will force the git log to use short hashes, making the output more readable by humans      ignore  * <provenanceRelation> ---> This provenance relation will not appear in the output. Multiple values are possible.        Example:  http://localhost:8905/git2prov?giturl=<your open git repository>&serialization=PROV-JSON&shortHashes=true&ignore=wasInformedBy&ignore=used    To start a proxy server:      node proxy.js <port> <target port>  for example:      node proxy.js 80 8905    ##Running as a service on a Linux/UNIX machine  This script is used in combination with init.d. You could also modify it to work with upstart.    Copy the startup script ""git2prov"" to your /etc/init.d directory:  ```  sudo cp scripts/git2prov /etc/init.d/git2prov  ```  Make it executable  ```  sudo chmod a+x /etc/init.d/git2prov  ```  add it to the startup services  ```  update-rc.d git2prov defaults  ```  You can now do commands such as  ```  sudo service git2prov start  sudo service git2prov restart  sudo service git2prov stop  ```    And the service should automatically start when the machine is rebooted. """
Semantic web;https://github.com/alangrafu/turtle-in-html;"""# Introduction  ##Version 0.0.0.1    Nowadays it is possible to embed RDF in Turtle [1], however it is not easy to see if there is RDF in a page. Inspired by Crowbar [2], I created a tool that displays the triples available in Turtle format. Just copy the link available at http://graves.cl/turtle-in-html/ to your bookmarks and click on it whenever you are in a page with turtle embedded.   turtle-in-html uses Masahide Kanzaki's excellent Turtle parser [3] and is licenses under LGPL.      1. http://www.w3.org/TR/turtle/#in-html  2. http://nytimes.github.io/svg-crowbar/  3. http://www.kanzaki.com/works/2006/misc/0308turtle.html      # Installation and use    Simply go to http://graves.cl/turtle-in-html/ and add the link in your bookrmark. Later, in a page containing turtle (such as http://graves.cl/turtle-in-html/), you can click on that bookmark and you'll see the triples.  # Known bugs    Currently it only works in Firefox and Chrome and hasn't been tested in other browsers. Feel free to report any bugs. """
Semantic web;https://github.com/gbv/dso;"""This repository contains the **Document Service Ontology (DSO)**    The URI of this ontology is going to be <http://purl.org/ontology/dso> and it's  URI namespace is going to be <http://purl.org/ontology/dso#> (not registered  yet).    The current version of this specification can be found at <http://gbv.github.io/dso/>  and a public git repository at <https://github.com/gbv/dso>.  [Feedback](https://github.com/gbv/dso/issues) is welcome!    The following diagram illustrates the classes and properties defined in this ontology.    ~~~      +---------------------+      |  dso:ServiceEvent   |      | +-----------------+ |  hasDocument    +-----------------------+      | | DocumentService |------------------>| ...any document class |      | |                 |<------------------|                       |      | |  Loan           | |  hasService     +-----------------------+      | |  Presentation   | |      | |  Interloan      | |      | |  OpenAccess     | |      | |  Digitization   | |      | |  Identification | |      | |  ...            | |      | +-----------------+ |      +---------------------+  ~~~     """
Semantic web;https://github.com/knakk/rdf;"""# rdf    This package introduces data structures for representing RDF resources, and includes functions for parsing and serialization of RDF data.    For complete documentation see [godoc](http://godoc.org/github.com/knakk/rdf). """
Semantic web;https://github.com/rdf-ext/rdf-ext;"""# RDF-Ext    [![Build Status](https://img.shields.io/github/workflow/status/rdf-ext/rdf-ext/CI)](https://github.com/rdf-ext/rdf-ext/actions/workflows/ci.yaml)    [![npm version](https://img.shields.io/npm/v/rdf-ext.svg)](https://www.npmjs.com/package/rdf-ext)    RDF-Ext is a JavaScript library that extends the [RDF/JS](#rdf-js) specs to handle RDF data in a developer-friendly way.    For more details, please check [rdf-ext.org](https://rdf-ext.org/) """
Semantic web;https://github.com/stardog-union/pellet;"""Pellet: An Open Source OWL DL reasoner for Java  -----------------------------------------------    [![Gitter](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/complexible/pellet?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    Pellet is the OWL 2 DL reasoner:      * [open source](https://github.com/complexible/pellet/blob/master/LICENSE.txt) (AGPL) or commercial license  * pure Java  * developed and [commercially supported](http://complexible.com/) by Complexible Inc.     Pellet can be used with Jena or OWL-API libraries. Pellet provides functionality to check consistency of ontologies, compute the classification hierarchy,   explain inferences, and answer SPARQL queries.    _Pellet 3.0, a closed source, next-gen version of Pellet, is embedded and available in [Stardog](http://stardog.com/), the RDF database._    Feel free to fork this repository and submit pull requests if you want to  see changes, new features, etc. in Pellet.    Documentation about how to use Pellet is in the doc/ directory and there are some   code samples in the examples/ directory.                                        Commercial support for Pellet is [available](http://complexible.com/). The [Pellet FAQ](http://clarkparsia.com/pellet/faq) answers some frequently asked questions.    There is a [pellet-users mailing list](https://groups.google.com/forum/?fromgroups#!forum/pellet-users) for questions and feedback. You can search [pellet-users archives](http://news.gmane.org/gmane.comp.web.pellet.user).   Bug reports and enhancement requests should be sent to the mailing list. Issues are on [Github](http://github.com/complexible/pellet/issues).    Thanks for using Pellet. """
Semantic web;https://github.com/mcollina/levelgraph-jsonld;"""LevelGraph-JSONLD  ===========    ![Logo](https://github.com/levelgraph/levelgraph/raw/master/logo.png)    [![Build Status](https://travis-ci.org/levelgraph/levelgraph-jsonld.png)](https://travis-ci.org/levelgraph/levelgraph-jsonld)  [![Coverage Status](https://coveralls.io/repos/levelgraph/levelgraph-jsonld/badge.png)](https://coveralls.io/r/levelgraph/levelgraph-jsonld)  [![Dependency Status](https://david-dm.org/levelgraph/levelgraph-jsonld.png?theme=shields.io)](https://david-dm.org/levelgraph/levelgraph-jsonld)  [![Sauce Labs Tests](https://saucelabs.com/browser-matrix/levelgraph-jsonld.svg)](https://saucelabs.com/u/levelgraph-jsonld)    __LevelGraph-JSONLD__ is a plugin for  [LevelGraph](http://github.com/levelgraph/levelgraph) that adds the  ability to store, retrieve and delete JSON-LD objects.  In fact, it is a full-blown Object-Document-Mapper (ODM) for  __LevelGraph__.    ## Install    ### Node.js    Adding support for JSON-LD to LevelGraph is easy:  ```shell  $ npm install level levelgraph levelgraph-jsonld --save  ```  Then in your code:  ```javascript  var level      = require('level'),      yourDB     = level('./yourdb'),      levelgraph = require('levelgraph'),      jsonld     = require('levelgraph-jsonld'),      db         = jsonld(levelgraph(yourDB));  ```    At the moment it requires node v0.10.x, but the port to node v0.8.x  should be straighforward.  If you need it, just open a pull request.    ## Browser    If you use [browserify](http://browserify.org/) you can use this package  in a browser just as in node.js. Please also take a look at [Browserify  section in LevelGraph package](https://github.com/levelgraph/levelgraph#browserify)    You can also use standalone browserified version from `./build`  directory or use [bower](http://bower.io)    ```shell  $ bower install levelgraph-jsonld --save  ```  It will also install its dependency levelgraph! Now you can simply:    ```html  <script src=""bower_components/levelgraph/build/levelgraph.js""></script>  <script src=""bower_components/levelgraph-jsonld/build/levelgraph-jsonld.js""></script>  <script>    var db = levelgraphJSONLD(levelgraph('yourdb'));  </script>  ```    ## Usage    We assume in following examples that you created database as explained  above!  ```js  var level  = require('level'),      yourDB = level('./yourdb'),      db     = levelgraphJSONLD(levelgraph(yourDB));  ```    `'base'` can also be specified when you create the db:  ```javascript  var level      = require('level'),      yourDB     = level('./yourdb'),      levelgraph = require('levelgraph'),      jsonld     = require('levelgraph-jsonld'),      opts       = { base: 'http://matteocollina.com/base' },      db         = jsonld(levelgraph(yourDB), opts);  ```    > From v1, overwriting and deleting is more conservative. If you rely on the previous behavior you can set the `overwrite` option to `true` (when creating the db or as options to `put` and `del`) to:  >  - overwrite all existing triples when using `put`  >  - delete all blank nodes recursively when using `del` (cf upcoming `cut` function)  > This old api will be phased out.    ### Put    Please keep in mind that LevelGraph-JSONLD __doesn't store the original  JSON-LD document but decomposes it into triples__! It stores literals  double quoted with datatype if other then string. If you use plain  LevelGraph methods, instead trying to match number `42` you need to try  matching `""42""^^http://www.w3.org/2001/XMLSchema#integer`     Storing triples from JSON-LD document is extremely easy:  ```javascript  var manu = {    ""@context"": {      ""name"": ""http://xmlns.com/foaf/0.1/name"",      ""homepage"": {        ""@id"": ""http://xmlns.com/foaf/0.1/homepage"",        ""@type"": ""@id""      }    },    ""@id"": ""http://manu.sporny.org#person"",    ""name"": ""Manu Sporny"",    ""homepage"": ""http://manu.sporny.org/""  };    db.jsonld.put(manu, function(err, obj) {    // do something after the obj is inserted  });  ```    if the top level objects have no `'@id'` key, one will be generated for  each, using a UUID and the `'base'` argument, like so:  ```javascript  delete manu['@id'];  db.jsonld.put(manu, { base: 'http://this/is/an/iri' }, function(err, obj) {    // obj['@id'] will be something like    // http://this/is/an/iri/b1e783b0-eda6-11e2-9540-d7575689f4bc  });  ```    `'base'` can also be [specified when you create the db](#usage).    __LevelGraph-JSONLD__ also support nested objects, like so:  ```javascript  var nested = {    ""@context"": {      ""name"": ""http://xmlns.com/foaf/0.1/name"",      ""knows"": ""http://xmlns.com/foaf/0.1/knows""    },    ""@id"": ""http://matteocollina.com"",    ""name"": ""Matteo"",    ""knows"": [{      ""name"": ""Daniele""    }, {      ""name"": ""Lucio""    }]  };    db.jsonld.put(nested, function(err, obj) {    // do something...  });  ```    ### Get    Retrieving a JSON-LD object from the store requires its `'@id'`:  ```javascript  db.jsonld.get(manu['@id'], { '@context': manu['@context'] }, function(err, obj) {    // obj will be the very same of the manu object  });  ```    The format of the loaded object is entirely specified by the  `'@context'`, so have fun :).    As with `'put'` it correctly support nested objects. If nested objects didn't originally include `'@id'` properties, now they will have them since `'put'` generates them by using UUID and formats  them as *blank node identifiers*:  ```javascript  var nested = {    ""@context"": {      ""name"": ""http://xmlns.com/foaf/0.1/name"",      ""knows"": ""http://xmlns.com/foaf/0.1/knows""    },    ""@id"": ""http://matteocollina.com"",    ""name"": ""Matteo"",    ""knows"": [{      ""name"": ""Daniele""    }, {      ""name"": ""Lucio""    }]  };    db.jsonld.put(nested, function(err, obj) {    // obj will be    // {    //   ""@context"": {    //     ""name"": ""http://xmlns.com/foaf/0.1/name"",    //     ""knows"": ""http://xmlns.com/foaf/0.1/knows""    //   },    //   ""@id"": ""http://matteocollina.com"",    //   ""name"": ""Matteo"",    //   ""knows"": [{    //     ""@id"": ""_:7053c150-5fea-11e3-a62e-adadc4e3df79"",    //     ""name"": ""Daniele""    //   }, {    //     ""@id"": ""_:9d2bb59d-3baf-42ff-ba5d-9f8eab34ada5"",    //     ""name"": ""Lucio""    //   }]    // }  });  ```    ### Delete    In order to delete an object, you need to pass the document to the `'del'` method which will delete only the properties specified in the document:  ```javascript  db.jsonld.del(manu, function(err) {    // do something after it is deleted!  });  ```    Note that blank nodes are ignored, so to delete blank nodes you need to pass the `cut: true` option (you can also add the `recurse: true`option) or use the `'cut'` method below.    > Note that since v1 `'del'` doesn't support passing an IRI anymore.    ### Cut    In order to delete the blank nodes object, you can just pass it's `'@id'` to the  `'cut'` method:  ```javascript  db.jsonld.cut(manu['@id'], function(err) {    // do something after it is cut!  });  ```    You can also pass an object, but in this case the properties are not used to determine which triples will be deleted and only the `@id`s are considered.    Using the `recurse` option you can follow all links and blank nodes (which might result in deleting more data than you expect)  ```javascript  db.jsonld.cut(manu['@id'], { recurse: true }, function(err) {    // do something after it is cut!  });  ```    ### Searching with LevelGraph    __LevelGraph-JSONLD__ does not support searching for objects, because  that problem is already solved by __LevelGraph__ itself. This example  search finds friends living near Paris:  ```javascript  var manu = {    ""@context"": {      ""@vocab"": ""http://xmlns.com/foaf/0.1/"",      ""homepage"": { ""@type"": ""@id"" },      ""knows"": { ""@type"": ""@id"" },      ""based_near"": { ""@type"": ""@id"" }    },    ""@id"": ""http://manu.sporny.org#person"",    ""name"": ""Manu Sporny"",    ""homepage"": ""http://manu.sporny.org/"",    ""knows"": [{      ""@id"": ""https://my-profile.eu/people/deiu/card#me"",      ""name"": ""Andrei Vlad Sambra"",      ""based_near"": ""http://dbpedia.org/resource/Paris""    }, {      ""@id"": ""http://melvincarvalho.com/#me"",      ""name"": ""Melvin Carvalho"",      ""based_near"": ""http://dbpedia.org/resource/Honolulu""    }, {      ""@id"": ""http://bblfish.net/people/henry/card#me"",      ""name"": ""Henry Story"",      ""based_near"": ""http://dbpedia.org/resource/Paris""    }, {      ""@id"": ""http://presbrey.mit.edu/foaf#presbrey"",      ""name"": ""Joe Presbrey"",      ""based_near"": ""http://dbpedia.org/resource/Cambridge""    }]  };    var paris = 'http://dbpedia.org/resource/Paris';    db.jsonld.put(manu, function(){    db.search([{      subject: manu['@id'],      predicate: 'http://xmlns.com/foaf/0.1/knows',      object: db.v('webid')    }, {      subject: db.v('webid'),      predicate: 'http://xmlns.com/foaf/0.1/based_near',      object: paris    }, {      subject: db.v('webid'),      predicate: 'http://xmlns.com/foaf/0.1/name',      object: db.v('name')    }    ], function(err, solution) {      // solution contains      // [{      //   webid: 'http://bblfish.net/people/henry/card#me',      //   name: '""Henry Story""'      // }, {      //   webid: 'https://my-profile.eu/people/deiu/card#me',      //   name: '""Andrei Vlad Sambra""'      // }]    });  });  ```  ## Changes    [CHANGELOG.md](https://github.com/levelgraph/levelgraph-jsonld/blob/master/CHANGELOG.md)  **including migration info for breaking changes**      ## Contributing to LevelGraph-JSONLD    * Check out the latest master to make sure the feature hasn't been    implemented or the bug hasn't been fixed yet  * Check out the issue tracker to make sure someone already hasn't    requested it and/or contributed it  * Fork the project  * Start a feature/bugfix branch  * Commit and push until you are happy with your contribution  * Make sure to add tests for it. This is important so I don't break it    in a future version unintentionally.  * Please try not to mess with the Makefile and package.json. If you    want to have your own version, or is otherwise necessary, that is    fine, but please isolate to its own commit so I can cherry-pick around    it.    ## LICENSE - ""MIT License""    Copyright (c) 2013-2017 Matteo Collina and LevelGraph-JSONLD contributors    Permission is hereby granted, free of charge, to any person  obtaining a copy of this software and associated documentation  files (the ""Software""), to deal in the Software without  restriction, including without limitation the rights to use,  copy, modify, merge, publish, distribute, sublicense, and/or sell  copies of the Software, and to permit persons to whom the  Software is furnished to do so, subject to the following  conditions:    The above copyright notice and this permission notice shall be  included in all copies or substantial portions of the Software.    THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,  EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES  OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND  NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT  HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,  WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR  OTHER DEALINGS IN THE SOFTWARE. """
Semantic web;https://github.com/nicholashauschild/kotlin-rdf;"""# kotlin-rdf  [![Build Status](https://img.shields.io/travis/nicholashauschild/kotlin-rdf/master.svg?style=flat-square)](https://travis-ci.org/nicholashauschild/kotlin-rdf)    > RDF DSL's written in Kotlin    ## What is it?  A series of DSL's to support creating and querying RDF Graphs.  This specific library is backed by Apache Jena.    ## Usage    ### Add dependency  Release dependencies: Not yet released.  Still experimental.    Snapshot dependencies:  ```  repositories {      maven {          url uri('https://oss.jfrog.org/artifactory/libs-snapshot')      }  }  dependencies {      compile ""com.github.nicholashauschild:kotlin-rdf:0.1.0-SNAPSHOT""  }  ```    ### DSL's    #### propertySchema  The `propertySchema` DSL is used to setup a property or predicate 'namespace'.    Here is an example that showcases the complete set of options available for using the propertySchema DSL:  ```  propertySchema(""http://example/schema/{{property}}"") {      ""price"" {          uri = ""http://example/schema/price""      }  }  ```    ...and here is a breakdown, mostly line-by-line, of what is happening...    `propertySchema(""http://example/schema/{{property}}"") {`    This line is doing two things.  1. It is establishing the start of the propertySchema DSL construct  2. It is providing a value for the propertySchema's namespace.    The namespace is useful for providing a default uri template, which will  allow us to remove some superfluous configuration.    ***    `    ""price"" {`    This line is doing two things.  1. It is providing a common name for a new property.  2. It is establishing the start of the definition for the new property.    ***    `        uri = ""http://example/schema/price""`    This line is defining the URI for the enclosing property.    ***    The last two lines are closing their respective constructs.    ##### Property Configuration  Here is a table with the configuration options available for a property:    | Field | Required | Description           | Default Value |  | ----- | -------- | --------------------- | ------------- |  | uri   | false    | URI for this property | 'common name' merged into namespace template    ##### Aliasing properties  If a property name is too long, or you would like to be allowed to refer  to it with additional names, then you can utilize  the `alias` keyword to create aliases for properties.    Example:  ```  propertySchema(""http://example/schema/{{property}}"") {      ""color"" { uri = ""http://example/schema/color"" } alias ""pigment""  }  ```    In the above example, 'color' and 'pigment' are two different names that refer to  the same property.    ##### propertySchema return type  The `propertySchema` DSL returns an object of type `PropertySchema`.  This object  has a function with signature `operator fun get(name: String): Property` which can be used to access the  underlying property objects, which are implementations of the Property interface of the Apache Jena API.    ```  val schema =      propertySchema(""http://example/schema/{{property}}"") {          ""height"" { uri = ""http://example/schema/height"" }      }        val aProperty: org.apache.jena.rdf.model.Property = schema[""height""]  assertEquals(""http://example/schema/height"", aProperty.getURI())  ```    ##### Reducing ceremonious syntax  The propertySchema definition shown at the beginning of this section can be written up  a bit more succinctly.    In general, the idea behind the namespace template is to be able to use a common 'base' URI  and derive the actual URI for each property from this template based on its name.  This is the  default behavior of the property definition.  With this information, we can rewrite our initial  propertySchema DSL definition like this, and we would get an equivalent result.    ```  propertySchema(""http://example/schema/{{property}}"") {      ""price"" {} // uri is the value of the merged namespace template and property name  }  ```    Pairing this with the `alias` keyword, and you utilize whatever 'common name' for a property  while still keeping the definition concise.  For example    ```  propertySchema(""http://example/schema/{{property}}"") {      ""some_silly_uri_prefix#price"" {} alias ""friendlyName""  }  ```    Going even further, you can use the unary plus operator to add a property that will provide no  configuration outside of default values.    ```  propertySchema(""http://example/schema/{{property}}"") {      +""price""  }  ```    *It is worth noting that these two variations may look the same now, but future versions of  this library will likely utilize further customization of a property.  The unary plus operator  will be creating a property with NO CUSTOMIZATION whatsoever, where the former syntax will  allow for a pick/choose type of customization*      #### rdfGraph  The `rdfGraph` DSL is meant to create an RDF graph or model  that can then be queried against.    Here is an example that showcases the complete set of options available for using the rdfGraph DSL:    *Note* this example uses the `propertySchema` DSL     ```  val props =            pSchema(""http://example/props/{{property}}"") {              +""enemies_with""              +""hair_color""              +""leg_count""          }    val model =            rdfGraph {              resources {                  ""dog""(""http://example/dog"")                  ""cat""(""http://example/cat"")                  ""parrot""(""http://example/parrot"")              }              statements {                  ""dog"" {                      props[""enemies_with""] of !""cat""                      props[""hair_color""] of ""golden""                      props[""leg_count""] of 4                  }                  ""cat"" {                      props[""enemies_with""] of !""parrot""                      props[""hair_color""] of ""black""                      props[""leg_count""] of 4                  }                  ""parrot"" {                      props[""leg_count""] of 2                  }              }          }  ```    ...and here is a breakdown, mostly line-by-line, of what is happening...    `rdfGraph {`    This line is establishing the start of the rdfGraph DSL construct    ***    `resources {`    This line is establishing the start of resource definitions.  Resources  defined within this construct are utilized later in statement creation.    ***    ```  ""dog""(""http://example/dog"")  ""cat""(""http://example/cat"")  ""parrot""(""http://example/parrot"")  ```                     These lines are creating resources, which can be referred to by the leading String,  using a short-hand syntax `!""name""`.  This syntax, when used in the `statements` dsl   construct will refer to the actual Resource objects themselves.    ***    `statements {`    This line is establishig the start of statement definitions.  Statements  are created with a Subject-Predicate-Object triple setup, with the Object  portion able to be literals or other Resources.    ***    `""dog"" {` or `""cat"" {` or `""parrot"" {`    These lines are the 'Subject' part of the triple.  A block is started as a  shorthand means of defining multiple predicate/object pairs for each subject.    ***    ```  props[""enemies_with""] of !""cat""  props[""hair_color""] of ""golden""  props[""leg_count""] of 4  ```    These lines show three different predicate/object pairs that will be assigned with  the enclosing subject to create triples.    The first maps the Predicate of name 'enemies_with' from the PropertySchema  to the 'cat' Resource (remember the !""cat"" syntax)    The second maps the Predicate of name 'hair_color' from the PropertySchema to  the string literal 'golden'.    The third maps the Predicate of name 'leg_count' from the PropertySchema to  the integer literal 4.    ##### rdfGraph return type  The `rdfGraph` DSL will return an object of type `org.apache.jena.rdf.model.Model` of the Apache Jena API.    ```  val model =      rdfGraph {          // ...      }        val numStatements = model.size()  ```    ##### Reducing ceremonious syntax  It is possible to simply references to PropertySchema properties.  It requires  providing the PropertySchemas to your `rdfGraph` DSL setup.    ```  val props1: PropertySchema = //...  val props2: PropertySchema = //...    val model =            rdfGraph(props1, props2) {              resources {                  ""dog""(""http://example/dog"")              }              statements {                  ""dog"" {                      ""someProp"" of 4                  }              }          }  ```    When providing more than one PropertySchema, they will be checked for a property  in the order provided, returning the first matched property.  If none are found,  an exception will be thrown.    It is worth pointing out that the String itself is NOT being considered a Property  on its own (unlike how !""name"" is used to refer to a Resource).  This works as is  only when used in the form of `""string"" of [literal/resource]`    ## Questions  1. Why are you doing this? To learn how to make a DSL in Kotlin and to learn more about RDF.  2. Why doesn't this support feature x/y/z?  I am new to RDF, and so my understanding of it is limited.  If you have any requests, please let me know via email, or via the github issue system.  Please note that a feature request is NOT a guarantee that I will implement something."""
Semantic web;https://github.com/edsu/csvw-template;"""This is a simple example of using [CSV on the Web] to document the semantics  of a CSV file. Fork it, and change it for your CSV data. With apologies   to [Dan Bricklin].    [CSV on the Web]: http://www.w3.org/2013/csvw/wiki/Main_Page  [Dan Bricklin]: https://en.wikipedia.org/wiki/Dan_Bricklin """
Semantic web;https://github.com/kasei/SPARQLKit;"""SPARQLKit  =========    An implementation of the SPARQL 1.1 Query and Update language in Objective-C.  ---------------    This code implements a full SPARQL 1.1 Query and Update engine in Objective-C.  The design is based on trait/role-based programming, where possible  allowing for natural extensibility and component selection/replacement  (e.g. using the Raptor RDF parser and a triple-store backed by the OS X Address Book).    The code depends on the [GTWSWBase framework](https://github.com/kasei/GTWSWBase).    Plugins  -------    The system provides an extensible plugin architecture for data sources and RDF parsers.  Plugins are automatically loaded from the `Library/Application Support/SPARQLKit/PlugIns` directory.    Some example plugins include:    * [GTWSPARQLProtocolStore](https://github.com/kasei/GTWSPARQLProtocolStore) provides triplestore access to remote remote data using the [SPARQL Protocol](http://www.w3.org/TR/sparql11-protocol/)  * [GTWRedland](https://github.com/kasei/GTWRedland) provides both a [librdf](http://librdf.org) in-memory triplestore and a [Raptor](http://librdf.org/raptor/) RDF parser  * [GTWAddressBookTripleStore](https://github.com/kasei/GTWAddressBookTripleStore) provides access to a users address book contacts  * [GTWApertureTripleStore](https://github.com/kasei/GTWApertureTripleStore) provides access to photo metadata (including geographic and depiction data) from [Aperture](http://www.apple.com/aperture/) libraries  * [GTWAOF](https://github.com/kasei/GTWAOF) provides a persistent, append-only quad store      Example  -------    The `gtwsparql` tool available in this package provides a command line interface to a  full SPARQL 1.1 Query and Update environment.    ### Loading Data    ```  % gtwsparql  sparql> LOAD <http://dbpedia.org/data/Objective-C.ttl> ;  OK  sparql> SELECT (COUNT(*) AS ?count) WHERE { ?s ?p ?o }  -------------  | # | count |   -------------  | 1 | 372   |   -------------  sparql> LOAD <http://dbpedia.org/data/SPARQL.ttl> INTO GRAPH <http://example.org/SPARQL> ;  OK  sparql> SELECT * WHERE { GRAPH ?g {} }  -----------------------------------  | # | g                           |   -----------------------------------  | 1 | <http://example.org/SPARQL> |   -----------------------------------  sparql> SELECT DISTINCT ?subject WHERE { GRAPH <http://example.org/SPARQL> { ?subject ?p ?o } } ORDER BY ?subject  -----------------------------------------------------------------------------  |  # | subject                                                              |   -----------------------------------------------------------------------------  |  1 | <http://dbpedia.org/resource/SPARQL>                                 |   |  2 | <http://fa.dbpedia.org/resource/اسپارکل>                             |   |  3 | <http://zh.dbpedia.org/resource/SPARQL>                              |   |  4 | <http://de.dbpedia.org/resource/SPARQL>                              |   |  5 | <http://dbpedia.org/resource/SPARQL_Protocol_and_RDF_Query_Language> |   |  6 | <http://ar.dbpedia.org/resource/سباركل>                              |   |  7 | <http://ru.dbpedia.org/resource/SPARQL>                              |   |  8 | <http://dbpedia.org/resource/Sparq>                                  |   |  9 | <http://lv.dbpedia.org/resource/SPARQL>                              |   | 10 | <http://vi.dbpedia.org/resource/SPARQL>                              |   | 11 | <http://nl.dbpedia.org/resource/SPARQL>                              |   | 12 | <http://uk.dbpedia.org/resource/SPARQL>                              |   | 13 | <http://ja.dbpedia.org/resource/SPARQL>                              |   | 14 | <http://dbpedia.org/resource/Sparql>                                 |   | 15 | <http://it.dbpedia.org/resource/SPARQL>                              |   | 16 | <http://hu.dbpedia.org/resource/Sparql>                              |   | 17 | <http://wikidata.dbpedia.org/resource/Q54871>                        |   | 18 | <http://en.wikipedia.org/wiki/SPARQL>                                |   | 19 | <http://pl.dbpedia.org/resource/SPARQL>                              |   | 20 | <http://fr.dbpedia.org/resource/SPARQL>                              |   | 21 | <http://es.dbpedia.org/resource/SPARQL>                              |   -----------------------------------------------------------------------------  ```    ### Namespace Completion    The tool can auto-complete prefix declarations (sourced from [prefix.cc](http://prefix.cc/).  By hitting TAB immediately after a prefix name (including colon), the full prefix IRI  is added to the query string:    `sparql> PREFIX foaf:`**&lt;TAB>**    `sparql> PREFIX foaf: <http://xmlns.com/foaf/0.1/> `    ### Configuring the Data Source    The `gtwsparql` tool can take a string argument to specify the data source configuration.  In its simplest form, this is just the name of a triple- or quad-store plugin.  For example, we can query over Aperture photo metadata loaded into the default graph:    ```  % gtwsparql -s GTWApertureTripleStore  sparql> PREFIX dcterms: <http://purl.org/dc/terms/> PREFIX foaf: <http://xmlns.com/foaf/0.1/> SELECT ?place (SAMPLE(?i) AS ?image) WHERE { ?i dcterms:spatial [ foaf:name ?place ] FILTER(REGEX(?place, ""Airport"")) } GROUP BY ?place ORDER BY ?place  -----------------------------------------------------------------------------------------------------------------------------------------------------  | # | place                              | image                                                                                                    |   -----------------------------------------------------------------------------------------------------------------------------------------------------  | 1 | ""Anchorage Airport""                | <file:///Users/greg/Pictures/Aperture Library.aplibrary/Masters/2013/03/27/20130327-000128/P1040654.RW2> |   | 2 | ""Cape Town Airport""                | <file:///Users/greg/Pictures/Aperture Library.aplibrary/Masters/2013/07/04/20130704-222928/IMG_1931.JPG> |   | 3 | ""Genoa Cristoforo Colombo Airport"" | <file:///Users/greg/Pictures/Aperture Library.aplibrary/Masters/2013/07/04/20130704-222928/IMG_1796.JPG> |   | 4 | ""Indira Gandhi Airport""            | <file:///Users/greg/Pictures/Aperture Library.aplibrary/Masters/2013/05/22/20130522-235054/IMG_1752.JPG> |   | 5 | ""Or Tambo Airport""                 | <file:///Users/greg/Pictures/Aperture Library.aplibrary/Masters/2013/07/04/20130704-222928/IMG_1924.JPG> |   | 6 | ""Vadodara Airport""                 | <file:///Users/greg/Pictures/Aperture Library.aplibrary/Masters/2013/05/22/20130522-235054/IMG_1762.JPG> |   | 7 | ""Wellington Airport""               | <file:///Users/greg/Pictures/Aperture Library.aplibrary/Masters/2013/04/10/20130410-213038/P1070371.RW2> |   -----------------------------------------------------------------------------------------------------------------------------------------------------  ```    The `SPKTripleModel` can be used to construct a dataset with multiple triplestores, each available in a separate graph.  We can query over both Aperture photo metadata and address book contacts:    ```  % gtwsparql -s '{ ""storetype"": ""SPKTripleModel"", ""graphs"": { ""tag:addressbook"": { ""storetype"": ""GTWAddressBookTripleStore"" }, ""tag:aperture"": { ""storetype"": ""GTWApertureTripleStore"" } } }'  sparql> SELECT * WHERE { GRAPH ?g {} }  -------------------------  | # | g                 |   -------------------------  | 1 | <tag:addressbook> |   | 2 | <tag:aperture>    |   -------------------------  ```    This allows us to find the number of photos depicting members of the same family by combining depiction data from Aperture with family name data from the address book (by constructing a query dataset using the `FROM` keyword):    ```  sparql> PREFIX foaf: <http://xmlns.com/foaf/0.1/> SELECT ?family (COUNT(*) AS ?count) FROM <tag:addressbook> FROM <tag:aperture> WHERE { ?image a foaf:Image ; foaf:depicts [ foaf:familyName ?family ] } GROUP BY ?family ORDER BY ?count  ----------------------------------  | # | count | family             |   ----------------------------------  | 1 | 1     | ""Kjernsmo""         |   | 2 | 6     | ""Heath""            |   | 3 | 14    | ""Brickley""         |   | 4 | 25    | ""Aastrand Grimnes"" |   | 5 | 104   | ""Acton""            |   | 6 | 116   | ""Gillis""           |   | 7 | 504   | ""Crawford""         |   | 8 | 2029  | ""Williams""         |   ----------------------------------  ```    ### Starting an Endpoint    A SPARQL endpoint can easily be started:    ```  sparql> endpoint 8080  Endpoint started on port 8080  ```    At this point, `http://localhost:8080/sparql` is a [SPARQL Protocol](http://www.w3.org/TR/sparql11-protocol/) endpoint URL that will respond to queries. """
Semantic web;https://github.com/theodi/csv2json;"""[![License](http://img.shields.io/:license-mit-blue.svg)](http://theodi.mit-license.org)    # CSV 2 JSON    A ruby gem to convert CSV to JSON, following the CSV on the Web specification."""
Semantic web;https://github.com/mifeet/LD-FusionTool;"""LD-FusionTool  ==========  ###Data Fusion and Conflict Resolution tool for Linked Data        LD-FusionTool is a standalone tool (and [a module](https://github.com/mifeet/FusionTool-DPU) for [UnifiedViews](http://unifiedviews.eu) ETL framework) executing the Data Fusion and Conflict Resolution steps in the integration process for RDF, where data are merged to produce consistent and clean representations of objects, and conflicts which emerged during data integration need to be resolved.    **Please visit [the official page of LD-FusionTool](http://mifeet.github.io/LD-FusionTool/) for more information about what LD-FusionTool is, how it works and how you can download it and run.**        Building from sources  ========    In order to use LD-FusionTool, download the sources, build them with Maven (run <code>mvn clean install</code> in the <code>sources</code> directory of the project). Locate the built binaries in <code>sources/odcsft-application/target</code> and execute<br/> <code>java -jar odcsft-application-&lt;version&gt;-executable.jar &lt;configuration-file&gt;.xml</code>. Running the executable without parameters shows more usage options and sample configuration files can be found at <a href=""https://github.com/mifeet/LD-FusionTool/tree/master/examples"">examples</a> (file <code>sample-config-full.xml</code> serves as the working documentation of the configuration file).     """
Semantic web;https://github.com/theodi/csv2rdf;"""[![License](http://img.shields.io/:license-mit-blue.svg)](http://theodi.mit-license.org)    # CSV 2 RDF    A ruby gem to convert CSV to RDF, following the CSV on the Web specification.      ## Installation      $ gem install specific_install      $ sudo gem specific_install -l https://github.com/theodi/csv2rdf        ## Usage        $ csv2rdf myfile.csv   """
Semantic web;https://github.com/uzh/signal-collect;"""Signal/Collect  ==============    Signal/Collect is a framework for computations on large graphs. The model allows to concisely express many iterated and data-flow algorithms, while the framework parallelizes and distributes the computation.    How to develop in Eclipse  -------------------------  Install the [Typesafe IDE for Scala 2.11](http://scala-ide.org/download/sdk.html).    Ensure that Eclipse uses a Java 8 library and JVM: Preferences → Java → Installed JREs → JRE/JDK 8 should be installed and selected.    Import the project into Eclipse: File → Import... → Maven → Existing Maven Projects → select ""signal-collect"" folder    Thanks a lot to  ---------------  * [University of Zurich](http://www.ifi.uzh.ch/ddis.html) and the [Hasler Foundation](http://www.haslerstiftung.ch/en/home) have generously funded the research on graph processing and the development of Signal/Collect.  * GitHub helps us by hosting our [code repositories](https://github.com/uzh/signal-collect).  * Travis.CI offers us very convenient [continuous integration](https://travis-ci.org/uzh/signal-collect).  * Codacy gives us automated [code reviews](https://www.codacy.com/public/uzh/signalcollect). """
Semantic web;https://github.com/AKSW/NSpM;"""# 🤖 Neural SPARQL Machines    [![Python 3.7](https://img.shields.io/badge/python-3.7-blue.svg)](https://www.python.org/downloads/release/python-370/)    A Machine-Translation Approach for Question Answering over Knowledge Graphs.    ![What does a NSpM do?](http://www.liberai.org/img/NSpM-image.png ""What does a NSpM do?"")    ## IMPORTANT    If you are looking for the code for papers _""SPARQL as a Foreign Language""_ and _""Neural Machine Translation for Query Construction and Composition""_ please checkout tag [v0.1.0-akaha](https://github.com/LiberAI/NSpM/tree/v0.1.0-akaha) or branch [v1](https://github.com/LiberAI/NSpM/tree/v1).    ## Install    ### Via pip    Coming soon!    ### Local setup    Clone the repository.    ```bash  pip install -r requirements.txt  ```    ## Example of usage    ### The Generator module    #### Pre-generated data    You can extract pre-generated data and model checkpoints from [here](https://nspm-models.s3.eu-west-2.amazonaws.com/v2/art_30.zip) (1.1 GB) in folders having the respective names.    #### Manual Generation (Alternative to using pre-generated data)    The template used in the paper can be found in a file such as `Annotations_F30_art.csv`. `data/art_30` will be the ID of the working dataset used throughout the tutorial. To generate the training data, launch the following command.    ```bash  mkdir -p data/art_30  python nspm/generator.py --templates data/templates/Annotations_F30_art.csv --output data/art_30  ```    Launch the command if you want to build dataset seprately else it will internally be called while training.    ```bash  python nspm/data_gen.py --input data/art_30 --output data/art_30  ```    ### The Learner module    Now go back to the initial directory and launch `learner.py` to train the model.     ```bash  python nspm/learner.py --input data/art_30 --output data/art_30  ```    This command will create a model checkpoints in `data/art_30` and some pickle files in `data/art_30/pickle_objects`.    ### The Interpreter module    Predict the SPARQL query for a given question it will store the detailed output in output_query.    ```bash  python nspm/interpreter.py --input data/art_30 --output data/art_30 --query ""yuncken freeman has architected in how many cities?""  ```  or, if you want to use NSpM with [airml](https://github.com/sahandilshan/airML) to install pre-trained models, follow these steps,  1. Install airML latest version from [here](https://pypi.org/project/airML/)  2. Navigate to the table.kns [here](https://github.com/sahandilshan/KBox/blob/dev/kns/2.0/table.kns) and check if your model is listed in that file.  3. Then copy the name of that model and use it with the `interpreter.py` as follows  ```bash  python interpreter.py --airml http://nspm.org/art --output data/art_30 --inputstr ""yuncken freeman has architected in how many cities?""  ```    ## Use cases & integrations    * Components of the [Adam Medical platform](https://www.graphen.ai/products/mi_feature.html) partly developed by [Jose A. Alvarado](https://www.linkedin.com/in/josealvaradoguzman/) at Graphen (including a humanoid robot called Dr Adam), rely on NSpM technology.  * The [Telegram NSpM chatbot](https://github.com/AKSW/NSpM/wiki/NSpM-Telegram-Bot) offers an integration of NSpM with the Telegram messaging platform.  * The [Google Summer of Code](https://summerofcode.withgoogle.com/) program has been supporting 6 students to work on NSpM-backed project ""[A neural question answering model for DBpedia](https://github.com/dbpedia/neural-qa)"" since 2018.  * A [question answering system](https://github.com/qasim9872/question-answering-system) was implemented on top of NSpM by [Muhammad Qasim](https://github.com/qasim9872).    ## Publications    ### SPARQL as a Foreign Language (2017)    * arXiv: https://arxiv.org/abs/1708.07624    ```  @inproceedings{soru-marx-2017,      author = ""Tommaso Soru and Edgard Marx and Diego Moussallem and Gustavo Publio and Andr\'e Valdestilhas and Diego Esteves and Ciro Baron Neto"",      title = ""{SPARQL} as a Foreign Language"",      year = ""2017"",      journal = ""13th International Conference on Semantic Systems (SEMANTiCS 2017) - Posters and Demos"",      url = ""https://arxiv.org/abs/1708.07624"",  }  ```    ### Neural Machine Translation for Query Construction and Composition (2018)    * NAMPI Website: https://uclnlp.github.io/nampi/  * arXiv: https://arxiv.org/abs/1806.10478    ```  @inproceedings{soru-marx-nampi2018,      author = ""Tommaso Soru and Edgard Marx and Andr\'e Valdestilhas and Diego Esteves and Diego Moussallem and Gustavo Publio"",      title = ""Neural Machine Translation for Query Construction and Composition"",      year = ""2018"",      journal = ""ICML Workshop on Neural Abstract Machines \& Program Induction (NAMPI v2)"",      url = ""https://arxiv.org/abs/1806.10478"",  }  ```    ### Exploring Sequence-to-Sequence Models for SPARQL Pattern Composition (2020)    * arXiv: https://arxiv.org/abs/2010.10900    ```  @inproceedings{panchbhai-2020,      author = ""Anand Panchbhai and Tommaso Soru and Edgard Marx"",      title = ""Exploring Sequence-to-Sequence Models for {SPARQL} Pattern Composition"",      year = ""2020"",      journal = ""First Indo-American Knowledge Graph and Semantic Web Conference"",      url = ""https://arxiv.org/abs/2010.10900"",  }  ```    ### Liber AI on Medium (2020)    * [What is a Neural SPARQL Machine?](https://medium.com/liber-ai/what-is-a-neural-sparql-machine-c35945a5d278)    ## Contact    ### Questions?  * Primary contacts: [Tommaso Soru](http://tommaso-soru.it) and [Edgard Marx](http://emarx.org).  * Neural SPARQL Machines [mailing list](https://groups.google.com/forum/#!forum/neural-sparql-machines).  * Join the conversation on [Gitter](https://gitter.im/LiberAI/community).    ### Follow us  * Follow the [project on ResearchGate](https://www.researchgate.net/project/Neural-SPARQL-Machines).  * Follow Liber AI Research on [Twitter](https://twitter.com/theLiberAI).    <p align=""center""><img tooltip=""Liber AI"" src=""http://www.liberai.org/img/Liber-AI-logo-name-200px.png"" alt=""Liber AI logo"" border=""0""></p> """
Semantic web;https://github.com/linkedpipes/etl;"""# LinkedPipes ETL  [![Build Status](https://travis-ci.com/linkedpipes/etl.svg?branch=develop)](https://travis-ci.com/linkedpipes/etl)    LinkedPipes ETL is an RDF based, lightweight ETL tool.  - [REST API](https://github.com/linkedpipes/etl/wiki) based set of components for easy integration  - [Library of components](https://etl.linkedpipes.com/components) to get you started faster  - [Sharing of configuration](https://etl.linkedpipes.com/templates/) among individual pipelines using templates  - RDF configuration of transformation pipelines    ## Requirements  - Linux, Windows, iOS  - [Docker], [Docker Compose]    ### For building locally  - [Java] 11 or 16  - [Git]  - [Maven], 3.2.5 or newer  - [Node.js] & npm    ## Installation and startup  You can run LP-ETL in Docker, or build it from the source.    ### Docker  To start LP-ETL ```master``` branch on ```http://localhost:8080```, you can use a one-liner:  ```  curl https://raw.githubusercontent.com/linkedpipes/etl/master/docker-compose.yml | docker-compose -f - up  ```  Note that on Windows, there is an [issue with buildkit](https://github.com/moby/buildkit/issues/1684).  See the [temporary workaround](https://github.com/linkedpipes/etl/issues/851#issuecomment-814058925).    When running this on Windows, you might get a build error. There is a [workaround](https://github.com/linkedpipes/etl/issues/851) for that.    Alternatively, you can build docker images from GitHub sources using a one-liner:  ```  curl https://raw.githubusercontent.com/linkedpipes/etl/master/docker-compose-github.yml | docker-compose -f - up  ```    You may need to run the commands as ```sudo``` or be in the ```docker``` group.    #### Configuration  Each component (executor, executor-monitor, storage, frontend) has separate ```Dockerfile```.    Environment variables:   * ```LP_ETL_BUILD_BRANCH``` - The ```Dockerfiles``` are designed to run build from the github repository, the branch is set using this property, default is ```master```.   * ```LP_ETL_BUILD_JAVA_TEST``` - Set to empty to allow to run Java tests, this will slow down the build.   * ```LP_ETL_DOMAIN``` - The URL of the instance, this is used instead of the ```domain.uri``` from the configuration.   * ```LP_ETL_FTP``` - The URL of the FTP server, this is used instead of the ```executor-monitor.ftp.uri``` from the configuration.      For [Docker Compose], there are additional environment variables:   * ```LP_ETL_PORT``` - Specify port mapping for frontend, this is where you can connect to your instance.  This does NOT have to be the same as port in ```LP_ETL_DOMAIN``` in case of reverse-proxying.    For example to run LP-ETL from ```develop``` branch on ```http://localhost:9080``` use can use following command:  ```  curl https://raw.githubusercontent.com/linkedpipes/etl/develop/docker-compose-github.yml | LP_ETL_PORT=9080 LP_ETL_DOMAIN=http://localhost:9080 LP_ETL_BUILD_BRANCH=develop docker-compose -f - up  ```    ```docker-compose``` utilizes several volumes that can be used to access/provide data.  See ```docker-compose.yml``` comments for examples and configuration.  You may want to create your own ```docker-compose.yml``` for custom configuration.    ### From source on Linux    #### Installation    ```sh  $ git clone https://github.com/linkedpipes/etl.git  $ cd etl  $ mvn install  ```    #### Configuration  The configuration file ```deploy/configuration.properties``` can be edited, mainly changing paths to working, storage, log and library directories.     #### Startup    ```sh  $ cd deploy  $ ./executor.sh >> executor.log &  $ ./executor-monitor.sh >> executor-monitor.log &  $ ./storage.sh >> storage.log &  $ ./frontend.sh >> frontend.log &  ```    #### Running LP-ETL as a systemd service  See example service files in the ```deploy/systemd``` folder.    ### From source on Windows  Note that it is also possible to use [Bash on Ubuntu on Windows] or [Cygwin] and proceed as with Linux.    #### Installation  ```sh  git clone https://github.com/linkedpipes/etl.git  cd etl  mvn install  ```  #### Configuration  The configuration file ```deploy/configuration.properties``` can be edited, mainly changing paths to working, storage, log and library directories.     #### Startup  In the ```deploy``` folder, run   * ```executor.bat```   * ```executor-monitor.bat```   * ```storage.bat```   * ```frontend.bat```    ## Plugins - Components  The components live in the ```jars``` directory.  Detailed description of how to create your own is coming soon, in the meantime, you can copy an existing component and change it.     ## Update notes  > Update note 5: 2019-09-03 breaking changes in the configuration file. Remove ```/api/v1``` from the ```executor-monitor.webserver.uri```, so it loolks like: ```executor-monitor.webserver.uri = http://localhost:8081```. You can also remove ```executor.execution.uriPrefix``` as the value is derived from ```domain.uri```.    > Update note 4: 2019-07-03 we changed the way frontend is run. If you do not use our script to run it, you need to update yours.     > Update note 3: When upgrading from develop prior to 2017-02-14, you need to delete ```{deploy}/jars``` and ```{deploy}/osgi```.     > Update note 2: When upgrading from master prior to 2016-11-04, you need to move your pipelines folder from e.g., ```/data/lp/etl/pipelines``` to ```/data/lp/etl/storage/pipelines```, update the configuration.properites file and possibly the update/restart scripts as there is a new component, ```storage```.    > Update note: When upgrading from master prior to 2016-04-07, you need to delete your old execution data (e.g., in /data/lp/etl/working/data)    [Java]: <http://www.oracle.com/technetwork/java/javase/downloads/index.html>  [Git]: <https://git-scm.com/>  [Maven]: <https://maven.apache.org/>  [Node.js]: <https://nodejs.org>  [Cygwin]: <https://www.cygwin.com/>  [Bash on Ubuntu on Windows]: <https://msdn.microsoft.com/en-us/commandline/wsl/about>  [Docker]: <https://www.docker.com/>  [Docker Compose]: <https://docs.docker.com/compose/> """
Semantic web;https://github.com/jiemakel/visu;"""VISU  ====    Visual SPARQL query tool, available at http://demo.seco.tkk.fi/visu/. Combines [YASQE](http://yasqe.yasgui.org/) and [YASR](http://yasr.yasgui.org/) from [YASGUI](http://yasgui.org/) with the [Google Visualization API](https://developers.google.com/chart/interactive/docs/reference), so that the results of SPARQL queries can be instantly visualized.    The tool is also able to save its state in the URL, so queries and visualizations can be shared. For example, [this query](http://goo.gl/QtDrzm) aggregates births and deaths by place and time in DBpedia, and visualizes the results as a motion chart. The chart shows for example that for some reason Wikipedia contains a disproportionate number of French people who died in the 1930s, while the number of Chinese people appearing is altogether very low. """
Semantic web;https://github.com/konradreiche/jtriple;"""# JTriple    JTriple is a Java tool which creates a RDF data model out of a Java object model by making use of reflection, a small set of annotations and Jena's flexible RDF/OWL API.    ### Why another RDF binding for Java?    The most popular tool for persisting Java objects to RDF is [JenaBean]. JTriple was developed, respectively JenaBean was not modified due to the following reasons:    * JenaBean aims for a persistence layer (object serialization). This fact is often expressed by missing confguration, for instance a field cannot be declared as transient.    * Not the whole functionality of JenaBean is required. Additional data is serialized, for instance the serialization of the package names. Package names are vital for deserialization but for the pure data translation (one-way) it only interferes.    * Data (RDF) and schema (OWL) should be translated into two separate RDF graphs. JenaBean creates only one graph.    ## Getting Started    JTriple can be deployed through Maven. Before, the following repository has to be added to your pom.xml    ```xml  <repository>       <id>berlin.reiche.jtriple</id>       <url>https://github.com/platzhirsch/jtriple/raw/master/repository/releases</url>  </repository>  ```    Then it can be added with this dependency    ```xml  <dependency>       <groupId>berlin.reiche.jtriple</groupId>       <artifactId>jtriple</artifactId>       <version>0.1-RELEASE</version>       <scope>compile</scope>  </dependency>  ```    Not using Maven? You can also get the [JAR] directly.    ### Example    Considering the following example. A class Philosopher    ```java  public class Philosopher {    	@RdfIdentifier  	String name;    	String nationality;  	List<Branch> interests;  }  ```    with an enum type Branch    ```java  public enum Branch {    	EPISTEMOLOGY(""Epistemology""),  	MATHEMATIC(""Mathematic""),  	METAPHYSISC(""Metaphysic""),  	PHILOSOPHY_OF_MIND(""Philosophy of Mind"");  	  	String name;  	  	Branch(String name) {  		this.name = name;  	}  }  ```  The only requirement is to annotate one field or method of a class with `@RdfIdentifier`. Binding objects to RDF is as easy as follows      ```java  // create data  Philosopher locke = new Philosopher();  locke.setName(""John Locke"");  locke.setNationality(""English"");    List<Branch> branches = new ArrayList<>();  branches.add(METAPHYSISC);  branches.add(EPISTEMOLOGY);  branches.add(PHILOSOPHY_OF_MIND);  locke.setInterests(branches);    // bind object  Binding binding = new Binding(DEFAULT_NAMESPACE);  Model model = binding.getModel();  model.setNsPrefix(""philosophy"", NAMESPACE);    binding.bind(locke);    // output RDF  model.write(System.out, ""TURTLE"");  ```    It is sufficient to produce this RDF    ```  @prefix philosophy:  <http://konrad-reiche.com/philosophy/> .    <http://konrad-reiche.com/philosophy/philosopher/John_locke>        a       <http://dbpedia.org/page/Philosopher> ;        philosophy:interests                <http://konrad-reiche.com/philosophy/branch/Metaphysisc> ,                <http://konrad-reiche.com/philosophy/branch/Philosophy_of_mind> ,                <http://konrad-reiche.com/philosophy/branch/Epistemology> ;        philosophy:name ""John Locke""^^<http://www.w3.org/2001/XMLSchema#string> ;        philosophy:nationality                ""English""^^<http://www.w3.org/2001/XMLSchema#string> .    <http://konrad-reiche.com/philosophy/branch/Epistemology>        a       philosophy:branch ;        philosophy:name ""Epistemology""^^<http://www.w3.org/2001/XMLSchema#string> .    <http://konrad-reiche.com/philosophy/branch/Metaphysisc>        a       philosophy:branch ;        philosophy:name ""Metaphysic""^^<http://www.w3.org/2001/XMLSchema#string> .    <http://konrad-reiche.com/philosophy/branch/Philosophy_of_mind>        a       philosophy:branch ;        philosophy:name ""Philosophy of Mind""^^<http://www.w3.org/2001/XMLSchema#string> .  ```    Now, to get more sophisticated results, annotations help to provide neccessary information    ```java  @RdfType(""http://dbpedia.org/page/Philosopher"")  public class Philosopher {    	@Label  	@RdfIdentifier  	String name;    	@RdfProperty(""http://www.foafrealm.org/xfoaf/0.1/nationality"")  	String nationality;    	List<Branch> interests;  }  ```    ```java  public enum Branch {    	@SameAs({ ""http://dbpedia.org/resource/Epistemology"" })  	EPISTEMOLOGY(""Epistemology""),  	  	@SameAs({ ""http://dbpedia.org/resource/Mathematic"" })  	MATHEMATIC(""Mathematic""),    	@SameAs({ ""http://dbpedia.org/resource/Metaphysic"" })  	METAPHYSISC(""Metaphysic""),    	@SameAs({ ""http://dbpedia.org/resource/Philosophy_of_mind"" })  	PHILOSOPHY_OF_MIND(""Philosophy of Mind"");  	  	@Label  	String name;  	  	Branch(String name) {  		this.name = name;  	}  }  ```    Leading to this RDF:    ```  @prefix rdfs:    <http://www.w3.org/2000/01/rdf-schema#> .  @prefix xfoaf:   <http://www.foafrealm.org/xfoaf/0.1/> .  @prefix philosophy:  <http://konrad-reiche.com/philosophy/> .  @prefix dbpedia:  <http://dbpedia.org/resource/> .    <http://konrad-reiche.com/philosophy/philosopher/John_locke>        a       <http://dbpedia.org/page/Philosopher> ;        rdfs:label ""John Locke""^^<http://www.w3.org/2001/XMLSchema#string> ;        philosophy:interests                <http://konrad-reiche.com/philosophy/branch/Metaphysisc> ,                <http://konrad-reiche.com/philosophy/branch/Philosophy_of_mind> ,                <http://konrad-reiche.com/philosophy/branch/Epistemology> ;        xfoaf:nationality ""English""^^<http://www.w3.org/2001/XMLSchema#string> .    <http://konrad-reiche.com/philosophy/branch/Metaphysisc>        a       philosophy:branch ;        rdfs:label ""Metaphysic""^^<http://www.w3.org/2001/XMLSchema#string> ;        <http://www.w3.org/2002/07/owl#sameAs>                dbpedia:Metaphysic .    <http://konrad-reiche.com/philosophy/branch/Philosophy_of_mind>        a       philosophy:branch ;        rdfs:label ""Philosophy of Mind""^^<http://www.w3.org/2001/XMLSchema#string> ;        <http://www.w3.org/2002/07/owl#sameAs>                dbpedia:Philosophy_of_mind .    <http://konrad-reiche.com/philosophy/branch/Epistemology>        a       philosophy:branch ;        rdfs:label ""Epistemology""^^<http://www.w3.org/2001/XMLSchema#string> ;        <http://www.w3.org/2002/07/owl#sameAs>                dbpedia:Epistemology .    ```    ### Annotations    What annotations are there and how can they be used?    <table>    <tr>      <th>Name</th><th>Use</th><th>Effect</th>    </tr>    <tr>      <td><code>@RdfIdentifier</code></td><td>Fields, Methods</td><td>Value to be used for constructing the resource URI</td>    </tr>    <tr>      <td><code>@RdfProperty</code></td><td>Fields, Methods</td><td>Value to define another property URI</td>    </tr>    <tr>      <td><code>@RdfType</code></td><td>Classes</td><td>Value to define a rdfs:type property on the resource</td>    </tr>    <tr>      <td><code>@Transient</code></td><td>Fields</td><td>Indicate that this field must not be converted</td>    </tr>    <tr>      <td><code>@SameAs</code></td><td>Enum Constants</td><td>Value to define a owl:sameAs property on the resource</td>    </tr>    <tr>      <td><code>@Label</code></td><td>Fields, Methods</td><td>Value to define a rdfs:label property on the resource</td>    </tr>  </table>    ## Future Work    Some ideas for the future development:    * Implement OWL binding  * Increase the configuration flexibility    If something is amiss, feel free to open an issue or make a pull request. The implementation is lightweight and allows to change the functionality very quickly.    [JenaBean]: http://code.google.com/p/jenabean/  [Jena API]: http://jena.apache.org/  [JAR]: https://github.com/platzhirsch/jtriple/raw/master/repository/releases/berlin/reiche/jtriple/jtriple/0.1/jtriple-0.1.jar """
Semantic web;https://github.com/RDFLib/geosparql-dggs;"""# RDFlib GeoSPARQL Functions for DGGS    This library provides support for the [GeoSPARQL 1.1 Simple Features Relation Family](https://opengeospatial.github.io/ogc-geosparql/geosparql11/spec.html#_simple_features_relation_family_relation_familysimple_features)  for geometries expressed as [DGGS Literals](https://opengeospatial.github.io/ogc-geosparql/geosparql11/spec.html#_rdfs_datatype_geodggsliteral).  Currently, [rHEALPix DGGS](https://iopscience.iop.org/article/10.1088/1755-1315/34/1/012012/pdf) Grids are supported.      ## Installation  From the python package index, PyPi: https://pypi.org/project/geosparql-dggs/    `pip install geosparql-dggs`    This package depends on to support the functions' use against graphs [RDFlib](https://pypi.org/project/rdflib/). The functions themselves depend on the [rHEAL-sf](https://github.com/surroundaustralia/rhealpix-sf/)   library, which in turn depends on the [rHEAL-geo](https://github.com/surroundaustralia/rhealpix-geo/) library for the base classes  which represent DGGS Cells and collections of Cells.   ## Use  These functions are implemented in RDFlib Python in the file `gsdggs/sf_functions.py` and are imported into `gsdggs/__init__.py` and registered there in RDFlib as SPARQL extension functions with their IRIs.    This means they can be used like this (full working script):    ```python  from rdflib import Literal, Graph, Namespace, URIRef  from gsdggs import DGGS    EX = Namespace(""http://example.com/"")  GEO = Namespace(""http://www.opengis.net/ont/geosparql#"")    # Define the DGGS Geometries, add them to an in-memory RDF graph  g = Graph()  g.add((URIRef('https://geom-a'), GEO.asDGGS, Literal('CELLLIST ((R0 R10 R13 R16 R30 R31 R32 R40))', EX.ausPixLiteral)))  g.add((URIRef('https://geom-b'), GEO.asDGGS, Literal('CELLLIST ((R06 R07 R30 R31))', EX.ausPixLiteral)))  g.add((URIRef('https://geom-c'), GEO.asDGGS, Literal('CELLLIST ((R11 R12 R14 R15))', EX.ausPixLiteral)))    # Query the in-memory graph  q = """"""      PREFIX geo: <http://www.opengis.net/ont/geosparql#>      PREFIX dggs: <https://placeholder.com/dggsfuncs/>            SELECT ?a ?b       WHERE {          ?a geo:asDGGS ?a_geom .          ?b geo:asDGGS ?b_geom .                    FILTER dggs:sfWithin(?a_geom, ?b_geom)      }""""""  # Interate through and print results  for r in g.query(q):      print(f""{r['a']} is within {r['b']}"")  ```  The above stript outputs:    ```bash  https://geom-b is within https://geom-a  ```    The functions can also be used directly (without RDFLib) by direct import from _source, for example:  ```python  from _source import sfEquals    sfEquals(""R1"", ""R1"")  ```  The above stript outputs:    ```bash  True  ```  ## Function Definitions  The Simple Feature relations have been interpreted in the following way for the context of a nested square DGGS grid (such as rHEALPix grids).      * **dggs:sfEqual:** Two sets of cells are equal if they have the same identifier.    * **dggs:sfWithin:** One set of cells (A) is within some other set of cells (B) if the addition of A's cells to B results in a set of cells equal to B, where A is not equal to B.    * **dggs:sfContains:** One set of cells (A) is contains some other set of cells (B) if the addition of A's cells to B results in a set of cells equal to A, where A is not equal to B.    * **dggs:sfIntersects:** One set of cells (A) intersects some other set of cells (B) where they share any two cells, or any cell in A is the parent or child of a cell in B, or any cell in A or B touches.    * **dggs:sfTouches:** One set of cells (A) touches some other set of cells (B) where the cells meet at an edge, or vertex.    * **dggs:sfDisjoint:** One set of cells (A) is disjoint with some other set of cells (B) where they do not share any two cells, no cell in A is the parent or child of a cell in B, and no cells in A and B touch.    * **dggs:sfOverlaps:** One set of cells (A) overlaps some other set of cells (B) where the addition of A's cells to B results in a set of cells different from A and B, and A and B are not disjoint and do not touch.    ## Testing  All tests are in `tests/` and implemented using [pytest](https://docs.pytest.org/en/6.2.x/index.html).    There are individual tests for each function, along with more granular tests for supporting Python classes (Cells and CellCollections), as well as application of the functions without RDF.     ## Contributing  Via GitHub, Issues & Pull Requests:     * <https://github.com/rdflib/geosparql-dggs>    ## License  This code is licensed with the BSD 3-clause license as per [LICENSE](LICENSE) which is the same license as used for [rdflib](https://pypi.org/project/rdflib/).    ## Citation  ```bibtex  @software{https://github.com/rdflib/geosparql-dggs,    author = {{David Habgood}},    title = {RDFlib GeoSPARQL Functions for DGGS},    version = {0.0.1},    date = {2021},    url = {https://github.com/rdflib/geosparql-dggs}  }  ```    ## Contact  _Creator & maintainer:_    **David Habgood**    _Application Architect_    [SURROUND Australia Pty Ltd](https://surroundaustralia.com)    <david.habgood@surroundaustrlaia.com>      https://orcid.org/0000-0002-3322-1868 """
Semantic web;https://github.com/AKSW/ORE;"""ORE - Ontology Repair and Enrichment  ===    The ORE (Ontology Repair and Enrichment) tool allows for knowledge engineers to improve an OWL ontology by fixing inconsistencies and making suggestions for adding further axioms to it.    Ontology Debugging: ORE uses OWL reasoning to detect inconsistencies and satisfiable classes. State-of-the-art methods are then used to detect the most likely sources for the problems. In a simple process, the user can create a repair plan to resolve a problem, while maintaining full control over desired and undesired inferences.    Ontology Enrichment: ORE uses the DL-Learner framework to suggest definitions and super classes for existing classes in the knowledge base. This works if instance data is available and can be used to detect potential problems and harmonise schema and data in the knowledge base. """
Semantic web;https://github.com/ontola/rdf-serializers;"""# RDF Serializers    <a href=""https://travis-ci.org/ontola/rdf-serializers""><img src=""https://travis-ci.org/ontola/rdf-serializers.svg?branch=master"" alt=""Build Status""></a>    ## About    RDF Serializers enables serialization to RDF formats. It uses [fast-jsonapi](https://github.com/fast-jsonapi/fast_jsonapi) serializers, with a few modifications.  The serialization itself is done by the [rdf](https://github.com/ruby-rdf/rdf) gem.    This was built at [Ontola](https://ontola.io/). If you want to know more about our passion for open data, send us [an e-mail](mailto:ontola@argu.co).    ## Installation    Add this line to your application's Gemfile:    ```  gem 'rdf-serializers'  ```    And then execute:    ```  $ bundle  ```    ## Getting started    First, register the formats you wish to serialize to. For example, add the following to `config/initializers/rdf_serializers.rb`:  ```ruby  require 'rdf/serializers/renderers'    RDF::Serializers::Renderers.register(:ntriples)  ```  This automatically registers the MIME type.    In your controllers, add:  ```ruby  respond_to do |format|    format.nt { render nt: model }  end  ```    ## Configuration    You can configure the gem using `RDF::Serializers.configure`.  ```  RDF::Serializers.configure do |config|    config.always_include_named_graphs = false # true by default. Whether to include named graphs when the serialization format does not support quads.    config.default_graph = RDF::URI('https://example.com/graph') # nil by default.  end    ```    ## Formats    You can register multiple formats, if you add the correct gems. For example, add `rdf-turtle` to your gemfile and put this in the initializer:  ```ruby  require 'rdf/serializers/renderers'    opts = {    prefixes: {      ns:   'http://rdf.freebase.com/ns/',      key:  'http://rdf.freebase.com/key/',      owl:  'http://www.w3.org/2002/07/owl#',      rdfs: 'http://www.w3.org/2000/01/rdf-schema#',      rdf:  'http://www.w3.org/1999/02/22-rdf-syntax-ns#',      xsd:  'http://www.w3.org/2001/XMLSchema#'    }  }    RDF::Serializers::Renderers.register(%i[ntriples turtle], opts)    ```    The RDF gem has a list of available [RDF Serialization Formats](https://github.com/ruby-rdf/rdf#rdf-serialization-formats), which includes:  * NTriples  * Turtle  * N3  * RDF/XML  * JSON::LD    and more    ## Serializing    Add a predicate to the attributes and relations you wish to serialize.    It's recommended to reuse existing vocabularies provided by the `rdf` gem and the [rdf-vocab](https://github.com/ruby-rdf/rdf-vocab) gem,   and add your own vocab for missing predicates. One way to be able to access the different vocabs throughout your application is by defining a module:  ```  require 'rdf'  require ""rdf/vocab""    module NS    SCHEMA = RDF::Vocab::SCHEMA    MY_VOCAB = RDF::Vocabulary.new('http://example.com/')  end  ```    Now add the predicates to your serializers.     Old:   ```ruby  class PostSerializer    include JSONAPI::Serializer    attributes :title, :body    belongs_to :author    has_many :comments  end  ```    New:  ```ruby  class PostSerializer    include RDF::Serializers::ObjectSerializer    attribute :title, predicate: NS::SCHEMA[:name]    attribute :body, predicate: NS::SCHEMA[:text]    belongs_to :author, predicate: NS::MY_VOCAB[:author]    has_many :comments, predicate: NS::MY_VOCAB[:comments]  end  ```    For RDF serialization, you are required to add an `iri` method to your model, which must return a `RDF::Resource`. For example:  ```ruby    def iri      RDF::URI(Rails.application.routes.url_helpers.comment_url(object))    end  ```    In contrast to the JSON API serializer, this rdf serializers don't automatically serialize the `type` and `id` of your model.   It's recommended to add `attribute :type, predicate: RDF[:type]` and a method defining the type to your serializers to fix this.    ### Custom statements per model    You can add custom statements to the serialization of a model in the serializer, for example:  ```ruby  class PostSerializer    include RDF::Serializers::ObjectSerializer    statements :my_custom_statements        def my_custom_statements      [RDF::Statement.new(RDF::URI('https://example.com'), NS::MY_VOCAB[:fooBar], 1)]    end  end  ```    ### Meta statements    You can add additional statements to the serialization in the controller, for example:  ```ruby  render nt: model, meta: [RDF::Statement.new(RDF::URI('https://example.com'), NS::MY_VOCAB[:fooBar], 1)]  ```    ## Contributing    The usual stuff. Open an issue to discuss a change, open pull requests directly for bugfixes and refactors. """
Semantic web;https://github.com/joachimvh/SPARQLAlgebra.js;"""# SPARQL to SPARQL Algebra converter    [![npm version](https://badge.fury.io/js/sparqlalgebrajs.svg)](https://www.npmjs.com/package/sparqlalgebrajs)  [![Build status](https://github.com/joachimvh/SPARQLAlgebra.js/workflows/CI/badge.svg)](https://github.com/joachimvh/SPARQLAlgebra.js/actions?query=workflow%3ACI)    2 components get exposed: the **translate** function and the **Algebra** object,  which contains all the output types that can occur.    Note that this is still a work in progress so naming conventions could change.  There is also support for 'non-algebra' entities such as ASK, FROM, etc.  to make sure the output contains all relevant information from the query.    ## Translate    Input for the translate function should either be a SPARQL string  or a result from calling [SPARQL.js](https://github.com/RubenVerborgh/SPARQL.js).    ```javascript  const { translate } = require('sparqlalgebrajs');  translate('SELECT * WHERE { ?x ?y ?z }');  ```  Returns:  ```json  {     ""type"": ""project"",    ""input"": {      ""type"": ""bgp"",        ""patterns"": [{          ""type"": ""pattern"",          ""termType"": ""Quad"",          ""subject"": { ""termType"": ""Variable"", ""value"": ""x"" },          ""predicate"": { ""termType"": ""Variable"", ""value"": ""y"" },          ""object"": { ""termType"": ""Variable"", ""value"": ""z"" },          ""graph"": { ""termType"": ""DefaultGraph"", ""value"": """" }        }]    },    ""variables"": [      { ""termType"": ""Variable"", ""value"": ""x"" },      { ""termType"": ""Variable"", ""value"": ""y"" },      { ""termType"": ""Variable"", ""value"": ""z"" }    ]  }    ```    Translating back to SPARQL can be done with the `toSparql` (or `toSparqlJs`) function.    ## Algebra object  The algebra object contains a `types` object,  which contains all possible values for the `type` field in the output results.  Besides that it also contains all the TypeScript interfaces of the possible output results.  The output of the `translate` function will always be an `Algebra.Operation` instance.    The best way to see what output would be generated is to look in the `test` folder,  where we have many SPARQL queries and their corresponding algebra output.    ## Deviations from the spec  This implementation tries to stay as close to the SPARQL 1.1  [specification](https://www.w3.org/TR/sparql11-query/#sparqlDefinition),  but some changes were made for ease of use.  These are mostly based on the Jena ARQ [implementation](https://jena.apache.org/documentation/query/).  What follows is a non-exhaustive list of deviations:    #### Named parameters  This is the biggest visual change.  The functions no longer take an ordered list of parameters but a named list instead.  The reason for this is to prevent having to memorize the order of parameters and also  due to seeing some differences between the spec and the Jena ARQ SSE output when ordering parameters.    #### Multiset/List conversion  The functions `toMultiset` and `toList` have been removed for brevity.  Conversions between the two are implied by the operations used.    #### Quads  The `translate` function has an optional second parameter  indicating whether patterns should be translated to triple or quad patterns.  In the case of quads the `graph` operation will be removed  and embedded into the patterns it contained.  The default value for this parameter is `false`.  ```  PREFIX : <http://www.example.org/>    SELECT ?x WHERE {      GRAPH ?g {?x ?y ?z}  }  ```    Default result:  ```json  {    ""type"": ""project"",      ""input"": {      ""type"": ""graph"",        ""input"": {        ""type"": ""bgp"",          ""patterns"": [{            ""type"": ""pattern"",            ""termType"": ""Quad"",            ""subject"": { ""termType"": ""Variable"", ""value"": ""x"" },            ""predicate"": { ""termType"": ""Variable"", ""value"": ""y"" },            ""object"": { ""termType"": ""Variable"", ""value"": ""z"" },            ""graph"": { ""termType"": ""DefaultGraph"", ""value"": """" }          }]      },      ""name"": { ""termType"": ""Variable"", ""value"": ""g"" }    },    ""variables"": [{ ""termType"": ""Variable"", ""value"": ""x"" }]  }    ```    With quads:  ```json  {    ""type"": ""project"",      ""input"": {      ""type"": ""bgp"",        ""patterns"": [{          ""type"": ""pattern"",          ""termType"": ""Quad"",          ""subject"": { ""termType"": ""Variable"", ""value"": ""x"" },          ""predicate"": { ""termType"": ""Variable"", ""value"": ""y"" },          ""object"": { ""termType"": ""Variable"", ""value"": ""z"" },          ""graph"": { ""termType"": ""Variable"", ""value"": ""g"" }        }]    },    ""variables"": [{ ""termType"": ""Variable"", ""value"": ""x"" }]  }    ```    ### Flattened operators  Several binary operators that can be nested,   such as the path operators,  can take an array of input entries to simply this notation.  For example, the following SPARQL:  ```sparql  SELECT * WHERE { ?x <a:a>|<b:b>|<c:c> ?z }  ```  outputs the following algebra:  ```json  {    ""type"": ""project"",    ""input"": {      ""type"": ""path"",      ""subject"": { ""termType"": ""Variable"", ""value"": ""x"" },      ""predicate"": {        ""type"": ""alt"",        ""input"": [          { ""type"": ""link"", ""iri"": { ""termType"": ""NamedNode"", ""value"": ""a:a"" }},          { ""type"": ""link"", ""iri"": { ""termType"": ""NamedNode"", ""value"": ""b:b"" }},          { ""type"": ""link"", ""iri"": { ""termType"": ""NamedNode"", ""value"": ""c:c"" }}        ]      },      ""object"": { ""termType"": ""Variable"", ""value"": ""z"" },      ""graph"": { ""termType"": ""DefaultGraph"", ""value"": """" }    },    ""variables"": [      { ""termType"": ""Variable"", ""value"": ""x"" },      { ""termType"": ""Variable"", ""value"": ""z"" }    ]  }    ```    #### SPARQL*    [SPARQL*](https://blog.liu.se/olafhartig/2019/01/10/position-statement-rdf-star-and-sparql-star/) queries can be parsed by setting `sparqlStar` to true in the `translate` options.    #### VALUES  For the VALUES block we return the following output:  ```  PREFIX dc:   <http://purl.org/dc/elements/1.1/>   PREFIX :     <http://example.org/book/>   PREFIX ns:   <http://example.org/ns#>     SELECT ?book ?title ?price  {     VALUES ?book { :book1 :book3 }     ?book dc:title ?title ;           ns:price ?price .  }  ```  ```json  {    ""type"": ""project"",    ""input"": {      ""type"": ""join"",      ""input"": [        {          ""type"": ""values"",          ""variables"": [{ ""termType"": ""Variable"", ""value"": ""book"" }],          ""bindings"": [            { ""?book"": { ""termType"": ""NamedNode"", ""value"": ""http://example.org/book/book1"" }},            { ""?book"": { ""termType"": ""NamedNode"", ""value"": ""http://example.org/book/book3"" }}          ]        },        {          ""type"": ""bgp"",          ""patterns"": [            {              ""type"": ""pattern"",              ""termType"": ""Quad"",              ""subject"": { ""termType"": ""Variable"", ""value"": ""book"" },              ""predicate"": { ""termType"": ""NamedNode"", ""value"": ""http://purl.org/dc/elements/1.1/title"" },              ""object"": { ""termType"": ""Variable"", ""value"": ""title"" },              ""graph"": { ""termType"": ""DefaultGraph"", ""value"": """" }            },            {              ""type"": ""pattern"",              ""termType"": ""Quad"",              ""subject"": { ""termType"": ""Variable"", ""value"": ""book"" },              ""predicate"": { ""termType"": ""NamedNode"", ""value"": ""http://example.org/ns#price"" },              ""object"": { ""termType"": ""Variable"", ""value"": ""price"" },              ""graph"": { ""termType"": ""DefaultGraph"", ""value"": """" }            }          ]        }      ]    },    ""variables"": [      { ""termType"": ""Variable"", ""value"": ""book"" },      { ""termType"": ""Variable"", ""value"": ""title"" },      { ""termType"": ""Variable"", ""value"": ""price"" }    ]  }    ```    #### Differences from Jena ARQ  Some differences from Jena (again, non-exhaustive):  no prefixes are used (all uris get expanded)  and the project operation always gets used (even in the case of `SELECT *`).    ## A note on tests  Every test consists of a sparql file and a corresponding json file containing the algebra result.  Tests ending with `(quads)` in their name are tested/generated with `quads: true` in the options.    If you need to regenerate the parsed JSON files in bulk, you can invoke `node test/generate-json.js`. """
Semantic web;https://github.com/utapyngo/owl2vcs;"""owl2vcs is a set of tools designed to facilitate version control of  [OWL 2 ontologies][owl2] using version control systems.        Contents  --------    -   owl2diff - a command line diff tool for OWL 2 ontologies;    -   a set of scripts to integrate the tools with [Git][git], [Mercurial][hg] and [Subversion][svn].        Features  --------    -   Detects axioms additions and removals;    -   Detects imports additions and removals;    -   Detects ontology annotations additions and removals;    -   Detects prefix additions, removals, modifications and renames;    -   Detects ontology IRI and version IRI changes;    -   Detects ontology format changes;    -   Supports RDF/XML, OWL/XML, OWL Functional Syntax, Manchester OWL Syntax,      Turtle;    -   Changeset serializer and parser;    -   Two formats of changes: compact (like OWL Functional Syntax) and indented      (same but uses indents instead of parentheses, more readable);    -   Four formats of IRIs: Simple, QName, Full, Label.        Requirements  ------------    -   Java 1.6 or higher and `java` in `PATH`;    -   For [Git][git]: `git` in `PATH;`    -   For [Mercurial][hg]: `hg` in `PATH`;        Installation instructions  -------------------------    1.  [Download][owl2vcs-latest];    2.  Unzip;    3.  [Add][path] to `PATH`.        Standalone usage  ----------------    After adding the directory to `PATH` you can use the `owl2diff` command to compare two versions of an ontology. See `owl2diff --help` for more information.        Usage with Git/Mercurial  ------------------------    1.  Open command shell and `cd` into your repository;    2.  Type `owl2enable`;    3.  Now you can view informative diffs for \*.owl, \*.rdf, and \*.ttl files with either `hg owl2diff` or `git diff`.    * If `git diff` hangs on Windows, use `sh -c ""git diff""` or `git difftool`.    4.  If you want owl2vcs to compare files with other extensions, edit your `.hg/hgrc` or `.git/info/attributes`.        Please help out  ---------------    This project is still under development. Feedback and suggestions are very welcome and I encourage you to use the [Issues list][issues] on Github to provide that feedback.    Feel free to [fork][fork] this repo and to commit your additions.    Contributing  ------------    1.  [Fork it][fork].    2.  Clone the **develop** branch to your machine: `git clone -b develop git@github.com:utapyngo/owl2vcs.git`.    3.  Create your feature branch: `git checkout -b my-new-feature`.    4.  Commit your changes: `git commit -am 'Added some feature'`.    5.  Push to the branch `git push origin my-new-feature`.    6.  Create new Pull Request.    [owl2]:   http://www.w3.org/TR/owl2-overview/    [git]:    http://git-scm.com/    [hg]:     http://mercurial.selenic.com/    [svn]:    http://subversion.apache.org/    [owl2vcs-latest]: http://j.mp/owl2vcs-latest    [path]:   https://github.com/utapyngo/owl2vcs/wiki/How-to-add-owl2vcs-to-PATH    [issues]: http://github.com/utapyngo/owl2vcs/issues    [fork]:   https://github.com/utapyngo/owl2vcs/fork_select"""
Semantic web;https://github.com/sebferre/sparklis;"""<meta charset=""UTF-8""/>    # What is Sparklis?    Sparklis is a query builder in natural language that allows people to explore and query SPARQL endpoints with all the power of SPARQL and without any knowledge of SPARQL, nor of the endpoint vocabulary.    Sparklis is a Web client running entirely in the browser. It directly connects to SPARQL endpoints to retrieve query results and suggested query elements. It covers a large subset of SPARQL 1.1 `SELECT` queries: basic graph patterns including cycles, `UNION`, `OPTIONAL`, `NOT EXISTS`, `FILTER`, `BIND`, complex expressions, aggregations, `GROUP BY`, `ORDER BY`. All those features can be combined in a flexible way, like in SPARQL. Results are presented as tables, and also on maps. A  configuration panel offers a few configuration options to adapt to different endpoints (e.g., GET/POST, labelling properties and language tags). Sparklis also includes the YASGUI editor to let advanced users access and modify the SPARQL translation of the query.    Sparklis reconciles expressivity and usability in semantic search by tightly combining a Query Builder, a Natural Language Interface, and a Faceted Search system. As a *Query Builder* it lets users build complex queries by composing elementary queries in an incremental fashion. An elementary query can be a class (e.g., ""a film""), a property (e.g., ""that has a director""), a RDF node (e.g., ""Tim Burton""), a reference to another node (e.g., ""the film""), or an operator (e.g., ""not"", ""or"", ""highest-to-lowest"", ""+"", ""average"").    As a *Faceted Search system*, at every step, the query under construction is well-formed, query results are computed and displayed, and the suggested query elements are derived from actual data - not only from schema - so as to prevent the construction of non-sensical or empty results. The display of results and data-relevant suggestions at every step provides constant and acurate feedback to users during the construction process. This supports exploratory search, serendipity, and confidence about final results.    As a *Natural Language Interface*, everything presented to users - queries, suggested query elements, and results - are verbalized in natural language, completely hidding SPARQL behind the user interface. Compared to Query Answering (QA) systems, the hard problem of spontaneous NL understanding is avoided by controlling query formulation through guided query construction, and replaced by the simpler problem of NL generation. The user interface lends itself to multilinguality, and is so far available in English, French, Spanish, and Dutch.    When refering to Sparklis in scientific documents, please use the following citation.    > Sébastien Ferré: *Sparklis: An expressive query builder for SPARQL endpoints with guidance in natural language.* Semantic Web 8(3): 405-418 (2017)    # Where can I try Sparklis?    Simply follow those steps:  1. Go to the [online application](http://www.irisa.fr/LIS/ferre/sparklis/)  2. Select a SPARQL endpoint in the dropdown list at the top (the default one is *Core English DBpedia*, a core subset of DBpedia)  3. Build your query incrementally by clicking suggested query elements (in the three lists of suggestions, and through the hamburger menu in the query), and clicking different parts of the query to change focus. The suggestions are relative to the current focus.    We recommend to visit the [*Examples page*](http://www.irisa.fr/LIS/ferre/sparklis/examples.html) where there are 100+ example queries over several datasets. Every example query can be opened in Sparklis in one click, and the query can then be further modified. For a number of them, there is a YouTube screencast to show how they are built step by step.    # How can I use Sparklis on my own RDF dataset?    It is enough to have a SPARQL endpoint for your dataset that is visible from your machine. It can be a publicly open endpoint (like for DBpedia or Wikidata), or a localhost endpoint (I personally use Apache Jena Fuseki but other stores should work too). The one important condition is that the endpoint server be [CORS-enabled](https://www.w3.org/wiki/CORS_Enabled) so that HTTP requests can be made to it from your browser, where Sparklis runs.    Here a few recommendations about the contents of your store for best results:    * include RDFS/OWL triples declaring classes (`rdfs:Class`, `owl:Class`) and properties (`rdf:Property`, `owl:ObjectProperty`, `owl:DataProperty`), as well as their labels (`rdfs:label`) and their hierarchy (`rdfs:subClassOf`, `rdfs:subPropertyOf`)  * ensure that all URI-resources have their label defined, preferably with `rdfs:label` and possibly with other standard properties (e.g., `skos:prefLabel`)  * if named graphs are used, make sure to configure your store so that the default graphs contains the union of those named graphs    The *Configure* menu offers a number of options to adapt Sparklis to your endpoint, and control the display. Here is a non-exhaustive list:    * Endpoint and queries: max numbers of results/suggestions, GET vs POST, credentials  * Ontology: display of class/property hierarchies, filtering of classes/properties, use of Sparklis-specific schema properties (see below)  * Language and labels: interface language, labelling properties, fulltext search support    Sparklis makes use of standard and non-standard properties to get more control on the building of queries, and on the display of suggestions and results. For ech property, there is generally a configuration option to activate its use.    * `sdo:position` (`sdo: = https://schema.org/`): it is possible to control the ordering of suggestions (classes, properties, and individuals) by setting this property with the desired rank of the suggestion. The related option is in *Configure advanced features*.  * `sdo:logo`: it is possible to have small icons in front of entity labels by setting this property with URLs to those icons. Several icons can be attached to a same entity. The related option is in *Configure advanced features*, where the size of icons can be defined.  * `rdfs:inheritsThrough`: suppose you have a `ex:location` property whose range `ex:Place` is organized into a hierarchy through the property `ex:isPartOf`. By adding to your dataset the triple `ex:location rdfs:inheritsThrough ex:isPartOf`, you get that whenever property `ex:location` is inserted into the query, inheritance through the place hierarchy is performed, and place suggestions are displayed as a tree. This is a generalization of the well-known `rdf:type` inheritance through `rdfs:subClassOf`. By adding triple `ex:Place rdfs:inheritsThrough ex:isPartOf`, the same effect is obtained when inserting class `ex:Place`. The related option in *Configure the ontology* must be activated.  * `lis-owl:transitiveOf` (`lis-owl: = http://www.irisa.fr/LIS/ferre/vocab/owl#`): the use of `rdfs:inheritsThrough` entails the insertion of transitive paths (e.g., `ex:isPartOf*`) in the SPARQL query, which are very costly to evaluate. One solution is to materialize the transitive closure as a new property `ex:isPartOf_star` in the dataset, and to add the triple `ex:isPartOf_star lis-owl:transitiveOf ex:isPartOf`. By activating the related option in *Configure the ontology*, property path `ex:isPartOf*` will be replaced by `ex:isPartOf_star`.  * `nary:subjectObject` (`nary: = http://www.irisa.fr/LIS/ferre/vocab/nary#`): this property handles the case of a reified relationship where there is a property `PS` from reification node to subject, and a property `PO` from reification node to object. By adding triple `PS nary:subjectObject PO`, the reification becomes transparent in Sparkls. See cyan properties on the Mondial endpoint for examples. The related option in *Configure the ontology* must be activated.  * `nary:eventObject`: this is similar as above, except there is a property `PE` from subject to reification node (instead of the inverse `PS`). See cyan properties in the Wikidata endpoint for examples.    Once you have found a good configuration of your endpoint, you can generate a *permalink* with the button at the top, which you can share to the endpoint users. Those permalinks also include the current query and current focus, so you can also share example queries and template queries. You can also save queries by simply adding bookmarks in your browser.    If you find your endpoint of general interest, you are welcome to suggest me to add it to the list of SPARQL endpoints.    # How do I reuse Sparklis in my web site?    As Sparklis is only client-side code, it is possible to integrate Sparklis into your website by simply copying the contents of the `webapp` folder among your website files, and adding links to it from your web pages, with URL arguments containing the endpoint, the configuration, and possibly an initial query. To get those URLs, simply navigate in Sparklis and copy (and adapt as needed) the browser URL.    You can adapt the appearance of the main HTML file (`osparklis.html`, `osparklis.css`) as long as you retain the *Sparklis* name, and the credits in the page footer. You can for instance hide some configuration options and elements, you can change the look-and-feel, and the layout of elements. Be careful not to delete element ids and classes that are used by the JS code of Sparklis.    Let me know of successful integrations, and also of problems you encounter in the process.    # Compiling Sparklis from the source code    Sparklis is developed in [OCaml](https://ocaml.org), and compiled to Javascript with the [js_of_ocaml](https://ocsigen.org/js_of_ocaml/latest/manual/overview) tool. It is strongly recommended to use the [opam](https://opam.ocaml.org/) tool to manage OCaml dependencies.    The following build steps were found to work on Ubuntu (20.04 LTS) by [waldenn](https://github.com/waldenn):    ```      bash -c ""sh <(curl -fsSL https://raw.githubusercontent.com/ocaml/opam/master/shell/install.sh)""      opam install csv lwt js_of_ocaml js_of_ocaml-lwt lwt_ppx js_of_ocaml-ppx str unix num xmlm xml-light lablgtk2      eval $(opam env)      sudo apt-get install camlp5      git clone https://github.com/sebferre/sparklis.git      cd sparklis      make  ```    # Credits    Author: [Sébastien Ferré](http://people.irisa.fr/Sebastien.Ferre/)    Affiliation: Univ. Rennes 1, team [SemLIS](http://www-semlis.irisa.fr/) at IRISA    Copyright © 2013 Sébastien Ferré, IRISA, Université de Rennes 1, France    Licence: Apache Licence 2.0    Citation: *Ferré, Sébastien. ‘Sparklis: An Expressive Query Builder for SPARQL Endpoints with Guidance in Natural Language’. Semantic Web 8(3) : 405-418. IOS Press, 2017.* [PDF](https://hal.inria.fr/hal-01485093/file/sparklis-preprint.pdf) """
Semantic web;https://github.com/sparna-git/Sparnatural;"""# Sparnatural - A natural way of building SPARQL queries    Sparnatural is a **visual SPARQL query builder written in javascript**.    It supports the creation of basic graph patterns with the selection of values with autocomplete search or dropdown lists. It can be configured through a JSON-LD or OWL configuration file (that can be edited in Protégé) that defines the classes and properties to be presented in the component.    ![](documentation/screencast-sparnatural-dbpedia-v3-en.gif)    You can play with **online demos at http://sparnatural.eu#demos**.    # Getting Started    To get started :    1. Read the following README;  2. Read [the documentation](https://docs.sparnatural.eu)  3. Look at how things work in file `sparnatural-demo-dbpedia/index.html`;   4. In particular look at how the specifications are written by looking at [the source of `sparnatural-demo-dbpedia/index.html`](https://github.com/sparna-git/Sparnatural/blob/master/sparnatural-demo-dbpedia/index.html#L100)  5. Adapt `sparnatural-demo-dbpedia/index.html` by changing the configuration and adapting the SPARQL endpoint URL;    To get started with docker :    1. Clone the git repository  2. Run ``docker-compose build``  3. Run ``docker-compose up``  4. Open your browser: http://127.0.0.1:8080    # Features    ## Query Structure    ### Basic query pattern    Select the type of entity to search...    ![](documentation/1-screenshot-class-selection.png)    ... then select the type of the related entity.    ![](documentation/2-screenshot-object-type-selection.png)    In this case there is only one possible type of relation that can connect the 2 entities, so it gets selected automatically. Then select a value for the related entity, in this case in a dropdown list :    ![](documentation/3-screenshot-value-selection.png)    Congratulations, your first SPARQL query criteria is complete !    ![](documentation/4-screenshot-criteria.png)    Now you can fetch the generated SPARQL query :    ![](documentation/5-screenshot-sparql.png)    ### ""WHERE""    This enables to navigate the graph :    ![](documentation/6-where.png)    ### ""AND""    Combine criterias :    ![](documentation/7-and.png)    ### ""OR""    Select multiple values for a criteria :    ![](documentation/8-or.png)    ## Values selection    Sparnatural offers currently 6 ways of selecting a value for a criteria : autocomplete field, dropdown list, simple string value, date range (year or date precision), date range with a search in a period name (e.g. ""bronze age""), or no selection at all.    ### Autocomplete field    ![](documentation/9-autocomplete.png)    ### Dropdown list    ![](documentation/10-list.png)    ### Tree selector    ![](documentation/17-tree.png)    ### String value (text search)    ![](documentation/11-search.png)    ### Date range (year or date precision)    ![](documentation/12-time-date.png)    ### Date range with search in period name (chronocultural periods)    ![](documentation/14-chronocultural-period.png)    (this requires data from [Perio.do](https://perio.do), a gazeeter of periods for linking and visualizing data)    ### Boolean selection    ![](documentation/15-boolean.png)    ### No value selection    This is useful when a type a of entity is used only to navigate the graph, but without the ability to select an instance of these entities.    ![](documentation/13-no-value.png)      ## Multilingual    Sparnatural is multilingual and supports displaying labels of classes and properties in multiple languages.    ## Support for OPTIONAL and FILTER NOT EXISTS    Sparnatural supports the `OPTIONAL` and `FILTER NOT EXISTS {}` keywords applied to a whole ""branch"" of the query.  See here how to search for French Museums and the name of Italian painters they display, _if any_ :    ![](documentation/16-optional.gif)      ## Limitations    ### No UNION or BIND, etc.    Sparnatural does not support the creation of UNION, SERVICE, BIND, etc...    ### SPARQL endpoint needs to be CORS-enabled    To send SPARQL queries to a service that is not hosted on the same domain name as the web page in which Sparnatural is included, the SPARQL endpoint needs to allow [Cross-Origin Resource Sharing (CORS)](https://enable-cors.org/). But we have SPARQL proxies for those who are not, don't worry ;-)    # Integration    ## Specification of classes and properties    The component is configurable using a an [OWL configuration file](https://docs.sparnatural.eu/OWL-based-configuration) editable in Protégé.. Look at the specification files of [the demos](https://github.com/sparna-git/sparnatural.eu/tree/main/demos) to get an idea.     Alternatively one can also use a [JSON(-LD) ontology file](https://docs.sparnatural.eu/JSON-based-configuration). A JSON(-LD) configuration file contains :    ### Class definition    ```json      {        ""@id"" : ""http://dbpedia.org/ontology/Museum"",        ""@type"" : ""Class"",        ""label"": [          {""@value"" : ""Museum"", ""@language"" : ""en""},          {""@value"" : ""Musée"",""@language"" : ""fr""}        ],        ""faIcon"":  ""fas fa-university""      },  ```    ### Property definitions with domains and ranges    ```json      {        ""@id"" : ""http://dbpedia.org/ontology/museum"",        ""@type"" : ""ObjectProperty"",        ""subPropertyOf"" : ""sparnatural:AutocompleteProperty"",        ""label"": [          {""@value"" : ""displayed at"",""@language"" : ""en""},          {""@value"" : ""exposée à"",""@language"" : ""fr""}        ],        ""domain"": ""http://dbpedia.org/ontology/Artwork"",        ""range"": ""http://dbpedia.org/ontology/Museum"",        ""datasource"" : ""datasources:search_rdfslabel_bifcontains""      },  ```    ### Using font-awesome icons    It is possible to directly use font-awesome icons in place of icons embedded in your application :    ```json  ""faIcon"":  ""fas fa-user"",  ```    ## How to integrate Sparnatural in a webpage    Look at [this page in the documentation](https://docs.sparnatural.eu/Javascript-integration).      ## Map the query structure to a different graph structure    Map classes or properties in the config to a corresponding SPARQL property path or a corresponding class URI, using the `sparqlString` JSON key, e.g. :    ```      {        ""@id"" : ""http://labs.sparna.fr/sparnatural-demo-dbpedia/onto#bornIn"",        ""@type"" : ""ObjectProperty"",        ...        ""sparqlString"": ""<http://dbpedia.org/ontology/birthPlace>/<http://dbpedia.org/ontology/country>"",      },  ```    Then call `expandSparql` on the `sparnatural` instance by passing the original SPARQL query, to replace all mentions of original classes and properties URI with the corresponding SPARQL string :    ```  queryString = sparnatural.expandSparql(queryString);  ``` """
Semantic web;https://github.com/linkeddata/rdflib.js;"""# rdflib.js  [![NPM Version](https://img.shields.io/npm/v/rdflib.svg?style=flat)](https://npm.im/rdflib)  [![Join the chat at https://gitter.im/linkeddata/rdflib.js](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/linkeddata/rdflib.js?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)    Javascript RDF library for browsers and Node.js.    - Reads and writes RDF/XML, Turtle and N3; Reads RDFa and JSON-LD  - Read/Write Linked Data client, using WebDav or SPARQL/Update  - Real-Time Collaborative editing with web sockets and PATCHes  - Local API for querying a store  - Compatible with [RDFJS task force spec](https://github.com/rdfjs/representation-task-force/blob/master/interface-spec.md)  - SPARQL queries (not full SPARQL - just graph match and optional)  - Smushing of nodes from `owl:sameAs`, and `owl:{f,inverseF}unctionProperty`  - Tracks provenance of triples keeps metadata (in RDF) from HTTP accesses    ## Documentation    See:    * The [API documentation](https://linkeddata.github.io/rdflib.js/doc/) is partial but useful  * [Tutorial: Using rdflib in a Solid web app](https://linkeddata.github.io/rdflib.js/Documentation/webapp-intro.html)  * [Tutorial: Using rdflib.js](https://github.com/solidos/solid-tutorial-rdflib.js)  * [Tutorial: Using Turtle](https://linkeddata.github.io/rdflib.js/Documentation/turtle-intro.html)  * [Using authenticated & alternate fetch methods](https://linkeddata.github.io/rdflib.js/Documentation/alternate-fetches.md)  * [Block diagram: rdflib modules](https://linkeddata.github.io/rdflib.js/Documentation/diagrams/rdflib-block-diagram.svg)  * [Block diagram: The Fetcher](https://linkeddata.github.io/rdflib.js/Documentation/diagrams/fetcher-block-diagram.svg)  * [Block diagram: The Fetcher - handling retries](https://linkeddata.github.io/rdflib.js/Documentation/diagrams/fetcher-block-diagram-2.svg)  * [Block diagram: The Update Manager](https://linkeddata.github.io/rdflib.js/Documentation/diagrams/update-manager-diagram.svg)      * [The Solid developer portal at Inrupt](https://solid.inrupt.com/)    for more information.    ## Install    #### Browser (using a bundler like Webpack)    ```bash  npm install rdflib  ```    #### Browser (generating a `<script>` file to include)    ```bash  git clone git@github.com:linkeddata/rdflib.js.git;  cd rdflib.js;  npm install;  ```    Generate the dist directory    ```bash  npm run build:browser  ```    #### Node.js    Make sure you have Node.js and Node Package Manager ([npm](https://npmjs.org/))  installed.    ```bash  npm install --save rdflib  ```    ## Contribute    #### Subdirectories    - `dist`: Where the bundled libraries are built. Run `npm run build` to generate them.  - `test`: Tests are here.  - `lib`: Transpiled, non-bundled library is built here when the library is    published to npm.    #### Dependencies        - XMLHTTPRequest (Node.js version)    ## Thanks    Thanks to the many contributors who have been involved along the way.  LinkedData team & TimBL    ## LICENSE  MIT """
Semantic web;https://github.com/kbss-cvut/jb4jsonld;"""# Java Binding for JSON-LD    [![Build Status](https://kbss.felk.cvut.cz/jenkins/buildStatus/icon?job=jaxb-jsonld)](https://kbss.felk.cvut.cz/jenkins/job/jaxb-jsonld)    Java Binding for JSON-LD (JB4JSON-LD) is a simple library for serialization of Java objects into JSON-LD and vice versa.    Note that this is the core, abstract implementation. For actual usage, a binding like [https://github.com/kbss-cvut/jb4jsonld-jackson](https://github.com/kbss-cvut/jb4jsonld-jackson)  has to be used.      ## Usage    JB4JSON-LD is based on annotations from [JOPA](https://github.com/kbss-cvut/jopa), which enable POJO attributes  to be mapped to ontological constructs (i.e. to object, data or annotation properties) and Java classes to ontological  classes.    Use `@OWLDataProperty` to annotate data fields and `@OWLObjectProperty` to annotate fields referencing other mapped entities.    See [https://github.com/kbss-cvut/jopa-examples/tree/master/jsonld](https://github.com/kbss-cvut/jopa-examples/tree/master/jsonld) for  an executable example of JB4JSON-LD in action (together with Spring and Jackson).      ## Example    ### Java    ```Java  @OWLClass(iri = ""http://onto.fel.cvut.cz/ontologies/ufo/Person"")  public class User {        @Id      public URI uri;        @OWLDataProperty(iri = ""http://xmlns.com/foaf/0.1/firstName"")      private String firstName;        @OWLDataProperty(iri = ""http://xmlns.com/foaf/0.1/lastName"")      private String lastName;            @OWLDataProperty(iri = ""http://xmlns.com/foaf/0.1/accountName"")      private String username;        @OWLDataProperty(iri = ""http://krizik.felk.cvut.cz/ontologies/jb4jsonld/role"")      private Role role;  // Role is an enum        @Properties      private Map<String, Set<String>> properties;            // Getters and setters follow  }  ```    ### JSON-LD    ```JSON  {    ""@context"": {      ""firstName"": ""http://xmlns.com/foaf/0.1/firstName"",      ""lastName"": ""http://xmlns.com/foaf/0.1/lastName"",      ""accountName"": ""http://xmlns.com/foaf/0.1/accountName"",      ""isAdmin"": ""http://krizik.felk.cvut.cz/ontologies/jb4jsonld/isAdmin"",      ""role"": ""http://krizik.felk.cvut.cz/ontologies/jb4jsonld/role""    },    ""@id"": ""http://krizik.felk.cvut.cz/ontologies/jb4jsonld#Catherine+Halsey"",    ""@type"": [      ""http://onto.fel.cvut.cz/ontologies/ufo/Person"",      ""http://krizik.felk.cvut.cz/ontologies/jb4jsonld/User"",      ""http://onto.fel.cvut.cz/ontologies/ufo/Agent""    ],    ""isAdmin"": true,    ""accountName"": ""halsey@unsc.org"",    ""firstName"": ""Catherine"",    ""lastName"": ""Halsey"",    ""role"": ""USER""  }  ```    ## Configuration    Parameter | Default value | Explanation  ----------|---------------|-----------  `ignoreUnknownProperties` | `false` | Whether to ignore unknown properties when deserializing JSON-LD. Default behavior throws an exception.  `scanPackage` | `""""` | Package in which the library should look for mapped classes. The scan is important for support for polymorphism in object deserialization.  It is highly recommended to specify this value, otherwise the library will attempt to load and scan all classes on the classpath.  `requireId` | `false` | Whether to require an identifier when serializing an object. If set to `true` and no identifier is found (either there is no `@Id` field or its value is `null`), an exception will be thrown. By default a blank node identifier is generated if no id is present.  `assumeTargetType` | `false` | Whether to allow assuming target type in case the JSON-LD object does not contain types (`@type`). If set to `true`, the provided Java type (deserialization invocation argument, field type) will be used as target type.  `enableOptimisticTargetTypeResolution` | `false` | Whether to enable optimistic target type resolution. If enabled, this allows to pick a target type even if there are multiple matching classes (which would normally end with an `AmbiguousTargetTypeException`).  `preferSuperclass` | `false` | Allows to further specify optimistic target type resolution. By default, any of the target classes may be selected. Setting this to `true` will make the resolver attempt to select a superclass of the matching classes (if it is also in the target set).  `serializeDatetimeAsMillis` | `false` | Whether to serialize datetime values as millis since Unix epoch. If false, datetime value are serialize as string in ISO format (default).  `datetimeFormat` |  | Format in which datetime values are serialized (and expected for deserialization). Default is undefined, meaning that the ISO 8601 format is used.    See `cz.cvut.kbss.jsonld.ConfigParam`.    ## Documentation    Documentation is on the [Wiki](https://github.com/kbss-cvut/jb4jsonld/wiki). API Javadoc is also [available](https://kbss.felk.cvut.cz/jenkins/view/Java%20Tools/job/jaxb-jsonld/javadoc/).    ## Getting JB4JSON-LD    There are two ways to get JB4JSON-LD:    * Clone repository/download zip and build it with Maven,  * Use a [Maven dependency](http://search.maven.org/#search%7Cga%7C1%7Ccz.cvut.kbss.jsonld):    ```XML  <dependency>      <groupId>cz.cvut.kbss.jsonld</groupId>      <artifactId>jb4jsonld</artifactId>  </dependency>  ```    Note that you will most likely need an integration with a JSON-serialization library like [JB4JSON-LD-Jackson](https://github.com/kbss-cvut/jb4jsonld-jackson).      ## License    LGPLv3 """
Semantic web;https://github.com/SDM-TIB/SDM-RDFizer;"""# SDM-RDFizer  [![License](https://img.shields.io/pypi/l/rdfizer.svg)](https://github.com/SDM-TIB/SDM-RDFizer/blob/master/LICENSE)  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6225573.svg)](https://doi.org/10.5281/zenodo.6225573)  [![Latest PyPI version](https://img.shields.io/pypi/v/rdfizer?style=flat)](https://pypi.org/project/rdfizer/)  [![Python Version](https://img.shields.io/pypi/pyversions/rdfizer.svg)](https://pypi.org/project/rdfizer/)  [![PyPI status](https://img.shields.io:/pypi/status/rdfizer?)](https://pypi.org/project/rdfizer/)    This project presents the SDM-RDFizer, an interpreter of mapping rules that allows the transformation of (un)structured data into RDF knowledge graphs. The current version of the SDM-RDFizer assumes mapping rules are defined in the [RDF Mapping Language (RML) by Dimou et al](https://rml.io/specs/rml/). The SDM-RDFizer implements optimized data structures and relational algebra operators that enable an efficient execution of RML triple maps even in the presence of Big data. SDM-RDFizer is able to process data from heterogeneous data sources (CSV, JSON, RDB, XML) processing each set of RML rules (TriplesMap) in a multi-thread safe procedure.    ![SDM-RDFizer workflow](https://raw.githubusercontent.com/SDM-TIB/SDM-RDFizer/beta/images/architecture.png ""SDM-RDFizer workflow"")    # The new features presented by SDM-RDFizer version4.0    In version 4.0 of SDM-RDFizer, we have addressed the problem of efficiency in KG creation in terms of memory storage. SDM-RDFizer version4.0 includes a new module called ""TriplesMap Planning"" a.k.a. TMP which defines an optimized evaluation plan for the execution of triples maps. Additionally, version4.0 extends the previously included module (i.e. TriplesMap Execution a.k.a. TME) by introducing a new operator for compressing data stored in the data structures. These new features can be configured using two new parameters added to the configuration file, named ""large_file"" and ""ordered"".     We have performed extensive empirical evaluation on SDM-RDFizer version4.0 in terms of execution time and memory usage. The experiments are set up to empirically compare the impact of data duplicate rates, data size, and the complexity and the execution order of the triples maps on two versions of SDM-RDFizer (i.e. version4.0 and version3.6) and other exisiting engines icluding [RMLMapper v4.7](https://github.com/RMLio/rmlmapper-java) and [RocketRML](https://github.com/semantifyit/RocketRML) ), in terms of execution time and memory usage. The experiments are performed on two different benchmarks:   - From [SDM-Genomic-datasets](https://figshare.com/articles/dataset/SDM-Genomic-Datasets/14838342/1), datasets including 10k, 100k, and 1M records with 25% and 75% duplicates rates, over six mapping rules with different complexities (1/4 simple object map, 2/5 object reference maps, 2/5 object join maps)  - From [GTFS-Madrid](https://github.com/oeg-upm/gtfs-bench), datasets with scale values of 1-csv, 5-csv, 10-csv, and 50-csv, over two different mapping rules (72 simple object maps and 11 object join maps).     The results of explained experiments can be summarized as the following:  ![Overview of Results (Execution Time Comparison)](https://raw.githubusercontent.com/SDM-TIB/SDM-RDFizer/beta/images/time.png ""Execution Time Comparison"")  As observed in the figures above, both versions of SDM-RDFizer completed all the testbeds successfully while the other two engines have cases of timeout. SDM-RDFizer version3.6 and RocketRML version 1.7.0 are competitve in simple testbeds, however, SDM-RDFizer version4.0 shows the best performance in all the testbeds.   ![Overview of Results (Memory Consumption Comparison)](https://raw.githubusercontent.com/SDM-TIB/SDM-RDFizer/beta/images/memory.png ""Memory Consumption Comparison"")  As illustrated in the figures above, SDM-RDFizer version4.0 has the smallest peak in memory usage compared to the previous version of SDM-RDFizer.        The results of the execution of SDM-RDFizer has been described in the following research reports:    - Enrique Iglesias, Samaneh Jozashoori, David Chaves-Fraga, Diego Collarana, and Maria-Esther Vidal. 2020. SDM-RDFizer: An RML Interpreter for the Efficient Creation of RDF Knowledge Graphs. The 29th ACM International Conference on Information and Knowledge Management (CIKM ’20).    - Samaneh Jozashoori, David Chaves-Fraga, Enrique Iglesias, Oscar Corcho, and Maria-Esther Vidal. 2020. FunMap: Efficient Execution of Functional Mappings for Knowledge Graph Creation. The 19th International Semantic Web Conference - Research Track (ISWC 2020).    - Samaneh Jozashoori and Maria-Esther Vidal. MapSDI: A Scaled-up Semantic Data Integrationframework for Knowledge Graph Creation. The 27th International Conference on Cooperative Information Systems (CoopIS 2019).     - David Chaves-Fraga, Kemele M. Endris, Enrique Iglesias, Oscar Corcho, and Maria-Esther Vidal. What are the Parameters that Affect the Construction of a Knowledge Graph?. The 18th International Conference on Ontologies, DataBases, and Applications of Semantics (ODBASE 2019).    - David Chaves-Fraga, Antón Adolfo, Jhon Toledo, and Oscar Corcho. ONETT: Systematic Knowledge Graph Generation for National Access Points. The 1st International Workshop on Semantics for Transport co-located with SEMANTiCS 2019.    - David Chaves-Fraga, Freddy Priyatna, Andrea Cimmino, Jhon Toledo, Edna Ruckhaus, and Oscar Corcho. GTFS-Madrid-Bench: A benchmark for virtual knowledge graph access in the transport domain. Journal of Web Semantics, 2020.    Additional References:    - Dimou et al. 2014. Dimou, A., Sande, M.V., Colpaert, P., Verborgh, R., Mannens, E., de Walle, R.V.:RML: A generic language for integrated RDF mappings of heterogeneous data. In:Proceedings of the Workshop on Linked Data on the Web co-located with the 23rdInternational World Wide Web Conference (WWW 2014)     # Projects where the SDM-RDFizer has been used    The SDM-RDFizer is used in the creation of the knowledge graphs of EU H2020 projects and national projects where the Scientific Data Management group participates. These projects include:    - iASiS (http://project-iasis.eu/): big data for precision medicine, based on patient data insights. The iASiS RDF knowledge graph comprises more than 1.2B RDF triples collected from more than 40 heterogeneous sources using over 1300 RML triple maps.   - BigMedilytics (https://www.bigmedilytics.eu/): lung cancer pilot. 800 RML triple maps are used to create the lung cancer knowledge graph from around 25 data sources with 500M RDF triples.  - CLARIFY (https://www.clarify2020.eu/): predict poor health status after specific oncological treatments  - P4-LUCAT (https://www.tib.eu/de/forschung-entwicklung/projektuebersicht/projektsteckbrief/p4-lucat)  - ImProVIT (https://www.tib.eu/de/forschung-entwicklung/projektuebersicht/projektsteckbrief/improvit)  - PLATOON (https://platoon-project.eu/)   - EUvsVirus Hackathon (April 2020) (https://blogs.tib.eu/wp/tib/2020/05/06/how-do-knowledge-graphs-contribute-to-understanding-covid-19-related-treatments/). SDM-RDFizer created the Knowledge4COVID-19 knowledge graph during the participation of the team of the Scientific Data Management group. By June 7th, 2020, this KG is comprised of 28M RDF triples describing at a fine-grained level 63527 COVID-19 scientific publications and COVID-19 related concepts (e.g., 5802 substances, 1.2M drug-drug interactions, and 103 molecular disfunctions).     The SDM-RDFizer is also used in EU H2020, EIT-Digital and Spanish national projects where the Ontology Engineering Group (Technical University of Madrid) participates. These projects, mainly focused on the transportation and smart cities domain, include:    - H2020 - SPRINT (http://sprint-transport.eu/): performance and scalability to test a semantic architecture for the Interoperability Framework on Transport across Europe.  - EIT-SNAP (https://www.snap-project.eu/): innovation project on the application of semantic technologies for national access points.  - Open Cities (https://ciudades-abiertas.es/): national project on creating common and shared vocabularies for Spanish Cities  - Drugs4Covid (https://drugs4covid.oeg.fi.upm.es/): NLP annotations and metadata from more than 60,000 scientific papers about COVID viruses are integrated in a KG with almost 44M of facts (triples). SDM-RDFizer was used for creating this KG.    Other projects were the SDM-RDFizer is also used:  -  Virtual Platform for the H2020 European Joint Programme on Rare Disease (https://www.ejprarediseases.org)      # Installing and Running the SDM-RDFizer   From PyPI (https://pypi.org/project/rdfizer/):  ```  python3 -m pip install rdfizer  python3 -m rdfizer -c /path/to/config/file  ```    From Github/Docker:  Visit the [wiki](https://github.com/SDM-TIB/SDM-RDFizer/wiki) of the repository to learn how to install and run the SDM-RDFizer. You can also take a look to our demo at: https://www.youtube.com/watch?v=DpH_57M1uOE  - Install and run the SDM-RDFizer: https://github.com/SDM-TIB/SDM-RDFizer/wiki/Install&Run  - Parameters to configure SDM-RDFizer: https://github.com/SDM-TIB/SDM-RDFizer/wiki/The-Parameters-of-the-Configuration-file  - FAQ: https://github.com/SDM-TIB/SDM-RDFizer/wiki/FAQ    ## Configurations  You can easily customize your own configurations from the set of features that SDM-RDFzier offers by changing the values of the parameters in the config file. The descriptions of each parameter and the possible values are provided [here](https://github.com/SDM-TIB/SDM-RDFizer/wiki/The-Parameters-of-the-Configuration-file); ""ordered"" and ""large_file"" are the new features provided by SDM-RDFizer version4.0.        ## Version   ```  4.3  ```    ## RML-Test Cases  See the results of the SDM-RDFizer over the RML test-cases at the [RML Implementation Report](http://rml.io/implementation-report/). SDM-RDFizer version4.0 is tested over the latest published [test cases](https://rml.io/test-cases/) before the release.    ## Experimental Evaluations  See the results of the experimental evaluations of SDM-RDFizer version 3.* at [SDM-RDFizer-Experiments repository](https://github.com/SDM-TIB/SDM-RDFizer-Experiments)      ## License  This work is licensed under Apache 2.0    # Authors  The SDM-RDFizer has been developed by members of the Scientific Data Management Group at TIB, as an ongoing research effort. The development is coordinated and supervised by Maria-Esther Vidal (maria.vidal@tib.eu). We strongly encourage you to please report any issues you have with the SDM-RDFizer. You can do that over our contact email or creating a new issue here on Github. The SDM-RDFizer has been implemented by Enrique Iglesias (current version, iglesias@l3s.de) and Guillermo Betancourt (version 0.1, guillermojbetancourt@gmail.com) under the supervision of Samaneh Jozashoori (samaneh.jozashoori@tib.eu), David Chaves-Fraga (dchaves@fi.upm.es), and Kemele Endris (kemele.endris@tib.eu)   """
Semantic web;https://github.com/RDFLib/rdflib;"""![](docs/_static/RDFlib.png)        RDFLib  ======  [![Build Status](https://drone.rdflib.ashs.dev/api/badges/RDFLib/rdflib/status.svg?ref=refs/heads/master)](https://drone.rdflib.ashs.dev/RDFLib/rdflib/branches)  [![Coveralls branch](https://img.shields.io/coveralls/RDFLib/rdflib/master.svg)](https://coveralls.io/r/RDFLib/rdflib?branch=master)  [![GitHub stars](https://img.shields.io/github/stars/RDFLib/rdflib.svg)](https://github.com/RDFLib/rdflib/stargazers)  [![PyPI](https://img.shields.io/pypi/v/rdflib.svg)](https://pypi.python.org/pypi/rdflib)  [![PyPI](https://img.shields.io/pypi/pyversions/rdflib.svg)](https://pypi.python.org/pypi/rdflib)    RDFLib is a pure Python package for working with [RDF](http://www.w3.org/RDF/). RDFLib contains most things you need to work with RDF, including:    * parsers and serializers for RDF/XML, N3, NTriples, N-Quads, Turtle, TriX, Trig and JSON-LD  * a Graph interface which can be backed by any one of a number of Store implementations  * store implementations for in-memory, persistent on disk (Berkeley DB) and remote SPARQL endpoints  * a SPARQL 1.1 implementation - supporting SPARQL 1.1 Queries and Update statements  * SPARQL function extension mechanisms    ## RDFlib Family of packages  The RDFlib community maintains many RDF-related Python code repositories with different purposes. For example:    * [rdflib](https://github.com/RDFLib/rdflib) - the RDFLib core  * [sparqlwrapper](https://github.com/RDFLib/sparqlwrapper) - a simple Python wrapper around a SPARQL service to remotely execute your queries  * [pyLODE](https://github.com/RDFLib/pyLODE) - An OWL ontology documentation tool using Python and templating, based on LODE.  * [pyrdfa3](https://github.com/RDFLib/pyrdfa3) - RDFa 1.1 distiller/parser library: can extract RDFa 1.1/1.0 from (X)HTML, SVG, or XML in general.  * [pymicrodata](https://github.com/RDFLib/pymicrodata) - A module to extract RDF from an HTML5 page annotated with microdata.   * [pySHACL](https://github.com/RDFLib/pySHACL) - A pure Python module which allows for the validation of RDF graphs against SHACL graphs.  * [OWL-RL](https://github.com/RDFLib/OWL-RL) - A simple implementation of the OWL2 RL Profile which expands the graph with all possible triples that OWL RL defines.    Please see the list for all packages/repositories here:    * <https://github.com/RDFLib>    Help with maintenance of all of the RDFLib family of packages is always welcome and appreciated.    ## Versions & Releases    * `6.2.0-alpha` current `master` branch  * `6.x.y` current release and support Python 3.7+ only. Many improvements over 5.0.0      * see [Releases](https://github.com/RDFLib/rdflib/releases)  * `5.x.y` supports Python 2.7 and 3.4+ and is [mostly backwards compatible with 4.2.2](https://rdflib.readthedocs.io/en/stable/upgrade4to5.html).    See <https://rdflib.dev> for the release overview.    ## Documentation  See <https://rdflib.readthedocs.io> for our documentation built from the code. Note that there are `latest`, `stable` `5.0.0` and `4.2.2` documentation versions, matching releases.    ## Installation  The stable release of RDFLib may be installed with Python's package management tool *pip*:        $ pip install rdflib    Alternatively manually download the package from the Python Package  Index (PyPI) at https://pypi.python.org/pypi/rdflib    The current version of RDFLib is 6.1.1, see the ``CHANGELOG.md`` file for what's new in this release.    ### Installation of the current master branch (for developers)    With *pip* you can also install rdflib from the git repository with one of the following options:        $ pip install git+https://github.com/rdflib/rdflib@master    or        $ pip install -e git+https://github.com/rdflib/rdflib@master#egg=rdflib    or from your locally cloned repository you can install it with one of the following options:        $ python setup.py install    or        $ pip install -e .    ## Getting Started  RDFLib aims to be a pythonic RDF API. RDFLib's main data object is a `Graph` which is a Python collection  of RDF *Subject, Predicate, Object* Triples:    To create graph and load it with RDF data from DBPedia then print the results:    ```python  from rdflib import Graph  g = Graph()  g.parse('http://dbpedia.org/resource/Semantic_Web')    for s, p, o in g:      print(s, p, o)  ```  The components of the triples are URIs (resources) or Literals  (values).    URIs are grouped together by *namespace*, common namespaces are included in RDFLib:    ```python  from rdflib.namespace import DC, DCTERMS, DOAP, FOAF, SKOS, OWL, RDF, RDFS, VOID, XMLNS, XSD  ```    You can use them like this:    ```python  from rdflib import Graph, URIRef, Literal  from rdflib.namespace import RDFS, XSD    g = Graph()  semweb = URIRef('http://dbpedia.org/resource/Semantic_Web')  type = g.value(semweb, RDFS.label)  ```  Where `RDFS` is the RDFS namespace, `XSD` the XML Schema Datatypes namespace and `g.value` returns an object of the triple-pattern given (or an arbitrary one if multiple exist).    Or like this, adding a triple to a graph `g`:    ```python  g.add((      URIRef(""http://example.com/person/nick""),      FOAF.givenName,      Literal(""Nick"", datatype=XSD.string)  ))  ```  The triple (in n-triples notation) `<http://example.com/person/nick> <http://xmlns.com/foaf/0.1/givenName> ""Nick""^^<http://www.w3.org/2001/XMLSchema#string> .`  is created where the property `FOAF.givenName` is the URI `<http://xmlns.com/foaf/0.1/givenName>` and `XSD.string` is the  URI `<http://www.w3.org/2001/XMLSchema#string>`.    You can bind namespaces to prefixes to shorten the URIs for RDF/XML, Turtle, N3, TriG, TriX & JSON-LD serializations:     ```python  g.bind(""foaf"", FOAF)  g.bind(""xsd"", XSD)  ```  This will allow the n-triples triple above to be serialised like this:   ```python  print(g.serialize(format=""turtle""))  ```    With these results:  ```turtle  PREFIX foaf: <http://xmlns.com/foaf/0.1/>  PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>    <http://example.com/person/nick> foaf:givenName ""Nick""^^xsd:string .  ```    New Namespaces can also be defined:    ```python  dbpedia = Namespace('http://dbpedia.org/ontology/')    abstracts = list(x for x in g.objects(semweb, dbpedia['abstract']) if x.language=='en')  ```    See also [./examples](./examples)      ## Features  The library contains parsers and serializers for RDF/XML, N3,  NTriples, N-Quads, Turtle, TriX, JSON-LD, RDFa and Microdata.    The library presents a Graph interface which can be backed by  any one of a number of Store implementations.    This core RDFLib package includes store implementations for  in-memory storage and persistent storage on top of the Berkeley DB.    A SPARQL 1.1 implementation is included - supporting SPARQL 1.1 Queries and Update statements.    RDFLib is open source and is maintained on [GitHub](https://github.com/RDFLib/rdflib/). RDFLib releases, current and previous  are listed on [PyPI](https://pypi.python.org/pypi/rdflib/)    Multiple other projects are contained within the RDFlib ""family"", see <https://github.com/RDFLib/>.    ## Running tests    ### Running the tests on the host    Run the test suite with `pytest`.  ```shell  pytest  ```    ### Running test coverage on the host with coverage report    Run the test suite and generate a HTML coverage report with `pytest` and `pytest-cov`.  ```shell  pytest --cov  ```    ### Running the tests in a Docker container    Run the test suite inside a Docker container for cross-platform support. This resolves issues such as installing BerkeleyDB on Windows and avoids the host and port issues on macOS.  ```shell  make tests  ```    Tip: If the underlying Dockerfile for the test runner changes, use `make build`.    ### Running the tests in a Docker container with coverage report    Run the test suite inside a Docker container with HTML coverage report.  ```shell  make coverage  ```    ### Viewing test coverage    Once tests have produced HTML output of the coverage report, view it by running:  ```shell  pytest --cov --cov-report term --cov-report html  python -m http.server --directory=htmlcov  ```    ## Contributing    RDFLib survives and grows via user contributions!  Please read our [contributing guide](https://rdflib.readthedocs.io/en/stable/developers.html) to get started.  Please consider lodging Pull Requests here:    * <https://github.com/RDFLib/rdflib/pulls>    You can also raise issues here:    * <https://github.com/RDFLib/rdflib/issues>    ## Support & Contacts  For general ""how do I..."" queries, please use https://stackoverflow.com and tag your question with `rdflib`.  Existing questions:    * <https://stackoverflow.com/questions/tagged/rdflib>    If you want to contact the rdflib maintainers, please do so via:    * the rdflib-dev mailing list: <https://groups.google.com/group/rdflib-dev>  * the chat, which is available at [gitter](https://gitter.im/RDFLib/rdflib) or via matrix [#RDFLib_rdflib:gitter.im](https://matrix.to/#/#RDFLib_rdflib:gitter.im) """
Semantic web;https://github.com/AKSW/Sparqlify;"""# Sparqlify SPARQL->SQL rewriter  [![Build Status](http://ci.aksw.org/jenkins/job/Sparqlify/badge/icon)](http://ci.aksw.org/jenkins/job/Sparqlify/)      ## Introduction    Sparqlify is a scalable SPARQL-SQL rewriter whose development began in April 2011 in the course of the [LinkedGeoData](http://linkedgeodata.org) project.    This system's features/traits are:  * Support of the ['Sparqlification Mapping Language' (SML)](http://sparqlify.org/wiki/SML), an intuitive language for expressing RDB-RDF mappings with only very little syntactic noise.  * Scalability: Sparqlify does not evaluate expressions in memory. All SPARQL filters end up in the corresponding SQL statement, giving the underlying RDBMS has maximum control over query planning.  * A powerful rewriting engine that analyzes filter expressions in order to eleminate self joins and joins with unsatisfiable conditions.  * Initial support for spatial datatypes and predicates.  * A subset of the SPARQL 1.0 query language plus sub queries are supported.  * Tested with PostgreSQL/Postgis and H2. Support for further databases is planned.  * CSV support  * R2RML will be supported soon    ## Functions  SPARQL-to-SQL function mappings are specified in the file [functions.xml](sparqlify-core/src/main/resources/functions.xml).    <details>    <summary>Standard SPARQL functions</summary>    | SPARQL function | SQL Definition  |  |-----------------| ----------------|  | boolean strstarts(string, string) | strpos($1$, $2$) = 1|  | TODO | |    </details>    <details>    <summary>Spatial Function Extensions </summary>      | SPARQL function | SQL Definition  |  |-----------------| ----------------|  | TODO | |      </details>            ## Supported SPARQL language features  * Join, LeftJoin (i.e. Optional), Union, Sub queries  * Filter predicates: comparison: (<=, <, =, >, >=) logical: (!, &&; ||) arithmetic: (+, -) spatial: st\_intersects, geomFromText; other: regex, lang, langMatches    * Aggregate functions: Count(\*)  * Order By is pushed into the SQL      ## Debian packages    Sparqlify Debian packages can be obtained by following means:  * Via the [Linked Data Stack](http://stack.linkeddata.org) (recommended)  * Download from the [Sparqlify website's download section](http://sparqlify.org/downloads/releases).  * Directly from source using maven (read down the README)    ### Public repositories    After setting up any of the repositories below, you can install sparqlify with apt using    * apt: `sudo apt-get install sparqlify-cli    #### Linked Data Stack (this is what you want)    Sparqlify is distributed at the [Linked Data Stack](http://stack.linkeddata.org), which offers many great tools done by various contributors of the Semantic Web community.    * The repository is available in the flavors `nightly`, `testing` and `stable` [here](http://stack.linkeddata.org/download/repo.php).    ```bash  # !!! Replace stable with nightly or testing as needed !!!    # Download the repository package  wget http://stack.linkeddata.org/ldstable-repository.deb    # Install the repository package  sudo dpkg -i ldstable-repository.deb    # Update the repository database  sudo apt-get update  ```      #### Bleeding Edge (Not recommended for production)  For the latest development version (built on every commit) perform the following steps    Import the public key with        wget -qO - http://cstadler.aksw.org/repos/apt/conf/packages.precise.gpg.key  | sudo apt-key add -    Add the repository        echo 'deb http://cstadler.aksw.org/repos/apt precise main contrib non-free' | sudo tee -a /etc/apt/sources.list.d/cstadler.aksw.org.list      Note that this also works with distros other than ""precise"" (ubuntu 12.04) such as ubuntu 14.04 or 16.04.        ## Building  Building the repository creates the JAR files providing the `sparqlify-*` tool suite.      ### Debian package  Building debian packages from this repo relies on the [Debian Maven Plugin](http://debian-maven.sourceforge.net]) plugin, which requires a debian-compatible environment.  If such an environment is present, the rest is simple:        # Install all shell scripts necessary for creating deb packages      sudo apt-get install devscripts        # Execute the follwing from the `<repository-root>/sparqlify-core` folder:      mvn clean install deb:package        # Upon sucessful completion, the debian package is located under `<repository-root>/sparqlify-core/target`      # Install using `dpkg`      sudo dpkg -i sparqlify_<version>.deb        # Uninstall using dpkg or apt:      sudo dpkg -r sparqlify      sudo apt-get remove sparqlify      ### Assembly based  Another way to build the project is run the following commands at `<repository-root>`        mvn clean install        cd sparqlify-cli      mvn assembly:assembly      This will generate a single stand-alone jar containing all necessary dependencies.  Afterwards, the shell scripts under `sparqlify-core/bin` should work.    ## Tool suite    If Sparqlify was installed from the debian package, the following commands are available system-wide:    * `sparqlify`: This is the main executable for running individual SPARQL queries, creating dumps and starting a stand-alone server.  * `sparqlify-csv`: This tool can create RDF dumps from CSV file based on SML view definitions.  * `sparqlify-platform`: A stand-alone server component integrating additional projects.    These tools write their output (such as RDF data in the N-TRIPLES format) to STDOUT. Log output goes to STDERR.    ### sparqlify  Usage: `sparqlify [options]`    Options are:    * Setup    * -m   SML view definition file    * Database Connectivity Settings    * -h   Hostname of the database (e.g. localhost or localhost:5432)    * -d   Database name    * -u   User name    * -p   Password    * -j   JDBC URI (mutually exclusive with both -h and -d)    * Quality of Service    * -n   Maximum result set size    * -t   Maximum query execution time in seconds (excluding rewriting time)    * Stand-alone Server Configuration    * -P   Server port [default: 7531]    * Run-Once (these options prevent the server from being started and are mutually exclusive with the server configuration)    * -D   Create an N-TRIPLES RDF dump on STDOUT     * -Q   [SPARQL query] Runs a SPARQL query against the configured database and view definitions    #### Example  The following command will start the Sparqlify HTTP server on the default port.        sparqlify -h localhost -u postgres -p secret -d mydb -m mydb-mappings.sml -n 1000 -t 30    Agents can now access the SPARQL endpoint at `http://localhost:7531/sparql`    ### sparqlify-csv  Usage: `sparqlify-csv [options]`    * Setup    * -m   SML view definition file    * -f   Input data file    * -v   View name (can be omitted if the view definition file only contains a single view)    * CSV Parser Settings    * -d   CSV field delimiter (default is '""')    * -e   CSV field escape delimiter (escapes the field delimiter) (default is '\')    * -s   CSV field separator (default is ',')    * -h   Use first row as headers. This option allows one to reference columns by name additionally to its index.      ### sparqlify-platform (Deprecated; about to be superseded by sparqlify-web-admin)  The Sparqlify Platform (under /sparqlify-platform) bundles Sparqlify with the Linked Data wrapper [Pubby](https://github.com/cygri/pubby) and the SPARQL Web interface [Snorql](https://github.com/kurtjx/SNORQL).    Usage: `sparqlify-platform config-dir [port]`     * `config-dir` Path to the configuration directory, e.g. `<repository-root/sparqlify-platform/config/example>`  * `port` Port on which to run the platform, default 7531.      For building, at the root of the project (outside of the sparqlify-\* directories), run `mvn compile` to build all modules.  Afterwards, lauch the platform using:        cd sparqlify-platform/bin      ./sparqlify-platform <path-to-config> <port>      Assuming the platform runs under `http://localhost:7531`, you can access the following services relative to this base url:  * `/sparql` is Sparqlify's SPARQL endpoint  * `/snorql` shows the SNORQL web frontend  * `/pubby` is the entry point to the Linked Data interface      #### Configuration  The configDirectory argument is mandatory and must contain a *sub-directory* for the context-path (i.e. `sparqlify-platform`) in turn contains the files:  * `platform.properties` This file contains configuration parameters that can be adjusted, such as the database connection.  * `views.sparqlify` The set of Sparqlify view definition to use.    I recommend to first create a copy of the files in `/sparqlify-platform/config/example` under a different location, then adjust the parameters and finally launch the platform with `-DconfigDirectory=...` set appropriately.    The platform *applies autoconfiguration to Pubby and Snorql*:  * Snorql: Namespaces are those of the views.sparqlify file.  * Pubby: The host name of all resources generated in the Sparqlify views is replaced with the URL of the platform (currently still needs to be configured via `platform.properties`)    Additionally you probably want to make the URIs nice by e.g. configuring an apache reverse proxy:    Enable the apache `proxy_http` module:    	sudo a2enmod proxy_http    Then in your `/etc/apache2/sites-available/default` add lines such as    	ProxyRequest Off  	ProxyPass /resource http://localhost:7531/pubby/bizer/bsbm/v01/ retry=1  	ProxyPassReverse /resource http://localhost:7531/pubby/bizer/bsbm/v01/    These entries will enable requests to `http://localhost/resource/...` rather than `http//localhost:7531/pubby/bizer/bsbm/v01/`.    The `retry=1` means, that apache only waits 1 seconds before retrying again when it encounters an error (e.g. HTTP code 500) from the proxied resource.    *IMPORTANT: ProxyRequests are off by default; DO NOT ENABLE THEM UNLESS YOU KNOW WHAT YOU ARE DOING. Simply enabling them potentially allows anyone to use your computer as a proxy.*      ## SML Mapping Syntax:  A Sparqlification Mapping Language (SML) configuration is essentially a set of CREATE VIEW statements, somewhat similar to the CREATE VIEW statement from SQL.  Probably the easiest way to learn to syntax is to look at the following resources:    * The [SML documentation](http://sparqlify.org/wiki/SML)  * The [SML test suite](https://github.com/AKSW/Sparqlify/tree/master/sparqlify-core/src/test/resources/org/aksw/sml/r2rml_tests) which is derived from the [R2RML test suite](https://github.com/AKSW/Sparqlify/tree/master/sparqlify-core/src/test/resources/org/w3c/r2rml_tests).    Two more examples are from    Additionally, for convenience, prefixes can be declared, which are valid throughout the config file.  As comments, you can use //, /\* \*/, and #.     For a first impression, here is a quick example:            /* This is a comment       * /* You can even nest them! */       */      // Prefixes are valid throughout the file      Prefix dbp:<http://dbpedia.org/ontology/>      Prefix ex:<http://ex.org/>        Create View myFirstView As          Construct {              ?s a dbp:Person .              ?s ex:workPage ?w .          }      With          ?s = uri('http://mydomain.org/person', ?id) // Define ?s to be an URI generated from the concatenation of a prefix with mytable's id-column.          ?w = uri(?work_page) // ?w is assigned the URIs in the column 'work_page' of 'mytable'      Constrain          ?w prefix ""http://my-organization.org/user/"" // Constraints can be used for optimization, e.g. to prune unsatisfiable join conditions      From          mytable; // If you want to use an SQL query, the query (without trailing semicolon) must be enclosed in double square brackets: [[SELECT id, work_page FROM mytable]]      ### Notes for sparqlify-csv  For `sparqlify-csv` view definition syntax is almost the same as above; the differences being:    * Instead of `Create View viewname As Construct` start your views with `CREATE VIEW TEMPLATE viewname As Construct`  * There is no FROM and CONSTRAINT clause    Colums can be referenced either by name (see the -h option) or by index (1-based).    #### Example        // Assume a CSV file with the following columns (osm stands for OpenStreetMap)      (city\_name, country\_name, osm\_entity\_type, osm\_id, longitude, latitude)        Prefix fn:<http://aksw.org/sparqlify/> //Needed for urlEncode and urlDecode.      Prefix rdfs:<http://www.w3.org/2000/01/rdf-schema#>      Prefix owl:<http://www.w3.org/2002/07/owl#>      Prefix xsd:<http://www.w3.org/2001/XMLSchema#>      Prefix geo:<http://www.w3.org/2003/01/geo/wgs84_pos#>        Create View Template geocode As        Construct {          ?cityUri            owl:sameAs ?lgdUri .            ?lgdUri            rdfs:label ?cityLabel ;            geo:long ?long ;            geo:lat ?lat .        }        With          ?cityUri = uri(concat(""http://fp7-pp.publicdata.eu/resource/city/"", fn:urlEncode(?2), ""-"", fn:urlEncode(?1)))          ?cityLabel = plainLiteral(?1)          ?lgdUri = uri(concat(""http://linkedgeodata.org/triplify/"", ?4, ?5))          ?long = typedLiteral(?6, xsd:float)          ?lat = typedLiteral(?7, xsd:float)       """
Semantic web;https://github.com/Wimmics/corese;"""<!-- markdownlint-configure-file { ""MD004"": { ""style"": ""consistent"" } } -->  <!-- markdownlint-disable MD033 -->    #    <p align=""center"">      <a href=""https://project.inria.fr/corese/"">          <img src=""https://user-images.githubusercontent.com/5692787/151987397-316a61f0-8098-4d37-a4e8-69180e33261a.svg"" width=""300"" height=""149"" alt=""Corese-logo"">      </a>      <br>      <strong>Software platform for the Semantic Web of Linked Data</strong>  </p>  <!-- markdownlint-enable MD033 -->    Corese is a software platform implementing and extending the standards of the Semantic Web. It allows to create, manipulate, parse, serialize, query, reason and validate RDF data.    Corese implement W3C standarts [RDF](https://www.w3.org/RDF/), [RDFS](https://www.w3.org/2001/sw/wiki/RDFS), [SPARQL1.1 Query & Update](https://www.w3.org/2001/sw/wiki/SPARQL), [OWL RL](https://www.w3.org/2005/rules/wiki/OWLRL), [SHACL](https://www.w3.org/TR/shacl/) …  It also implements extensions like [STTL SPARQL](https://files.inria.fr/corese/doc/sttl.html), [SPARQL Rule](https://files.inria.fr/corese/doc/rule.html) and [LDScript](https://files.inria.fr/corese/doc/ldscript.html).    There are three versions of Corese:    - **Corese-library:** Java library to process RDF data and use Corese features via an API.  - **Corese-server:** Tool to easily create, configure and manage SPARQL endpoints.  - **Corese-gui:** Graphical interface that allows an easy and visual use of Corese features.    ## Download and install    ### Corese-library    - Download from [maven-central](https://search.maven.org/search?q=g:fr.inria.corese)    ```xml  <dependency>      <groupId>fr.inria.corese</groupId>      <artifactId>corese-core</artifactId>      <version>4.3.0</version>  </dependency>    <dependency>      <groupId>fr.inria.corese</groupId>      <artifactId>corese-rdf4j</artifactId>      <version>4.3.0</version>  </dependency>  ```    Documentation: [Getting Started With Corese-library](https://notes.inria.fr/s/hiiedLfVe#)    ### Corese-server    - Download from [Docker-hub](https://hub.docker.com/r/wimmics/corese)    ```sh  docker run --name my-corese \      -p 8080:8080 \      -d wimmics/corese  ```    - Download [Corese-server jar file](https://project.inria.fr/corese/download/).    ```sh  # Replace ${VERSION} with the desired version number (e.g: 4.3.0)  wget ""files.inria.fr/corese/distrib/corese-server-${VERSION}.jar""  java -jar ""corese-server-${VERSION}.jar""  ```    Documentation: [Getting Started With Corese-server](https://notes.inria.fr/s/SoyFglO_1#)    ### Corese-GUI    - Download [Corese-gui jar file](https://project.inria.fr/corese/download/).    ```sh  # Replace ${VERSION} with the desired version number (e.g: 4.3.0)  wget ""files.inria.fr/corese/distrib/corese-gui-${VERSION}.jar""  java -jar ""corese-gui-${VERSION}.jar""  ```    ## Compilation from source    Download source code and compile.    ```shell  git clone ""https://github.com/Wimmics/corese.git""  cd corese  mvn -Dmaven.test.skip=true package  ```    ## Contributions and discussions    For support questions, comments, and any ideas for improvements you'd like to discuss, please use our [discussion forum](https://github.com/Wimmics/corese/discussions/).  We welcome everyone to contribute to [issue reports](https://github.com/Wimmics/corese/issues), suggest new features, and create [pull requests](https://github.com/Wimmics/corese/pulls).    ## General informations    - [Corese website](https://project.inria.fr/corese)  - [Source code](https://github.com/Wimmics/corese)  - [Corese server demo](http://corese.inria.fr/)  - [Changelog](https://notes.inria.fr/s/TjriAbX14#)  - **Mailing list:** corese-users at inria.fr  - **Subscribe to the mailing list:** corese-users-request at inria.fr **subject:** subscribe """
Semantic web;https://github.com/comunica/comunica;"""<p align=""center"">    <a href=""https://comunica.dev/"">      <img alt=""Comunica"" src=""https://comunica.dev/img/comunica_red.svg"" width=""200"">    </a>  </p>    <p align=""center"">    <strong>A knowledge graph querying framework for JavaScript</strong>    <br />    <i>Flexible SPARQL and GraphQL over decentralized RDF on the Web.</i>  </p>    <p align=""center"">  <a href=""https://github.com/comunica/comunica/actions?query=workflow%3ACI""><img src=""https://github.com/comunica/comunica/workflows/CI/badge.svg"" alt=""Build Status""></a>  <a href=""https://coveralls.io/github/comunica/comunica?branch=master""><img src=""https://coveralls.io/repos/github/comunica/comunica/badge.svg?branch=master"" alt=""Coverage Status""></a>  <a href=""https://zenodo.org/badge/latestdoi/107345960""><img src=""https://zenodo.org/badge/107345960.svg"" alt=""DOI""></a>  <a href=""https://gitter.im/comunica/Lobby""><img src=""https://badges.gitter.im/comunica.png"" alt=""Gitter chat""></a>  </p>    **[Learn more about Comunica on our website](https://comunica.dev/).**    Comunica is an open-source project that is used by [many other projects](https://github.com/comunica/comunica/network/dependents),  and is being maintained by a [group of volunteers](https://github.com/comunica/comunica/graphs/contributors).  If you would like to support this project, you may consider:    * Contributing directly by [writing code or documentation](https://comunica.dev/contribute/); or  * Contributing indirectly by funding this project via [Open Collective](https://opencollective.com/comunica-association).    ## Supported by    Comunica is a community-driven project, sustained by the [Comunica Association](https://comunica.dev/association/).  If you are using Comunica, [becoming a sponsor or member](https://opencollective.com/comunica-association) is a way to make Comunica sustainable in the long-term.    Our top sponsors are shown below!    <a href=""https://opencollective.com/comunica-association/sponsor/0/website"" target=""_blank""><img src=""https://opencollective.com/comunica-association/sponsor/0/avatar.svg""></a>  <a href=""https://opencollective.com/comunica-association/sponsor/1/website"" target=""_blank""><img src=""https://opencollective.com/comunica-association/sponsor/1/avatar.svg""></a>  <a href=""https://opencollective.com/comunica-association/sponsor/2/website"" target=""_blank""><img src=""https://opencollective.com/comunica-association/sponsor/2/avatar.svg""></a>  <a href=""https://opencollective.com/comunica-association/sponsor/3/website"" target=""_blank""><img src=""https://opencollective.com/comunica-association/sponsor/3/avatar.svg""></a>    ## Query with Comunica    Read one of our [guides to **get started** with querying](https://comunica.dev/docs/query/getting_started/):    * [Querying from the command line](https://comunica.dev/docs/query/getting_started/query_cli/)  * [Updating from the command line](https://comunica.dev/docs/query/getting_started/update_cli/)  * [Querying local files from the command line](https://comunica.dev/docs/query/getting_started/query_cli_file/)  * [Querying in a JavaScript app](https://comunica.dev/docs/query/getting_started/query_app/)  * [Updating in a JavaScript app](https://comunica.dev/docs/query/getting_started/update_app/)  * [Querying in a JavaScript browser app](https://comunica.dev/docs/query/getting_started/query_browser_app/)  * [Setting up a SPARQL endpoint](https://comunica.dev/docs/query/getting_started/setup_endpoint/)  * [Querying from a Docker container](https://comunica.dev/docs/query/getting_started/query_docker/)  * [Setting up a Web client](https://comunica.dev/docs/query/getting_started/setup_web_client/)  * [Query using the latest development version](https://comunica.dev/docs/query/getting_started/query_dev_version/)    Or jump right into one of the available query engines:  * [Comunica SPARQL](https://github.com/comunica/comunica/tree/master/engines/query-sparql#readme): SPARQL/GraphQL querying from JavaScript applications or the CLI ([Browser-ready via a CDN](https://github.com/rdfjs/comunica-browser))     - Source Customisation     * [Comunica SPARQL File](https://github.com/comunica/comunica/tree/master/engines/query-sparql-file#readme): Engine to query over local RDF files     * [Comunica SPARQL RDFJS](https://github.com/comunica/comunica/tree/master/engines/query-sparql-rdfjs#readme): Engine to query over in-memory [RDFJS-compliant sources](https://rdf.js.org/stream-spec/#source-interface).     * [Comunica SPARQL HDT](https://github.com/comunica/comunica-actor-init-sparql-hdt#readme): Library to query over local [HDT](https://www.rdfhdt.org/) files     - Solid Customisation     * [Comunica SPARQL Solid](https://github.com/comunica/comunica-feature-solid/tree/master/engines/query-sparql-solid#readme): Engine to query over files behind [Solid access control](https://solidproject.org/).     - Link Traversal Research     * [Comunica SPARQL Link Traversal](https://github.com/comunica/comunica-feature-link-traversal/tree/master/engines/query-sparql-link-traversal#readme): Engine to query over multiple files by following links between them.     * [Comunica SPARQL Link Traversal Solid](https://github.com/comunica/comunica-feature-link-traversal/tree/master/engines/query-sparql-link-traversal-solid#readme): Engine to query within [Solid data vaults](https://solidproject.org/) by following links between documents.     - Reasoning Support     * [Comunica SPARQL Reasoning](https://github.com/comunica/comunica-feature-reasoning/tree/master/engines/query-sparql-reasoning): Engine that adds support for reasoning     * [Comunica SPARQL Reasoning File](https://github.com/comunica/comunica-feature-reasoning/tree/master/engines/query-sparql-file-reasoning): Engine to query over local RDF files with support for reasoning    ## Modify or Extending Comunica    [Read one of our guides to **get started** with modifying Comunica](https://comunica.dev/docs/modify/),  or have a look at some [examples](https://github.com/comunica/examples):    * [Querying with a custom configuration from the command line](https://comunica.dev/docs/modify/getting_started/custom_config_cli/)  * [Querying with a custom configuration in a JavaScript app](https://comunica.dev/docs/modify/getting_started/custom_config_app/)  * [Exposing your custom config as an npm package](https://comunica.dev/docs/modify/getting_started/custom_init/)  * [Exposing your custom config in a Web client](https://comunica.dev/docs/modify/getting_started/custom_web_client/)  * [Contributing a new query operation actor to the Comunica repository](https://comunica.dev/docs/modify/getting_started/contribute_actor/)  * [Adding a config parameter to an actor](https://comunica.dev/docs/modify/getting_started/actor_parameter/)    ## Contribute    Interested in contributing? Have a look at our [contribution guide](https://comunica.dev/contribute/).    ## Development Setup    _(JSDoc: https://comunica.github.io/comunica/)_    This repository should be used by Comunica module **developers** as it contains multiple Comunica modules that can be composed.  This repository is managed as a [monorepo](https://github.com/babel/babel/blob/master/doc/design/monorepo.md)  using [Lerna](https://lernajs.io/).    If you want to develop new features  or use the (potentially unstable) in-development version,  you can set up a development environment for Comunica.    Comunica requires [Node.JS](http://nodejs.org/) 8.0 or higher and the [Yarn](https://yarnpkg.com/en/) package manager.  Comunica is tested on OSX, Linux and Windows.    This project can be setup by cloning and installing it as follows:    ```bash  $ git clone https://github.com/comunica/comunica.git  $ cd comunica  $ yarn install  ```    **Note: `npm install` is not supported at the moment, as this project makes use of Yarn's [workspaces](https://yarnpkg.com/lang/en/docs/workspaces/) functionality**    This will install the dependencies of all modules, and bootstrap the Lerna monorepo.  After that, all [Comunica packages](https://github.com/comunica/comunica/tree/master/packages) are available in the `packages/` folder  and can be used in a development environment, such as querying with [Comunica SPARQL (`@comunica/query-sparql`)](https://github.com/comunica/comunica/tree/master/engines/query-sparql).    Furthermore, this will add [pre-commit hooks](https://www.npmjs.com/package/pre-commit)  to build, lint and test.  These hooks can temporarily be disabled at your own risk by adding the `-n` flag to the commit command.    ## Benchmarking    If you want to do benchmarking with Comunica in Node.js,  make sure to run Node.js in production mode as follows:    ```bash  > NODE_ENV=production node packages/some-package/bin/some-bin.js  ```    The reason for this is that Comunica extensively generates  internal `Error` objects.  In non-production mode, these also produce long stacktraces,  which may in some cases impact performance.    ## Cite    If you are using or extending Comunica as part of a scientific publication,  we would appreciate a citation of our [article](https://comunica.github.io/Article-ISWC2018-Resource/).    ```bibtex  @inproceedings{taelman_iswc_resources_comunica_2018,    author    = {Taelman, Ruben and Van Herwegen, Joachim and Vander Sande, Miel and Verborgh, Ruben},    title     = {Comunica: a Modular SPARQL Query Engine for the Web},    booktitle = {Proceedings of the 17th International Semantic Web Conference},    year      = {2018},    month     = oct,    url       = {https://comunica.github.io/Article-ISWC2018-Resource/}  }  ```    ## License  This code is copyrighted by [the Comunica Association](https://comunica.dev/association/) and [Ghent University – imec](http://idlab.ugent.be/)  and released under the [MIT license](http://opensource.org/licenses/MIT). """
Semantic web;https://github.com/phenoscape/scowl;"""# Scowl    [![status](http://joss.theoj.org/papers/1b9d09aab8754997884a04c081cfc019/status.svg)](http://joss.theoj.org/papers/1b9d09aab8754997884a04c081cfc019)    Scowl provides a Scala DSL allowing a declarative approach to composing OWL expressions and axioms using the [OWL API](http://owlapi.sourceforge.net).    ## Usage    Since version 1.2.1, Scowl is available via Maven Central. Add the dependency to your `build.sbt` if you are using OWL API 4.x:    ```scala  libraryDependencies += ""org.phenoscape"" %% ""scowl"" % ""1.4.1""  ```    For OWL API 5.x:    ```scala  libraryDependencies += ""org.phenoscape"" %% ""scowl-owlapi5"" % ""1.4.1""  ```    Import `org.phenoscape.scowl._`, and Scowl implicit conversions will add pseudo Manchester syntax methods to native OWL API objects. Additionally, functional syntax-style constructors and extractors will be in scope.    Scowl 1.2+ is built with OWL API 4.x (and from 1.4.1, additionally OWL API 5.x). For OWL API 3.5, use Scowl 1.0.2. Scowl is cross-compiled to support Scala 2.13 and Scala 3.    ## Examples  The easiest way to get started is to see how the DSL can be used to implement all the examples from the [OWL 2 Web Ontology Language   Primer](https://www.w3.org/TR/owl2-primer/):    * [Manchester syntax style](https://github.com/phenoscape/scowl/blob/master/src/main/scala/org/phenoscape/scowl/example/OWL2PrimerManchester.scala)  * [Functional syntax style](https://github.com/phenoscape/scowl/blob/master/src/main/scala/org/phenoscape/scowl/example/OWL2PrimerFunctional.scala)    The examples below are also available in   [code](https://github.com/phenoscape/scowl/blob/master/src/main/scala/org/phenoscape/scowl/example/ReadMeExamples.scala).    ### Scowl expressions use and return native OWL API objects  ```scala  import org.phenoscape.scowl._  // import org.phenoscape.scowl._    val hasParent = ObjectProperty(""http://www.co-ode.org/roberts/family-tree.owl#hasParent"")  // hasParent: org.semanticweb.owlapi.model.OWLObjectProperty = <http://www.co-ode.org/roberts/family-tree.owl#hasParent>    val isParentOf = ObjectProperty(""http://www.co-ode.org/roberts/family-tree.owl#isParentOf"")  // isParentOf: org.semanticweb.owlapi.model.OWLObjectProperty = <http://www.co-ode.org/roberts/family-tree.owl#isParentOf>    val isSiblingOf = ObjectProperty(""http://www.co-ode.org/roberts/family-tree.owl#isSiblingOf"")  // isSiblingOf: org.semanticweb.owlapi.model.OWLObjectProperty = <http://www.co-ode.org/roberts/family-tree.owl#isSiblingOf>    val Person = Class(""http://www.co-ode.org/roberts/family-tree.owl#Person"")  // Person: org.semanticweb.owlapi.model.OWLClass = <http://www.co-ode.org/roberts/family-tree.owl#Person>    val FirstCousin = Class(""http://www.co-ode.org/roberts/family-tree.owl#FirstCousin"")  // FirstCousin: org.semanticweb.owlapi.model.OWLClass = <http://www.co-ode.org/roberts/family-tree.owl#FirstCousin>    val axiom = FirstCousin EquivalentTo (Person and (hasParent some (Person and (isSiblingOf some (Person and (isParentOf some Person))))))  // axiom: org.semanticweb.owlapi.model.OWLEquivalentClassesAxiom = EquivalentClasses(<http://www.co-ode.org/roberts/family-tree.owl#FirstCousin> ObjectIntersectionOf(<http://www.co-ode.org/roberts/family-tree.owl#Person> ObjectSomeValuesFrom(<http://www.co-ode.org/roberts/family-tree.owl#hasParent> ObjectIntersectionOf(<http://www.co-ode.org/roberts/family-tree.owl#Person> ObjectSomeValuesFrom(<http://www.co-ode.org/roberts/family-tree.owl#isSiblingOf> ObjectIntersectionOf(<http://www.co-ode.org/roberts/family-tree.owl#Person> ObjectSomeValuesFrom(<http://www.co-ode.org/roberts/family-tree.owl#isParentOf> <http://www.co-ode.org/roberts/family-tree.owl#Person>)))))) )  ```  ### Add some axioms and programmatically generated GCIs to an ontology  ```scala  val manager = OWLManager.createOWLOntologyManager()  val ontology = manager.createOntology()  val PartOf = ObjectProperty(""http://example.org/part_of"")  val HasPart = ObjectProperty(""http://example.org/has_part"")  val DevelopsFrom = ObjectProperty(""http://example.org/develops_from"")  val Eye = Class(""http://example.org/eye"")  val Head = Class(""http://example.org/head"")  val Tail = Class(""http://example.org/tail"")    manager.addAxiom(ontology, Eye SubClassOf (PartOf some Head))  manager.addAxiom(ontology, Eye SubClassOf (not(PartOf some Tail)))    val gcis = for {    term <- ontology.getClassesInSignature(true)  } yield {    (not(HasPart some term)) SubClassOf (not(HasPart some (DevelopsFrom some term)))  }  manager.addAxioms(ontology, gcis)  ```    ### Using pattern matching extractors to implement negation normal form  ```scala  def nnf(expression: OWLClassExpression): OWLClassExpression = expression match {    case Class(_)                                                          => expression    case ObjectComplementOf(Class(_))                                      => expression    case ObjectComplementOf(ObjectComplementOf(expression))                => nnf(expression)    case ObjectUnionOf(operands)                                           => ObjectUnionOf(operands.map(nnf))    case ObjectIntersectionOf(operands)                                    => ObjectIntersectionOf(operands.map(nnf))    case ObjectComplementOf(ObjectUnionOf(operands))                       => ObjectIntersectionOf(operands.map(c => nnf(ObjectComplementOf(c))))    case ObjectComplementOf(ObjectIntersectionOf(operands))                => ObjectUnionOf(operands.map(c => nnf(ObjectComplementOf(c))))    case ObjectAllValuesFrom(property, filler)                             => ObjectAllValuesFrom(property, nnf(filler))    case ObjectSomeValuesFrom(property, filler)                            => ObjectSomeValuesFrom(property, nnf(filler))    case ObjectMinCardinality(num, property, filler)                       => ObjectMinCardinality(num, property, nnf(filler))    case ObjectMaxCardinality(num, property, filler)                       => ObjectMaxCardinality(num, property, nnf(filler))    case ObjectExactCardinality(num, property, filler)                     => ObjectExactCardinality(num, property, nnf(filler))    case ObjectComplementOf(ObjectAllValuesFrom(property, filler))         => ObjectSomeValuesFrom(property, nnf(ObjectComplementOf(filler)))    case ObjectComplementOf(ObjectSomeValuesFrom(property, filler))        => ObjectAllValuesFrom(property, nnf(ObjectComplementOf(filler)))    case ObjectComplementOf(ObjectMinCardinality(num, property, filler))   => ObjectMaxCardinality(math.max(num - 1, 0), property, nnf(filler))    case ObjectComplementOf(ObjectMaxCardinality(num, property, filler))   => ObjectMinCardinality(num + 1, property, nnf(filler))    case ObjectComplementOf(ObjectExactCardinality(num, property, filler)) => ObjectUnionOf(ObjectMinCardinality(num + 1, property, nnf(filler)), ObjectMaxCardinality(math.max(num - 1, 0), property, nnf(filler)))    case _                                                                 => ???  }  ```    ### Using pattern matching extractors in for comprehensions  ```scala  // Print all properties and fillers used in existential restrictions in subclass axioms  for {    SubClassOf(_, subclass, ObjectSomeValuesFrom(property, filler)) <- ontology.getAxioms  } yield {    println(s""$property $filler"")  }    // Make an index of language tags to label values  val langValuePairs = for {    AnnotationAssertion(_, RDFSLabel, _, value @@ Some(lang)) <- ontology.getAxioms(Imports.INCLUDED)  } yield {    lang -> value  }  val langToValues: Map[String, Set[String]] = langValuePairs.foldLeft(Map.empty[String, Set[String]]) {    case (langIndex, (lang, value)) =>      langIndex.updated(lang, langIndex.getOrElse(value, Set.empty) ++ Set(value))  }  ```    ## Question or problem?  If you have questions about how to use Scowl, feel free to send an email to balhoff@gmail.com, or [open an issue on the tracker](https://github.com/phenoscape/scowl/issues). [Contributions are welcome](CONTRIBUTING.md).    ## Funding  Development of Scowl has been supported by National Science Foundation grant DBI-1062404 to the University of North Carolina.    ## License    Scowl is open source under the [MIT License](http://opensource.org/licenses/MIT).  See [LICENSE](LICENSE) for more information. """
Semantic web;https://github.com/marcelotto/rdf-ex;"""<img src=""rdf-logo.png"" align=""right"" />    # RDF.ex    [![CI](https://github.com/rdf-elixir/rdf-ex/workflows/CI/badge.svg?branch=master)](https://github.com/rdf-elixir/rdf-ex/actions?query=branch%3Amaster+workflow%3ACI)  [![Hex.pm](https://img.shields.io/hexpm/v/rdf.svg?style=flat-square)](https://hex.pm/packages/rdf)  [![Hex Docs](https://img.shields.io/badge/hex-docs-lightgreen.svg)](https://hexdocs.pm/rdf/)  [![Total Download](https://img.shields.io/hexpm/dt/rdf.svg)](https://hex.pm/packages/rdf)  [![License](https://img.shields.io/hexpm/l/rdf.svg)](https://github.com/rdf-elixir/rdf-ex/blob/master/LICENSE.md)      An implementation of the [RDF](https://www.w3.org/TR/rdf11-primer/) data model in Elixir.    The API documentation can be found [here](https://hexdocs.pm/rdf/). For a guide and more information about RDF.ex and it's related projects, go to <https://rdf-elixir.dev>.    Migration guides for the various versions can be found in the [Wiki](https://github.com/rdf-elixir/rdf-ex/wiki).      ## Features    - fully compatible with the RDF 1.1 specification  - support of the [RDF-star] extension  - in-memory data structures for RDF descriptions, RDF graphs and RDF datasets  - basic graph pattern matching against the in-memory data structures with streaming-support  - execution of [SPARQL] queries against the in-memory data structures with the [SPARQL.ex] package or against any SPARQL endpoint with the [SPARQL.Client] package  - RDF vocabularies as Elixir modules for safe, i.e. compile-time checked and concise usage of IRIs  - most of the important XML schema datatypes for RDF literals  - support for custom datatypes for RDF literals, incl. as derivations of XSD datatypes via facets   - sigils for the most common types of nodes, i.e. IRIs, literals, blank nodes and lists  - a DSL resembling Turtle to build RDF descriptions or full RDF graphs in Elixir  - implementations for the [N-Triples], [N-Quads] and [Turtle] serialization formats (including the respective RDF-star extensions); [JSON-LD] and [RDF-XML] are available with the separate [JSON-LD.ex] and [RDF-XML.ex] packages  - validation of RDF data against [ShEx] schemas with the [ShEx.ex] package  - mapping of RDF data structures to Elixir structs and back with [Grax]       ## Contributing    There's still much to do for a complete RDF ecosystem for Elixir, which means there are plenty of opportunities to contribute. Here are some suggestions:    - more serialization formats, like [RDFa], [N3], [CSVW], [HDT] etc.  - more XSD datatypes  - improving the documentation    See [CONTRIBUTING](CONTRIBUTING.md) for details.      ## Consulting    If you need help with your Elixir and Linked Data projects, just contact [NinjaConcept](https://www.ninjaconcept.com/) via <contact@ninjaconcept.com>.      ## Acknowledgements    The development of this project was partly sponsored by [NetzeBW](https://www.netze-bw.de/) for [NETZlive](https://www.netze-bw.de/unsernetz/netzinnovationen/digitalisierung/netzlive).    [JetBrains](https://www.jetbrains.com/?from=RDF.ex) supports the project with complimentary access to its development environments.      ## License and Copyright    (c) 2017-present Marcel Otto. MIT Licensed, see [LICENSE](LICENSE.md) for details.      [RDF.ex]:               https://hex.pm/packages/rdf  [JSON-LD.ex]:           https://hex.pm/packages/json_ld  [RDF-XML.ex]:           https://hex.pm/packages/rdf_xml  [SPARQL.ex]:            https://hex.pm/packages/sparql  [SPARQL.Client]:        https://hex.pm/packages/sparql_client  [ShEx.ex]:              https://hex.pm/packages/shex  [Grax]:                 https://hex.pm/packages/grax  [RDF-star]:             https://w3c.github.io/rdf-star/cg-spec  [N-Triples]:            https://www.w3.org/TR/n-triples/  [N-Quads]:              https://www.w3.org/TR/n-quads/  [Turtle]:               https://www.w3.org/TR/turtle/  [N3]:                   https://www.w3.org/TeamSubmission/n3/  [JSON-LD]:              https://www.w3.org/TR/json-ld/  [RDFa]:                 https://www.w3.org/TR/rdfa-syntax/  [RDF-XML]:              https://www.w3.org/TR/rdf-syntax-grammar/  [CSVW]:                 https://www.w3.org/TR/tabular-data-model/  [HDT]:                  http://www.rdfhdt.org/  [SPARQL]:               https://www.w3.org/TR/sparql11-overview/  [ShEx]:                 https://shex.io/ """
Semantic web;https://github.com/RubenVerborgh/N3.js;"""# Lightning fast, asynchronous, streaming RDF for JavaScript  [![Build Status](https://github.com/rdfjs/N3.js/workflows/CI/badge.svg)](https://github.com/rdfjs/N3.js/actions)  [![Coverage Status](https://coveralls.io/repos/github/rdfjs/N3.js/badge.svg)](https://coveralls.io/github/rdfjs/N3.js)  [![npm version](https://badge.fury.io/js/n3.svg)](https://www.npmjs.com/package/n3)  [![DOI](https://zenodo.org/badge/3058202.svg)](https://zenodo.org/badge/latestdoi/3058202)    The N3.js library is an implementation of the [RDF.js low-level specification](http://rdf.js.org/) that lets you handle [RDF](https://www.w3.org/TR/rdf-primer/) in JavaScript easily.  It offers:    - [**Parsing**](#parsing) triples/quads from    [Turtle](https://www.w3.org/TR/turtle/),    [TriG](https://www.w3.org/TR/trig/),    [N-Triples](https://www.w3.org/TR/n-triples/),    [N-Quads](https://www.w3.org/TR/n-quads/),    [RDF*](https://blog.liu.se/olafhartig/2019/01/10/position-statement-rdf-star-and-sparql-star/)    and [Notation3 (N3)](https://www.w3.org/TeamSubmission/n3/)  - [**Writing**](#writing) triples/quads to    [Turtle](https://www.w3.org/TR/turtle/),    [TriG](https://www.w3.org/TR/trig/),    [N-Triples](https://www.w3.org/TR/n-triples/),    [N-Quads](https://www.w3.org/TR/n-quads/)    and [RDF*](https://blog.liu.se/olafhartig/2019/01/10/position-statement-rdf-star-and-sparql-star/)  - [**Storage**](#storing) of triples/quads in memory    Parsing and writing is:  - **asynchronous** – triples arrive as soon as possible  - **streaming** – streams are parsed as data comes in, so you can parse files larger than memory  - **fast** – once the [fastest spec-compatible parser in JavaScript](https://github.com/rdfjs/N3.js/tree/master/perf)    (but then [graphy.js](https://github.com/blake-regalia/graphy.js) came along)    ## Installation  For Node.js, N3.js comes as an [npm package](https://npmjs.org/package/n3).    ```Bash  $ npm install n3  ```    ```JavaScript  const N3 = require('n3');  ```    N3.js seamlessly works in browsers via [webpack](https://webpack.js.org/)  or [browserify](http://browserify.org/).  If you're unfamiliar with these tools,  you can read  [_webpack: Creating a Bundle – getting started_](https://webpack.js.org/guides/getting-started/#creating-a-bundle)  or  [_Introduction to browserify_](https://writingjavascript.org/posts/introduction-to-browserify).  You will need to create a ""UMD bundle"" and supply a name (e.g. with the `-s N3` option in browserify).    You can also load it via CDN:  ```html  <script src=""https://unpkg.com/n3/browser/n3.min.js""></script>  ```    ## Creating triples/quads  N3.js follows the [RDF.js low-level specification](http://rdf.js.org/).    `N3.DataFactory` will give you the [factory](http://rdf.js.org/#datafactory-interface) functions to create triples and quads:    ```JavaScript  const { DataFactory } = N3;  const { namedNode, literal, defaultGraph, quad } = DataFactory;  const myQuad = quad(    namedNode('https://ruben.verborgh.org/profile/#me'),    namedNode('http://xmlns.com/foaf/0.1/givenName'),    literal('Ruben', 'en'),    defaultGraph(),  );  console.log(myQuad.termType);              // Quad  console.log(myQuad.value);                 // ''  console.log(myQuad.subject.value);         // https://ruben.verborgh.org/profile/#me  console.log(myQuad.object.value);          // Ruben  console.log(myQuad.object.datatype.value); // http://www.w3.org/1999/02/22-rdf-syntax-ns#langString  console.log(myQuad.object.language);       // en  ```    In the rest of this document, we will treat “triples” and “quads” equally:  we assume that a quad is simply a triple in a named or default graph.    ## Parsing    ### From an RDF document to quads    `N3.Parser` transforms Turtle, TriG, N-Triples, or N-Quads document into quads through a callback:  ```JavaScript  const parser = new N3.Parser();  parser.parse(    `PREFIX c: <http://example.org/cartoons#>     c:Tom a c:Cat.     c:Jerry a c:Mouse;             c:smarterThan c:Tom.`,    (error, quad, prefixes) => {      if (quad)        console.log(quad);      else        console.log(""# That's all, folks!"", prefixes);    });  ```  The callback's first argument is an optional error value, the second is a quad.  If there are no more quads,  the callback is invoked one last time with `null` for `quad`  and a hash of prefixes as third argument.  <br>  Pass a second callback to `parse` to retrieve prefixes as they are read.  <br>  If no callbacks are provided, parsing happens synchronously.    By default, `N3.Parser` parses a permissive superset of Turtle, TriG, N-Triples, and N-Quads.  <br>  For strict compatibility with any of those languages, pass a `format` argument upon creation:    ```JavaScript  const parser1 = new N3.Parser({ format: 'N-Triples' });  const parser2 = new N3.Parser({ format: 'application/trig' });  ```    Notation3 (N3) is supported _only_ through the `format` argument:    ```JavaScript  const parser3 = new N3.Parser({ format: 'N3' });  const parser4 = new N3.Parser({ format: 'Notation3' });  const parser5 = new N3.Parser({ format: 'text/n3' });  ```    It is possible to provide the base IRI of the document that you want to parse.  This is done by passing a `baseIRI` argument upon creation:  ```JavaScript  const parser = new N3.Parser({ baseIRI: 'http://example.org/' });  ```    By default, `N3.Parser` will prefix blank node labels with a `b{digit}_` prefix.  This is done to prevent collisions of unrelated blank nodes having identical  labels. The `blankNodePrefix` constructor argument can be used to modify the  prefix or, if set to an empty string, completely disable prefixing:  ```JavaScript  const parser = new N3.Parser({ blankNodePrefix: '' });  ```    ### From an RDF stream to quads    `N3.Parser` can parse [Node.js streams](http://nodejs.org/api/stream.html) as they grow,  returning quads as soon as they're ready.    ```JavaScript  const parser = new N3.Parser(),        rdfStream = fs.createReadStream('cartoons.ttl');  parser.parse(rdfStream, console.log);  ```    `N3.StreamParser` is a [Node.js stream](http://nodejs.org/api/stream.html) and [RDF.js Sink](http://rdf.js.org/#sink-interface) implementation.  This solution is ideal if your consumer is slower,  since source data is only read when the consumer is ready.    ```JavaScript  const streamParser = new N3.StreamParser(),        rdfStream = fs.createReadStream('cartoons.ttl');  rdfStream.pipe(streamParser);  streamParser.pipe(new SlowConsumer());    function SlowConsumer() {    const writer = new require('stream').Writable({ objectMode: true });    writer._write = (quad, encoding, done) => {      console.log(quad);      setTimeout(done, 1000);    };    return writer;  }  ```    A dedicated `prefix` event signals every prefix with `prefix` and `term` arguments.    ## Writing    ### From quads to a string    `N3.Writer` serializes quads as an RDF document.  Write quads through `addQuad`.    ```JavaScript  const writer = new N3.Writer({ prefixes: { c: 'http://example.org/cartoons#' } });  writer.addQuad(    namedNode('http://example.org/cartoons#Tom'),    namedNode('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),    namedNode('http://example.org/cartoons#Cat')  );  writer.addQuad(quad(    namedNode('http://example.org/cartoons#Tom'),    namedNode('http://example.org/cartoons#name'),    literal('Tom')  ));  writer.end((error, result) => console.log(result));  ```    By default, `N3.Writer` writes Turtle (or TriG if some quads are in a named graph).  <br>  To write N-Triples (or N-Quads) instead, pass a `format` argument upon creation:    ```JavaScript  const writer1 = new N3.Writer({ format: 'N-Triples' });  const writer2 = new N3.Writer({ format: 'application/trig' });  ```    ### From quads to an RDF stream    `N3.Writer` can also write quads to a Node.js stream.    ```JavaScript  const writer = new N3.Writer(process.stdout, { end: false, prefixes: { c: 'http://example.org/cartoons#' } });  writer.addQuad(    namedNode('http://example.org/cartoons#Tom'),    namedNode('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),    namedNode('http://example.org/cartoons#Cat')  );  writer.addQuad(quad(    namedNode('http://example.org/cartoons#Tom'),    namedNode('http://example.org/cartoons#name'),    literal('Tom')  ));  writer.end();  ```    ### From a quad stream to an RDF stream    `N3.StreamWriter` is a [Node.js stream](http://nodejs.org/api/stream.html) and [RDF.js Sink](http://rdf.js.org/#sink-interface) implementation.    ```JavaScript  const streamParser = new N3.StreamParser(),        inputStream = fs.createReadStream('cartoons.ttl'),        streamWriter = new N3.StreamWriter({ prefixes: { c: 'http://example.org/cartoons#' } });  inputStream.pipe(streamParser);  streamParser.pipe(streamWriter);  streamWriter.pipe(process.stdout);  ```    ### Blank nodes and lists  You might want to use the `[…]` and list `(…)` notations of Turtle and TriG.  However, a streaming writer cannot create these automatically:  the shorthand notations are only possible if blank nodes or list heads are not used later on,  which can only be determined conclusively at the end of the stream.    The `blank` and `list` functions allow you to create them manually instead:  ```JavaScript  const writer = new N3.Writer({ prefixes: { c: 'http://example.org/cartoons#',                                         foaf: 'http://xmlns.com/foaf/0.1/' } });  writer.addQuad(    writer.blank(      namedNode('http://xmlns.com/foaf/0.1/givenName'),      literal('Tom', 'en')),    namedNode('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),    namedNode('http://example.org/cartoons#Cat')  );  writer.addQuad(quad(    namedNode('http://example.org/cartoons#Jerry'),    namedNode('http://xmlns.com/foaf/0.1/knows'),    writer.blank([{      predicate: namedNode('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),      object:    namedNode('http://example.org/cartoons#Cat'),    },{      predicate: namedNode('http://xmlns.com/foaf/0.1/givenName'),      object:    literal('Tom', 'en'),    }])  ));  writer.addQuad(    namedNode('http://example.org/cartoons#Mammy'),    namedNode('http://example.org/cartoons#hasPets'),    writer.list([      namedNode('http://example.org/cartoons#Tom'),      namedNode('http://example.org/cartoons#Jerry'),    ])  );  writer.end((error, result) => console.log(result));  ```    ## Storing    `N3.Store` allows you to store triples in memory and find them fast.    In this example, we create a new store and add the triples `:Pluto a :Dog.` and `:Mickey a :Mouse`.  <br>  Then, we find triples with `:Mickey` as subject.    ```JavaScript  const store = new N3.Store();  store.addQuad(    namedNode('http://ex.org/Pluto'),    namedNode('http://ex.org/type'),    namedNode('http://ex.org/Dog')  );  store.addQuad(    namedNode('http://ex.org/Mickey'),    namedNode('http://ex.org/type'),    namedNode('http://ex.org/Mouse')  );    // Retrieve all quads  for (const quad of store)    console.log(quad);  // Retrieve Mickey's quads  for (const quad of store.readQuads(namedNode('http://ex.org/Mickey'), null, null))    console.log(quad);  ```    ### Addition and deletion of quads  The store provides the following manipulation methods  ([documentation](http://rdfjs.github.io/N3.js/docs/N3Store.html)):  - `addQuad` to insert one quad  - `addQuads` to insert an array of quads  - `removeQuad` to remove one quad  - `removeQuads` to remove an array of quads  - `remove` to remove a stream of quads  - `removeMatches` to remove all quads matching the given pattern  - `deleteGraph` to remove all quads with the given graph  - `createBlankNode` returns an unused blank node identifier    ### Searching quads or entities  The store provides the following search methods  ([documentation](http://rdfjs.github.io/N3.js/docs/N3Store.html)):  - `readQuads` returns a generator of quads matching the given pattern  - `getQuads` returns an array of quads matching the given pattern  - `match` returns a stream of quads matching the given pattern  - `countQuads` counts the number of quads matching the given pattern  - `forEach` executes a callback on all matching quads  - `every` returns whether a callback on matching quads always returns true  - `some`  returns whether a callback on matching quads returns true at least once  - `getSubjects` returns an array of unique subjects occurring in matching quads  - `forSubjects` executes a callback on unique subjects occurring in matching quads  - `getPredicates` returns an array of unique predicates occurring in matching quad  - `forPredicates` executes a callback on unique predicates occurring in matching quads  - `getObjects` returns an array of unique objects occurring in matching quad  - `forObjects` executes a callback on unique objects occurring in matching quads  - `getGraphs` returns an array of unique graphs occurring in matching quad  - `forGraphs` executes a callback on unique graphs occurring in matching quads    ## Compatibility  ### Format specifications  The N3.js parser and writer is fully compatible with the following W3C specifications:  - [RDF 1.1 Turtle](https://www.w3.org/TR/turtle/)    – [EARL report](https://raw.githubusercontent.com/rdfjs/N3.js/earl/n3js-earl-report-turtle.ttl)  - [RDF 1.1 TriG](https://www.w3.org/TR/trig/)    – [EARL report](https://raw.githubusercontent.com/rdfjs/N3.js/earl/n3js-earl-report-trig.ttl)  - [RDF 1.1 N-Triples](https://www.w3.org/TR/n-triples/)    – [EARL report](https://raw.githubusercontent.com/rdfjs/N3.js/earl/n3js-earl-report-ntriples.ttl)  - [RDF 1.1 N-Quads](https://www.w3.org/TR/n-quads/)    – [EARL report](https://raw.githubusercontent.com/rdfjs/N3.js/earl/n3js-earl-report-nquads.ttl)    In addition, the N3.js parser also supports [Notation3 (N3)](https://www.w3.org/TeamSubmission/n3/) (no official specification yet).    The N3.js parser and writer are also fully compatible with the RDF* variants  of the W3C specifications.    The default mode is permissive  and allows a mixture of different syntaxes, including RDF*.  Pass a `format` option to the constructor with the name or MIME type of a format  for strict, fault-intolerant behavior.  If a format string contains `star` or `*`  (e.g., `turtlestar` or `TriG*`),  RDF* support for that format will be enabled.    ### Interface specifications  The N3.js submodules are compatible with the following [RDF.js](http://rdf.js.org) interfaces:    - `N3.DataFactory` implements    [`DataFactory`](http://rdf.js.org/data-model-spec/#datafactory-interface)    - the terms it creates implement [`Term`](http://rdf.js.org/data-model-spec/#term-interface)      and one of      [`NamedNode`](http://rdf.js.org/data-model-spec/#namednode-interface),      [`BlankNode`](http://rdf.js.org/data-model-spec/#blanknode-interface),      [`Literal`](http://rdf.js.org/data-model-spec/#literal-interface),      [`Variable`](http://rdf.js.org/data-model-spec/#variable-interface),      [`DefaultGraph`](http://rdf.js.org/data-model-spec/#defaultgraph-interface)    - the triples/quads it creates implement      [`Term`](http://rdf.js.org/data-model-spec/#term-interface),      [`Triple`](http://rdf.js.org/data-model-spec/#triple-interface)      and      [`Quad`](http://rdf.js.org/data-model-spec/#quad-interface)  - `N3.StreamParser` implements    [`Stream`](http://rdf.js.org/stream-spec/#stream-interface)    and    [`Sink`](http://rdf.js.org/stream-spec/#sink-interface)  - `N3.StreamWriter` implements    [`Stream`](http://rdf.js.org/stream-spec/#stream-interface)    and    [`Sink`](http://rdf.js.org/stream-spec/#sink-interface)  - `N3.Store` implements    [`Store`](http://rdf.js.org/stream-spec/#store-interface)    [`Source`](http://rdf.js.org/stream-spec/#source-interface)    [`Sink`](http://rdf.js.org/stream-spec/#sink-interface)    ## License and contributions  The N3.js library is copyrighted by [Ruben Verborgh](https://ruben.verborgh.org/)  and released under the [MIT License](https://github.com/rdfjs/N3.js/blob/master/LICENSE.md).    Contributions are welcome, and bug reports or pull requests are always helpful.  If you plan to implement a larger feature, it's best to contact me first. """
Semantic web;https://github.com/totogo/awesome-knowledge-graph;"""# Awesome Knowledge Graph [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)    > A curated list of Knowledge Graph related learning materials, databases, tools and other resources    ## Contents    * [Infrastructure](#infrastructure)    * [Graph Databases](#graph-databases)    * [Triple Stores](#triple-stores)     * [Graph Computing Frameworks](#graph-computing-frameworks)    * [Graph Visualization](#graph-visualization)    * [Languages](#languages)    * [Managed Hosting Services](#managed-hosting-services)  * [Knowledge Engineering](#knowledge-engineering)    * [Knowledge Fusion](#knowledge-fusion)  * [Knowledge Graph Dataset](#knowledge-graph-dataset)    * [General](#general)    * [Semantic Network](#semantic-network)    * [Academic](#academic)  * [Learning Materials](#learning-materials)    * [Official Documentations](#official-documentations)    * [Community Effort](#community-effort)    ## Infrastructure    ### Graph Databases    * [AgensGraph](https://bitnine.net/agensgraph/) - multi-model graph database with SQL and Cypher support based on PostgreSQL  * [ArangoDB](https://www.arangodb.com/) - highly available Multi-Model NoSQL database  * [Atomic-Server](https://crates.io/crates/atomic-server/) - open-source type-safe graph database server with GUI, written in rust. Supports [Atomic Data](docs.atomicdata.dev/), JSON & RDF.  * [Blazegraph](https://github.com/blazegraph/database) - GPU accelerated graph database  * [Cayley](https://github.com/cayleygraph/cayley) - open source database written in Go  * [CosmosDB](https://docs.microsoft.com/en-us/azure/cosmos-db/graph-introduction) - cloud-based multi-model database with support for TinkerPop3  * [Dgraph](https://dgraph.io) - Fast, Transactional, Distributed Graph Database (open source, written in Go)  * [DSE Graph](https://www.datastax.com/products/datastax-enterprise-graph) - Graph layer on top of DataStax Enterprise (Cassandra, SolR, Spark)  * [Grakn.AI](https://grakn.ai/) - a distributed hyper-relational database for knowledge-oriented systems, i.e. a distributed knowledge base  * [Graphd](https://github.com/google/graphd) - the Metaweb/Freebase Graph Repository  * [JanusGraph](http://janusgraph.org) - an open-source, distributed graph database with pluggable storage and indexing backends  * [Memgraph](https://memgraph.com/) - High Performance, In-Memory, Transactional Graph Database  * [Neo4j](http://tinkerpop.apache.org/docs/currentg/#neo4j-gremlin) - OLTP graph database  * [Sparksee](http://www.sparsity-technologies.com/#sparksee) - makes space and performance compatible with a small footprint and a fast analysis of large networks  * [Stardog](http://stardog.com/) - RDF graph database with OLTP and OLAP support  * [OrientDB](http://orientdb.com/orientdb/) - Distributed Multi-Model NoSQL Database with a Graph Database Engine  * [TigerGraph](https://www.tigergraph.com) - a complete, distributed, parallel graph computing platform for enterprise, supporting web-scale data analytics in real-time.   * [Nebula Graph](https://nebula-graph.io/) - A truly distributed, linear scalable, lightning-fast graph database, using SQL-like query language.   * [HugeGraph](https://github.com/hugegraph/hugegraph) - An open source TinkerPop 3 compliant OLTP Graph Database with pluggable storage bakcend which is similar to JanusGraph. It also supports OLAP through Spark GraphX.   * [Diffbot](https://diffbot.com/products/knowledge-graph) - One of three Western entities to crawl a majority of the web. Largest commercially available knowledge graph.   * [Weaver](https://www.weaverhq.com/) - A graph database built on top of Postgres, which allows you to query the dataset in both SQL and graph query languages including SQL, SPARQL, and GraphQL.    ### Triple Stores    * [AllegroGraph](https://franz.com/agraph/allegrograph/) - high-performance, persistent graph database that scales to billions of quads  * [Apache Jena](https://jena.apache.org/) - open source Java framework for building Semantic Web and Linked Data applications  * [Eclipse RDF4J](http://rdf4j.org/) - (formerly known as Sesame) is an open source Java framework for processing RDF data. This includes parsing, storing, inferencing and querying of/over such data. It offers an easy-to-use API that can be connected to all leading RDF storage solutions. It allows you to connect with SPARQL endpoints and create applications that leverage the power of linked data and Semantic Web.  * [GraphDB](http://graphdb.ontotext.com/graphdb/) - enterprise ready Semantic Graph Database, compliant with W3C Standards  * [Virtuoso](https://virtuoso.openlinksw.com/) - a ""Data Junction Box"" that drives enterprise and individual agility by deriving a Semantic Web of Linked Data from existing data silos  * [Hoply](https://github.com/amirouche/hoply/) - explore bigger than RAM relational data in the comfort of Python.  * [Apache Marmotta](https://marmotta.apache.org/) - (retired Apache project) an open platform for linked data.    ### Graph Computing Frameworks    * [Apache Giraph](https://giraph.apache.org/) - an iterative graph processing system built for high scalability  * [Apache TinkerPop](https://tinkerpop.apache.org/) - a graph computing framework for both graph databases (OLTP) and graph analytic systems (OLAP)  * [Apache Spark - GraphX](https://spark.apache.org/graphx/) - Apache Spark's API for graphs and graph-parallel computation  * [Tencent Plato](https://github.com/tencent/plato) - a fast distributed graph computation and machine learning framework used by WeChat.     ### Graph Visualization    * [AntV G6](https://github.com/antvis/g6) - Simple, easy and complete high performance graph visualization engine written in JavaScript, from Ant Financial  * [Gephi](https://gephi.org/) - Graph visualization platform software runs on Windows, Mac and Linux.  * [KeyLines & ReGraph](https://cambridge-intelligence.com/) - Graph visualization tookits for JavaScript and React developer from Cambridge Intelligence.  * [Linkurious](https://linkurio.us) - Linkurious is an enterprise ready on-premises graph visualization and analysis platform.  * [Cytoscape](https://cytoscape.org/) - Open source graph visualization platform software runs on Windows, Mac and Linux.  * [Cytoscape.js](https://js.cytoscape.org/) - Graph visualization tookit for JavaScript.  * [Sigma.js](https://www.sigmajs.org/) - JavaScript library aimed at visualizing larger graphs.     ### Languages    * [Cypher](http://www.opencypher.org/)  * [Gremlin](https://tinkerpop.apache.org/gremlin.html)  * [SPARQL](https://en.wikipedia.org/wiki/SPARQL)  * [GraphQL+-](https://docs.dgraph.io/query-language/) - The query language of Dgraph, which is based on Facebook's GraphQL  * [GQL](https://gql.today/) - An initiative to create a standard query language for property graph database, just like SQL for relational database.     ### Managed Hosting Services    * [CosmosDB @ Microsoft](https://docs.microsoft.com/en-us/azure/cosmos-db/graph-introduction) - Azure Cosmos DB is Microsoft's globally distributed, multi-model (Key-value, Document, Column, Graph) database service.    * [JanusGraph @ IBM Compose](https://www.compose.com/databases/janusgraph)  * [JanusGraph @ Google Cloud Platform](https://cloud.google.com/solutions/running-janusgraph-with-bigtable) - JanusGraph on Google Kubernetes Engine backed by Google Cloud Bigtable  * [JanusGraph @ Amazon Web Services Labs](https://github.com/awslabs/dynamodb-janusgraph-storage-backend) - The Amazon DynamoDB Storage Backend for JanusGraph  * [Neo4j @ Graphene](https://www.graphenedb.com/)  * [Neo4j @ Graph Story](https://www.graphstory.com/) - End-to-end Graph Database hosting for Community and Enterprise Neo4j with expert help for development  * [Neptune @ Amazon Web Services](https://aws.amazon.com/neptune/) - a fast, reliable, fully-managed graph database service that makes it easy to build and run applications that work with highly connected datasets  * [Graph Engine Service @ Huawei Cloud](https://www.huaweicloud.com/en-us/product/ges.html) - Fully-managed, distributed, at-scale graph query and analysis service that provides a visualized interactive analytics platform.  * [Graph Database (beta) @ Aliyun (Alibaba Cloud)](https://www.aliyun.com/product/gdb) - highly reliable and available property graph database that supports ACID and TinkerPop Gremlin query language.   * [Tencent Knowledge Graph @ Tencent Cloud](https://cloud.tencent.com/product/tkg) - One stop platform for Graph database, computing and visualization. Currently available in beta test and only in Chinese.   * [WoordLift](https://wordlift.io/) - Easy-to-use SEO-focused Graph Database hosting for web and e-commerce websites running on Apache Marmotta.   * [Baidu Knowledge Graph @ Baidu AI Platform](https://ai.baidu.com/solution/kgaas) - One-stop AI platform to build knowledge graph and its applications.     ## Knowledge Engineering    * [YAGA-NAGA](https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/) - Harvesting, Searching, and Ranking Knowledge from the Web    ### Knowledge Fusion    * [Dedupe](https://github.com/dedupeio/dedupe) - dedupe is a python library that uses machine learning to perform fuzzy matching, deduplication and entity resolution quickly on structured data.  * [LIMES](https://github.com/dice-group/LIMES) - Link Discovery Framework for Metric Spaces.    ## Knowledge Graph Dataset    ### General    * [BabelNet](https://babelnet.org/) - Both a **multilingual encyclopedic dictionary**, with lexicographic and encyclopedic coverage of terms, and a **semantic network** which connects concepts and named entities in a very large network of semantic relations, made up of about 16 million entries, called Babel synsets. Each Babel synset represents a given meaning and contains all the synonyms which express that meaning in a range of different languages.  * [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) - Wikidata is a free, collaborative, multilingual, secondary database, collecting structured data to provide support for Wikipedia, Wikimedia Commons, the other wikis of the Wikimedia movement, and to anyone in the world.  * [Google Knowledge Graph](https://developers.google.com/knowledge-graph/) - Google’s Knowledge Graph has millions of entries that describe real-world entities like people, places, and things.  * [Freebase](https://developers.google.com/freebase/) - Large scale knowledge base originally stated by Metaweb. Later aquired by Google and used in [Google Knowledge Graph](https://blog.google/products/search/introducing-knowledge-graph-things-not/).  * [DBpedia](https://wiki.dbpedia.org/) - DBpedia is a crowd-sourced community effort to extract structured content from the information created in various Wikimedia projects.  * [XLore](https://xlore.org/) - A large-scale English-Chinese bilingual knowledge graph by structuring and integrating Chinese Wikipedia, English Wikipedia, French Wikipedia, and Baidu Baike.   * [The GDELT Project](https://www.gdeltproject.org/) - The GDELT Project monitors the world's broadcast, print, and web news from nearly every corner of every country in over 100 languages and identifies the people, locations, organizations, themes, sources, emotions, counts, quotes, images and events driving our global society  every second of every day, creating a free open platform for computing on the entire world.  * [YAGO](http://yago-knowledge.org/) - A huge semantic knowledge base, derived from [Wikipedia](http://en.wikipedia.org/),  [WordNet](http://wordnet.princeton.edu/) and [GeoNames](http://www.geonames.org/). Currently, YAGO has knowledge of more than 10 million entities (like persons, organizations, cities, etc.) and contains more than 120 million facts about these entities. The source code of YAGO is in this Github [repo](https://github.com/yago-naga/yago3).   * [Zhishi.me](http://zhishi.me/) - Knowledge Graph data extracted from the largest Chinese encyclopedias,  [Baidu Baike](https://baike.baidu.com/), [Hudong Baike](https://www.baike.com/) and [Chinese Wikipedia](https://zh.wikipedia.org/).  * [NELL](http://rtw.ml.cmu.edu/rtw/) - Never-Ending Language Learner, read the web and extract facts from text found in web pages continuously and improve itself.    ### Semantic Network    * [ConceptNet](http://conceptnet.io/) - ConceptNet is a freely-available semantic network, designed to help computers understand the meanings of words that people use.  * [Microsoft Concept Graph](https://concept.research.microsoft.com/) - For Short Text Understanding  * [OpenHowNet](https://openhownet.thunlp.org) - An Open Sememe-based Lexical Knowledge Base in Chinese.  * [WordNet](http://wordnet.princeton.edu/) - A free large lexical database of English from Princeton University.    ### Academic & Research    * [AMiner](https://www.aminer.cn/) - Aminer aims to provide comprehensive search and mining services for researcher social networks.  * [Microsoft Academic](https://academic.microsoft.com/) - Microsoft Academic (MA) employs advances in machine learning, semantic inference and knowledge discovery to help you explore scholarly information in more powerful ways than ever before.  * [AceMap](https://www.acemap.info/) - Academic search engine based on knowledge graph which includes entities like paper, author, institution and etc.   * [Semantic Scholar](https://www.semanticscholar.org/) - A free, AI-powered research tool for scientific literature. Collaborating with academic publishers to build a trustworthy and authoritative scientific knowledge graph.    ### Other Domain    * [Lynx](https://lynx-project.eu/) - an ecosystem of smart cloud services to better manage compliance, based on a Legal Knowledge Graph (LKG) which integrates and links heterogeneous compliance data sources including legislation, case law, standards and other private contracts.  * [ResearchSpace](https://researchspace.org/) - A culture heritage knowledge graph from the British Museum.   * [Unified Medical Language System (UMLS)](https://www.nlm.nih.gov/research/umls/index.html) - The UMLS integrates and distributes key terminology, classification and coding standards, and associated resources to promote creation of more effective and interoperable biomedical information systems and services, including electronic health records.  * [DrugBank](https://go.drugbank.com/) - Knowledge base for drug interactions, pharmacology, chemical structures, targets, metabolism, and more.   * [STRING](https://string-db.org/) - A database of known and predicted protein-protein interactions.    ## Learning Materials    ### Official Documentations    * [Cypher](https://neo4j.com/developer/cypher-query-language/) - reference documentation  * [Gremlin](http://tinkerpop.apache.org/docs/current/reference/#traversal) - reference documentation    ### Community Effort    * [Graph Book](https://github.com/krlawrence/graph) - TinkerPop3 centric book written by [Kelvin R. Lawrence](https://twitter.com/gfxman)  * [SQL2Gremlin](http://sql2gremlin.com/) - transition from SQL to Gremlin by [Daniel Kuppitz](https://twitter.com/dkuppitz)  * [The Gremlin Compendium](http://www.doanduyhai.com/blog/?p=13460) - minimum survival kit for any Gremlin user, 10 blog post series by [Doan DuyHai](https://twitter.com/doanduyhai)    ## Conferences    * [Graph Connect](http://graphconnect.com/) - powered by Neo4j  * [Graph Day](http://graphday.com/) - an Independent Graph Conference from the Data Day folks  * [Connected Data London](https://connected-data.london/) - Connected Data London brings together 160+ Artificial Intelligence, Semantic Technology, Linked Data and Graph Database innovators, thought leaders and practitioners annually in one great conference. The conference has expanded its themes and tracks, from its roots as the primary conference for Knowledge Graphs, Linked Data and Semantics to include related Graph Database and AI / Machine Learning technologies and practical use cases.    ## Contribute    Contributions welcome! Read the [contribution guidelines](contributing.md) first.    Some of the content were copied from other awesome lists:      * [awesome-graph](https://github.com/jbmusso/awesome-graph) - Graph, the infrastructure for Knowledge Graph  * [awesome-knowledge-graph](https://github.com/husthuke/awesome-knowledge-graph) - Knowledge graph related materials but all in Chinese    ## License    [![CC0](http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](http://creativecommons.org/publicdomain/zero/1.0)    To the extent possible under law, Sitao Z. has waived all copyright and related or neighboring rights to this work. """
Semantic web;https://github.com/Merck/Halyard;"""# Halyard    [![CI](https://api.travis-ci.org/Merck/Halyard.svg?branch=master)](https://travis-ci.org/Merck/Halyard)  [![Coverage](https://codecov.io/github/Merck/Halyard/coverage.svg?branch=master)](https://codecov.io/gh/Merck/Halyard/)    Halyard is an extremely horizontally scalable triple store with support for named graphs, designed for integration of extremely large semantic data models and for storage and SPARQL 1.1 querying of complete Linked Data universe snapshots. Halyard implementation is based on Eclipse RDF4J framework and Apache HBase database, and it is completely written in Java.    **Author: [Adam Sotona](mailto:adam.sotona@merck.com)**    **Discussion group: <https://groups.google.com/d/forum/halyard-users>**    **Documentation: <https://merck.github.io/Halyard>**    ## Get started    [Download](https://github.com/Merck/Halyard/releases) and unzip the latest `halyard-sdk-<version>.zip` bundle to a Apache Hadoop cluster node with configured Apache HBase client.    Halyard is expected to run on an Apache Hadoop cluster node with configured Apache HBase client. Apache Hadoop and Apache HBase components are not bundled with Halyard. The runtime requirements are:     * Apache Hadoop version 2.5.1 or higher   * Apache HBase version 1.1.2 or higher   * Java 8 Runtime    *Note: Recommended Apache Hadoop distribution is the latest version of Hortonworks Data Platform (HDP) or Amazon Elastic Map Reduce (EMR).*    See [Documentation](https://merck.github.io/Halyard) for usage examples, architecture information, and more.    ## Repository contents     * `common` - a library for direct mapping between an RDF data model and Apache HBase   * `strategy` - a generic parallel asynchronous implementation of RDF4J Evaluation Strategy   * `sail` - an implementation of the RDF4J Storage and Inference Layer on top of Apache HBase   * `tools` - a set of command line and Apache Hadoop MapReduce tools for loading, updating, querying, and exporting the data with maximum performance   * `sdk` - a distributable bundle of Eclipse RDF4J and Halyard for command line use on an Apache Hadoop cluster with configured HBase   * `webapps` - a re-distribution of Eclipse RDF4J Web Applications (RDF4J-Server and RDF4J-Workbench), patched and enhanced to include Halyard as another RDF repository option """
Semantic web;https://github.com/stardog-union/pinto;"""# Pinto    ![Pinto](https://github.com/Complexible/pinto/raw/master/pinto.png)    Pinto is a Java framework for transforming Java beans into RDF (and back).    The current version is 2.0 as of 2016-06-14.  We're using [git flow](http://nvie.com/posts/a-successful-git-branching-model/)  for development.    Inspired by [Jackson](https://github.com/FasterXML/jackson) and [Empire](http://github.com/mhgrove/Empire), it aims to  be simple and easy to use.  No annotations or configuration are required.  If you have a compliant Java bean Pinto will  turn it into RDF.    The [rdf4j](http://rdf4j.com) framework is used to represent RDF primitives such as Graph and Statement.    ## Building    To create the artifacts:    ```bash  $ gradle jar  ```    And to run the tests:    ```bash  $ gradle test  ```    ## Example Usage    Given this simple Java Bean:    ```java  public static final class Person {      private String mName;        public Person() {      }        public Person(final String theName) {          mName = theName;      }        public String getName() {          return mName;      }        public void setName(final String theName) {          mName = theName;      }        @Override      public int hashCode() {          return Objects.hashCode(mName);      }        @Override      public boolean equals(final Object theObj) {          if (theObj == this) {              return true;          }          else if (theObj instanceof Person) {              return Objects.equal(mName, ((Person) theObj).mName);          }          else {              return false;          }      }  }  ```    You can easily serialize it as RDF:    ```java  Graph aGraph = RDFMapper.create().writeValue(new Person(""Michael Grove""));  ```    And `aGraph` serialized as NTriples:    ```  <tag:complexible:pinto:f97658c7048377a026111c7806bd7280> <tag:complexible:pinto:name> ""Michael Grove""^^<http://www.w3.org/2001/XMLSchema#string> .  ```    And if you have that RDF, you can turn it back into a `Person`:    ```java  final Person aPerson RDFMapper.create().readValue(aGraph, Person.class)  ```    This is the quick and dirty example, but for more detailed examples, check out the tests.    ## Annotations    Pinto does not require annotations to serialize Beans as RDF, but does support a few basic annotations so you can  control how the object is serialized.    ### `@RdfId`    An annotation which specifies the properties to be used for generating the URI of the object.  By default, a hash of  the object itself is used to generate a URI.  But when present on a getter or setter of one or more properties on the  bean, the values of those properties will be used as the seed for the URI.    Note: There is a secondary mechanism for controlling the URI of an object.  If the object implements `Identifiable` the  mapper will use the URI returned by `#id` ignoring any `@RdfId` annotated properties.    ### `@RdfProperty`    An annotation which can be applied to a property on a bean, either the getter or the setter, which specifies the  URI of the property when generating RDF for the bean.  Normally, a URI for the property is auto-generated, but when  this annotation is present, the URI specified in the annotation is used instead.  The value of the annotation can also  be a QName.    ### `@RdfsClass`    An annotation which can be applied to a class to specify the `rdf:type` of the class when generating the RDF.  Can be  a QName or a URI.  When not present, no `rdf:type` assertion is generated for the object.    ### `@Iri`    Annotation which can be used to control the URI assigned to an `Enum`.  Normally the URI's are generated by Pinto, but  if you want to map them to specific constants in your ontology, you can use `Iri` to explicitly identify the objects.    ```java  public enum TestEnum {      @Iri(""urn:my_ontology:test:Foo"")      Foo,        @Iri(""urn:my_ontology:test:Bar"")      Bar,  }  ```    ## Configuration    By default, `RDFMapper` does not require any configuration, it's meant to generate reasonable RDF out of the box.  There  are a couple (de)serialization options which are specified by `MappingOptions`:    * `REQUIRE_IDS` - By default, Pinto will auto-generate URIs for objects when `@RdfId` is not specified.  By setting this property to `true` the mapper will not auto-generate URIs, they must be specified explicitly. (default: `false`)  * `SERIALIZE_COLLECTIONS_AS_LISTS` - When true, collections are serialized as RDF lists.  Otherwise, they're serialized using `Collection#size` separate property assertions. (default: `false`)  * `IGNORE_INVALID_ANNOTATIONS` - Whether or not to ignore an annotation which is invalid, such as `@RdfProperty` which defines a property with an invalid URI.  Properties with invalid/ignored annotations are simply not used when generating a Bean or RDF. (default: `true`)    Beyond these configuration options, `RDFMapper` has a few other configuration mechanisms that can be specified on its  `Builder` when creating the mapper:    * `#map(URI, Class)` - Specify the provided type corresponds to instances of the given Java class.  Functions like the `@RdfsClass` annotation.  * `#namespace(...)` - Methods to specify namespace mappings which are used to expand any QNames used in the annotations  * `#valueFactory(ValueFactory)` - Provide the `ValueFactory` to be used when creating RDF from a bean  * `#collectionFactory(CollectionFactory)` - The factory to be used for creating instances of `java.util.Collection`.  Defaults to `DefaultCollectionFactory`  * `#mapFactory(MapFactory)` - The factory to be used for creating instances of `java.util.Map`.  Defaults to `DefaultMapFactory`    ## Custom serialization    In some cases, an object won't ahere to the Java Bean specification, or it's a third-party class that you don't control  so you cannot add annotations, but you need a specific serialization.  For these cases `RDFCodec` can be used.  It's  a small plugin to `RDFMapper` which will handle transforming a Java object to/from RDF.  Pinto includes an example  implementation of a codec for `java.util.UUID` called `UUIDCodec`.    Codecs are registered when the `RDFMapper` is created via its builder: `Builder.codec(Class<T>, RDFCodec<T>)`    ## Why Pinto?    Why create Pinto when there are similar frameworks available?  Well, the other frameworks, like  [Empire](http://github.com/mhgrove/Empire) or [Alibaba](https://bitbucket.org/openrdf/alibaba) are focused on more than  just transforming Beans into RDF and back. Neither are a good fit for _just_ round-tripping between beans and RDF.    A good example is if you're building a JAX-RS based web service and you have some bean in your domain that you'd like  to serialize as RDF, or accept as RDF, that's normally done with a custom implementation of  `MessageBodyReader`/`MessageBodyWriter`.  But that implementation is not straight-forward with the heavier-weight  frameworks.  With Pinto, it's a single line of code."""
Semantic web;https://github.com/hectorcorrea/ldpserver;"""This is a mini LDP Server in Go.    Linked Data Platform (LDP) is a W3C recommendation that defines rules for how to  implement an HTTP API for read-write Linked Data. The official recommendation can  be found [here](http://www.w3.org/TR/ldp/).  You can also find a more gentle introduction to LDP in  [my blog](http://hectorcorrea.com/blog/introduction-to-ldp/67).    *Warning*: This is my sandbox project as I learn both Go and LDP. The code in this repo very likely does not follow Go's best practices and it certainly does not conform to the LDP spec (yet).      ## Compile and run the server  If Go is installed on your machine:        cd ~/src      git clone git@github.com:hectorcorrea/ldpserver.git      cd ldpserver      go build      ./ldpserver    If you are new to Go follow these steps instead:        # Download and install Go from: http://golang.org/doc/install      #      # Go is very picky about the location of the code (e.g. the code must be      # inside an src folder.) Here is a setup that will work with minimal effort      # and configuration on your part. You can skip the first step if you      # already have an ~/src folder.      #      mkdir ~/src      export GOPATH=~/      cd ~/src      git clone git@github.com:hectorcorrea/ldpserver.git      cd ldpserver      go build      ./ldpserver    If you don't care about the source code, the fastest way to get started is to [download the executable for your platform](https://github.com/hectorcorrea/ldpserver/releases) from the release tab, make it an executable on your box, and run it.      ## Operations supported  With the server running, you can use `cURL` to submit requests to it. For example, to fetch the root node        curl localhost:9001    POST to the root (the Slug defaults to ""node"" + a sequential number)        curl -X POST localhost:9001    Fetch the node created        curl localhost:9001/node1    POST a non-RDF to the root        curl -X POST --header ""Content-Type: text/plain"" --data ""hello world"" localhost:9001        curl -X POST --header ""Content-Type: image/jpeg"" --data-binary ""@filename.jpg"" localhost:9001    Fetch the non-RDF created        curl localhost:9001/node2    HTTP HEAD operations are supported        curl -I localhost:9001/      curl -I localhost:9001/node1      curl -I localhost:9001/node2    Add an RDF source to add a child node (you can only add to RDF sources)        curl -X POST localhost:9001/node1    See that the child was added        curl localhost:9001/node1    Fetch the child        curl localhost:9001/node1/node3    Create a node with a custom Slug        curl -X POST --header ""Slug: demo"" localhost:9001    Fetch node created        curl localhost:9001/demo    Create an *LDP Direct Container* `/dc1` that uses `/node1` as its `membershipResource` and `someRel` as the member relation...        curl -X POST --header ""Content-Type: text/turtle"" --header ""Slug: dc1"" -d ""<> <http://www.w3.org/ns/ldp#hasMemberRelation> someRel ; <http://www.w3.org/ns/ldp#membershipResource> <http://localhost:9001/node1> ."" localhost:9001    ...add a node to the direct container        curl -X POST --header ""Slug: child1"" localhost:9001/dc1    ...fetch `/node1` and notice that it references `/dc1/child1` with the predicate `someRel` that we defined in the direct container:        curl localhost:9001/dc1    ## Demo  Take a look at `demo.sh` file for an example of a shell script that executes some of the operations supported. To run this demo make sure the LDP Server is running in a separate terminal window, for example:        # Run the LDP Server in one terminal window      ./ldpserver        # Run the demo script in a separate terminal window      ./demo.sh      ## Storage  Every resource (RDF or non-RDF) is saved in a folder inside the data folder.    Every RDF source is saved on its own folder with single file inside of it. This file is always `meta.rdf` and it has the triples of the node.    Non-RDF are also saved on their own folder and with a `meta.rdf` file for their metadata but also a file `data.bin` with the non-RDF content.    For example, if we have two nodes (blog1 and blog2) and blog1 is an RDF node and blog2 is a non-RDF then the data structure would look as follow:        /data/meta.rdf          (root node)      /data/blog1/meta.rdf    (RDF for blog1)      /data/blog2/meta.rdf    (RDF for blog2)      /data/blog2/data.bin    (binary for blog2)      ## Overview of the Code    * `main.go` is the launcher program. It's only job is to kick off the web server.  * `web/web.go` is the web server. It's job is to handle HTTP requests and responses. This is the only part of the code that is aware of the web.  * `server/server.go` handles most of the operations like creating new nodes and fetching existing ones.  * `ldp/node.go` handles operations at the individual node level (fetching and saving.)  * `rdf/` contains utilities to parse and update RDF triples and graphs.      ## TODO  A lot.    * Add validation to make sure the data in the root node matches the URL (host:port) where the server is running.    * Support isMemberOfRelation in Direct Containers.    * Support Indirect Containers.      ## LDP Test Suite  The W3C provides a test suite to make sure LDP server implementations meet a minimum criteria. The test suite can be found at http://w3c.github.io/ldp-testsuite/    In order to run the suite against this repo you need to do the following:      1. Clone the ldp-testsuite repo: `git clone https://github.com/w3c/ldp-testsuite`    1. Download maven from http://maven.apache.org/download.cgi    1. Unzip maven to the `ldp-testsuite` folder    1. `cd ldp-testsuite`    1. Run `bin/mvn package`    ...and then you can run the following command against the LDP Server to execute an individual test:        java -jar target/ldp-testsuite-0.2.0-SNAPSHOT-shaded.jar --server http://localhost:9001 --test name_of_test_goes_here --basic    ...or as follow to run all basic container tests (including support for non-RDF):        java -jar target/ldp-testsuite-0.2.0-SNAPSHOT-shaded.jar --server http://localhost:9001 --basic --non-rdf    As of 1/9/2016 these are the results of all basic container tests (including support for non-RDF):        LDP Test Suite      Total tests run: 112, Failures: 4, Skips: 28      TODO: Document how to test DC and the results 97/5/27 """
Semantic web;https://github.com/epimorphics/qonsole;"""# Qonsole - a simple console for running SPARQL queries    Suppose you have a linked-data dataset that you've published on the web, and that  you'd like to provide your users with the means to run SPARQL queries to explore   your data. Most SPARQL end-points allow you to use HTTP `get` and `post` commands  to send a query and get back results, so it's easy enough to put up an HTML form for   your users to type in a query and get back some results. But that's not necessarily  that helpful for your users, because:      * in some browsers, hitting the 'back' button after posting a query means that the    text input control is emptied. So your users will lose the query that they have typed,     which makes iterative development of the query hard.    * you can provide at most one sample query to get your users started with suggestions    and ideas.    * you don't have any to manage prefixes, which can make a huge difference to the    readability of a query (compare `<http://www.w3.org/2000/01/rdf-schema#comment>` to    `rdfs:comment`, for example).    * the returned values from a SPARQL form are generally represented as a text format, or    using a MIME type (e.g. `text/csv` that makes your user's computer open the returning file    in another program (e.g. a spreadsheet). Plain text can be hard to explore, while opening a    different application takes your user's focus away from the task of editing the query and    exploring your data.    See a [demo of qonsole](http://epimorphics.github.io/qonsole/demo-vertical.html),   using the [UK bathing water quality dataset](http://environment.data.gov.uk/bwq/).    Qonsole provides the following features:      * An edit control with SPARQL syntax highlighting, undo, and other familiar code editing     features (courtesy of [CodeMirror](http://codemirror.net/)).    * Display of the query and the return result on one page    * A configurable set of example queries your users can select with one click    * A configurable set of pre-declared prefixes, and the ability to add new prefixes dynamically,    including looking a well-know prefix up on [prefix.cc](http://prefix.cc). Users can add or    remove prefix declarations from the edit window with one click.    * A choice of return formats, including XML, JSON and plain text. XML and JSON results    display in a structured editor window for easier browsing. By default, results are returned    in a table control, with sortable columns, paging and search.    Qonsole is free open-source software under an   [Apache license](http://www.apache.org/licenses/LICENSE-2.0.html), and was developed by    [Epimorphics Ltd](http://www.epimorphics.com).    ## Adding qonsole to your page    Clone the git repository ([epimorphics/qonsole](http://github.com/epimorphics/qonsole)), or  download the files individually from GitHub. The `demo-` pages show a few variants on laying  out the elements of the page body that Qonsole needs, and the JavaScript and CSS files you'll  need to include.    Qonsole is configured by passing a JSON data structure to the `qonsole.init()` call. Currently,  this config object allows you to specify:    <dl>    <dt><code>elements</code> &ndash; available SPARQL end-points</dt>    <dd>The value of this config element is a JSON object, whose keys are short    references to available endpoints and whose values are URLs. One `default` end-point    should always be provided. The goal here is to allow different example queries potentially    to be run against different specific SPARQL endpoints. Example:  <pre>      endpoints: {        ""default"":  ""http://environment.data.gov.uk/sparql/bwq/query"",      }  </pre>    </dd>    <dt><code>prefixes</code> &ndash; shared list of pre-defined prefixes</dt>    <dd>The prefixes listed in this element will be added to each query, and may be    selected on or off by a single click. The value is a JSON object, whose keys are the    prefix short-name, and whose values are URIs. Example:    <pre>      prefixes: {        ""bw"":       ""http://environment.data.gov.uk/def/bathing-water/"",        ""bwq"":      ""http://environment.data.gov.uk/def/bathing-water-quality/"",        ""ef-sp"":    ""http://location.data.gov.uk/def/ef/SamplingPoint/"",        ""interval"": ""http://reference.data.gov.uk/def/intervals/"",        ""stats"":    ""http://statistics.data.gov.uk/def/administrative-geography/"",        ""sr"":       ""http://data.ordnancesurvey.co.uk/ontology/spatialrelations/"",        ""rdf"":      ""http://www.w3.org/1999/02/22-rdf-syntax-ns#"",        ""rdfs"":     ""http://www.w3.org/2000/01/rdf-schema#"",        ""owl"":      ""http://www.w3.org/2002/07/owl#"",        ""xsd"":      ""http://www.w3.org/2001/XMLSchema#""      },  </pre>    </dd>    <dt><code>queries</code> &ndash; pre-defined example queries</dt>    <dd>    This element defines the example queries that users can select run, or    to base their own queries on. The value is a JSON array, each element of    which is one example query. Note that prefixes do not need to be    declared in the example queries. The query text can be declared in the config    structure itself, with the `query` key, or accessed remotely from a different URL    using the `queryURL` key:  <pre>      queries: [        { ""name"": ""Properties of a named bathing water"",          ""query"": ""select ?predicate ?object\nwhere {\n"" +                   ""  ?bw rdfs:label \""Spittal\""@en ;\n"" +                   ""      ?predicate ?object\n}""        },        { ""name"": ""all OWL classes"",          ""query"": ""select ?class ?label ?description\nwhere {\n"" +                   ""  ?class a owl:Class.\n"" +                   ""  optional { ?class rdfs:label ?label}\n"" +                   ""  optional { ?class rdfs:comment ?description}\n}""        },        { ""name"": ""Properties of a named bathing water"",          ""queryURL"": ""list-properties.rq""        }      ]  </pre>    By default, each query gets all of the shared prefixes declared in the configuration  object (see above). However this isn't always necessary. Simpler example queries may be easier  to read if the prefixes shown are only those actually in use. There are two mechanisms  you can use to control the prefixes used when displaying a query. Firstly, if the query  itself includes <code>prefix</code> declarations, then only those prefixes will be shown. Alternatively,  you can list the prefix keys that should be used with the query with a <code>prefixes</code> key  in the configuration object:    <pre>    queries: [      { ""name"": ""Properties of a named bathing water"",        ""query"": ""select ?predicate ?object\nwhere {\n"" +                 ""  ?bw rdfs:label \""Spittal\""@en ;\n"" +                 ""      ?predicate ?object\n}"",        ""prefixes"": [rdfs"", ""bw""]      }]  </pre>    </dd>  </dl>   """
Semantic web;https://github.com/klaussinani/awesome-prolog;"""# Awesome Prolog [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)    > A curated list of open-source Prolog frameworks, libraries and resources.    ## Contents    - [API Interfaces](#api-interfaces)  - [Artificial Intelligence](#artificial-intelligence)  - [Build Systems](#build-systems)  - [Compilers](#compilers)  - [Database](#database)  - [Date](#date)  - [Development](#development)  - [IDE](#ide)  - [Interpreters](#interpreters)  - [JSON](#json)  - [Logging](#logging)  - [Machine Learning](#machine-learning)  - [Math](#math)  - [Miscellaneous](#miscellaneous)  - [Native](#native)  - [Object Oriented Programming](#object-oriented-programming)  - [Parsing](#parsing)  - [Regular Expressions](#regular-expressions)  - [REST Frameworks](#rest-frameworks)  - [Server](#server)  - [Testing](#testing)  - [Text Editor Extensions](#text-editor-extensions)  - [Utilities](#utilities)  - [Resources](#resources)  	- [Tutorials](#tutorials)  	- [Videos](#videos)  	- [Free Courses](#free-courses)  	- [Books](#books)  	- [Community](#community)  - [Contributing](#contributing)  - [License](#license)    ## API interfaces    - [twitter_pack](https://github.com/samwalrus/twitter_pack) - Twitter API interface.  - [amazon_api](http://packs.ndrix.com/amazon_api/index.html) - Amazon API interface.  - [blog_core](https://blog-core.net) - Blog/CMS framework.  - [irc_client](https://github.com/eazar001/irc_client) - Low-level IRC interface.  - [geoip](https://github.com/JanWielemaker/geoip) - GeoIP interface.  - [pl_omdb](https://github.com/eazar001/pl_omdb) - OMDB API interface.    ## Artificial Intelligence    - [zamia-ai](https://github.com/gooofy/zamia-ai) - Open source AI system.    ## Build Systems    - [biomake](https://github.com/evoldoers/biomake) - GNU-Make-like building utility.    ## Compilers    - [SWI-Prolog](http://www.swi-prolog.org) - Widely used, comprehensive Prolog compiler.  - [GNU Prolog](http://www.gprolog.org) - ISO Prolog compiler with useful extensions.  - [YAP](https://github.com/vscosta/yap-6.3/) - High-performance Prolog compiler.  - [Ciao](https://github.com/ciao-lang/ciao) - A Prolog system with many extensions.  - [Eclipse CLP](https://eclipseclp.org/) - A constraint logic programming system.  - [XSB](http://xsb.sourceforge.net/) - XSB logic programming and deductive database system.    ## Database    - [prolongo](https://github.com/khueue/prolongo) - MongoDB driver.  - [docstore](https://github.com/rla/docstore) - Document-oriented database.  - [db_facts](http://stoics.org.uk/~nicos/sware/db_facts) - Centric abstraction layer for SQL.  - [bio_db](http://stoics.org.uk/~nicos/sware/bio_db) - Biological datasets manager.  - [prosqlite](http://stoics.org.uk/~nicos/sware/prosqlite) - SQLite interface.  - [TerminuDB](https://github.com/terminusdb/terminusdb) - for knowledge graph representation     ## Date    - [julian](https://github.com/mndrix/julian) - Date-time library.    ## Development    - [typedef](http://www.swi-prolog.org/pack/list?p=typedef) - Type definitions.  - [lambda](http://www.swi-prolog.org/pack/list?p=lambda) - Lambda expressions.  - [func](https://github.com/mndrix/func/) - Function application and composition.  - [condition](https://github.com/mndrix/condition) - Common Lisp-like condition system.  - [function_expansion](https://github.com/mndrix/function_expansion) - Function-like macros.  - [fsyntax](http://ciao-lang.org/ciao/build/doc/ciao.html/fsyntax_doc.html) - Ciao functional syntax.    ## IDE    - [swish](https://github.com/SWI-Prolog/swish) - SWI-Prolog web IDE.  - [Prolog Development Tool](http://sewiki.iai.uni-bonn.de/research/pdt/docs/start) - Prolog IDE for Eclipse.  - [VSC-Prolog](https://github.com/arthwang/vsc-prolog) - Prolog extension for Visual Studio Code.    ## Interpreters    - [golog](https://github.com/mndrix/golog) - Interpreter in Go.  - [erlog](https://github.com/rvirding/erlog) - Interpreter in Erlang.  - [chalk](https://github.com/rust-lang-nursery/chalk) - Interpreter in Rust.  - [jiprolog](https://github.com/jiprolog/jiprolog) - Interpreter in Java.  - [prolog.js](https://github.com/jldupont/prolog.js) - Interpreter in JavaScript.  - [prolog](https://github.com/Erdwolf/prolog) - Interpreter in Haskell.  - [ELPI](https://github.com/LPCIC/elpi) - Embeddable λProlog Interpreter in OCaml.  - [scryer prolog](https://github.com/mthom/scryer-prolog) - ISO-compatible interpreter in Rust.    ## JSON    - [djson](http://packs.ndrix.com/djson/index.html) - Declarative JSON.    ## Logging    - [log4p](https://github.com/hargettp/log4p) - Logging library.  - [httplog](https://github.com/JanWielemaker/httplog) - HTTP logfiles.    ## Machine Learning    - [mlu](http://stoics.org.uk/~nicos/sware/mlu) - Probabilistic logic programs.  - [cplint](https://github.com/friguzzi/cplint) -  Reasoning suite for probabilistic logic programs.  - [cplint_datasets](https://github.com/friguzzi/cplint_datasets) - Dataset for machine learning.    ## Math    - [matrix](https://github.com/friguzzi/matrix) - Matrix operations.  - [nan_numerics_prime](https://github.com/jp-diegidio/Nan.Numerics.Prime-Prolog) - Prime numbers library.  - [pljulia](https://github.com/samer--/pljulia) - Numerical and technical computing.    ## Miscellaneous    - [Online compiler](https://www.tutorialspoint.com/execute_prolog_online.php) - Execute Prolog online.  - [turing](https://bitbucket.org/ttmrichter/turing/src) - Turing Machine simulator.  - [yesbot](https://github.com/eazar001/yesbot) - IRC Bot.  - [webconsole](http://www.swi-prolog.org/pack/list?p=webconsole) - Browser as HTML console.  - [prologmud](https://github.com/TeamSPoon/prologmud_samples) - Online text adventure game.    ## Native    - [by_unix](http://stoics.org.uk/~nicos/sware/by_unix) - Unix primitives library.  - [inotify](https://github.com/JanWielemaker/inotify) - Linux notification library.  - [plmidi](https://github.com/samer--/plmidi) - Macos MIDI interface.  - [plgi](http://www.swi-prolog.org/pack/list?p=plgi) - Gnome bindings.    ## Object Oriented Programming    - [logtalk](http://www.swi-prolog.org/pack/list?p=logtalk) - Object oriented logic programming.    ## Parsing    - [atom_feed](http://packs.ndrix.com/atom_feed/index.html) - Atom and RSS feeds parser.  - [tokenize](https://github.com/aBathologist/tokenize) - Straightforward text tokenizing.  - [markdown](https://github.com/rla/prolog-markdown) - Markdown parser.  - [yaml](http://www.swi-prolog.org/pack/list?p=yaml) - YAML parser.    ## Regular Expressions    - [regex](https://github.com/mndrix/regex) - Regular expressions library.    ## REST Frameworks    - [arouter](https://github.com/rla/alternative-router) - HTTP router/dispatcher.    ## Server    - [httppl](https://github.com/jamesbvaughan/httppl) - Simple HTTP server.  - [letswicrypt](https://github.com/triska/letswicrypt) - HTTPS server.    ## Testing    - [tap](https://github.com/mndrix/tap) - Automated TAP testing.  - [quickcheck](https://github.com/mndrix/quickcheck) - QuickCheck randomized testing.  - [PlUnit](http://www.swi-prolog.org/pldoc/package/plunit.html) - Prolog unit tests.    ## Text Editor Extensions    - [prolog.vim](https://github.com/adimit/prolog.vim) - Prolog integration for Vim.  - [ediprolog](https://github.com/triska/ediprolog) - Prolog integration for Emacs.  - [sublimeprolog](https://github.com/alnkpa/sublimeprolog) - Prolog integration    for Sublime Text.  - [repl.vim](https://github.com/ujihisa/repl.vim) - VIM REPL with support for SWIProlog.  - [Ciao mode for Emacs](https://github.com/ciao-lang/ciao_emacs) - Emacs mode for Ciao and Prolog.    ## Utilities    - [pac](http://www.swi-prolog.org/pack/list?p=pac) - Anonymous predicates expansion utility.  - [list_util](https://github.com/mndrix/list_util) - List utility predicates.  - [gvterm](http://www.swi-prolog.org/pack/list?p=gvterm) - Reveal terms using Graphviz.  - [odf_sheet](http://www.swi-prolog.org/pack/list?p=odf_sheet) - Load and analyze ODF spreadsheets.  - [graphml](http://www.swi-prolog.org/pack/list?p=graphml) - Create GraphML files.  - [fileutils](http://www.swi-prolog.org/pack/list?p=fileutils) - File manipulation library.  - [dia](http://www.swi-prolog.org/pack/list?p=dia) - UML loading predicates.  - [cplint_r](https://github.com/frnmst/cplint_r) - R plotting predicates.  - [tuProlog](http://apice.unibo.it/xwiki/bin/view/Tuprolog/WebHome) - Light-weight system for distributed applications and infrastructures.    ## Resources    ### Tutorials    #### Beginner    - [Learn Prolog Now!](http://learnprolognow.org/lpnpage.php?pageid=online) - Thorough introductory course to programming in Prolog.  - [P-99: Ninety-Nine Prolog Problems](https://sites.google.com/site/prologsite/prolog-problems) - Practice problems in Prolog and Logic programming.  - [The Power of Prolog](https://www.metalevel.at/prolog) - Introduction to modern Prolog.  - [Simply Logical](http://book.simply-logical.space) - Intelligent reasoning by example.  - [Prolog Wikibook](https://en.wikibooks.org/wiki/Prolog) - Thorough Prolog overview.    #### Advanced    - [Prolog for Software Engineering](https://www.cs.auckland.ac.nz/~j-hamer/07.363/prolog-for-se.html) - Software engineering fundamentals using Prolog.  - [Prolog Modules](http://chiselapp.com/user/ttmrichter/repository/gng/doc/trunk/output/tutorials/swiplmodtut.html) - Fundamentals in SWI-Prolog Modules.  - [Web Applications in SWI-Prolog](http://www.pathwayslms.com/swipltuts/html) - Building web application with SWI-Prolog.  - [Constraint Logic Programming over Finite Domains](https://github.com/triska/clpz) - Fundamentals of Constraint Systems on Finite Domains.  - [Printing Messages in SWI-Prolog](http://www.pathwayslms.com/swipltuts/message/index.html) - IO handling in libraries.    ### Videos    - [Prolog Tutorial](https://www.youtube.com/watch?v=SykxWpFwMGs) - General hour-long video tutorial.  - [Introduction to Prolog](https://www.youtube.com/watch?v=GHLfeGN5OMk) - Hour-long introduction to Prolog.  - [Programming In Prolog](https://youtu.be/gJOZZvYijqk) - Four-part video introduction.    ### Free Courses  - [Prolog Online Training](https://www.youtube.com/playlist?list=PLWPirh4EWFpFLjsd3IUqymnCVBaz1Yagg) - Tutorials Point's series on Prolog.  - [Prolog in Artificial Intelligence](https://www.youtube.com/playlist?list=PLWPirh4EWFpEYxjEJyDoqplBhJF91Mwkp) - Tutorials Point's series on Prolog with lectures relating Prolog to AI.    ### Books    - [Adventure in Prolog](http://www.amzi.com/AdventureInProlog/apreface.php) - Pragmatic approach to Prolog.  - [Prolog Programming in Depth](http://www.covingtoninnovations.com/books.html#ppid) - Full coverage of the Prolog programming language.  - [Logic, Programming and Prolog](http://www.j-paine.org/prolog/mathnotes/files/pms/node1.html) - Foundations of Logic programming and programming techniques in Prolog.  - [Natural Language Processing for Prolog Programmers](http://www.covingtoninnovations.com/books.html#nlp) - Turning theories into practical techniques.  - [Artificial Intelligence through Prolog](http://faculty.nps.edu/ncrowe/book/book.html) - In-depth coverage of key concepts on artificial intelligence.  - [Building Expert Systems in Prolog](http://www.amzi.com/ExpertSystemsInProlog/index.htm) - Build prototype expert system shells and their underlying inference engines.  - [Coding Guidelines for Prolog](https://arxiv.org/abs/0911.2899) - Coding standards and good practices for Prolog.  - [The Art of Prolog](https://mitpress.mit.edu/books/art-prolog-second-edition) - Advanced programming techniques for Prolog.    ### Community    - [Prolog FAQ](https://www.metalevel.at/prolog/faq) - Frequently Asked Questions list of the SWI-Prolog newsgroup.  - [SWI-Prolog Discourse](https://swi-prolog.discourse.group/) - Official SWI-Prolog Discourse board.  - [SWI-Prolog Mailing List](http://www.swi-prolog.org/Mailinglist.html) - Announcements, questions and discussion among SWI-Prolog users.  - [SWI-Prolog Freenode](http://webchat.freenode.net/?channels=##prolog) - IRC channel of the SWI-Prolog community.  - [SWI-Prolog Google Group](https://groups.google.com/forum/#!forum/swi-prolog) - SWI-Prolog user    discussion forum **DEPRECATED**.  - [Stack Overflow Prolog](https://stackoverflow.com/questions/tagged/prolog) - User questions under the Prolog tag.  - [Reddit Prolog](http://www.reddit.com/r/prolog) - Logic Programming and Prolog subreddit.    ## Contributing    Contributions are super welcome!    Have a look at the [Contributing Guidelines](contributing.md) on how to get started.    ## License    [![CC0](http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](https://creativecommons.org/publicdomain/zero/1.0) """
Semantic web;https://github.com/arachne-framework/aristotle;"""# Aristotle    [![CircleCI](https://circleci.com/gh/arachne-framework/aristotle.svg?style=svg)](https://circleci.com/gh/arachne-framework/aristotle)    An RDF/OWL library for Clojure, providing a data-oriented wrapper for  Apache Jena.    Key features:    - Read/write RDF graphs using idiomatic Clojure data structures.  - SPARQL queries expressed using Clojure data structures.  - Pluggable inferencing and reasoners.  - Pluggable validators.    ##  Rationale    RDF is a powerful framework for working with highly-annotated data in very abstract ways. Although it isn't perfect, it is highly researched, well defined and understood, and the industry standard for ""rich"" semi-structured, open-ended information modeling.    Most of the existing Clojure tools for RDF are focused mostly on creating and manipulating RDF graphs in pure Clojure at a low level. I desired a more comprehensive library with the specific objective of bridging existing idioms for working with Clojure data to RDF graphs.    Apache Jena is a very capable, well-designed library for working with RDF and the RDF ecosystem. It uses the Apache software license, which unlike many other RDF tools is compatible with Clojure's EPL. However, Jena's core APIs can only be described as agressively object-oriented. Since RDF is at its core highly data-oriented, and Clojure is also data-oriented, using an object-oriented or imperative API seems especially cumbersome. Aristotle attempts to preserve ""good parts"" of Jena, while replacing the cumbersome APIs with clean data-driven interfaces.    Aristotle does not provide direct access to other RDF frameworks (such as RDF4j, JSONLD, Commons RDF, OWL API, etc.) However, Jena itself is highly pluggable, so if you need to interact with one of these other systems it is highly probably that a Jena adapter already exists or can be easily created.    ## Index    - [Data Model](#data-model)    - [Literals](#literals)    - [Data Structures](#data-structures)  - [API](#api)  - [Query](#query)  - [Validation](#validation)    ## Data Model    To express RDF data as Clojure, Aristotle provides two protocols. `arachne.aristotle.graph/AsNode` converts Clojure literals to RDF Nodes of the appropriate type, while `arachne.aristotle.graph/AsTriples` converts Clojure data structures to sets of RDF triples.    ### Literals    Clojure primitive values map to Jena Node objects of the appropriate type.    |Clojure Type|RDF Node|  |------------|--------|  |long|XSD Long|  |double|XSD Double|  |boolean|XSD Boolean|  |java.math.BigDecimal|XSD Decimal|  |java.util.Date|XSD DateTime|  |java.util.Calendar|XSD DateTime|  |string enclosed by angle brackets<br>(e.g, `""<http://foo.com/#bar>""`)| IRI  |other strings| XSD String|  |keyword|IRI (see explanation of IRI/keyword registry below)|  |java.net.URL|IRI|  java.net.URI|IRI|  |symbols starting with `?`| variable node (for patterns or queries)|  |the symbol `_`|unique blank node|  |symbols starting with `_`| named blank node|  |other symbols| IRI of the form `<urn:clojure:namespace/name>`.    #### IRI/Keyword Registry    Since IRIs are usually long strings, and tend to be used repeatedly, using the full string expression can be cumbersome. Furthermore, Clojure tends to prefer keywords to strings, especially for property/attribute names and enumerated or constant values.    Therefore, Aristotle provides a mechanism to associate a namespace with an IRI prefix. Keywords with a registered namespace will be converted to a corresponding IRI.    Use the `arachne.aristotle.registry/prefix` function to declare a prefix. For example,    ```  (reg/prefix 'foaf ""http://xmlns.com/foaf/0.1/"")  ```    Then, keywords with a `:foaf` namespace will be interpreted as IRI nodes. For example, with the above declaration `:foaf/name` will be interpreted as `<http://xmlns.com/foaf/0.1/name>`.    The following common namespace prefixes are defined by default:    |Namespace |IRI Prefix|  |----|-------|  |rdf|`<http://www.w3.org/1999/02/22-rdf-syntax-ns#>`|  |rdfs|`<http://www.w3.org/2000/01/rdf-schema#>`|  |xsd|`<http://www.w3.org/2001/XMLSchema#>`|  |owl|`<http://www.w3.org/2002/07/owl#>`|  |owl2|`<http://www.w3.org/2006/12/owl2#>`|    The registry is stored in the global dynamic Var `arachne.aristotle.registry/*registry*`, which can be also overridden on a thread-local basis using the `arachne.aristotle.registry/with` macro, which takes a map of namespaces (as keywords) and IRI prefixes. For example:    ```clojure  (reg/with {'foaf ""http://xmlns.com/foaf/0.1/""             'dc ""http://purl.org/dc/elements/1.1/""}    ;; Code using keywords with :foaf and :dc namespaces    )  ```    You can also register a prefix in RDF/EDN data, using the `#rdf/prefix` tagged literal. The prefix will be added to the thread-local binding and is scoped to the same triple expansion. This allows you to define a prefix alongside the data that uses it, without installing it globally or managing it in your code. For example:    ```clojure  [#rdf/prefix [:ex ""http://example.com/""]   {:rdf/about :ex/luke    :foaf/name ""Luke""}]    ```    #### Wildcard Prefixes    Aristotle now allows you to register a RDF IRI prefix for a namespace *prefix*, rather than a fully specified namespace. To do so, use an asterisk in the symbol you provide to the `prefix` function:    ```clojure  (reg/prefix 'arachne.* ""http://arachne-framework.org/vocab/1.0/"")  ```    This means that keywords with a namespace that starts with an `arachne` namespace segment will use the supplied prefix. Any additional namespace segments will be appended to the prefix, separated by a forward slash (`/`).    Given the registration above, for example, the keyword `:arachne.http.request/body` would be interpreted as the IRI ""<http://arachne-framework.org/vocab/1.0/http/request/body>""    If multiple wildcard prefixes overlap, the system will use whichever is more specific, and will prefer non-wildcard registrations to wildcard registrations in the case of ambiguity.    Using `#` or any other character as a prefix separator for wildcard prefixes, instead of `/`, is currently not supported.    ### Data Structures    You can use the `arachne.aristotle.graph/triples` function to convert any compatible Clojure data structure to a collection of RDF Triples (usually in practice it isn't necessary to call `triples` explicitly, as the higher-level APIs do it for you.)    #### Single Triple    A 3-element vector can be used to represent a single RDF Triple. For example:     ```clojure  (ns arachne.aristotle.example    (:require [arachne.aristotle.registry :as reg]              [arachne.aristotle.graph :as g]))    (reg/prefix 'arachne.aristotle.example ""http://arachne-framework.org/example#"")    (g/triples [::luke :foaf/firstName ""Luke""])  ```    The call to `g/triples` returns a collection containing a single Jena Triple with a subject of `<http://arachne-framework.org/example#luke>`, a predicate of `<http://xmlns.com/foaf/0.1/firstName>` an the string literal `""Luke""` as the object.    #### Collections of Triples    A collection of multiple triples works the same way.    For example,     ```clojure  (g/triples '[[luke :foaf/firstName ""Luke""]               [luke :foaf/knows nola]               [nola :foaf/firstName ""Nola""]])   ```    Note the use of symbols; in this case, the nodes for both Luke and Nola are represented as blank nodes (without explicit IRIs.)    #### Maps    Maps may be used to represent multiple statements about a single subject, with each key indicating an RDF property. The subject of the map is indicated using the special `:rdf/about` key, which is *not* interpreted as a property, but rather as identifying the subject of the map. If no `:rdf/about` key is present,  a blank node will be used as the subject.    For example:    ```clojure  (g/triples {:rdf/about ::luke              :foaf/firstName ""Luke""              :foaf/lastName ""VanderHart""})  ```    This is equivalent to two triples:    ```  <http://arachne-framework.org/example#luke> <http://xmlns.com/foaf/0.1/firstName> ""Luke""  <http://arachne-framework.org/example#luke> <http://xmlns.com/foaf/0.1/lastName> ""VanderHart""  ```    ##### Multiple Values    If the value for a key is a single literal, it is interpreted as a single triple. If the value is a collection, it is intererpreted as multiple values for the same property. For example:    ```clojure  (g/triples {:rdf/about ::luke              :foaf/made [::arachne ::aristotle ::quiescent]})  ```                            Expands to:         <http://arachne-framework.org/example#luke> <http://xmlns.com/foaf/0.1/made> <http://arachne-framework.org/example#arachne>      <http://arachne-framework.org/example#luke> <http://xmlns.com/foaf/0.1/made> <http://arachne-framework.org/example#quiescent>      <http://arachne-framework.org/example#luke> <http://xmlns.com/foaf/0.1/made> <http://arachne-framework.org/example#aristotle>        ##### Nested Maps    In addition to literals, the values of keys may be additional maps (or collections of maps). The subject of the nested map will be both the object of the property under which it is specified, and the subject if statements in its own map.    ```clojure  (g/triples {:rdf/about ::luke              :foaf/knows [{:rdf/about ::nola                            :foaf/name ""Nola""                            :foaf/knows ::luke}}                           {:rdf/about ::Jim                            :foaf/name ""Jim""}}])  ```     Expressed in expanded triples, this is:        <http://arachne-framework.org/example#luke> <http://xmlns.com/foaf/0.1/knows> <http://arachne-framework.org/example#nola>      <http://arachne-framework.org/example#nola> <http://xmlns.com/foaf/0.1/name> ""Nola""      <http://arachne-framework.org/example#nola> <http://xmlns.com/foaf/0.1/knows> <http://arachne-framework.org/example#luke>      <http://arachne-framework.org/example#luke> <http://xmlns.com/foaf/0.1/knows> <http://arachne-framework.org/example#jim>      <http://arachne-framework.org/example#jim> <http://xmlns.com/foaf/0.1/name> ""Jim""    ## API    Aristotle's primary API is exposed in its top-level namespace, `arachne.aristotle`, which defines functions to create and interact with _graphs_.    A graph is a collection of RDF data, together with (optionally) logic and/or inferencing engines. Graphs may be stored in memory or be a facade to an external RDF database (although all the graph constructors shipped with Aristotle are for in-memory graphs.)    Graphs are instances of `org.apache.jena.graph.Graph`, which are  stateful mutable objects (mutability is too deeply ingrained in Jena  to provide an immutable facade.) However, Aristotle's APIs are  consistent in returning the model from any update operations, as if  graphs were immutable Clojure-style collections. It is reccomended to  rely on the return value of update operations, as if graphs were  immutable, so your code does not break if immutable graph  representations are ever supported.    Jena Graphs are not thread-safe by default; make sure you limit concurrent graph access.    ### Creating a Graph    To create a new graph, invoke the `arachne.aristotle/graph` multimethod. The first argument to `graph` is a keyword specifying the type of graph to construct, additional arguments vary depending on the type of graph.    Graph constructors provided by Aristotle include:    |type|model|  |----|-----|  |:simple| Basic in-memory triple store with no inferencing capability |  |:jena-mini| In-memory triple store that performs OWL 1 inferencing using Jena's ""Mini"" inferencer (a subset of OWL Full with restrictions on some of the less useful forward entailments.)  |:jena-rules| In-memory triple store supporting custom rules, using Jena's [hybrid backward/forward rules engine](https://jena.apache.org/documentation/inference/#rules). Takes a collection of `org.apache.jena.reasoner.rulesys.Rule` objects as an additional argument (the prebuilt collection of rules for Jena Mini is provided at `arachne.aristotle.inference/mini-rules`) |    Clients may wish to provide additional implementations of the `graph` multimethod to support additional underlying graphy or inference types; the only requirement is that the method return an instance of `org.apache.jena.rdf.graph.Graph`. For example, for your project, you may wish to create a Graph backed by on-disk or database storag, or which uses the more powerful Pellet reasoner, which has Jena integration but is not shipped with Aristotle due to license restrictions.    Example:    ```  (require '[arachne.aristotle :as aa])  (def m (aa/graph :jena-mini))  ```    ### Adding data to a graph    In order to do anything useful with a graph, you must add additional facts. Facts may be added either programatically in your code, or by reading serialized data from a file or remote URL.    #### Adding data programatically    To add data programatically, use the `arachne.aristotle/add` function, which takes a graph and some data to add. The data is processed into RDF triples using  `arachne.aristotle.graph/triples`, using the data format documented above. For example:    ```  (require '[arachne.aristotle :as aa])    (def g (aa/graph :jena-mini))    (aa/add g {:rdf/about ::luke             :foaf/firstName ""Luke""             :foaf/lastName ""VanderHart""})  ```    #### Adding data from a file    To add data from a file, use the `arachne.aristotle/read` function, which takes a graph and a file. The file may be specified by a:      - String of the absolute or process-relative filename    - java.net.URI    - java.net.URL    - java.io.File    Jena will detect what format the file is in, which may be one of RDF/XML, Turtle, N3, or N-Triples. All of the statements it contains will be added to the graph. Example:    ## Query    Aristotle provides a data-oriented interface to Jena's SPARQL query engine. Queries themselves are expressed as Clojure data, and can be programatically generated and combined (similar to queries in Datomic.)    To invoke a query, use the `arachne.aristotle.query/query` function, which takes a query data structure, a graph, and any query inputs. It returns the results of the query.    SPARQL itself is string oriented, with a heavily lexical grammar that does not translate cleanly to data structures. However, SPARQL has an internal algebra that *is* very clean and composable. Aristotle's query data uses this internal SPARQL alegebra (which is exposed by Jena's ARQ data graph) ignoring SPARQL syntax. All queries expressible in SPARQL syntax are also expressible in Aristotle's query data, modulo some features that are not implemented yet (e.g, query fedration across remote data sources.)    Unfortunately, the SPARQL algebra has no well documented syntax. A [rough overview](https://www.w3.org/2011/09/SparqlAlgebra/ARQalgebra) is available, and this readme will document some of the more common forms. For more details, see the [query specs](https://github.com/arachne-framework/aristotle/blob/master/src/arachne/aristotle/query/spec.clj) with their associated docstrings.    Aristotle queries are expressed as compositions of algebraic operations, using the generalized form `[operation expression* sub-operation*]` These operation vectors may be nested arbitrarily.    Expressions are specified using a Clojure list form, with the expression type as a symbol. These expressions take the general form `(expr-type arg*)`.    ### Running Queries    To run a query, use the `arachne.aristotle.query/run` function. This function takes a graph, an (optional) binding vector, a query, and (optionally) a map of variable bindings which serve as query inputs.    If a binding vector is given, results will be returned as a set of tuples, one for each unique binding of the variables in the binding vector.    If no binding vector is supplied, results will be returned as a sequence of query solutions, with each solution represented as a map of the variables it binds. In this case, solutions may not be unique (unless the query specifically inclues a `:distinct` operation.)    Some examples follow:    #### Sample: simple query    ```clojure  (require '[arachne.aristotle.query :as q])    (q/run my-graph '[:bgp [:example/luke :foaf/knows ?person]                         [?person :foaf/name ?name]])  ```    This query is a single pattern match (using a ""basic graph pattern"" or ""bgp""), binding the `:foaf/name` property of each entity that is the subject of `:foaf/knows` for an entity identified by `:example/luke`.     An example of the results that might be returned by this query is:    ```clojure  ({?person <http://example.com/person#james> ?name ""Jim""},   {?person <http://example.com/person#sara> ?name ""Sara""},   {?person <http://example.com/person#jules> ?name ""Jules""})  ```    #### Sample: simple query with result binding    This is the same query, but using a binding vector    ```clojure  (q/run my-graph '[?name]         '[:bgp [:example/luke :foaf/knows ?person]                [?person :foaf/name ?name]])  ```  In this case, results would look like:    ```clojure  #{[""Jim""]    [""Sara""]    [""Jules""]}  ```    #### Sample: query with filtering expression    This example expands on the previous query, using a `:filter` operation with an expression to only return acquaintances above the age of 18:     ```clojure  (q/run my-graph '[?name]         '[:filter (< 18 ?age)           '[:bgp [:example/luke :foaf/knows ?person]                  [?person :foaf/name ?name]                  [?person :foaf/age ?age]]])  ```    #### Sample: providing inputs    This example is the same as those above, except instead of hardcoding the base individual as `:example/luke`, the starting individual is bound in a separate binding map provided to `q/run`.     ```clojure  (q/run my-graph '[?name]          [:bgp [?individual :foaf/knows ?person]                [?person :foaf/name ?name]]    '{?individual :example/luke})  ```    It is also possible to bind multiple possibilities for the value of `?individual`:     ```clojure  (q/run my-graph '[?name]          [:bgp [?individual :foaf/knows ?person]                [?person :foaf/name ?name]]    '{?individual #{:example/luke                    :example/carin                    :example/dan}})  ```    This will find the names of all persons who are known by Luke, Carin OR Dan.    ### Precompiled Queries    Queries can also be precompiled into a Jena Operation object, meaning they do not need to be parsed, interpreted, and optimized again every time they are invoked. To precompile a query, use the `arachne.aristotle.query/build` function:     ```clojure  (def friends-q (q/build '[:bgp [?individual :foaf/knows ?person]                                 [?person :foaf/name ?name]]))  ```    You can then use the precompiled query object (bound in this case to `friends-q` in calls to `arachne.aristotle.query/run`:    ```clojure  (q/run my-graph friends-q '{?individual :example/luke})  ```    The results will be exactly the same as using the inline version.    ## Validation    One common use case is to take a given Graph and ""validate"" it, ensuring its internal consistency (including whether entities in it conform to any OWL or RDFS schema that is present.)    To do this, run the `arachne.aristotle.validation/validate` function. Passed only a graph, it will return any errors returned by the Jena Reasoner that was used when constructing the graph. The `:simple` reasoner will never return any errors, the `:jena-mini` reasoner will return OWL inconsistencies, etc.    If validation is successfull, the validator will return nil or an empty list. If there were any errors, each error will be returned as a map containing details about the specific error type.    #### Closed-World validation    The built-in reasoners use the standard open-world assumption of RDF and OWL. This means that many scenarios that would intuitively be ""invalid"" to a human (such as a missing min-cardinality attribute) will not be identified, because the reasoner alwas operates under the assumption that it doesn't yet know all the facts.    However, for certain use cases, it can be desirable to assert that yes, the graph actually does contain all pertinent facts, and that we want to make some assertions based on what the graph *actually* knows at a given moment, never mind what facts may be added in the future.    To do this, you can pass additional validator functions to `validate`, providing  a sequence of optional validators as a second argument.    Each of these validator functions takes a graph as its argument, and returns a sequence of validation error maps. An empty sequence implies that the graph is valid.    The ""min-cardinality"" situation mentioned above has a built in validator, `arachne.aristotle.validators/min-cardinality`. It works by running a SPARQL query on the provided graph that detects if any min-cardinality attributes are missing from entities known to be of an OWL class where they are supposed to be present.    To use it, just provide it in the list of custom validators passed to `validate`:     ```clojure  (v/validate m [v/min-cardinality])  ```    This will return the set not only of built in OWL validation errors, but also any min-cardinality violations that are discovered.    Of course, you can provide any additional validator functions as well. """
Semantic web;https://github.com/sindresorhus/awesome;"""<div align=""center"">  	<a href=""https://vshymanskyy.github.io/StandWithUkraine"">  		<img width=""500"" height=""350"" src=""media/logo-ua.svg"" alt=""Awesome"">  		<img src=""https://raw.githubusercontent.com/vshymanskyy/StandWithUkraine/main/banner2-direct.svg"">  	</a>  	<br>  	<br>  	<br>  	<br>  	<br>  	<br>  	<br>  	<hr>  	<p>  		<p>  			<sup>  				<a href=""https://github.com/sponsors/sindresorhus"">My open source work is supported by the community</a>  			</sup>  		</p>  		<sup>Special thanks to:</sup>  		<br>  		<br>  		<a href=""https://standardresume.co/tech"">  			<img src=""https://sindresorhus.com/assets/thanks/standard-resume-logo.svg"" width=""160""/>  		</a>  		<br>  		<br>  		<a href=""https://retool.com/?utm_campaign=sindresorhus"">  			<img src=""https://sindresorhus.com/assets/thanks/retool-logo.svg"" width=""210""/>  		</a>  		<br>  		<br>  		<a href=""https://doppler.com/?utm_campaign=github_repo&utm_medium=referral&utm_content=awesome&utm_source=github"">  			<div>  				<img src=""https://dashboard.doppler.com/imgs/logo-long.svg"" width=""230"" alt=""Doppler"">  			</div>  			<b>All your environment variables, in one place</b>  			<div>  				<sub>Stop struggling with scattered API keys, hacking together home-brewed tools,</sub>  				<br>  				<sup>and avoiding access controls. Keep your team and servers in sync with Doppler.</sup>  			</div>  		</a>  		<br>  		<a href=""https://workos.com/?utm_campaign=github_repo&utm_medium=referral&utm_content=awesome&utm_source=github"">  			<div>  				<img src=""https://sindresorhus.com/assets/thanks/workos-logo-white-bg.svg"" width=""200"" alt=""WorkOS"">  			</div>  			<b>Your app, enterprise-ready.</b>  			<div>  				<sub>Start selling to enterprise customers with just a few lines of code.</sub>  				<br>  				<sup>Add Single Sign-On (and more) in minutes instead of months.</sup>  			</div>  		</a>  		<br>  		<a href=""https://strapi.io/?ref=sindresorhus"">  			<div>  				<img src=""https://sindresorhus.com/assets/thanks/strapi-logo-white-bg.png"" width=""200"" alt=""Strapi"">  			</div>  			<b>Strapi is the leading open-source headless CMS.</b>  			<div>  				<sup>It’s 100% JavaScript, fully customizable, and developer-first.</sup>  			</div>  		</a>  		<br>  		<a href=""https://oss.capital"">  			<div>  				<img src=""https://sindresorhus.com/assets/thanks/oss-capital-logo-white-bg.svg"" width=""300"" alt=""OSS Capital"">  			</div>  			<div>  				<sup><b>Founded in 2018, OSS Capital is the first and only venture capital platform focused<br>exclusively on supporting early-stage COSS (commercial open source) startup founders.</b></sup>  			</div>  		</a>  		<br>  		<br>  		<a href=""https://bit.io/?utm_campaign=github_repo&utm_medium=referral&utm_content=awesome&utm_source=github"">  			<div>  				<img src=""https://sindresorhus.com/assets/thanks/bitio-logo.svg"" width=""190"" alt=""bit.io"">  			</div>  			<b>Instant, shareable cloud PostgreSQL database</b>  			<div>  				<sup>Import any dataset in seconds, share with anyone with a click, try without signing up</sup>  			</div>  		</a>  		<br>  		<br>  		<a href=""https://www.gitpod.io/?utm_campaign=sindresorhus&utm_medium=referral&utm_content=awesome&utm_source=github"">  			<div>  				<img src=""https://sindresorhus.com/assets/thanks/gitpod-logo-white-bg.svg"" width=""220"" alt=""Gitpod"">  			</div>  			<b>Dev environments built for the cloud</b>  			<div>  				<sub>  				Natively integrated with GitLab, GitHub, and Bitbucket, Gitpod automatically and continuously prebuilds dev  				<br>  				environments for all your branches. As a result team members can instantly start coding with fresh dev environments  				<br>  				for each new task - no matter if you are building a new feature, want to fix a bug, or work on a code review.  				</sub>  			</div>  		</a>  		<br>  		<br>  		<a href=""https://keygen.sh"">  			<div>  				<img src=""https://sindresorhus.com/assets/thanks/keygen-logo.svg"" width=""210"" alt=""Keygen"">  			</div>  			<b>A dead-simple software licensing and distribution API built for developers</b>  		</a>  		<br>  		<br>  		<br>  		<a href=""https://getstream.io/chat/sdk/ios/?utm_source=Github&utm_medium=Github_Repo_Content_Ad&utm_content=Developer&utm_campaign=Github_Jan2022_iOSChatSDK&utm_term=Sindresorhus#gh-light-mode-only"">  			<div>  				<img src=""https://sindresorhus.com/assets/thanks/stream-logo.svg"" width=""220"" alt=""Stream"">  			</div>  			<br>  			<div>  				<b>Build Scalable Feeds & Chat Applications with Powerful APIs and Front End Components</b>  			</div>  		</a>  		<a href=""https://getstream.io/chat/sdk/ios/?utm_source=Github&utm_medium=Github_Repo_Content_Ad&utm_content=Developer&utm_campaign=Github_Jan2022_iOSChatSDK&utm_term=Sindresorhus#gh-dark-mode-only"">  			<div>  				<img src=""https://sindresorhus.com/assets/thanks/stream-logo-dark.svg"" width=""220"" alt=""Stream"">  			</div>  			<br>  			<div>  				<b>Build Scalable Feeds & Chat Applications with Powerful APIs and Front End Components</b>  			</div>  		</a>  		<br>  		<br>  	</p>  	<hr>  	<br>  	<br>  	<br>  	<br>  </div>    <p align=""center"">  	<a href=""awesome.md"">What is an awesome list?</a>&nbsp;&nbsp;&nbsp;  	<a href=""contributing.md"">Contribution guide</a>&nbsp;&nbsp;&nbsp;  	<a href=""create-list.md"">Creating a list</a>&nbsp;&nbsp;&nbsp;  	<a href=""https://twitter.com/awesome__re"">Twitter</a>&nbsp;&nbsp;&nbsp;  	<a href=""https://www.redbubble.com/people/sindresorhus/works/30364188-awesome-logo"">Stickers & t-shirts</a>  </p>    <br>    <div align=""center"">  	<b>Follow the <a href=""https://twitter.com/awesome__re"">Awesome Twitter account</a> for updates on new list additions.</b>  </div>    <br>    <p align=""center"">  	<sub>Just type <a href=""https://awesome.re""><code>awesome.re</code></a> to go here. Check out my <a href=""https://blog.sindresorhus.com"">blog</a> and follow me on <a href=""https://twitter.com/sindresorhus"">Twitter</a>.</sub>  </p>  <br>    ## Contents    - [Platforms](#platforms)  - [Programming Languages](#programming-languages)  - [Front-End Development](#front-end-development)  - [Back-End Development](#back-end-development)  - [Computer Science](#computer-science)  - [Big Data](#big-data)  - [Theory](#theory)  - [Books](#books)  - [Editors](#editors)  - [Gaming](#gaming)  - [Development Environment](#development-environment)  - [Entertainment](#entertainment)  - [Databases](#databases)  - [Media](#media)  - [Learn](#learn)  - [Security](#security)  - [Content Management Systems](#content-management-systems)  - [Hardware](#hardware)  - [Business](#business)  - [Work](#work)  - [Networking](#networking)  - [Decentralized Systems](#decentralized-systems)  - [Higher Education](#higher-education)  - [Events](#events)  - [Testing](#testing)  - [Miscellaneous](#miscellaneous)  - [Related](#related)    ## Platforms    - [Node.js](https://github.com/sindresorhus/awesome-nodejs#readme) - Async non-blocking event-driven JavaScript runtime built on Chrome's V8 JavaScript engine.  	- [Cross-Platform](https://github.com/bcoe/awesome-cross-platform-nodejs#readme) - Writing cross-platform code on Node.js.  - [Frontend Development](https://github.com/dypsilon/frontend-dev-bookmarks#readme)  - [iOS](https://github.com/vsouza/awesome-ios#readme) - Mobile operating system for Apple phones and tablets.  - [Android](https://github.com/JStumpp/awesome-android#readme) - Mobile operating system developed by Google.  - [IoT & Hybrid Apps](https://github.com/weblancaster/awesome-IoT-hybrid#readme)  - [Electron](https://github.com/sindresorhus/awesome-electron#readme) - Cross-platform native desktop apps using JavaScript/HTML/CSS.  - [Cordova](https://github.com/busterc/awesome-cordova#readme) - JavaScript API for hybrid apps.  - [React Native](https://github.com/jondot/awesome-react-native#readme) - JavaScript framework for writing natively rendering mobile apps for iOS and Android.  - [Xamarin](https://github.com/XamSome/awesome-xamarin#readme) - Mobile app development IDE, testing, and distribution.  - [Linux](https://github.com/inputsh/awesome-linux#readme)  	- [Containers](https://github.com/Friz-zy/awesome-linux-containers#readme)  	- [eBPF](https://github.com/zoidbergwill/awesome-ebpf#readme) - Virtual machine that allows you to write more efficient and powerful tracing and monitoring for Linux systems.  	- [Arch-based Projects](https://github.com/PandaFoss/Awesome-Arch#readme) - Linux distributions and projects based on Arch Linux.  	- [AppImage](https://github.com/AppImage/awesome-appimage#readme) - Package apps in a single file that works on various mainstream Linux distributions.  - macOS - Operating system for Apple's Mac computers.  	- [Screensavers](https://github.com/agarrharr/awesome-macos-screensavers#readme)  	- [Apps](https://github.com/jaywcjlove/awesome-mac#readme)  	- [Open Source Apps](https://github.com/serhii-londar/open-source-mac-os-apps#readme)  - [watchOS](https://github.com/yenchenlin/awesome-watchos#readme) - Operating system for the Apple Watch.  - [JVM](https://github.com/deephacks/awesome-jvm#readme)  - [Salesforce](https://github.com/mailtoharshit/awesome-salesforce#readme)  - [Amazon Web Services](https://github.com/donnemartin/awesome-aws#readme)  - [Windows](https://github.com/Awesome-Windows/Awesome#readme)  - [IPFS](https://github.com/ipfs/awesome-ipfs#readme) - P2P hypermedia protocol.  - [Fuse](https://github.com/fuse-compound/awesome-fuse#readme) - Mobile development tools.  - [Heroku](https://github.com/ianstormtaylor/awesome-heroku#readme) - Cloud platform as a service.  - [Raspberry Pi](https://github.com/thibmaek/awesome-raspberry-pi#readme) - Credit card-sized computer aimed at teaching kids programming, but capable of a lot more.  - [Qt](https://github.com/JesseTG/awesome-qt#readme) - Cross-platform GUI app framework.  - [WebExtensions](https://github.com/fregante/Awesome-WebExtensions#readme) - Cross-browser extension system.  - [Smart TV](https://github.com/vitalets/awesome-smart-tv#readme) - Create apps for different TV platforms.  - [GNOME](https://github.com/Kazhnuz/awesome-gnome#readme) - Simple and distraction-free desktop environment for Linux.  - [KDE](https://github.com/francoism90/awesome-kde#readme) - A free software community dedicated to creating an open and user-friendly computing experience.  - [.NET](https://github.com/quozd/awesome-dotnet#readme)  	- [Core](https://github.com/thangchung/awesome-dotnet-core#readme)  	- [Roslyn](https://github.com/ironcev/awesome-roslyn#readme) - Open-source compilers and code analysis APIs for C# and VB.NET languages.  - [Amazon Alexa](https://github.com/miguelmota/awesome-amazon-alexa#readme) - Virtual home assistant.  - [DigitalOcean](https://github.com/jonleibowitz/awesome-digitalocean#readme) - Cloud computing platform designed for developers.  - [Flutter](https://github.com/Solido/awesome-flutter#readme) - Google's mobile SDK for building native iOS and Android apps from a single codebase written in Dart.  - [Home Assistant](https://github.com/frenck/awesome-home-assistant#readme) - Open source home automation that puts local control and privacy first.  - [IBM Cloud](https://github.com/victorshinya/awesome-ibmcloud#readme) - Cloud platform for developers and companies.  - [Firebase](https://github.com/jthegedus/awesome-firebase#readme) - App development platform built on Google Cloud Platform.  - [Robot Operating System 2.0](https://github.com/fkromer/awesome-ros2#readme) - Set of software libraries and tools that help you build robot apps.  - [Adafruit IO](https://github.com/adafruit/awesome-adafruitio#readme) - Visualize and store data from any device.  - [Cloudflare](https://github.com/irazasyed/awesome-cloudflare#readme) - CDN, DNS, DDoS protection, and security for your site.  - [Actions on Google](https://github.com/ravirupareliya/awesome-actions-on-google#readme) - Developer platform for Google Assistant.  - [ESP](https://github.com/agucova/awesome-esp#readme) - Low-cost microcontrollers with WiFi and broad IoT applications.  - [Deno](https://github.com/denolib/awesome-deno#readme) - A secure runtime for JavaScript and TypeScript that uses V8 and is built in Rust.  - [DOS](https://github.com/balintkissdev/awesome-dos#readme) - Operating system for x86-based personal computers that was popular during the 1980s and early 1990s.  - [Nix](https://github.com/nix-community/awesome-nix#readme) - Package manager for Linux and other Unix systems that makes package management reliable and reproducible.  - [Integration](https://github.com/stn1slv/awesome-integration#readme) - Linking together different IT systems (components) to functionally cooperate as a whole.  - [Node-RED](https://github.com/naimo84/awesome-nodered#readme) - A programming tool for wiring together hardware devices, APIs, and online services.  - [Low Code](https://github.com/zenitysec/awesome-low-code#readme) - Allowing business professionals to address their needs on their own with little to no coding skills.    ## Programming Languages    - [JavaScript](https://github.com/sorrycc/awesome-javascript#readme)  	- [Promises](https://github.com/wbinnssmith/awesome-promises#readme)  	- [Standard Style](https://github.com/standard/awesome-standard#readme) - Style guide and linter.  	- [Must Watch Talks](https://github.com/bolshchikov/js-must-watch#readme)  	- [Tips](https://github.com/loverajoel/jstips#readme)  	- [Network Layer](https://github.com/Kikobeats/awesome-network-js#readme)  	- [Micro npm Packages](https://github.com/parro-it/awesome-micro-npm-packages#readme)  	- [Mad Science npm Packages](https://github.com/feross/awesome-mad-science#readme) - Impossible sounding projects that exist.  	- [Maintenance Modules](https://github.com/maxogden/maintenance-modules#readme) - For npm packages.  	- [npm](https://github.com/sindresorhus/awesome-npm#readme) - Package manager.  	- [AVA](https://github.com/avajs/awesome-ava#readme) - Test runner.  	- [ESLint](https://github.com/dustinspecker/awesome-eslint#readme) - Linter.  	- [Functional Programming](https://github.com/stoeffel/awesome-fp-js#readme)  	- [Observables](https://github.com/sindresorhus/awesome-observables#readme)  	- [npm scripts](https://github.com/RyanZim/awesome-npm-scripts#readme) - Task runner.  	- [30 Seconds of Code](https://github.com/30-seconds/30-seconds-of-code#readme) - Code snippets you can understand in 30 seconds.  	- [Ponyfills](https://github.com/Richienb/awesome-ponyfills#readme) - Like polyfills but without overriding native APIs.  - [Swift](https://github.com/matteocrippa/awesome-swift#readme) - Apple's compiled programming language that is secure, modern, programmer-friendly, and fast.  	- [Education](https://github.com/hsavit1/Awesome-Swift-Education#readme)  	- [Playgrounds](https://github.com/uraimo/Awesome-Swift-Playgrounds#readme)  - [Python](https://github.com/vinta/awesome-python#readme) - General-purpose programming language designed for readability.  	- [Asyncio](https://github.com/timofurrer/awesome-asyncio#readme) - Asynchronous I/O in Python 3.  	- [Scientific Audio](https://github.com/faroit/awesome-python-scientific-audio#readme) - Scientific research in audio/music.  	- [CircuitPython](https://github.com/adafruit/awesome-circuitpython#readme) - A version of Python for microcontrollers.  	- [Data Science](https://github.com/krzjoa/awesome-python-data-science#readme) - Data analysis and machine learning.  	- [Typing](https://github.com/typeddjango/awesome-python-typing#readme) - Optional static typing for Python.  	- [MicroPython](https://github.com/mcauser/awesome-micropython#readme) - A lean and efficient implementation of Python 3 for microcontrollers.  - [Rust](https://github.com/rust-unofficial/awesome-rust#readme)  - [Haskell](https://github.com/krispo/awesome-haskell#readme)  - [PureScript](https://github.com/passy/awesome-purescript#readme)  - [Go](https://github.com/avelino/awesome-go#readme)  - [Scala](https://github.com/lauris/awesome-scala#readme)  	- [Scala Native](https://github.com/tindzk/awesome-scala-native#readme) - Optimizing ahead-of-time compiler for Scala based on LLVM.  - [Ruby](https://github.com/markets/awesome-ruby#readme)  - [Clojure](https://github.com/razum2um/awesome-clojure#readme)  - [ClojureScript](https://github.com/hantuzun/awesome-clojurescript#readme)  - [Elixir](https://github.com/h4cc/awesome-elixir#readme)  - [Elm](https://github.com/sporto/awesome-elm#readme)  - [Erlang](https://github.com/drobakowski/awesome-erlang#readme)  - [Julia](https://github.com/svaksha/Julia.jl#readme) - High-level dynamic programming language designed to address the needs of high-performance numerical analysis and computational science.  - [Lua](https://github.com/LewisJEllis/awesome-lua#readme)  - [C](https://github.com/inputsh/awesome-c#readme)  - [C/C++](https://github.com/fffaraz/awesome-cpp#readme) - General-purpose language with a bias toward system programming and embedded, resource-constrained software.  - [R](https://github.com/qinwf/awesome-R#readme) - Functional programming language and environment for statistical computing and graphics.  	- [Learning](https://github.com/iamericfletcher/awesome-r-learning-resources#readme)  - [D](https://github.com/dlang-community/awesome-d#readme)  - [Common Lisp](https://github.com/CodyReichert/awesome-cl#readme) - Powerful dynamic multiparadigm language that facilitates iterative and interactive development.  	- [Learning](https://github.com/GustavBertram/awesome-common-lisp-learning#readme)  - [Perl](https://github.com/hachiojipm/awesome-perl#readme)  - [Groovy](https://github.com/kdabir/awesome-groovy#readme)  - [Dart](https://github.com/yissachar/awesome-dart#readme)  - [Java](https://github.com/akullpp/awesome-java#readme) - Popular secure object-oriented language designed for flexibility to ""write once, run anywhere"".  	- [RxJava](https://github.com/eleventigers/awesome-rxjava#readme)  - [Kotlin](https://github.com/KotlinBy/awesome-kotlin#readme)  - [OCaml](https://github.com/ocaml-community/awesome-ocaml#readme)  - [ColdFusion](https://github.com/seancoyne/awesome-coldfusion#readme)  - [Fortran](https://github.com/rabbiabram/awesome-fortran#readme)  - [PHP](https://github.com/ziadoz/awesome-php#readme) - Server-side scripting language.  	- [Composer](https://github.com/jakoch/awesome-composer#readme) - Package manager.  - [Pascal](https://github.com/Fr0sT-Brutal/awesome-pascal#readme)  - [AutoHotkey](https://github.com/ahkscript/awesome-AutoHotkey#readme)  - [AutoIt](https://github.com/J2TeaM/awesome-AutoIt#readme)  - [Crystal](https://github.com/veelenga/awesome-crystal#readme)  - [Frege](https://github.com/sfischer13/awesome-frege#readme) - Haskell for the JVM.  - [CMake](https://github.com/onqtam/awesome-cmake#readme) - Build, test, and package software.  - [ActionScript 3](https://github.com/robinrodricks/awesome-actionscript3#readme) - Object-oriented language targeting Adobe AIR.  - [Eta](https://github.com/sfischer13/awesome-eta#readme) - Functional programming language for the JVM.  - [Idris](https://github.com/joaomilho/awesome-idris#readme) - General purpose pure functional programming language with dependent types influenced by Haskell and ML.  - [Ada/SPARK](https://github.com/ohenley/awesome-ada#readme) - Modern programming language designed for large, long-lived apps where reliability and efficiency are essential.  - [Q#](https://github.com/ebraminio/awesome-qsharp#readme) - Domain-specific programming language used for expressing quantum algorithms.  - [Imba](https://github.com/koolamusic/awesome-imba#readme) - Programming language inspired by Ruby and Python and compiles to performant JavaScript.  - [Vala](https://github.com/desiderantes/awesome-vala#readme) - Programming language designed to take full advantage of the GLib and GNOME ecosystems, while preserving the speed of C code.  - [Coq](https://github.com/coq-community/awesome-coq#readme) - Formal language and environment for programming and specification which facilitates interactive development of machine-checked proofs.  - [V](https://github.com/vlang/awesome-v#readme) - Simple, fast, safe, compiled language for developing maintainable software.    ## Front-End Development    - [ES6 Tools](https://github.com/addyosmani/es6-tools#readme)  - [Web Performance Optimization](https://github.com/davidsonfellipe/awesome-wpo#readme)  - [Web Tools](https://github.com/lvwzhen/tools#readme)  - [CSS](https://github.com/awesome-css-group/awesome-css#readme) - Style sheet language that specifies how HTML elements are displayed on screen.  	- [Critical-Path Tools](https://github.com/addyosmani/critical-path-css-tools#readme)  	- [Scalability](https://github.com/davidtheclark/scalable-css-reading-list#readme)  	- [Must-Watch Talks](https://github.com/AllThingsSmitty/must-watch-css#readme)  	- [Protips](https://github.com/AllThingsSmitty/css-protips#readme)  	- [Frameworks](https://github.com/troxler/awesome-css-frameworks#readme)  - [React](https://github.com/enaqx/awesome-react#readme) - App framework.  	- [Relay](https://github.com/expede/awesome-relay#readme) - Framework for building data-driven React apps.  	- [React Hooks](https://github.com/glauberfc/awesome-react-hooks#readme) - A new feature that lets you use state and other React features without writing a class.  - [Web Components](https://github.com/mateusortiz/webcomponents-the-right-way#readme)  - [Polymer](https://github.com/Granze/awesome-polymer#readme) - JavaScript library to develop Web Components.  - [Angular](https://github.com/PatrickJS/awesome-angular#readme) - App framework.  - [Backbone](https://github.com/sadcitizen/awesome-backbone#readme) - App framework.  - [HTML5](https://github.com/diegocard/awesome-html5#readme) - Markup language used for websites & web apps.  - [SVG](https://github.com/willianjusten/awesome-svg#readme) - XML-based vector image format.  - [Canvas](https://github.com/raphamorim/awesome-canvas#readme)  - [KnockoutJS](https://github.com/dnbard/awesome-knockout#readme) - JavaScript library.  - [Dojo Toolkit](https://github.com/petk/awesome-dojo#readme) - JavaScript toolkit.  - [Inspiration](https://github.com/NoahBuscher/Inspire#readme)  - [Ember](https://github.com/ember-community-russia/awesome-ember#readme) - App framework.  - [Android UI](https://github.com/wasabeef/awesome-android-ui#readme)  - [iOS UI](https://github.com/cjwirth/awesome-ios-ui#readme)  - [Meteor](https://github.com/Urigo/awesome-meteor#readme)  - [BEM](https://github.com/sturobson/BEM-resources#readme)  - [Flexbox](https://github.com/afonsopacifer/awesome-flexbox#readme)  - [Web Typography](https://github.com/deanhume/typography#readme)  - [Web Accessibility](https://github.com/brunopulis/awesome-a11y#readme)  - [Material Design](https://github.com/sachin1092/awesome-material#readme)  - [D3](https://github.com/wbkd/awesome-d3#readme) - Library for producing dynamic, interactive data visualizations.  - [Emails](https://github.com/jonathandion/awesome-emails#readme)  - [jQuery](https://github.com/petk/awesome-jquery#readme) - Easy to use JavaScript library for DOM manipulation.  	- [Tips](https://github.com/AllThingsSmitty/jquery-tips-everyone-should-know#readme)  - [Web Audio](https://github.com/notthetup/awesome-webaudio#readme)  - [Offline-First](https://github.com/pazguille/offline-first#readme)  - [Static Website Services](https://github.com/agarrharr/awesome-static-website-services#readme)  - [Cycle.js](https://github.com/cyclejs-community/awesome-cyclejs#readme) - Functional and reactive JavaScript framework.  - [Text Editing](https://github.com/dok/awesome-text-editing#readme)  - [Motion UI Design](https://github.com/fliptheweb/motion-ui-design#readme)  - [Vue.js](https://github.com/vuejs/awesome-vue#readme) - App framework.  - [Marionette.js](https://github.com/sadcitizen/awesome-marionette#readme) - App framework.  - [Aurelia](https://github.com/aurelia-contrib/awesome-aurelia#readme) - App framework.  - [Charting](https://github.com/zingchart/awesome-charting#readme)  - [Ionic Framework 2](https://github.com/candelibas/awesome-ionic#readme)  - [Chrome DevTools](https://github.com/ChromeDevTools/awesome-chrome-devtools#readme)  - [PostCSS](https://github.com/jdrgomes/awesome-postcss#readme) - CSS tool.  - [Draft.js](https://github.com/nikgraf/awesome-draft-js#readme) - Rich text editor framework for React.  - [Service Workers](https://github.com/TalAter/awesome-service-workers#readme)  - [Progressive Web Apps](https://github.com/TalAter/awesome-progressive-web-apps#readme)  - [choo](https://github.com/choojs/awesome-choo#readme) - App framework.  - [Redux](https://github.com/brillout/awesome-redux#readme) - State container for JavaScript apps.  - [Browserify](https://github.com/browserify/awesome-browserify#readme) - Module bundler.  - [Sass](https://github.com/Famolus/awesome-sass#readme) - CSS preprocessor.  - [Ant Design](https://github.com/websemantics/awesome-ant-design#readme) - Enterprise-class UI design language.  - [Less](https://github.com/LucasBassetti/awesome-less#readme) - CSS preprocessor.  - [WebGL](https://github.com/sjfricke/awesome-webgl#readme) - JavaScript API for rendering 3D graphics.  - [Preact](https://github.com/preactjs/awesome-preact#readme) - App framework.  - [Progressive Enhancement](https://github.com/jbmoelker/progressive-enhancement-resources#readme)  - [Next.js](https://github.com/unicodeveloper/awesome-nextjs#readme) - Framework for server-rendered React apps.  - [lit](https://github.com/web-padawan/awesome-lit#readme) - Library for building web components with a declarative template system.  - [JAMstack](https://github.com/automata/awesome-jamstack#readme) - Modern web development architecture based on client-side JavaScript, reusable APIs, and prebuilt markup.  - [WordPress-Gatsby](https://github.com/henrikwirth/awesome-wordpress-gatsby#readme) - Web development technology stack with WordPress as a back end and Gatsby as a front end.  - [Mobile Web Development](https://github.com/myshov/awesome-mobile-web-development#readme) - Creating a great mobile web experience.  - [Storybook](https://github.com/lauthieb/awesome-storybook#readme) - Development environment for UI components.  - [Blazor](https://github.com/AdrienTorris/awesome-blazor#readme) - .NET web framework using C#/Razor and HTML that runs in the browser with WebAssembly.  - [PageSpeed Metrics](https://github.com/csabapalfi/awesome-pagespeed-metrics#readme) - Metrics to help understand page speed and user experience.  - [Tailwind CSS](https://github.com/aniftyco/awesome-tailwindcss#readme) - Utility-first CSS framework for rapid UI development.  - [Seed](https://github.com/seed-rs/awesome-seed-rs#readme) - Rust framework for creating web apps running in WebAssembly.  - [Web Performance Budget](https://github.com/pajaydev/awesome-web-performance-budget#readme) - Techniques to ensure certain performance metrics for a website.  - [Web Animation](https://github.com/sergey-pimenov/awesome-web-animation#readme) - Animations in the browser with JavaScript, CSS, SVG, etc.  - [Yew](https://github.com/jetli/awesome-yew#readme) - Rust framework inspired by Elm and React for creating multi-threaded frontend web apps with WebAssembly.  - [Material-UI](https://github.com/nadunindunil/awesome-material-ui#readme) - Material Design React components for faster and easier web development.  - [Building Blocks for Web Apps](https://github.com/componently-com/awesome-building-blocks-for-web-apps#readme) - Standalone features to be integrated into web apps.  - [Svelte](https://github.com/TheComputerM/awesome-svelte#readme) - App framework.  - [Design systems](https://github.com/klaufel/awesome-design-systems#readme) - Collection of reusable components, guided by rules that ensure consistency and speed.  - [Inertia.js](https://github.com/innocenzi/awesome-inertiajs#readme) - Make single-page apps without building an API.  - [MDBootstrap](https://github.com/mdbootstrap/awesome-mdbootstrap#readme) - Templates, layouts, components, and widgets to rapidly build websites.    ## Back-End Development    - [Flask](https://github.com/mjhea0/awesome-flask#readme) - Python framework.  - [Docker](https://github.com/veggiemonk/awesome-docker#readme)  - [Vagrant](https://github.com/iJackUA/awesome-vagrant#readme) - Automation virtual machine environment.  - [Pyramid](https://github.com/uralbash/awesome-pyramid#readme) - Python framework.  - [Play1 Framework](https://github.com/PerfectCarl/awesome-play1#readme)  - [CakePHP](https://github.com/friendsofcake/awesome-cakephp#readme) - PHP framework.  - [Symfony](https://github.com/sitepoint-editors/awesome-symfony#readme) - PHP framework.  	- [Education](https://github.com/pehapkari/awesome-symfony-education#readme)  - [Laravel](https://github.com/chiraggude/awesome-laravel#readme) - PHP framework.  	- [Education](https://github.com/fukuball/Awesome-Laravel-Education#readme)  	- [TALL Stack](https://github.com/livewire/awesome-tall-stack#readme) - Full-stack development solution featuring libraries built by the Laravel community.  - [Rails](https://github.com/gramantin/awesome-rails#readme) - Web app framework for Ruby.  	- [Gems](https://github.com/hothero/awesome-rails-gem#readme) - Packages.  - [Phalcon](https://github.com/phalcon/awesome-phalcon#readme) - PHP framework.  - [Useful `.htaccess` Snippets](https://github.com/phanan/htaccess#readme)  - [nginx](https://github.com/fcambus/nginx-resources#readme) - Web server.  - [Dropwizard](https://github.com/stve/awesome-dropwizard#readme) - Java framework.  - [Kubernetes](https://github.com/ramitsurana/awesome-kubernetes#readme) - Open-source platform that automates Linux container operations.  - [Lumen](https://github.com/unicodeveloper/awesome-lumen#readme) - PHP micro-framework.  - [Serverless Framework](https://github.com/pmuens/awesome-serverless#readme) - Serverless computing and serverless architectures.  - [Apache Wicket](https://github.com/PhantomYdn/awesome-wicket#readme) - Java web app framework.  - [Vert.x](https://github.com/vert-x3/vertx-awesome#readme) - Toolkit for building reactive apps on the JVM.  - [Terraform](https://github.com/shuaibiyy/awesome-terraform#readme) - Tool for building, changing, and versioning infrastructure.  - [Vapor](https://github.com/vapor-community/awesome-vapor#readme) - Server-side development in Swift.  - [Dash](https://github.com/ucg8j/awesome-dash#readme) - Python web app framework.  - [FastAPI](https://github.com/mjhea0/awesome-fastapi#readme) - Python web app framework.  - [CDK](https://github.com/kolomied/awesome-cdk#readme) - Open-source software development framework for defining cloud infrastructure in code.  - [IAM](https://github.com/kdeldycke/awesome-iam#readme) - User accounts, authentication and authorization.  - [Slim](https://github.com/nekofar/awesome-slim#readme) - PHP framework.    ## Computer Science    - [University Courses](https://github.com/prakhar1989/awesome-courses#readme)  - [Data Science](https://github.com/academic/awesome-datascience#readme)  	- [Tutorials](https://github.com/siboehm/awesome-learn-datascience#readme)  - [Machine Learning](https://github.com/josephmisiti/awesome-machine-learning#readme)  	- [Tutorials](https://github.com/ujjwalkarn/Machine-Learning-Tutorials#readme)  	- [ML with Ruby](https://github.com/arbox/machine-learning-with-ruby#readme) - Learning, implementing, and applying Machine Learning using Ruby.  	- [Core ML Models](https://github.com/likedan/Awesome-CoreML-Models#readme) - Models for Apple's machine learning framework.  	- [H2O](https://github.com/h2oai/awesome-h2o#readme) - Open source distributed machine learning platform written in Java with APIs in R, Python, and Scala.  	- [Software Engineering for Machine Learning](https://github.com/SE-ML/awesome-seml#readme) - From experiment to production-level machine learning.  	- [AI in Finance](https://github.com/georgezouq/awesome-ai-in-finance#readme) - Solving problems in finance with machine learning.  	- [JAX](https://github.com/n2cholas/awesome-jax#readme) - Automatic differentiation and XLA compilation brought together for high-performance machine learning research.  	- [XAI](https://github.com/altamiracorp/awesome-xai#readme) - Providing insight, explanations, and interpretability to machine learning methods.  - [Speech and Natural Language Processing](https://github.com/edobashira/speech-language-processing#readme)  	- [Spanish](https://github.com/dav009/awesome-spanish-nlp#readme)  	- [NLP with Ruby](https://github.com/arbox/nlp-with-ruby#readme)  	- [Question Answering](https://github.com/seriousran/awesome-qa#readme) - The science of asking and answering in natural language with a machine.  	- [Natural Language Generation](https://github.com/accelerated-text/awesome-nlg#readme) - Generation of text used in data to text, conversational agents, and narrative generation applications.  - [Linguistics](https://github.com/theimpossibleastronaut/awesome-linguistics#readme)  - [Cryptography](https://github.com/sobolevn/awesome-cryptography#readme)  	- [Papers](https://github.com/pFarb/awesome-crypto-papers#readme) - Theory basics for using cryptography by non-cryptographers.  - [Computer Vision](https://github.com/jbhuang0604/awesome-computer-vision#readme)  - [Deep Learning](https://github.com/ChristosChristofidis/awesome-deep-learning#readme) - Neural networks.  	- [TensorFlow](https://github.com/jtoy/awesome-tensorflow#readme) - Library for machine intelligence.  	- [TensorFlow.js](https://github.com/aaronhma/awesome-tensorflow-js#readme) - WebGL-accelerated machine learning JavaScript library for training and deploying models.  	- [TensorFlow Lite](https://github.com/margaretmz/awesome-tensorflow-lite#readme) - Framework that optimizes TensorFlow models for on-device machine learning.  	- [Papers](https://github.com/terryum/awesome-deep-learning-papers#readme) - The most cited deep learning papers.  	- [Education](https://github.com/guillaume-chevalier/awesome-deep-learning-resources#readme)  - [Deep Vision](https://github.com/kjw0612/awesome-deep-vision#readme)  - [Open Source Society University](https://github.com/ossu/computer-science#readme)  - [Functional Programming](https://github.com/lucasviola/awesome-functional-programming#readme)  - [Empirical Software Engineering](https://github.com/dspinellis/awesome-msr#readme) - Evidence-based research on software systems.  - [Static Analysis & Code Quality](https://github.com/analysis-tools-dev/static-analysis#readme)  - [Information Retrieval](https://github.com/harpribot/awesome-information-retrieval#readme) - Learn to develop your own search engine.  - [Quantum Computing](https://github.com/desireevl/awesome-quantum-computing#readme) - Computing which utilizes quantum mechanics and qubits on quantum computers.  - [Theoretical Computer Science](https://github.com/mostafatouny/awesome-theoretical-computer-science#readme) - The interplay of computer science and pure mathematics, distinguished by its emphasis on mathematical rigour and technique.    ## Big Data    - [Big Data](https://github.com/0xnr/awesome-bigdata#readme)  - [Public Datasets](https://github.com/awesomedata/awesome-public-datasets#readme)  - [Hadoop](https://github.com/youngwookim/awesome-hadoop#readme) - Framework for distributed storage and processing of very large data sets.  - [Data Engineering](https://github.com/igorbarinov/awesome-data-engineering#readme)  - [Streaming](https://github.com/manuzhang/awesome-streaming#readme)  - [Apache Spark](https://github.com/awesome-spark/awesome-spark#readme) - Unified engine for large-scale data processing.  - [Qlik](https://github.com/ambster-public/awesome-qlik#readme) - Business intelligence platform for data visualization, analytics, and reporting apps.  - [Splunk](https://github.com/sduff/awesome-splunk#readme) - Platform for searching, monitoring, and analyzing structured and unstructured machine-generated big data in real-time.    ## Theory    - [Papers We Love](https://github.com/papers-we-love/papers-we-love#readme)  - [Talks](https://github.com/JanVanRyswyck/awesome-talks#readme)  - [Algorithms](https://github.com/tayllan/awesome-algorithms#readme)  	- [Education](https://github.com/gaerae/awesome-algorithms-education#readme) - Learning and practicing.  - [Algorithm Visualizations](https://github.com/enjalot/algovis#readme)  - [Artificial Intelligence](https://github.com/owainlewis/awesome-artificial-intelligence#readme)  - [Search Engine Optimization](https://github.com/marcobiedermann/search-engine-optimization#readme)  - [Competitive Programming](https://github.com/lnishan/awesome-competitive-programming#readme)  - [Math](https://github.com/rossant/awesome-math#readme)  - [Recursion Schemes](https://github.com/passy/awesome-recursion-schemes#readme) - Traversing nested data structures.    ## Books    - [Free Programming Books](https://github.com/EbookFoundation/free-programming-books#readme)  - [Go Books](https://github.com/dariubs/GoBooks#readme)  - [R Books](https://github.com/RomanTsegelskyi/rbooks#readme)  - [Mind Expanding Books](https://github.com/hackerkid/Mind-Expanding-Books#readme)  - [Book Authoring](https://github.com/TalAter/awesome-book-authoring#readme)  - [Elixir Books](https://github.com/sger/ElixirBooks#readme)    ## Editors    - [Sublime Text](https://github.com/dreikanter/sublime-bookmarks#readme)  - [Vim](https://github.com/mhinz/vim-galore#readme)  - [Neovim](https://github.com/rockerBOO/awesome-neovim#readme) - Vim-fork focused on extensibility and usability.  - [Emacs](https://github.com/emacs-tw/awesome-emacs#readme)  - [Atom](https://github.com/mehcode/awesome-atom#readme) - Open-source and hackable text editor.  - [Visual Studio Code](https://github.com/viatsko/awesome-vscode#readme) - Cross-platform open-source text editor.    ## Gaming    - [Game Development](https://github.com/ellisonleao/magictools#readme)  - [Game Talks](https://github.com/hzoo/awesome-gametalks#readme)  - [Godot](https://github.com/godotengine/awesome-godot#readme) - Game engine.  - [Open Source Games](https://github.com/leereilly/games#readme)  - [Unity](https://github.com/RyanNielson/awesome-unity#readme) - Game engine.  - [Chess](https://github.com/hkirat/awesome-chess#readme)  - [LÖVE](https://github.com/love2d-community/awesome-love2d#readme) - Game engine.  - [PICO-8](https://github.com/pico-8/awesome-PICO-8#readme) - Fantasy console.  - [Game Boy Development](https://github.com/gbdev/awesome-gbdev#readme)  - [Construct 2](https://github.com/ConstructCommunity/awesome-construct#readme) - Game engine.  - [Gideros](https://github.com/stetso/awesome-gideros#readme) - Game engine.  - [Minecraft](https://github.com/bs-community/awesome-minecraft#readme) - Sandbox video game.  - [Game Datasets](https://github.com/leomaurodesenv/game-datasets#readme) - Materials and datasets for Artificial Intelligence in games.  - [Haxe Game Development](https://github.com/Dvergar/awesome-haxe-gamedev#readme) - A high-level strongly typed programming language used to produce cross-platform native code.  - [libGDX](https://github.com/rafaskb/awesome-libgdx#readme) - Java game framework.  - [PlayCanvas](https://github.com/playcanvas/awesome-playcanvas#readme) - Game engine.  - [Game Remakes](https://github.com/radek-sprta/awesome-game-remakes#readme) - Actively maintained open-source game remakes.  - [Flame](https://github.com/flame-engine/awesome-flame#readme) - Game engine for Flutter.  - [Discord Communities](https://github.com/mhxion/awesome-discord-communities#readme) - Chat with friends and communities.  - [CHIP-8](https://github.com/tobiasvl/awesome-chip-8#readme) - Virtual computer game machine from the 70s.  - [Games of Coding](https://github.com/michelpereira/awesome-games-of-coding#readme) - Learn a programming language by making games.  - [Esports](https://github.com/strift/awesome-esports#readme) - Video games played as a sport.    ## Development Environment    - [Quick Look Plugins](https://github.com/sindresorhus/quick-look-plugins#readme) - For macOS.  - [Dev Env](https://github.com/jondot/awesome-devenv#readme)  - [Dotfiles](https://github.com/webpro/awesome-dotfiles#readme)  - [Shell](https://github.com/alebcay/awesome-shell#readme)  - [Fish](https://github.com/jorgebucaran/awsm.fish#readme) - User-friendly shell.  - [Command-Line Apps](https://github.com/agarrharr/awesome-cli-apps#readme)  - [ZSH Plugins](https://github.com/unixorn/awesome-zsh-plugins#readme)  - [GitHub](https://github.com/phillipadsmith/awesome-github#readme) - Hosting service for Git repositories.  	- [Browser Extensions](https://github.com/stefanbuck/awesome-browser-extensions-for-github#readme)  	- [Cheat Sheet](https://github.com/tiimgreen/github-cheat-sheet#readme)  	- [Pinned Gists](https://github.com/matchai/awesome-pinned-gists#readme) - Dynamic pinned gists for your GitHub profile.  - [Git Cheat Sheet & Git Flow](https://github.com/arslanbilal/git-cheat-sheet#readme)  - [Git Tips](https://github.com/git-tips/tips#readme)  - [Git Add-ons](https://github.com/stevemao/awesome-git-addons#readme) - Enhance the `git` CLI.  - [Git Hooks](https://github.com/compscilauren/awesome-git-hooks#readme) - Scripts for automating tasks during `git` workflows.  - [SSH](https://github.com/moul/awesome-ssh#readme)  - [FOSS for Developers](https://github.com/tvvocold/FOSS-for-Dev#readme)  - [Hyper](https://github.com/bnb/awesome-hyper#readme) - Cross-platform terminal app built on web technologies.  - [PowerShell](https://github.com/janikvonrotz/awesome-powershell#readme) - Cross-platform object-oriented shell.  - [Alfred Workflows](https://github.com/alfred-workflows/awesome-alfred-workflows#readme) - Productivity app for macOS.  - [Terminals Are Sexy](https://github.com/k4m4/terminals-are-sexy#readme)  - [GitHub Actions](https://github.com/sdras/awesome-actions#readme) - Create tasks to automate your workflow and share them with others on GitHub.    ## Entertainment    - [Science Fiction](https://github.com/sindresorhus/awesome-scifi#readme) - Scifi.  - [Fantasy](https://github.com/RichardLitt/awesome-fantasy#readme)  - [Podcasts](https://github.com/ayr-ton/awesome-geek-podcasts#readme)  - [Email Newsletters](https://github.com/zudochkin/awesome-newsletters#readme)  - [IT Quotes](https://github.com/victorlaerte/awesome-it-quotes#readme)    ## Databases    - [Database](https://github.com/numetriclabz/awesome-db#readme)  - [MySQL](https://github.com/shlomi-noach/awesome-mysql#readme)  - [SQLAlchemy](https://github.com/dahlia/awesome-sqlalchemy#readme)  - [InfluxDB](https://github.com/mark-rushakoff/awesome-influxdb#readme)  - [Neo4j](https://github.com/neueda/awesome-neo4j#readme)  - [MongoDB](https://github.com/ramnes/awesome-mongodb#readme) - NoSQL database.  - [RethinkDB](https://github.com/d3viant0ne/awesome-rethinkdb#readme)  - [TinkerPop](https://github.com/mohataher/awesome-tinkerpop#readme) - Graph computing framework.  - [PostgreSQL](https://github.com/dhamaniasad/awesome-postgres#readme) - Object-relational database.  - [CouchDB](https://github.com/quangv/awesome-couchdb#readme) - Document-oriented NoSQL database.  - [HBase](https://github.com/rayokota/awesome-hbase#readme) - Distributed, scalable, big data store.  - [NoSQL Guides](https://github.com/erictleung/awesome-nosql-guides#readme) - Help on using non-relational, distributed, open-source, and horizontally scalable databases.  - [Database Tools](https://github.com/mgramin/awesome-db-tools#readme) - Everything that makes working with databases easier.  - [TypeDB](https://github.com/vaticle/typedb-awesome#readme) - Logical database to organize large and complex networks of data as one body of knowledge.  - [Cassandra](https://github.com/Anant/awesome-cassandra#readme) - Open-source, distributed, wide column store, NoSQL database management system.    ## Media    - [Creative Commons Media](https://github.com/shime/creative-commons-media#readme)  - [Fonts](https://github.com/brabadu/awesome-fonts#readme)  - [Codeface](https://github.com/chrissimpkins/codeface#readme) - Text editor fonts.  - [Stock Resources](https://github.com/neutraltone/awesome-stock-resources#readme)  - [GIF](https://github.com/davisonio/awesome-gif#readme) - Image format known for animated images.  - [Music](https://github.com/ciconia/awesome-music#readme)  - [Open Source Documents](https://github.com/44bits/awesome-opensource-documents#readme)  - [Audio Visualization](https://github.com/willianjusten/awesome-audio-visualization#readme)  - [Broadcasting](https://github.com/ebu/awesome-broadcasting#readme)  - [Pixel Art](https://github.com/Siilwyn/awesome-pixel-art#readme) - Pixel-level digital art.  - [FFmpeg](https://github.com/transitive-bullshit/awesome-ffmpeg#readme) - Cross-platform solution to record, convert and stream audio and video.  - [Icons](https://github.com/notlmn/awesome-icons#readme) - Downloadable SVG/PNG/font icon projects.  - [Audiovisual](https://github.com/stingalleman/awesome-audiovisual#readme) - Lighting, audio and video in professional environments.  - [VLC](https://github.com/mfkl/awesome-vlc#readme) - Cross-platform media player software and streaming server.    ## Learn    - [CLI Workshoppers](https://github.com/therebelrobot/awesome-workshopper#readme) - Interactive tutorials.  - [Learn to Program](https://github.com/karlhorky/learn-to-program#readme)  - [Speaking](https://github.com/matteofigus/awesome-speaking#readme)  - [Tech Videos](https://github.com/lucasviola/awesome-tech-videos#readme)  - [Dive into Machine Learning](https://github.com/hangtwenty/dive-into-machine-learning#readme)  - [Computer History](https://github.com/watson/awesome-computer-history#readme)  - [Programming for Kids](https://github.com/HollyAdele/awesome-programming-for-kids#readme)  - [Educational Games](https://github.com/yrgo/awesome-educational-games#readme) - Learn while playing.  - [JavaScript Learning](https://github.com/micromata/awesome-javascript-learning#readme)  - [CSS Learning](https://github.com/micromata/awesome-css-learning#readme) - Mainly about CSS – the language and the modules.  - [Product Management](https://github.com/dend/awesome-product-management#readme) - Learn how to be a better product manager.  - [Roadmaps](https://github.com/liuchong/awesome-roadmaps#readme) - Gives you a clear route to improve your knowledge and skills.  - [YouTubers](https://github.com/JoseDeFreitas/awesome-youtubers#readme) - Watch video tutorials from YouTubers that teach you about technology.    ## Security    - [Application Security](https://github.com/paragonie/awesome-appsec#readme)  - [Security](https://github.com/sbilly/awesome-security#readme)  - [CTF](https://github.com/apsdehal/awesome-ctf#readme) - Capture The Flag.  - [Malware Analysis](https://github.com/rshipp/awesome-malware-analysis#readme)  - [Android Security](https://github.com/ashishb/android-security-awesome#readme)  - [Hacking](https://github.com/carpedm20/awesome-hacking#readme)  - [Honeypots](https://github.com/paralax/awesome-honeypots#readme) - Deception trap, designed to entice an attacker into attempting to compromise the information systems in an organization.  - [Incident Response](https://github.com/meirwah/awesome-incident-response#readme)  - [Vehicle Security and Car Hacking](https://github.com/jaredthecoder/awesome-vehicle-security#readme)  - [Web Security](https://github.com/qazbnm456/awesome-web-security#readme) - Security of web apps & services.  - [Lockpicking](https://github.com/fabacab/awesome-lockpicking#readme) - The art of unlocking a lock by manipulating its components without the key.  - [Cybersecurity Blue Team](https://github.com/fabacab/awesome-cybersecurity-blueteam#readme) - Groups of individuals who identify security flaws in information technology systems.  - [Fuzzing](https://github.com/cpuu/awesome-fuzzing#readme) - Automated software testing technique that involves feeding pseudo-randomly generated input data.  - [Embedded and IoT Security](https://github.com/fkie-cad/awesome-embedded-and-iot-security#readme)  - [GDPR](https://github.com/bakke92/awesome-gdpr#readme) - Regulation on data protection and privacy for all individuals within EU.  - [DevSecOps](https://github.com/TaptuIT/awesome-devsecops#readme) - Integration of security practices into [DevOps](https://en.wikipedia.org/wiki/DevOps).  - [Executable Packing](https://github.com/dhondta/awesome-executable-packing#readme) - Packing and unpacking executable formats.    ## Content Management Systems    - [Umbraco](https://github.com/umbraco-community/awesome-umbraco#readme)  - [Refinery CMS](https://github.com/refinerycms-contrib/awesome-refinerycms#readme) - Ruby on Rails CMS.  - [Wagtail](https://github.com/springload/awesome-wagtail#readme) - Django CMS focused on flexibility and user experience.  - [Textpattern](https://github.com/drmonkeyninja/awesome-textpattern#readme) - Lightweight PHP-based CMS.  - [Drupal](https://github.com/nirgn975/awesome-drupal#readme) - Extensible PHP-based CMS.  - [Craft CMS](https://github.com/craftcms/awesome#readme) - Content-first CMS.  - [Sitecore](https://github.com/MartinMiles/Awesome-Sitecore#readme) - .NET digital marketing platform that combines CMS with tools for managing multiple websites.  - [Silverstripe CMS](https://github.com/wernerkrauss/awesome-silverstripe-cms#readme) - PHP MVC framework that serves as a classic or headless CMS.    ## Hardware    - [Robotics](https://github.com/Kiloreux/awesome-robotics#readme)  - [Internet of Things](https://github.com/HQarroum/awesome-iot#readme)  - [Electronics](https://github.com/kitspace/awesome-electronics#readme) - For electronic engineers and hobbyists.  - [Bluetooth Beacons](https://github.com/rabschi/awesome-beacon#readme)  - [Electric Guitar Specifications](https://github.com/gitfrage/guitarspecs#readme) - Checklist for building your own electric guitar.  - [Plotters](https://github.com/beardicus/awesome-plotters#readme) - Computer-controlled drawing machines and other visual art robots.  - [Robotic Tooling](https://github.com/protontypes/awesome-robotic-tooling#readme) - Free and open tools for professional robotic development.  - [LIDAR](https://github.com/szenergy/awesome-lidar#readme) - Sensor for measuring distances by illuminating the target with laser light.    ## Business    - [Open Companies](https://github.com/opencompany/awesome-open-company#readme)  - [Places to Post Your Startup](https://github.com/mmccaff/PlacesToPostYourStartup#readme)  - [OKR Methodology](https://github.com/domenicosolazzo/awesome-okr#readme) - Goal setting & communication best practices.  - [Leading and Managing](https://github.com/LappleApple/awesome-leading-and-managing#readme) - Leading people and being a manager in a technology company/environment.  - [Indie](https://github.com/mezod/awesome-indie#readme) - Independent developer businesses.  - [Tools of the Trade](https://github.com/cjbarber/ToolsOfTheTrade#readme) - Tools used by companies on Hacker News.  - [Clean Tech](https://github.com/nglgzz/awesome-clean-tech#readme) - Fighting climate change with technology.  - [Wardley Maps](https://github.com/wardley-maps-community/awesome-wardley-maps#readme) - Provides high situational awareness to help improve strategic planning and decision making.  - [Social Enterprise](https://github.com/RayBB/awesome-social-enterprise#readme) - Building an organization primarily focused on social impact that is at least partially self-funded.  - [Engineering Team Management](https://github.com/kdeldycke/awesome-engineering-team-management#readme) - How to transition from software development to engineering management.  - [Developer-First Products](https://github.com/agamm/awesome-developer-first#readme) - Products that target developers as the user.  - [Billing](https://github.com/kdeldycke/awesome-billing#readme) - Payments, invoicing, pricing, accounting, marketplace, fraud, and business intelligence.    ## Work    - [Slack](https://github.com/matiassingers/awesome-slack#readme) - Team collaboration.  	- [Communities](https://github.com/filipelinhares/awesome-slack#readme)  - [Remote Jobs](https://github.com/lukasz-madon/awesome-remote-job#readme)  - [Productivity](https://github.com/jyguyomarch/awesome-productivity#readme)  - [Niche Job Boards](https://github.com/tramcar/awesome-job-boards#readme)  - [Programming Interviews](https://github.com/DopplerHQ/awesome-interview-questions#readme)  - [Code Review](https://github.com/joho/awesome-code-review#readme) - Reviewing code.  - [Creative Technology](https://github.com/j0hnm4r5/awesome-creative-technology#readme) - Businesses & groups that specialize in combining computing, design, art, and user experience.  - [Internships](https://github.com/lodthe/awesome-internships#readme) - CV writing guides and companies that hire interns.    ## Networking    - [Software-Defined Networking](https://github.com/sdnds-tw/awesome-sdn#readme)  - [Network Analysis](https://github.com/briatte/awesome-network-analysis#readme)  - [PCAPTools](https://github.com/caesar0301/awesome-pcaptools#readme)  - [Real-Time Communications](https://github.com/rtckit/awesome-rtc#readme) - Network protocols for near simultaneous exchange of media and data.    ## Decentralized Systems    - [Bitcoin](https://github.com/igorbarinov/awesome-bitcoin#readme) - Bitcoin services and tools for software developers.  - [Ripple](https://github.com/vhpoet/awesome-ripple#readme) - Open source distributed settlement network.  - [Non-Financial Blockchain](https://github.com/machinomy/awesome-non-financial-blockchain#readme) - Non-financial blockchain applications.  - [Mastodon](https://github.com/tleb/awesome-mastodon#readme) - Open source decentralized microblogging network.  - [Ethereum](https://github.com/ttumiel/Awesome-Ethereum#readme) - Distributed computing platform for smart contract development.  - [Blockchain AI](https://github.com/steven2358/awesome-blockchain-ai#readme) - Blockchain projects for artificial intelligence and machine learning.  - [EOSIO](https://github.com/DanailMinchev/awesome-eosio#readme) - A decentralized operating system supporting industrial-scale apps.  - [Corda](https://github.com/chainstack/awesome-corda#readme) - Open source blockchain platform designed for business.  - [Waves](https://github.com/msmolyakov/awesome-waves#readme) - Open source blockchain platform and development toolset for Web 3.0 apps and decentralized solutions.  - [Substrate](https://github.com/substrate-developer-hub/awesome-substrate#readme) - Framework for writing scalable, upgradeable blockchains in Rust.  - [Golem](https://github.com/golemfactory/awesome-golem#readme) - Open source peer-to-peer marketplace for computing resources.  - [Stacks](https://github.com/friedger/awesome-stacks-chain#readme) - A smart contract platform secured by Bitcoin.  - [Algorand](https://github.com/aorumbayev/awesome-algorand#readme) - An open-source, proof of stake blockchain and smart contract computing platform.    ## Higher Education    - [Computational Neuroscience](https://github.com/eselkin/awesome-computational-neuroscience#readme) - A multidisciplinary science which uses computational approaches to study the nervous system.  - [Digital History](https://github.com/maehr/awesome-digital-history#readme) - Computer-aided scientific investigation of history.  - [Scientific Writing](https://github.com/writing-resources/awesome-scientific-writing#readme) - Distraction-free scientific writing with Markdown, reStructuredText and Jupyter notebooks.    ## Events    - [Creative Tech Events](https://github.com/danvoyce/awesome-creative-tech-events#readme) - Events around the globe for creative coding, tech, design, music, arts and cool stuff.  - [Events in Italy](https://github.com/ildoc/awesome-italy-events#readme) - Tech-related events in Italy.  - [Events in the Netherlands](https://github.com/awkward/awesome-netherlands-events#readme) - Tech-related events in the Netherlands.    ## Testing    - [Testing](https://github.com/TheJambo/awesome-testing#readme) - Software testing.  - [Visual Regression Testing](https://github.com/mojoaxel/awesome-regression-testing#readme) - Ensures changes did not break the functionality or style.  - [Selenium](https://github.com/christian-bromann/awesome-selenium#readme) - Open-source browser automation framework and ecosystem.  - [Appium](https://github.com/SrinivasanTarget/awesome-appium#readme) - Test automation tool for apps.  - [TAP](https://github.com/sindresorhus/awesome-tap#readme) - Test Anything Protocol.  - [JMeter](https://github.com/aliesbelik/awesome-jmeter#readme) - Load testing and performance measurement tool.  - [k6](https://github.com/grafana/awesome-k6#readme) - Open-source, developer-centric performance monitoring and load testing solution.  - [Playwright](https://github.com/mxschmitt/awesome-playwright#readme) - Node.js library to automate Chromium, Firefox and WebKit with a single API.  - [Quality Assurance Roadmap](https://github.com/fityanos/awesome-quality-assurance-roadmap#readme) - How to start & build a career in software testing.  - [Gatling](https://github.com/aliesbelik/awesome-gatling#readme) - Open-source load and performance testing framework based on Scala, Akka, and Netty.    ## Miscellaneous    - [JSON](https://github.com/burningtree/awesome-json#readme) - Text based data interchange format.  	- [GeoJSON](https://github.com/tmcw/awesome-geojson#readme)  	- [Datasets](https://github.com/jdorfman/awesome-json-datasets#readme)  - [CSV](https://github.com/secretGeek/awesomeCSV#readme) - A text file format that stores tabular data and uses a comma to separate values.  - [Discounts for Student Developers](https://github.com/AchoArnold/discount-for-student-dev#readme)  - [Radio](https://github.com/kyleterry/awesome-radio#readme)  - [Awesome](https://github.com/sindresorhus/awesome#readme) - Recursion illustrated.  - [Analytics](https://github.com/0xnr/awesome-analytics#readme)  - [REST](https://github.com/marmelab/awesome-rest#readme)  - [Continuous Integration and Continuous Delivery](https://github.com/cicdops/awesome-ciandcd#readme)  - [Services Engineering](https://github.com/mmcgrana/services-engineering#readme)  - [Free for Developers](https://github.com/ripienaar/free-for-dev#readme)  - [Answers](https://github.com/cyberglot/awesome-answers#readme) - Stack Overflow, Quora, etc.  - [Sketch](https://github.com/diessica/awesome-sketch#readme) - Design app for macOS.  - [Boilerplate Projects](https://github.com/melvin0008/awesome-projects-boilerplates#readme)  - [Readme](https://github.com/matiassingers/awesome-readme#readme)  - [Design and Development Guides](https://github.com/NARKOZ/guides#readme)  - [Software Engineering Blogs](https://github.com/kilimchoi/engineering-blogs#readme)  - [Self Hosted](https://github.com/awesome-selfhosted/awesome-selfhosted#readme)  - [FOSS Production Apps](https://github.com/DataDaoDe/awesome-foss-apps#readme)  - [Gulp](https://github.com/alferov/awesome-gulp#readme) - Task runner.  - [AMA](https://github.com/sindresorhus/amas#readme) - Ask Me Anything.  	- [Answers](https://github.com/stoeffel/awesome-ama-answers#readme)  - [Open Source Photography](https://github.com/ibaaj/awesome-OpenSourcePhotography#readme)  - [OpenGL](https://github.com/eug/awesome-opengl#readme) - Cross-platform API for rendering 2D and 3D graphics.  - [GraphQL](https://github.com/chentsulin/awesome-graphql#readme)  - [Urban & Regional Planning](https://github.com/APA-Technology-Division/urban-and-regional-planning-resources#readme) - Concerning the built environment and communities.  - [Transit](https://github.com/CUTR-at-USF/awesome-transit#readme)  - [Research Tools](https://github.com/emptymalei/awesome-research#readme)  - [Data Visualization](https://github.com/javierluraschi/awesome-dataviz#readme)  - [Social Media Share Links](https://github.com/vinkla/shareable-links#readme)  - [Microservices](https://github.com/mfornos/awesome-microservices#readme)  - [Unicode](https://github.com/jagracey/Awesome-Unicode#readme) - Unicode standards, quirks, packages and resources.  	- [Code Points](https://github.com/Codepoints/awesome-codepoints#readme)  - [Beginner-Friendly Projects](https://github.com/MunGell/awesome-for-beginners#readme)  - [Katas](https://github.com/gamontal/awesome-katas#readme)  - [Tools for Activism](https://github.com/drewrwilson/toolsforactivism#readme)  - [Citizen Science](https://github.com/dylanrees/citizen-science#readme) - For community-based and non-institutional scientists.  - [MQTT](https://github.com/hobbyquaker/awesome-mqtt#readme) - ""Internet of Things"" connectivity protocol.  - [Hacking Spots](https://github.com/daviddias/awesome-hacking-locations#readme)  - [For Girls](https://github.com/cristianoliveira/awesome4girls#readme)  - [Vorpal](https://github.com/vorpaljs/awesome-vorpal#readme) - Node.js CLI framework.  - [Vulkan](https://github.com/vinjn/awesome-vulkan#readme) - Low-overhead, cross-platform 3D graphics and compute API.  - [LaTeX](https://github.com/egeerardyn/awesome-LaTeX#readme) - Typesetting language.  - [Economics](https://github.com/antontarasenko/awesome-economics#readme) - An economist's starter kit.  - [Funny Markov Chains](https://github.com/sublimino/awesome-funny-markov#readme)  - [Bioinformatics](https://github.com/danielecook/Awesome-Bioinformatics#readme)  - [Cheminformatics](https://github.com/hsiaoyi0504/awesome-cheminformatics#readme) - Informatics techniques applied to problems in chemistry.  - [Colorful](https://github.com/Siddharth11/Colorful#readme) - Choose your next color scheme.  - [Steam](https://github.com/scholtzm/awesome-steam#readme) - Digital distribution platform.  - [Bots](https://github.com/hackerkid/bots#readme) - Building bots.  - [Site Reliability Engineering](https://github.com/dastergon/awesome-sre#readme)  - [Empathy in Engineering](https://github.com/KimberlyMunoz/empathy-in-engineering#readme) - Building and promoting more compassionate engineering cultures.  - [DTrace](https://github.com/xen0l/awesome-dtrace#readme) - Dynamic tracing framework.  - [Userscripts](https://github.com/bvolpato/awesome-userscripts#readme) - Enhance your browsing experience.  - [Pokémon](https://github.com/tobiasbueschel/awesome-pokemon#readme) - Pokémon and Pokémon GO.  - [ChatOps](https://github.com/exAspArk/awesome-chatops#readme) - Managing technical and business operations through a chat.  - [Falsehood](https://github.com/kdeldycke/awesome-falsehood#readme) - Falsehoods programmers believe in.  - [Domain-Driven Design](https://github.com/heynickc/awesome-ddd#readme) - Software development approach for complex needs by connecting the implementation to an evolving model.  - [Quantified Self](https://github.com/woop/awesome-quantified-self#readme) - Self-tracking through technology.  - [SaltStack](https://github.com/hbokh/awesome-saltstack#readme) - Python-based config management system.  - [Web Design](https://github.com/nicolesaidy/awesome-web-design#readme) - For digital designers.  - [Creative Coding](https://github.com/terkelg/awesome-creative-coding#readme) - Programming something expressive instead of something functional.  - [No-Login Web Apps](https://github.com/aviaryan/awesome-no-login-web-apps#readme) - Web apps that work without login.  - [Free Software](https://github.com/johnjago/awesome-free-software#readme) - Free as in freedom.  - [Framer](https://github.com/podo/awesome-framer#readme) - Prototyping interactive UI designs.  - [Markdown](https://github.com/BubuAnabelas/awesome-markdown#readme) - Markup language.  - [Dev Fun](https://github.com/mislavcimpersak/awesome-dev-fun#readme) - Funny developer projects.  - [Healthcare](https://github.com/kakoni/awesome-healthcare#readme) - Open source healthcare software for facilities, providers, developers, policy experts, and researchers.  - [Magento 2](https://github.com/DavidLambauer/awesome-magento2#readme) - Open Source eCommerce built with PHP.  - [TikZ](https://github.com/xiaohanyu/awesome-tikz#readme) - Graph drawing packages for TeX/LaTeX/ConTeXt.  - [Neuroscience](https://github.com/analyticalmonk/awesome-neuroscience#readme) - Study of the nervous system and brain.  - [Ad-Free](https://github.com/johnjago/awesome-ad-free#readme) - Ad-free alternatives.  - [Esolangs](https://github.com/angrykoala/awesome-esolangs#readme) - Programming languages designed for experimentation or as jokes rather than actual use.  - [Prometheus](https://github.com/roaldnefs/awesome-prometheus#readme) - Open-source monitoring system.  - [Homematic](https://github.com/homematic-community/awesome-homematic#readme) - Smart home devices.  - [Ledger](https://github.com/sfischer13/awesome-ledger#readme) - Double-entry accounting on the command-line.  - [Web Monetization](https://github.com/thomasbnt/awesome-web-monetization#readme) - A free open web standard service that allows you to send money directly in your browser.  - [Uncopyright](https://github.com/johnjago/awesome-uncopyright#readme) - Public domain works.  - [Crypto Currency Tools & Algorithms](https://github.com/Zheaoli/awesome-coins#readme) - Digital currency where encryption is used to regulate the generation of units and verify transfers.  - [Diversity](https://github.com/folkswhocode/awesome-diversity#readme) - Creating a more inclusive and diverse tech community.  - [Open Source Supporters](https://github.com/zachflower/awesome-open-source-supporters#readme) - Companies that offer their tools and services for free to open source projects.  - [Design Principles](https://github.com/robinstickel/awesome-design-principles#readme) - Create better and more consistent designs and experiences.  - [Theravada](https://github.com/johnjago/awesome-theravada#readme) - Teachings from the Theravada Buddhist tradition.  - [inspectIT](https://github.com/inspectit-labs/awesome-inspectit#readme) - Open source Java app performance management tool.  - [Open Source Maintainers](https://github.com/nayafia/awesome-maintainers#readme) - The experience of being an open source maintainer.  - [Calculators](https://github.com/xxczaki/awesome-calculators#readme) - Calculators for every platform.  - [Captcha](https://github.com/ZYSzys/awesome-captcha#readme) - A type of challenge–response test used in computing to determine whether or not the user is human.  - [Jupyter](https://github.com/markusschanta/awesome-jupyter#readme) - Create and share documents that contain code, equations, visualizations and narrative text.  - [FIRST Robotics Competition](https://github.com/andrewda/awesome-frc#readme) - International high school robotics championship.  - [Humane Technology](https://github.com/humanetech-community/awesome-humane-tech#readme) - Open source projects that help improve society.  - [Speakers](https://github.com/karlhorky/awesome-speakers#readme) - Conference and meetup speakers in the programming and design community.  - [Board Games](https://github.com/edm00se/awesome-board-games#readme) - Table-top gaming fun for all.  - [Software Patreons](https://github.com/uraimo/awesome-software-patreons#readme) - Fund individual programmers or the development of open source projects.  - [Parasite](https://github.com/ecohealthalliance/awesome-parasite#readme) - Parasites and host-pathogen interactions.  - [Food](https://github.com/jzarca01/awesome-food#readme) - Food-related projects on GitHub.  - [Mental Health](https://github.com/dreamingechoes/awesome-mental-health#readme) - Mental health awareness and self-care in the software industry.  - [Bitcoin Payment Processors](https://github.com/alexk111/awesome-bitcoin-payment-processors#readme) - Start accepting Bitcoin.  - [Scientific Computing](https://github.com/nschloe/awesome-scientific-computing#readme) - Solving complex scientific problems using computers.  - [Amazon Sellers](https://github.com/ScaleLeap/awesome-amazon-seller#readme)  - [Agriculture](https://github.com/brycejohnston/awesome-agriculture#readme) - Open source technology for farming and gardening.  - [Product Design](https://github.com/ttt30ga/awesome-product-design#readme) - Design a product from the initial concept to production.  - [Prisma](https://github.com/catalinmiron/awesome-prisma#readme) - Turn your database into a GraphQL API.  - [Software Architecture](https://github.com/simskij/awesome-software-architecture#readme) - The discipline of designing and building software.  - [Connectivity Data and Reports](https://github.com/stevesong/awesome-connectivity-info#readme) - Better understand who has access to telecommunication and internet infrastructure and on what terms.  - [Stacks](https://github.com/stackshareio/awesome-stacks#readme) - Tech stacks for building different apps and features.  - [Cytodata](https://github.com/cytodata/awesome-cytodata#readme) - Image-based profiling of biological phenotypes for computational biologists.  - [IRC](https://github.com/davisonio/awesome-irc#readme) - Open source messaging protocol.  - [Advertising](https://github.com/cenoura/awesome-ads#readme) - Advertising and programmatic media for websites.  - [Earth](https://github.com/philsturgeon/awesome-earth#readme) - Find ways to resolve the climate crisis.  - [Naming](https://github.com/gruhn/awesome-naming#readme) - Naming things in computer science done right.  - [Biomedical Information Extraction](https://github.com/caufieldjh/awesome-bioie#readme) - How to extract information from unstructured biomedical data and text.  - [Web Archiving](https://github.com/iipc/awesome-web-archiving#readme) - An effort to preserve the Web for future generations.  - [WP-CLI](https://github.com/schlessera/awesome-wp-cli#readme) - Command-line interface for WordPress.  - [Credit Modeling](https://github.com/mourarthur/awesome-credit-modeling#readme) - Methods for classifying credit applicants into risk classes.  - [Ansible](https://github.com/ansible-community/awesome-ansible#readme) - A Python-based, open source IT configuration management and automation platform.  - [Biological Visualizations](https://github.com/keller-mark/awesome-biological-visualizations#readme) - Interactive visualization of biological data on the web.  - [QR Code](https://github.com/make-github-pseudonymous-again/awesome-qr-code#readme) - A type of matrix barcode that can be used to store and share a small amount of information.  - [Veganism](https://github.com/sdassow/awesome-veganism#readme) - Making the plant-based lifestyle easy and accessible.  - [Translations](https://github.com/mbiesiad/awesome-translations#readme) - The transfer of the meaning of a text from one language to another.  - [Scriptable](https://github.com/dersvenhesse/awesome-scriptable#readme) - An iOS app for automations in JavaScript.  - [WebXR](https://github.com/msub2/awesome-webxr#readme) - Enables immersive virtual reality and augmented reality content on the web.    ## Related    - [All Awesome Lists](https://github.com/topics/awesome) - All the Awesome lists on GitHub.  - [Awesome Indexed](https://awesome-indexed.mathew-davies.co.uk) - Search the Awesome dataset.  - [Awesome Search](https://awesomelists.top) - Quick search for Awesome lists.  - [StumbleUponAwesome](https://github.com/basharovV/StumbleUponAwesome) - Discover random pages from the Awesome dataset using a browser extension.  - [Awesome CLI](https://github.com/umutphp/awesome-cli) - A simple command-line tool to dive into Awesome lists.  - [Awesome Viewer](https://awesome.digitalbunker.dev) - A visualizer for all of the above Awesome lists.  - [Track Awesome List](https://www.trackawesomelist.com) - View the latest updates of Awesome lists. """
