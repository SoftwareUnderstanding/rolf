Text: Getting the source code   $ wget https://github.com/facebookresearch/fastText/archive/v0.9.2.zip   $ cd fastText-0.9.2  $ make   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ mkdir build &amp;&amp; cd build &amp;&amp; cmake ..  $ make &amp;&amp; make install   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ pip install .   You can also quantize a supervised model to reduce its memory usage with the following command:   You can find our [latest stable release](https://github.com/facebookresearch/fastText/releases/latest) in the usual place.  There is also the master branch that contains all of our most recent work  but comes along with all the usual caveats of an unstable branch. You might want to use this if you are a developer or power-user.   This library has two main use cases: word representation learning and text classification. These were described in the two papers [1](#enriching-word-vectors-with-subword-information) and [2](#bag-of-tricks-for-efficient-text-classification).   , 
 Label: Natural Language Processing 

Text: We provide **RecAdam** (Recall Adam) optimizer to facilitate fine-tuning deep pretrained language models (e.g.  BERT  ALBERT) with less forgetting.  For a detailed description and experimental results  please refer to our paper: [Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting](https://www.aclweb.org/anthology/2020.emnlp-main.634/) (Accepted by EMNLP 2020).     --output_dir /path/to/output/$TASK_NAME/ \     --output_dir /path/to/output/$TASK_NAME/ \   , 
 Label: Natural Language Processing 

Text: Getting the source code   $ wget https://github.com/facebookresearch/fastText/archive/v0.9.2.zip   $ cd fastText-0.9.2  $ make   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ mkdir build &amp;&amp; cd build &amp;&amp; cmake ..  $ make &amp;&amp; make install   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ pip install .   You can also quantize a supervised model to reduce its memory usage with the following command:   You can find our [latest stable release](https://github.com/facebookresearch/fastText/releases/latest) in the usual place.  There is also the master branch that contains all of our most recent work  but comes along with all the usual caveats of an unstable branch. You might want to use this if you are a developer or power-user.   This library has two main use cases: word representation learning and text classification. These were described in the two papers [1](#enriching-word-vectors-with-subword-information) and [2](#bag-of-tricks-for-efficient-text-classification).   , 
 Label: Natural Language Processing 

Text: We provide **RecAdam** (Recall Adam) optimizer to facilitate fine-tuning deep pretrained language models (e.g.  BERT  ALBERT) with less forgetting.  For a detailed description and experimental results  please refer to our paper: [Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting](https://www.aclweb.org/anthology/2020.emnlp-main.634/) (Accepted by EMNLP 2020).     --output_dir /path/to/output/$TASK_NAME/ \     --output_dir /path/to/output/$TASK_NAME/ \   , 
 Label: Natural Language Processing 

Text: Getting the source code   $ wget https://github.com/facebookresearch/fastText/archive/v0.9.2.zip   $ cd fastText-0.9.2  $ make   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ mkdir build &amp;&amp; cd build &amp;&amp; cmake ..  $ make &amp;&amp; make install   $ git clone https://github.com/facebookresearch/fastText.git  $ cd fastText  $ pip install .   You can also quantize a supervised model to reduce its memory usage with the following command:   You can find our [latest stable release](https://github.com/facebookresearch/fastText/releases/latest) in the usual place.  There is also the master branch that contains all of our most recent work  but comes along with all the usual caveats of an unstable branch. You might want to use this if you are a developer or power-user.   This library has two main use cases: word representation learning and text classification. These were described in the two papers [1](#enriching-word-vectors-with-subword-information) and [2](#bag-of-tricks-for-efficient-text-classification).   , 
 Label: Natural Language Processing 

Text: 1. Clone this repository   ```Shell   #: Make sure to clone with --recursive   git clone --recursive https://github.com/vincentzhang/roi-fcn.git    ```   If you didn't clone with the `--recursive` flag  then you'll need to manually clone the `caffe-roi` submodule:   ```Shell     git submodule update --init --recursive   ```  2. Build Caffe and pycaffe   **Note:** Caffe *must* be built with support for Python layers!     ```Shell     #: ROOT refers to the directory that you cloned this repo into.     cd $ROOT/caffe-roi     #: Now follow the Caffe installation instructions here:     #:   http://caffe.berkeleyvision.org/installation.html          #: In your Makefile.config  make sure to have this line uncommented             WITH_PYTHON_LAYER := 1         #: Unrelatedly  it's also recommended that you use CUDNN             USE_CUDNN := 1      #: Compile     make -j8 && make pycaffe     ```   You can download my [Makefile.config](https://drive.google.com/open?id=1NSeWp7INxGWUrSdCTCwol8NmV_0-Ar5k) for reference.  3. Build the Cython modules     ```Shell     cd $ROOT/lib     make     ```  4. Download the ImageNet pre-trained VGG16 weights (adapted to be fully convolutional):     ```Shell     cd $ROOT/data/scripts     ./fetch_vgg16_fcn.sh     ```      This will populate the `$ROOT/data/imagenet_models` folder with `VGG16.v2.fcn-surgery-all.caffemodel`.   ./experiments/scripts/test_socket_scratch_n_1e-4_fg150_roils.sh test all 4586 1 16 1   To run the demo  first download the pretrained weights: ```Shell cd $ROOT/data/scripts ./fetch_socket_models.sh ``` Run the demo script: ```Shell cd $ROOT python ./tools/demo.py ``` The demo runs the segmentation network trained on the acetabulum data used in the paper.  To show the generalization of the algorithm  the input images stored in `$ROOT/data/samples` are anonymized clinical images that are not in the training or testing dataset.   , 
 Label: Computer Vision 

